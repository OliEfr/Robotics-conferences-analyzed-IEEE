<!DOCTYPE HTML>
<html>
 <head>
  <meta content="en-us" http-equiv="Content-Language"/>
  <meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
  <meta content="width=device-width" name="viewport"/>
  <script src="https://ras.papercept.net/conferences/scripts/dom-drag.js" type="text/javascript">
  </script>
  <script src="jquery-1.11.1.min.js">
  </script>
  <title>
   IROS 2024 Program | Thursday October 17, 2024
  </title>
  <style type="text/css">
   body, table, td, th{
	Font-Family : sans-serif;
	Font-Size : 10pt;
}
.r {text-align: right}
.blue {color: #0000FF;}
td {vertical-align: top; text-align: left}
.c {text-align: center}
table.s {
	border-collapse:collapse;
	border-width: 1px;
}
table.s td{
	border-width: 1px;
	padding: 4px;
	border-style: solid;
	border-color: gray;
}
table.t {
	border-collapse: collapse;
	border-width: 0px;
}
table.t td{
	border-width: 0px;
	padding: 4px;
	border-style: solid;
	border-color: gray;
}
.dots {
    background:url('./images/dot.gif') repeat-x center;
}
.field {
    background-color: #FFFFFF;
}
#pTitle { /* Page title */
   font-size: 14pt;
   line-height: 1.5em;
}
#pSubTitle { /* Page subtitle */
   color: #909090;
   font-size: 10pt; 
   line-height: 1.5em;
}
#container {
	position: absolute;
	width: 100%;
	margin-top: 2px;
/*	overflow: hidden; */
}

.sHdr {   /* Session header Content list */
   background-color: #F0E68C
}
      
.sSHdr {   /* Subsession header Content list */
   background-color: #f8f3c6 
}
      
table.trk { /* Track table Content list */
   border-collapse: collapse;
   border-width: 0px;
   margin: auto;
/**   width: 640px; **/
   width: 720px;
}
table.trk td{
   border-width: 0px;
   padding: 4px;
   border-style: solid;
   border-color: gray;
 }
      
.pHdr {  /* Paper header Content list */
   background-color: #E6E6FA;
   color: black;
}
hr.thin { /* Horizontal rule content list */
   border: 0px; 
   height: .8px; 
   background-color: #8888FF;
}
      
.pTtl {  /* Paper title Content list */
   font-size: 11pt;
   font-style: italic;
}
      
.ssHdr {  /* Subsession header container session Content list */
   background-color: #DDDDDD;
   color: black;
}
      
.ssTtl {  /* Subsession title container session Content list */
   font-size: 10pt;
   font-style: normal;
   font-weight: bold;
}
  </style>
  <script language="JavaScript">
   function initXMLHttp(){
   var oRequest = false;
   try {
      oRequest = new XMLHttpRequest();
   }  catch (trymicrosoft) {
      try {
         oRequest = new ActiveXObject("Msxml2.XMLHTTP");
      }  catch (othermicrosoft) {
         try {
            oRequest = new ActiveXObject("Microsoft.XMLHTTP");
         }  catch (failed) {
            oRequest = false;
         }
      }
   }
   if (!oRequest){
      alert("Error initializing XMLHttpRequest! Your browser does not support AJAX");
   }
   return oRequest;
}
function modify(number,kk){
   var check = document.getElementById('mod' + kk).checked;
   if (check){
      action = 'Add';
   }
   else{
      action = 'Delete';
   }
   
//   url = 'https://ras.papercept.net/conferences/scripts/myprogram.pl?ConfID=425&' + action + number;
//   window.open(url,'myprogrampage');

   modifyItem("https://ras.papercept.net/conferences/scripts/myprogram_aja.pl","425",action,number)

}


function modifyItem(url,ConfID,action,number){
   var oRequest = initXMLHttp();
   if (!oRequest){return;}
   
   // Send the request

   oRequest.open("post",url, true);
   var sParams = 'ConfID=' + ConfID + '&Action=' + action + '&Number=' + number;
   document.body.style.cursor = 'wait';
   oRequest.send(sParams);

   // Process the response
   
   oRequest.onreadystatechange = function(){ 
      if (oRequest && oRequest.readyState && oRequest.readyState == 4){
         document.body.style.cursor = 'auto';
         var responseText = oRequest.responseText;
         if (responseText.substring(0,5) == 'Error'){
            alert(responseText);
         }
         else{
         
//            alert(responseText);

         }
      }
   } 
}

var iIntervalId;  // Global variable
function modsession(id,kk){
   var check = document.getElementById('mod' + kk).checked;
   if (check){
      action = 'AddSession';
   }
   else{
      action = 'DelSession';
   }

//   url = 'https://ras.papercept.net/conferences/scripts/myprogram.pl?ConfID=425&' + action + id;
//   window.open(url,'myprogrampage');

   modifySession("https://ras.papercept.net/conferences/scripts/myprogram_aja.pl","425",action,id)

}

function modifySession(url,ConfID,action,id){
   var oRequest = initXMLHttp();
   if (!oRequest){return;}
   
   // Send the request

   oRequest.open("post",url, true);
   var sParams = 'ConfID=' + ConfID + '&Action=' + action + '&ID=' + id;
   document.body.style.cursor = 'wait';
   oRequest.send(sParams);

   // Process the response
   
   oRequest.onreadystatechange = function(){ 
      if (oRequest && oRequest.readyState && oRequest.readyState == 4){
         document.body.style.cursor = 'auto';
         var responseText = oRequest.responseText;
         if (responseText.substring(0,5) == 'Error'){
            alert(responseText);
         }
         else{
         
//            alert(responseText);

         }
      }
   } 
}

function getCookie(sName){
   var sRE = "(?:; )?" + sName + "=([^;]*);?";   
   var oRE = new RegExp(sRE);
   if (oRE.test(document.cookie)){
      return decodeURIComponent(RegExp["$1"]);}
   else{
      return null;
   }
}
function loadprogram(){
   var list = getCookie("IROS24");
   if (list){
      var List = list.split(",");
      for (var i=0; i<List.length; i++){
         var names = document.getElementsByName('modify' + List[i]);
         if (names.length){
            for (var j=0; j<names.length; j++){
               names[j].checked = true;
            }
         }
      }
   }
}
function reset(){

   // Uncheck all modify and addsession checkboxes

   var ins = document.getElementsByTagName('input');
   for (var i=0; i<ins.length; i++){
      if (ins[i].type == 'checkbox' && ins[i].id && ins[i].id.substring(0,3) == 'mod'){
         ins[i].checked = false;
      }
   }
   
   // Reload the program
   
   loadprogram();
}
function startreset(){
   iIntervalId = setInterval(reset,2000);
}
function viewAbstract(number){
   var box = document.getElementById('Ab' + number);
   if (box.style.display == 'block'){
      box.style.display = 'none';
   }
   else if (box && box.style.display == 'none'){
      box.style.display = 'block';
   }
}
function openAllAbstracts(){
   var d = document.getElementsByTagName('div');
   var count = d.length;
   if (count == 0){return;}
   for (var i=0; i<d.length; i++){
      if (d[i].id && d[i].id.substring(0,2) == 'Ab' && d[i].style.display == 'none'){
         d[i].style.display = 'block';
      }
   }
}
function closeAllAbstracts(){
   var d = document.getElementsByTagName('div');
   for (var i=0; i<d.length; i++){
      if (d[i].id && d[i].id.substring(0,2) == 'Ab'){
         d[i].style.display = 'none';
      }
   }
}
var uhash;
var pColor;
$(function() { 

   // Check for URL hash

   uhash = location.hash.substring(1);
   if (uhash.slice(-1) == '_'){
      uhash = null;
   }
   if (uhash){
   
      // Mark the session
   
      pColor = $('#' + uhash).parent().css('backgroundColor');
      $('#' + uhash).parent().css('backgroundColor','#FF8888');
   }
});


$(function() { 

   // Check for URL hash

   uhash = location.hash.substring(1);
   if (uhash.slice(-1) == '_'){
      uhash = null;
   }
   if (uhash){

      // Set the widths
      
      setwidth();
      
      // If claasical view is required then return

      if (!ghit){return;} 
      
      // Reset left margin for FF

      document.getElementById('container').scrollLeft = 0;;

      // Discover the table and the block and determine the block Id
   
      var rt = $('#' + uhash);
      var done = false;
      while (!done){
         rt = rt.parent();    
         var etype = rt.get(0).tagName;  
         if (rt.is("table")){      
            done = true;
         }
      }
      rt = rt.parent().parent().parent();
      var iid = rt.attr('id')

      // Show the block

      initialize();
      $('#' + iid).show();
      $( '#A' + iid ).focus();
      var ypos = $('#' + iid).offset().top;      
      window.scrollTo(0,ypos);

      // Cancel the scroll to uhash

      var url = location.href;
      url += '_';
      location.href = url;
      
      // Scroll into view

      var leftPosition = $('#' + uhash).parent().position().left;
      var topOffset = $('#' + uhash).parent().offset().top;
      var divOffset = $('#' + iid).find('div').offset().top;
      var topPosition = topOffset-divOffset;
      $('#' + iid).find('div').scrollLeft(leftPosition);
      $('#' + iid).find('div').scrollTop(topPosition);
   }
   else{
      setwidth();
      initialize();
   }
});

var ghit = false;
function setwidth(){
   var viewportwidth = $( window ).width();
   var viewportheight = $( window ).height();
   var sdiv = $( ".sdiv" );
   for (var i=0; i<sdiv.length; i++){
      $(sdiv[i]).css({width: .98*viewportwidth + 'px'});
      $(sdiv[i]).css("height", .9*viewportheight-50 + 'px');      
   }

   // Detect horizontal overflow on any of the divs
   
   var divs = document.getElementsByTagName('div');
   for (var i=0; i<divs.length; i++){
      if (divs[i].id && divs[i].id.substring(0,3) == 'div'){
         if (divs[i].scrollWidth > divs[i].clientWidth){
            ghit = true;
            break;
         }
      }
   }
   if (!ghit){
      for (var i=0; i<divs.length; i++){
         divs[i].style.height = 'auto';
      }
   }
}

function selfollowing(hsh){
   $('#' + uhash).parent().css('backgroundColor',pColor);
   setwidth();
   initialize();
   if (hsh == 'TheTop'){
      var ypos = $('#container').offset().top;
      window.scrollTo(0,ypos)
   }
   else{
      $('#' + hsh).show();
      $( '#A' + hsh ).focus();
      var ypos = $('#' + hsh).offset().top;
      window.scrollTo(0,ypos)
   }
}

function initialize(){

   // Show all day blocks
   
   var blcks = $('.blck');
   for (var i=0; i<blcks.length; i++){
      blcks[i].style.display = 'block';
   }

   // Detect horizontal overflow on any of the divs
   
   var hit = false;
   var divs = document.getElementsByTagName('div');
   for (var i=0; i<divs.length; i++){
      if (divs[i].id && divs[i].id.substring(0,3) == 'div'){
         if (divs[i].scrollWidth > divs[i].clientWidth || divs[i].scrollHeight > divs[i].clientHeight){
            hit = true;
            break;
         }
      }
   }
   if (hit){
   
      // Set overflow hidden on body. This will prevent it from scrolling
      
      $("body").css("overflow", "hidden");
      document.getElementById('start').style.display = 'inline';
      
      // Hide all day blocks
   
      var blcks = $('.blck');
      for (var i=0; i<blcks.length; i++){
         blcks[i].style.display = 'none';
      }
      var scrlis = $('.scrlis');
      for (var i=0; i<scrlis.length; i++){
         scrlis.show();
      }
   }
   else{
      $("body").css("overflow", "auto");
      document.getElementById('start').style.display = 'none';
      var blcks = $('.sdiv');
      for (var i=0; i<blcks.length; i++){
        blcks[i].style.height = 'auto';
      }
      var scrlis = $('.scrlis');
      for (var i=0; i<scrlis.length; i++){
         scrlis.hide();
      }
   }
   return;
}
  </script>
 </head>
 <body onresize="setwidth(); initialize()">
  <form action="https://ras.papercept.net/conferences/scripts/myprogram.pl" name="myprogram">
   <div id="container">
    <body leftmargin="0" marginheight="0" marginwidth="0" topmargin="0">
     <table border="0" cellpadding="0" cellspacing="0" width="100%">
      <tr>
       <td border="0" height="140" style="background:linear-gradient(to right,#53A8E5,#F78B00);" width="100%">
        <img alt="" border="0" height="140" src="/images/iros/iros24_l.png" style="position:absolute;left:200px;top:0px;"/>
        <img alt="" border="0" height="130" src="/images/iros/iros24_r.png" style="position:absolute;right:200px;top:5px;"/>
       </td>
      </tr>
     </table>
     <table border="0" cellpadding="0" cellspacing="0" height="80%" width="100%">
      <tr>
       <td height="100%" style="background-color:#9F7F59;" width="5">
       </td>
       <td width="5">
       </td>
       <td height="100%" valign="top" width="100%">
        <br/>
        <div class="c" id="TheTop">
         <span id="pTitle">
          <a href="http://iros2024-abudhabi.org" target="_blank">
           <b>
            2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
           </b>
          </a>
          <br/>
         </span>
         <span id="pSubTitle">
          <b>
           October 14-18, 2024, Abu Dhabi, UAE
          </b>
         </span>
         <br/>
         <br/>
        </div>
        <div class="c" style="position: relative">
         <a href="IROS24_ProgramAtAGlanceWeb.html">
          Program at a Glance
         </a>
         <a href="IROS24_ContentListWeb_1.html">
          Monday
         </a>
         <a href="IROS24_ContentListWeb_2.html">
          Tuesday
         </a>
         <a href="IROS24_ContentListWeb_3.html">
          Wednesday
         </a>
         <a href="IROS24_ContentListWeb_4.html">
          Thursday
         </a>
         <a href="IROS24_ContentListWeb_5.html">
          Friday
         </a>
         <a href="IROS24_AuthorIndexWeb.html">
          Author Index
         </a>
         <a href="IROS24_KeywordIndexWeb.html">
          Keyword Index
         </a>
        </div>
        <div class="c">
         <p style="color: gray">
          Last updated on October 5, 2024. This conference program is tentative and subject to change
         </p>
        </div>
        <div class="c">
         <h3>
          Technical Program for Thursday October 17, 2024
         </h3>
        </div>
        <p class="c">
        </p>
        <div class="c">
         <span style="color:gray ">
          To show or hide the keywords and abstract (text summary) of a paper (if available), click on the paper title
         </span>
         <br/>
         <a href="javascript:openAllAbstracts()" title="Click to open all abstracts">
          Open all abstracts
         </a>
         <a href="javascript:closeAllAbstracts()" title="Click to close all abstracts">
          Close all abstracts
         </a>
        </div>
        <div class="c">
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thpi4t1">
             <b>
              ThPI4T1
             </b>
            </a>
           </td>
           <td class="r">
            Room 1
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thpi4t1" title="Click to go to the Program at a Glance">
             <b>
              Legged Robot Systems I
             </b>
            </a>
           </td>
           <td class="r">
            Teaser Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#203949" title="Click to go to the Author Index">
             Zhao, Ding
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#202313" title="Click to go to the Author Index">
             Zou, Ting
            </a>
           </td>
           <td class="r">
            Memorial University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t1_01">
             09:00-10:00, Paper ThPI4T1.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('425'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Explosive Legged Robotic Hopping: Energy Accumulation and Power Amplification Via Pneumatic Augmentation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#350589" title="Click to go to the Author Index">
             Chen, Yifei
            </a>
           </td>
           <td class="r">
            Southern University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#384642" title="Click to go to the Author Index">
             Arturo, Gamboa-Gonzalez
            </a>
           </td>
           <td class="r">
            University of Wisconsin-Madison
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#152935" title="Click to go to the Author Index">
             Wehner, Michael
            </a>
           </td>
           <td class="r">
            University of Wisconsin, Madison
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#178177" title="Click to go to the Author Index">
             Xiong, Xiaobin
            </a>
           </td>
           <td class="r">
            University of Wisconsin Madison
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab425" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#hydraulic_pneumatic_actuators" title="Click to go to the Keyword Index">
               Hydraulic/Pneumatic Actuators
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mechanism_design" title="Click to go to the Keyword Index">
               Mechanism Design
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present a novel pneumatic augmentation to traditional electric motor-actuated legged robot to increase intermittent power density to perform infrequent explosive hopping behaviors. The pneumatic system is composed of a pneumatic pump, a tank, and a pneumatic actuator. The tank is charged up by the pump during regular hopping motion that is created by the electric motors. At any time after reaching a desired air pressure in the tank, a solenoid valve is utilized to rapidly release the air pressure to the pneumatic actuator (piston) which is used in conjunction with the electric motors to perform explosive hopping, increasing maximum hopping height for one or subsequent cycles. We show that, on a custom-designed one-legged hopping robot, without any additional power source and with this novel pneumatic augmentation system, their associated system identification and optimal control, the robot is able to realize highly explosive hopping with power amplification per cycle by a factor of approximately 5.4 times the power of electric motor actuation alone.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t1_02">
             09:00-10:00, Paper ThPI4T1.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('630'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Real-Time Perceptive Motion Control Using Control Barrier Functions with Analytical Smoothing for Six-Wheeled-Telescopic-Legged Robot Tachyon 3
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#195618" title="Click to go to the Author Index">
             Takasugi, Noriaki
            </a>
           </td>
           <td class="r">
            Sony Group Corporation
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#295379" title="Click to go to the Author Index">
             Kinoshita, Masaya
            </a>
           </td>
           <td class="r">
            Sony Group Corporation
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#116464" title="Click to go to the Author Index">
             Kamikawa, Yasuhisa
            </a>
           </td>
           <td class="r">
            Sony Group Corporation
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#360854" title="Click to go to the Author Index">
             Tsuzaki, Ryoichi
            </a>
           </td>
           <td class="r">
            Sony Group Corporation
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#295384" title="Click to go to the Author Index">
             Sakamoto, Atsushi
            </a>
           </td>
           <td class="r">
            Sony Group Corporation
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#295371" title="Click to go to the Author Index">
             Kai, Toshimitsu
            </a>
           </td>
           <td class="r">
            Sony Group Corporation
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#113926" title="Click to go to the Author Index">
             Kawanami, Yasunori
            </a>
           </td>
           <td class="r">
            Sony Group Corporation
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab630" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             To achieve safe legged locomotion, it is crucial to generate motion in real-time considering various constraints in robots and environments. In this study, we propose a lightweight real-time perceptive motion control system for the newly developed six-wheeled-telescopic-legged robot, Tachyon 3. In the proposed method, analytically smoothed constraints including Smooth Separating Axis Theorem (SSAT) as a novel higher order differentiable collision detection for 3D shapes is applied to the Control Barrier Function (CBF). The proposed system integrating the CBF achieves online motion generation in a short control cycle of 1 ms that satisfies joint limitations, environmental collision avoidance and safe convex foothold constraints. The efficiency of SSAT is shown from the collision detection time of 1 us or less and the CBF constraint computation time for Tachyon 3 of several us. Furthermore, the effectiveness of the proposed system is verified through the stair-climbing motion, integrating online recognition in a simulation and a real machine.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t1_03">
             09:00-10:00, Paper ThPI4T1.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('820'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              State Estimation Transformers for Agile Legged Locomotion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#313646" title="Click to go to the Author Index">
             Yu, Chen
            </a>
           </td>
           <td class="r">
            Center for Robotics and Biosystems
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#394173" title="Click to go to the Author Index">
             Yang, Yichu
            </a>
           </td>
           <td class="r">
            ByteDance
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#230260" title="Click to go to the Author Index">
             Liu, Tianlin
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#183257" title="Click to go to the Author Index">
             You, Yangwei
            </a>
           </td>
           <td class="r">
            Xiaomi
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#351514" title="Click to go to the Author Index">
             Zhou, Mingliang
            </a>
           </td>
           <td class="r">
            Beijing Xiaomi Mobile Software Co., Ltd
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#377150" title="Click to go to the Author Index">
             Xiang, Diyun
            </a>
           </td>
           <td class="r">
            XIAOMI
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab820" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#machine_learning_for_robot_control" title="Click to go to the Keyword Index">
               Machine Learning for Robot Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We propose a state estimation method that can accurately predict the robot's privileged states to push the limits of quadruped robots in executing advanced skills such as jumping in the wild. In particular, we present the State Estimation Transformers (SET), an architecture that casts the state estimation problem as conditional sequence modeling. SET outputs the robot states that are hard to obtain directly in the real world, such as the body height and velocities, by leveraging a causally masked Transformer. By conditioning an autoregressive model on the robot's past states, our SET model can predict these privileged observations accurately even in highly dynamic locomotions. We evaluate our methods on three tasks --- running jumping, running backflipping, and running sideslipping --- on a low-cost quadruped robot, Cyberdog2. Results show that SET can outperform other methods in estimation accuracy and transferability in the simulation as well as success rates of jumping and triggering a recovery controller in the real world, suggesting the superiority of such a Transformer-based explicit state estimator in highly dynamic locomotion tasks.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t1_04">
             09:00-10:00, Paper ThPI4T1.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1015'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              The Design of the Barkour Benchmark for Robot Agility
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#198071" title="Click to go to the Author Index">
             Yu, Wenhao
            </a>
           </td>
           <td class="r">
            Google
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#139378" title="Click to go to the Author Index">
             Caluwaerts, Ken
            </a>
           </td>
           <td class="r">
            Google
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#219951" title="Click to go to the Author Index">
             Iscen, Atil
            </a>
           </td>
           <td class="r">
            Google
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#246539" title="Click to go to the Author Index">
             Kew, J. Chase
            </a>
           </td>
           <td class="r">
            Google Robotics
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#173944" title="Click to go to the Author Index">
             Zhang, Tingnan
            </a>
           </td>
           <td class="r">
            Google
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#325868" title="Click to go to the Author Index">
             Freeman, Daniel
            </a>
           </td>
           <td class="r">
            Google LLC
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#344188" title="Click to go to the Author Index">
             Lee, Lisa
            </a>
           </td>
           <td class="r">
            Google
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#138907" title="Click to go to the Author Index">
             Saliceti, Stefano
            </a>
           </td>
           <td class="r">
            Google DeepMind
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395105" title="Click to go to the Author Index">
             Zhuang, Vincent
            </a>
           </td>
           <td class="r">
            Google DeepMind
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395107" title="Click to go to the Author Index">
             Batchelor, Nathan
            </a>
           </td>
           <td class="r">
            Google DeepMind
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#211003" title="Click to go to the Author Index">
             Bohez, Steven
            </a>
           </td>
           <td class="r">
            DeepMind
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395108" title="Click to go to the Author Index">
             Casarini, Federico
            </a>
           </td>
           <td class="r">
            Google DeepMind
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#204263" title="Click to go to the Author Index">
             Chen, Jose Enrique
            </a>
           </td>
           <td class="r">
            DeepMind
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#223803" title="Click to go to the Author Index">
             Coumans, Erwin
            </a>
           </td>
           <td class="r">
            Google Inc
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395109" title="Click to go to the Author Index">
             Dostmohamed, Adil
            </a>
           </td>
           <td class="r">
            Google DeepMind
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#285989" title="Click to go to the Author Index">
             Dulac-Arnold, Gabriel
            </a>
           </td>
           <td class="r">
            Google
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#285844" title="Click to go to the Author Index">
             Escontrela, Alejandro
            </a>
           </td>
           <td class="r">
            Google
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395111" title="Click to go to the Author Index">
             Frey, Erik
            </a>
           </td>
           <td class="r">
            Google DeepMind
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#108351" title="Click to go to the Author Index">
             Hafner, Roland
            </a>
           </td>
           <td class="r">
            Google DeepMind
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#245808" title="Click to go to the Author Index">
             Jain, Deepali
            </a>
           </td>
           <td class="r">
            Robotics at Google
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395112" title="Click to go to the Author Index">
             Jyenis, Bauyrjan
            </a>
           </td>
           <td class="r">
            Google
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395114" title="Click to go to the Author Index">
             Kuang, Yuheng
            </a>
           </td>
           <td class="r">
            Google DeepMind
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#243491" title="Click to go to the Author Index">
             Lee, Edward
            </a>
           </td>
           <td class="r">
            Google
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#166153" title="Click to go to the Author Index">
             Nachum, Ofir
            </a>
           </td>
           <td class="r">
            Google
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#220321" title="Click to go to the Author Index">
             Oslund, Kenneth
            </a>
           </td>
           <td class="r">
            Google
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#165208" title="Click to go to the Author Index">
             Romano, Francesco
            </a>
           </td>
           <td class="r">
            DeepMind
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#205952" title="Click to go to the Author Index">
             Sadeghi, Fereshteh
            </a>
           </td>
           <td class="r">
            University of Washington
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395115" title="Click to go to the Author Index">
             Tabanpour, Baruch
            </a>
           </td>
           <td class="r">
            Google DeepMind
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395116" title="Click to go to the Author Index">
             Zheng, Daniel
            </a>
           </td>
           <td class="r">
            Google DeepMind
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#166565" title="Click to go to the Author Index">
             Neunert, Michael
            </a>
           </td>
           <td class="r">
            Google
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#111364" title="Click to go to the Author Index">
             Hadsell, Raia
            </a>
           </td>
           <td class="r">
            DeepMind
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#239449" title="Click to go to the Author Index">
             Heess, Nicolas
            </a>
           </td>
           <td class="r">
            Google Deepmind
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#111052" title="Click to go to the Author Index">
             Nori, Francesco
            </a>
           </td>
           <td class="r">
            Google DeepMind
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395117" title="Click to go to the Author Index">
             Seto, Jeff
            </a>
           </td>
           <td class="r">
            Google DeepMind
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#341525" title="Click to go to the Author Index">
             Parada, Carolina
            </a>
           </td>
           <td class="r">
            Google
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#205788" title="Click to go to the Author Index">
             Sindhwani, Vikas
            </a>
           </td>
           <td class="r">
            Google Brain, NYC
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#179038" title="Click to go to the Author Index">
             Vanhoucke, Vincent
            </a>
           </td>
           <td class="r">
            Google
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#219934" title="Click to go to the Author Index">
             Tan, Jie
            </a>
           </td>
           <td class="r">
            Google
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#324421" title="Click to go to the Author Index">
             Lee, Kuang-Huei
            </a>
           </td>
           <td class="r">
            Google
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1015" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#performance_evaluation_and_benchmarking" title="Click to go to the Keyword Index">
               Performance Evaluation and Benchmarking
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#machine_learning_for_robot_control" title="Click to go to the Keyword Index">
               Machine Learning for Robot Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper, we describe the design of the Barkour benchmark for measuring robot agility in navigating complex environments. Despite the growing interest in developing agile robot locomotion skills, the field lacks systematic benchmarks to measure the performance of robotic control systems and hardware in agility-focused tasks. This motivated us to propose the Barkour benchmark, an obstacle course designed to quantify agility across various robotic platforms. Inspired by dog agility competitions, the course features diverse obstacles and a time-based scoring mechanism, encouraging researchers to develop controllers that enable robots to move quickly, precisely, and with adaptability. This benchmark is challenging as it demands diverse motion skills and the time-based scoring requires control precision at high speed. Along with the design details presented in the paper, we release our simulated environment setups in MuJoCo-XLA and the CAD model of a custom-designed quadruped robot to facilitate future research to reproduce the Barkour setup (available at sites.google.com/view/barkour). We hope these together will accelerate the pace of robot agility research.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t1_05">
             09:00-10:00, Paper ThPI4T1.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1677'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Task-Space Riccati Feedback Based Whole Body Control for Underactuated Legged Locomotion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#284863" title="Click to go to the Author Index">
             Yang, Shunpeng
            </a>
           </td>
           <td class="r">
            Hong Kong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#284107" title="Click to go to the Author Index">
             Hong, Zejun
            </a>
           </td>
           <td class="r">
            Southern University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#355265" title="Click to go to the Author Index">
             Li, Sen
            </a>
           </td>
           <td class="r">
            Department of Civil and Environment Engineering, Hong Kong Univ
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#131965" title="Click to go to the Author Index">
             Wensing, Patrick M.
            </a>
           </td>
           <td class="r">
            University of Notre Dame
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#142430" title="Click to go to the Author Index">
             Zhang, Wei
            </a>
           </td>
           <td class="r">
            Southern University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#251380" title="Click to go to the Author Index">
             Chen, Hua
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1677" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#whole_body_motion_planning_and_control" title="Click to go to the Keyword Index">
               Whole-Body Motion Planning and Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#underactuated_robots" title="Click to go to the Keyword Index">
               Underactuated Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This manuscript primarily aims to enhance the performance of whole-body controllers(WBC) for underactuated legged locomotion. We introduce a systematic parameter design mechanism for the floating-base feedback control within the WBC. The proposed approach involves utilizing the linearized model of unactuated dynamics to formulate a Linear Quadratic Regulator(LQR) and solving a Riccati gain while accounting for potential physical constraints through a second-order approximation of the log-barrier function. And then the user-tuned feedback gain for the floating base task is replaced by a new one constructed from the solved Riccati gain. Extensive simulations conducted in MuJoCo with a point bipedal robot, as well as real-world experiments performed on a quadruped robot, demonstrate the effectiveness of the proposed method. In the different bipedal locomotion tasks, compared with the user-tuned method, the proposed approach is at least 12% better and up to 50% better at linear velocity tracking, and at least 7% better and up to 47% better at angular velocity tracking. In the quadruped experiment, linear velocity tracking is improved by at least 3% and angular velocity tracking is improved by at least 23% using the proposed method.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t1_06">
             09:00-10:00, Paper ThPI4T1.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             SLIP Embodied Robust Quadruped Robot Control
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#378077" title="Click to go to the Author Index">
             Hong, Jin song
            </a>
           </td>
           <td class="r">
            DGIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#378150" title="Click to go to the Author Index">
             Yeo, Changmin
            </a>
           </td>
           <td class="r">
            DGIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#237225" title="Click to go to the Author Index">
             Bae, Sangjin
            </a>
           </td>
           <td class="r">
            Daegu Gyeongbuk Institute of Science &amp; Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#350976" title="Click to go to the Author Index">
             Hong, Jeongwoo
            </a>
           </td>
           <td class="r">
            Daegu Gyeongbuk Institute of Science and Technology (DGIST)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#129982" title="Click to go to the Author Index">
             Oh, Sehoon
            </a>
           </td>
           <td class="r">
            DGIST
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t1_07">
             09:00-10:00, Paper ThPI4T1.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1966'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Enhancing Leg Odometry in Legged Robots with Learned Contact Bias: An LSTM Recurrent Neural Network Approach
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396682" title="Click to go to the Author Index">
             Gu, Yaru
            </a>
           </td>
           <td class="r">
            Memorial University of Newfoundland
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397550" title="Click to go to the Author Index">
             Liu, Ze
            </a>
           </td>
           <td class="r">
            Simon Fraser University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#202313" title="Click to go to the Author Index">
             Zou, Ting
            </a>
           </td>
           <td class="r">
            Memorial University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1966" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             To address the leg odometry drift caused by the non-stationary foot contact, this paper introduces a novel data-driven based leg odometry technique for legged robots. By leveraging a Long Short-Term Memory (LSTM) Recurrent Neural Network (RNN), the method learns the biases in the robot's foot contact locations from sequential IMU measurements and ground reaction forces (GRF). This learned contact bias is then incorporated into the state estimation process using a Kalman filter (KF), significantly improving the precision of leg odometry for legged robots operating in real time. This method, which combines deep learning approaches with conventional filtering techniques, is termed the Deep Learning Kalman Filter (DLKF). The effectiveness of the DLKF is demonstrated through simulation and experimental trials using a Unitree Go1 robot across various challenging environments, including uneven terrain, slopes, and stairs, where foot slippage occurs frequently. Our results indicate an average 64.93% reduction in translational errors in leg odometry when the learned contact bias is applied. Further improvements are observed in a fused LiDAR and leg odometry state estimation system, especially in feature-deprived areas, indicating that the proposed leg odometry system can be easily fused with other sensor measurements to get a more precise state estimation.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t1_08">
             09:00-10:00, Paper ThPI4T1.8
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2306'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Distilling Reinforcement Learning Policies for Interpretable Robot Locomotion: Gradient Boosting Machines and Symbolic Regression
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#310550" title="Click to go to the Author Index">
             Acero, Fernando
            </a>
           </td>
           <td class="r">
            University College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#125824" title="Click to go to the Author Index">
             Li, Zhibin (Alex)
            </a>
           </td>
           <td class="r">
            University College London
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2306" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Recent advancements in reinforcement learning (RL) have led to remarkable achievements in robot locomotion capabilities. However, the complexity and ``black-box'' nature of neural network-based RL policies hinder their interpretability and broader acceptance, particularly in applications demanding high levels of safety and reliability. This paper introduces a novel approach to distill neural RL policies into more interpretable forms using Gradient Boosting Machines (GBMs), Explainable Boosting Machines (EBMs) and Symbolic Regression. By leveraging the inherent interpretability of generalized additive models, decision trees, and analytical expressions, we transform opaque neural network policies into more transparent "glass-box" models. We train expert neural network policies using RL and subsequently distill them into (i) GBMs, (ii) EBMs, and (iii) symbolic policies. To address the inherent distribution shift challenge of behavioral cloning, we propose to use the Dataset Aggregation (DAgger) algorithm with a curriculum of episode-dependent alternation of actions between expert and distilled policies, to enable efficient distillation of feedback control policies. We evaluate our approach on various robot locomotion gaits -- walking, trotting, bounding, and pacing -- and study the importance of different observations in joint actions for distilled policies using various methods. We train neural expert policies for 205 hours of simulated experience and distill interpretable policies with only 10 minutes of simulated interaction for each gait using the proposed method.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t1_09">
             09:00-10:00, Paper ThPI4T1.9
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2434'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Dynamic Object Catching with Quadruped Robot Front Legs
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398209" title="Click to go to the Author Index">
             Schakkal, Andr
            </a>
           </td>
           <td class="r">
            EPFL
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#216565" title="Click to go to the Author Index">
             Bellegarda, Guillaume
            </a>
           </td>
           <td class="r">
            EPFL
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#105018" title="Click to go to the Author Index">
             Ijspeert, Auke
            </a>
           </td>
           <td class="r">
            EPFL
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2434" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#motion_control" title="Click to go to the Keyword Index">
               Motion Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a framework for dynamic object catching using a quadruped robot's front legs while it stands on its rear legs. The system integrates computer vision, trajectory prediction, and leg control to enable the quadruped to visually detect, track, and successfully catch a thrown object using an onboard camera. Leveraging a fine-tuned YOLOv8 model for object detection and a regression-based trajectory prediction module, the quadruped adapts its front leg positions iteratively to anticipate and intercept the object. The catching maneuver involves identifying the optimal catching position, controlling the front legs with Cartesian PD control, and closing the legs together at the right moment. We propose and validate three different methods for selecting the optimal catching position: 1) intersecting the predicted trajectory with a vertical plane, 2) selecting the point on the predicted trajectory with the minimal distance to the center of the robots legs in their nominal position, and 3) selecting the point on the predicted trajectory with the highest likelihood on a Gaussian Mixture Model (GMM) modelling the robot's reachable space. Experimental results demonstrate robust catching capabilities across various scenarios, with the GMM method achieving the best performance, leading to an 80% catching success rate.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t1_10">
             09:00-10:00, Paper ThPI4T1.10
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2727'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Improving Legged Robot Locomotion by Quantifying Morphological Computation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398537" title="Click to go to the Author Index">
             Chandiramani, Vijay
            </a>
           </td>
           <td class="r">
            University of Bristol
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#110193" title="Click to go to the Author Index">
             Hauser, Helmut
            </a>
           </td>
           <td class="r">
            University of Bristol
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#166553" title="Click to go to the Author Index">
             Conn, Andrew
            </a>
           </td>
           <td class="r">
            University of Bristol
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2727" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#biologically_inspired_robots" title="Click to go to the Keyword Index">
               Biologically-Inspired Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#compliant_joints_and_mechanisms" title="Click to go to the Keyword Index">
               Compliant Joints and Mechanisms
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Many robotic and biological systems exploit their morphologys interaction with the environment to become more adaptable, more energy efficient, and to simplify their control. The principles of morphological computation (MC) have been increasingly studied in recent years and researchers have investigated theoretical approaches to quantify the contribution of MC for a variety of robotic systems using only simulated models. In this work, we quantify MC in a physical robotic system, utilizing position-controlled legs with two degrees of freedom in two designs of different elastic compliance, on a bespoke test rig to execute a walking gait. The contribution of morphology was estimated by applying a theoretical model at various stages within the gait cycle to quantify the MC. The relationships between MC and the ground reaction forces and actuator energy consumption are analyzed. The results indicating that increasing the compliance in the leg morphology increases the mean MC value (7.701.49) relative to a traditional leg design (5.032.27). Periods of high MC were found to occur during the swing phase of the walking gait and reduced during the stance phase with ground reaction forces, which correlates with the findings of prior theoretical studies of MC in hopping gaits. The benefits of refining the leg morphology for higher MC is demonstrated by the measurements of cost of transport (COT), where the leg with the higher mean MC of 7.70 has a lower mean COT of 102.8 compared to the other legs mean COT of 153.8. The results demonstrate how real-world measurements of MC may help design the morphology of improved robotics systems.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t1_11">
             09:00-10:00, Paper ThPI4T1.11
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2925'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Harnessing Natural Oscillations for High-Speed, Efficient Asymmetrical Locomotion in Quadrupedal Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#216951" title="Click to go to the Author Index">
             Cheng, Jing
            </a>
           </td>
           <td class="r">
            Syracuse University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#354189" title="Click to go to the Author Index">
             Alqaham, Yasser G.
            </a>
           </td>
           <td class="r">
            Syracuse University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#171554" title="Click to go to the Author Index">
             Gan, Zhenyu
            </a>
           </td>
           <td class="r">
            Syracuse University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2925" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#whole_body_motion_planning_and_control" title="Click to go to the Keyword Index">
               Whole-Body Motion Planning and Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This study explores the dynamics of asymmetrical bounding gaits in quadrupedal robots, focusing on the integration of torso pitching and hip motion to enhance speed and stability. Traditional control strategies often enforce a fixed posture, minimizing natural body movements to simplify the control problem. However, this approach may overlook the inherent dynamical advantages found in natural locomotion. By considering the robot as two interconnected segments, we concentrate on stance leg motion while allowing passive torso oscillation, drawing inspiration from natural dynamics and underactuated robotics principles. Our control scheme employs Linear Inverted Pendulum (LIP) and Spring-Loaded Inverted Pendulum (SLIP) models to govern front and rear leg movements independently. This approach has been validated through extensive simulations and hardware experiments, demonstrating successful high-speed locomotion with top speeds nearing 4 m/s and reduced ground reaction forces, indicating a more efficient gait. Furthermore, unlike conventional methods, our strategy leverages natural torso oscillations to aid leg circulation and stride length, aligning robot dynamics more closely with biological counterparts. Our findings suggest that embracing the natural dynamics of quadrupedal movement, particularly in asymmetrical gaits like bounding, can lead to more stable, efficient, and high-speed robotic locomotion. This investigation lays the groundwork for future studies on versatile and dynamic quadrupedal gaits and their potential applications in scenarios demanding rapid and effective locomotion.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t1_12">
             09:00-10:00, Paper ThPI4T1.12
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2927'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Learning Safe Locomotion for Quadrupedal Robots by Derived-Action Optimization
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#314225" title="Click to go to the Author Index">
             Zhu, Deye
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#368276" title="Click to go to the Author Index">
             Zhu, Chengrui
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#270935" title="Click to go to the Author Index">
             Zhang, Zhen
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#346208" title="Click to go to the Author Index">
             Xin, Shuo
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#122966" title="Click to go to the Author Index">
             Liu, Yong
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2927" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#machine_learning_for_robot_control" title="Click to go to the Keyword Index">
               Machine Learning for Robot Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Deep reinforcement learning controllers with exteroception have enabled quadrupedal robots to traverse terrain robustly. However, most of these controllers heavily depend on complex reward functions and suffer from poor convergence. This work proposes a novel learning framework called derived-action optimization. The derived action is defined as a high-level representation of a policy and can be introduced into the reward function to guide decision-making behaviors. The proposed derived-action optimization is applied to learning safe quadrupedal locomotion, achieving fast convergence and good performance. Specifically, we choose the foothold as the derived action and optimize the flatness of the terrain around the foothold to reduce potential slippage and collisions. Extensive experiments demonstrate the high safety and effectiveness of our method.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t1_13">
             09:00-10:00, Paper ThPI4T1.13
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2932'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              LocoMan: Advancing Versatile Quadrupedal Dexterity with Lightweight Loco-Manipulators
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#336562" title="Click to go to the Author Index">
             Lin, Changyi
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#295050" title="Click to go to the Author Index">
             Liu, Xingyu
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#220165" title="Click to go to the Author Index">
             Yang, Yuxiang
            </a>
           </td>
           <td class="r">
            Robotics at Google
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#220764" title="Click to go to the Author Index">
             Niu, Yaru
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#198071" title="Click to go to the Author Index">
             Yu, Wenhao
            </a>
           </td>
           <td class="r">
            Google
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#173944" title="Click to go to the Author Index">
             Zhang, Tingnan
            </a>
           </td>
           <td class="r">
            Google
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#219934" title="Click to go to the Author Index">
             Tan, Jie
            </a>
           </td>
           <td class="r">
            Google
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#151779" title="Click to go to the Author Index">
             Boots, Byron
            </a>
           </td>
           <td class="r">
            University of Washington
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#203949" title="Click to go to the Author Index">
             Zhao, Ding
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2932" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#methods_and_tools_for_robot_system_design" title="Click to go to the Keyword Index">
               Methods and Tools for Robot System Design
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#whole_body_motion_planning_and_control" title="Click to go to the Keyword Index">
               Whole-Body Motion Planning and Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Quadrupedal robots have emerged as versatile agents capable of locomoting and manipulating in complex environments. Traditional designs typically rely on the robots inherent body parts or incorporate top-mounted arms for manipulation tasks. However, these configurations may limit the robots operational dexterity, efficiency, and adaptability, particularly in cluttered or constrained spaces. In this work, we present LocoMan, a dexterous quadrupedal robot with a novel morphology to perform versatile manipulation in diverse constrained environments. By equipping a Unitree Go1 robot with two low-cost and lightweight modular 3-DoF loco manipulators on its front calves, LocoMan leverages the combined mobility and functionality of the legs and grippers for complex manipulation tasks that require precise 6D positioning of the end effector in a wide workspace. To harness the loco-manipulation capabilities of LocoMan, we introduce a unified control framework that extends the whole-body controller (WBC) to integrate the dynamics of loco-manipulators. Through experiments, we validate that the proposed whole-body controller can accurately and stably follow desired 6D trajectories of the end effector and torso, which, when combined with the large workspace from our design, facilitates a diverse set of challenging dexterous loco-manipulation tasks in confined spaces, such as opening doors, plugging into sockets, picking objects in narrow and low-lying spaces, and bimanual manipulation.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t1_14">
             09:00-10:00, Paper ThPI4T1.14
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3118'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Versatile Locomotion Skills for Hexapod Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#337739" title="Click to go to the Author Index">
             Qu, Tomson
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398711" title="Click to go to the Author Index">
             Li, Dichen
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#127841" title="Click to go to the Author Index">
             Zakhor, Avideh
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#198071" title="Click to go to the Author Index">
             Yu, Wenhao
            </a>
           </td>
           <td class="r">
            Google
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#173944" title="Click to go to the Author Index">
             Zhang, Tingnan
            </a>
           </td>
           <td class="r">
            Google
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3118" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#climbing_robots" title="Click to go to the Keyword Index">
               Climbing Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Hexapod robots are potentially suitable for carrying out tasks in cluttered environments since they are stable, compact, and light weight. They also have multi-joint legs and variable height bodies that make them good candidates for tasks such as stairs climbing and squeezing under objects in a typical home environment or an attic. Expanding on our previous work on joist climbing in attics, we train a legged hexapod equipped with a depth camera and visual inertial odometry (VIO) to perform three tasks: climbing stairs, avoiding obstacles, and squeezing under obstacles such as a table. Our policies are trained with simulation data only and can be deployed on low-cost hardware not requiring real-time joint state feedback. We train our model in a teacher-student model with 2 phases: In phase 1, we use reinforcement learning with access to privileged information such as height maps and joint feedback. In phase 2, we use supervised learning to distill the model into one with access to only onboard observations, consisting of egocentric depth images and robot pose captured by a tracking VIO camera. By manipulating available privileged information, constructing simulation terrains, and refining reward functions during phase 1 training, we are able to train the robots with skills that are robust in non-ideal physical environments. We demonstrate successful sim-to-real transfer and achieve high success rates across all three tasks in physical experiments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t1_15">
             09:00-10:00, Paper ThPI4T1.15
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3198'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Modeling and Analysis of Passive Quadruped Walker with Compliant Torso on Low-Friction Environment
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#307066" title="Click to go to the Author Index">
             Xiang, Yuxuan
            </a>
           </td>
           <td class="r">
            Japan Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#194561" title="Click to go to the Author Index">
             Zheng, Yanqiu
            </a>
           </td>
           <td class="r">
            Ritsumeikan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#103082" title="Click to go to the Author Index">
             Asano, Fumihiko
            </a>
           </td>
           <td class="r">
            Japan Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3198" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#passive_walking" title="Click to go to the Keyword Index">
               Passive Walking
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#biologically_inspired_robots" title="Click to go to the Keyword Index">
               Biologically-Inspired Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The quadrupeds have wider active territory than humans. Their bodies can adapt various environments through evolution, enabling the efficient, elegant gait for their legged locomotion. Previous researches have indicated lots of examples of utilizing the advantages of body to achieve environment adaptive and stable gait, and for legged locomotion, especially with quadruped robot determining how to generate environment-adaptive mobile locomotion remains a significant challenge. In this study, we discussed the adaptability to environments of quadruped robots, specific walking stability and gait convergence in low-friction environments with Compliant Torso. The numerically simulations are proposed for observing the trend of walking performance with various friction coefficient. By analyzing the typical walking gait, the adaptability of quadruped walkers with compliant torso are found. These conclusions contribute to the design and development of compliant torso for quadruped walkers.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t1_16">
             09:00-10:00, Paper ThPI4T1.16
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3278'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Leveraging Symmetry in RL-Based Legged Locomotion Control
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398788" title="Click to go to the Author Index">
             Su, Zhi
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#341937" title="Click to go to the Author Index">
             Huang, Xiaoyu
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#382255" title="Click to go to the Author Index">
             Ordonez Apraez, Daniel Felipe
            </a>
           </td>
           <td class="r">
            Italian Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#299378" title="Click to go to the Author Index">
             Li, Yunfei
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#237641" title="Click to go to the Author Index">
             Li, Zhongyu
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#325511" title="Click to go to the Author Index">
             Liao, Qiayuan
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#171363" title="Click to go to the Author Index">
             Pontil, Massimiliano
            </a>
           </td>
           <td class="r">
            Department of Computer Science, University College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#108792" title="Click to go to the Author Index">
             Semini, Claudio
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#277908" title="Click to go to the Author Index">
             Turrisi, Giulio
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#239398" title="Click to go to the Author Index">
             Wu, Yi
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#138299" title="Click to go to the Author Index">
             Sreenath, Koushil
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3278" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Model-free reinforcement learning is a promising approach for autonomously solving challenging robotics control problems, but faces exploration difficulty without information of the robot's kinematics and dynamics morphology. The under-exploration of multiple modalities with symmetric states leads to behaviors that are often unnatural and sub-optimal. This issue becomes particularly pronounced in the context of robotic systems with morphological symmetries, such as legged robots for which the resulting asymmetric and aperiodic behaviors compromise performance, robustness, and transferability to real hardware. To mitigate this challenge, we can leverage symmetry to guide and improve the exploration in policy learning via equivariance/invariance constraints. In this paper, we investigate the efficacy of two approaches to incorporate symmetry: modifying the network architectures to be strictly equivariant/invariant, and leveraging data augmentation to approximate equivariant/invariant actor-critics. We implement the methods on challenging loco-manipulation and bipedal locomotion tasks and compare with an unconstrained baseline. We find that the strictly equivariant policy consistently outperforms other methods in sample efficiency and task performance in simulation. In addition, symmetry-incorporated approaches exhibit better gait quality, higher robustness and can be deployed zero-shot in real-world experiments.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thpi4t2">
             <b>
              ThPI4T2
             </b>
            </a>
           </td>
           <td class="r">
            Room 2
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thpi4t2" title="Click to go to the Program at a Glance">
             <b>
              Robotics in Healthcare III
             </b>
            </a>
           </td>
           <td class="r">
            Teaser Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#172565" title="Click to go to the Author Index">
             Stilli, Agostino
            </a>
           </td>
           <td class="r">
            University College London
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#117023" title="Click to go to the Author Index">
             Tamadazte, Brahim
            </a>
           </td>
           <td class="r">
            CNRS
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t2_01">
             09:00-10:00, Paper ThPI4T2.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1312'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Optimizing Base Placement of Surgical Robot: Kinematics Data-Driven Approach by Analyzing Working Pattern
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#373140" title="Click to go to the Author Index">
             Yoon, Jeonghyeon
            </a>
           </td>
           <td class="r">
            DGIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#353715" title="Click to go to the Author Index">
             Park, Junhyun
            </a>
           </td>
           <td class="r">
            DGIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#374268" title="Click to go to the Author Index">
             Park, Hyojae
            </a>
           </td>
           <td class="r">
            Daegu Gyeongbuk Institute of Science and Technology (DGIST)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#374269" title="Click to go to the Author Index">
             Lee, Hakyoon
            </a>
           </td>
           <td class="r">
            DGIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#377186" title="Click to go to the Author Index">
             Lee, Sang Won
            </a>
           </td>
           <td class="r">
            Daegu Gyeongbuk Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#145094" title="Click to go to the Author Index">
             Hwang, Minho
            </a>
           </td>
           <td class="r">
            Daegu Gyeongbuk Instituute of Science and Technology (DGIST)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1312" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#surgical_robotics__laparoscopy" title="Click to go to the Keyword Index">
               Surgical Robotics: Laparoscopy
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In robot-assisted minimally invasive surgery (RAMIS), optimal placement of the surgical robot base is crucial for successful surgery. Improper placement can hinder performance because of manipulator limitations and inaccessible workspaces. Conventional base placement relies on the experience of trained medical staff. This study proposes a novel method for determining the optimal base pose based on the surgeons working pattern. The proposed method analyzes recorded end-effector poses using a machine learning-based clustering technique to identify key positions and orientations preferred by the surgeon. We introduce two scoring metrics to address the joint limit and singularity issues: joint margin and manipulability scores. We then train a multi-layer perceptron regressor to predict the optimal base pose based on these scores. Evaluation in a simulated environment using the da Vinci Research Kit shows unique base pose score maps for four volunteers, highlighting the individuality of the working patterns. Results comparing with 20,000 randomly selected base poses suggest that the score obtained using the proposed method is 28.2% higher than that obtained by random base placement. These results emphasize the need for operator-specific optimization during base placement in RAMIS.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t2_02">
             09:00-10:00, Paper ThPI4T2.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2547'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              2mm Diameter Continuum Robot Tools for Suturing in Open Spina Bifida Repair
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#375517" title="Click to go to the Author Index">
             Law, Arion
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398271" title="Click to go to the Author Index">
             Nimal, Nillan
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#340419" title="Click to go to the Author Index">
             Kang, Paul Hoseok
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#209550" title="Click to go to the Author Index">
             Gondokaryono, Radian
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#157335" title="Click to go to the Author Index">
             Drake, James
            </a>
           </td>
           <td class="r">
            Hospital for Sick Children, University of Toronto
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#378062" title="Click to go to the Author Index">
             Van Mieghem, Tim
            </a>
           </td>
           <td class="r">
            Sinai Health
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#109669" title="Click to go to the Author Index">
             Looi, Thomas
            </a>
           </td>
           <td class="r">
            Hospital for Sick Children
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2547" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#surgical_robotics__laparoscopy" title="Click to go to the Keyword Index">
               Surgical Robotics: Laparoscopy
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#compliant_joints_and_mechanisms" title="Click to go to the Keyword Index">
               Compliant Joints and Mechanisms
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Open Spina Bifida (OSB) is a congenital neural tube defect where a major component of the procedure to repair the defect involves the closure of a lesion wound through suturing. For a minimally invasive approach, tools entering the uterus to access the fetus should be as thin as possible to minimize maternal risk. This work presents the design of a 3 degrees-of-freedom, 2mm diameter tool wrist with a bending range of motion from 0 to 90. This wrist is capable of generating up to 2N of force measured from the end of the wrist and achieving a bending curvature of 107m-1 (9.35mm bending radius). A pseudo-rigid body kinematic model has been implemented for the control of this tool with a protocol for singularity mitigation and avoidance. Timed teleoperation studies explicitly demonstrate that the tool is able to reliably execute suturing with a fastest achieved time of under 3 minutes for a simple interrupted suturing technique.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t2_03">
             09:00-10:00, Paper ThPI4T2.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('729'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              BronchoCopilot: Towards Autonomous Robotic Bronchoscopy Via Multimodal Reinforcement Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#374239" title="Click to go to the Author Index">
             Zhao, Jianbo
            </a>
           </td>
           <td class="r">
            Institute of Automation, Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#361812" title="Click to go to the Author Index">
             Chen, Hao
            </a>
           </td>
           <td class="r">
            University of Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#335224" title="Click to go to the Author Index">
             Tian, Qingyao
            </a>
           </td>
           <td class="r">
            University of Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#317513" title="Click to go to the Author Index">
             Chen, Jian
            </a>
           </td>
           <td class="r">
            Hong Kong Institute of Science and Innovation, Chinese Academy O
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#376266" title="Click to go to the Author Index">
             Yang, Bingyu
            </a>
           </td>
           <td class="r">
            Institute of Automation, Chinese Academy of Sciences Sch
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#336380" title="Click to go to the Author Index">
             Zhang, Zihui
            </a>
           </td>
           <td class="r">
            Institute of Automation, Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#104575" title="Click to go to the Author Index">
             Liu, Hongbin
            </a>
           </td>
           <td class="r">
            Hong Kong Institute of Science &amp; Innovation, Chinese Academy Of
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab729" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#surgical_robotics__planning" title="Click to go to the Keyword Index">
               Surgical Robotics: Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Bronchoscopy plays a significant role in the early diagnosis and treatment of lung diseases. This process demands physicians to maneuver the flexible endoscope for reaching distal lesions, particularly requiring substantial expertise when examining the airways of the upper lung lobe. With the development of artificial intelligence and robotics, reinforcement learning (RL) method has been applied to the manipulation of interventional surgical robots. However, unlike human physicians who utilize multimodal information, most of the current RL methods rely on a single modality, limiting their performance. In this paper, we propose BronchoCopilot, a multimodal RL agent designed to acquire manipulation skills for autonomous bronchoscopy. BronchoCopilot specifically integrates images from the bronchoscope camera and estimated robot poses, aiming for a higher success rate within challenging airway environment. We employ auxiliary reconstruction tasks to compress multimodal data and utilize attention mechanisms to achieve an efficient latent representation of this data, serving as input for the RL module. This framework adopts a stepwise training and fine-tuning approach to mitigate the challenges of training difficulty. Our evaluation in the realistic simulation environment reveals that BronchoCopilot, by effectively harnessing multimodal information, attains a success rate of approximately 90% in fifth generation airways with consistent movements. Additionally, it demonstrates a robust capacity to adapt to diverse cases.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t2_04">
             09:00-10:00, Paper ThPI4T2.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2254'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Autonomous Guidewire Navigation in Dynamic Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#340523" title="Click to go to the Author Index">
             Scarponi, Valentina
            </a>
           </td>
           <td class="r">
            Inria, University of Strasbourg
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396675" title="Click to go to the Author Index">
             Lecomte, Franois
            </a>
           </td>
           <td class="r">
            INRIA
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#340480" title="Click to go to the Author Index">
             Duprez, Michel
            </a>
           </td>
           <td class="r">
            Inria
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#112297" title="Click to go to the Author Index">
             Nageotte, Florent
            </a>
           </td>
           <td class="r">
            University of Strasbourg
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#107722" title="Click to go to the Author Index">
             Cotin, Stephane
            </a>
           </td>
           <td class="r">
            INRIA
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2254" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#machine_learning_for_robot_control" title="Click to go to the Keyword Index">
               Machine Learning for Robot Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#autonomous_agents" title="Click to go to the Keyword Index">
               Autonomous Agents
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Cardiovascular disease treatment involves the challenging task of navigating guidewires and catheters through the vascular anatomy. This often results in prolonged procedures where both the patient and clinician are subjected to X-ray radiation. As a potential solution, Deep Reinforcement Learning methods have demonstrated potential in learning this task, paving the way for automated catheter navigation during robotic interventions. However, current works show a limited ability to generalize to unseen and/or deforming anatomies. In this paper, we extend our previous reinforcement learning approach in two main areas: we improve the training strategy to learn a control of the device even when the vascular anatomy is deforming and we propose a method to estimate the motion of the anatomy from single view fluoroscopy images. The combination of these two contributions makes it possible to automatically navigate across a moving vascular anatomy under fluoroscopic imaging, even without injecting a contrast agent. We validate our method on two scenarios: a simulated beating heart and a liver subjected to breathing motion. Our approach leads to an average success rate of 95% in reaching random targets within these anatomies. Our framework is also computationally efficient, enabling the training of our controller to be completed in about 6 hours.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t2_05">
             09:00-10:00, Paper ThPI4T2.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('774'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Towards a Surgeon-In-The-Loop Ophthalmic Robotic Apprentice Using Reinforcement and Imitation Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#373656" title="Click to go to the Author Index">
             Gomaa, Amr
            </a>
           </td>
           <td class="r">
            DFKI, Saarland Informatics Campus
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#373657" title="Click to go to the Author Index">
             Mahdy, Bilal
            </a>
           </td>
           <td class="r">
            DFKI, Saarland Informatics Campus
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#319314" title="Click to go to the Author Index">
             Kleer, Niko
            </a>
           </td>
           <td class="r">
            DFKI, Saarland Informatics Campus
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#373699" title="Click to go to the Author Index">
             Krger, Antonio
            </a>
           </td>
           <td class="r">
            DFKI, Saarland Informatics Campus
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab774" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#surgical_robotics__planning" title="Click to go to the Keyword Index">
               Surgical Robotics: Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robot-assisted surgical systems have demonstrated significant potential in enhancing surgical precision and minimizing human errors. However, existing systems cannot accommodate individual surgeons' unique preferences and requirements. Additionally, they primarily focus on general surgeries (e.g., laparoscopy) and are unsuitable for highly precise microsurgeries, such as ophthalmic procedures. Thus, we propose an image-guided approach for surgeon-centered autonomous agents that can adapt to the individual surgeon's skill level and preferred surgical techniques during ophthalmic cataract surgery. Our approach trains reinforcement and imitation learning agents simultaneously using curriculum learning approaches guided by image data to perform all tasks of the incision phase of cataract surgery. By integrating the surgeon's actions and preferences into the training process, our approach enables the robot to implicitly learn and adapt to the individual surgeon's unique techniques through surgeon-in-the-loop demonstrations. This results in a more intuitive and personalized surgical experience for the surgeon while ensuring consistent performance for the autonomous robotic apprentice. We define and evaluate the effectiveness of our approach in a simulated environment using our proposed metrics and highlight the trade-off between a generic agent and a surgeon-centered adapted agent. Finally, our approach has the potential to extend to other ophthalmic and microsurgical procedures, opening the door to a new generation of surgeon-in-the-loop autonomous surgical robots. We provide an open-source simulation framework for future development and reproducibility at https://github.com/amrgomaaelhady/CataractAdaptSurgRobot.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t2_06">
             09:00-10:00, Paper ThPI4T2.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1501'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Pedicle Drilling Planning Transfer for Spine Surgery Using Functional Map Correspondences
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#223870" title="Click to go to the Author Index">
             Leblanc, Lilyan
            </a>
           </td>
           <td class="r">
            Sorbonne Universit
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#364357" title="Click to go to the Author Index">
             Vialle, Raphael
            </a>
           </td>
           <td class="r">
            ISIR, Sorbonne Universit, CNRS UMR 7222, INSERM U1150
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#286610" title="Click to go to the Author Index">
             de Farias, Cristiana
            </a>
           </td>
           <td class="r">
            University of Birmingham
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#345726" title="Click to go to the Author Index">
             Saghbiny, Elie
            </a>
           </td>
           <td class="r">
            ISIR, UMR 7222 Sorbonne University, CNRS, ERL AGATHE, U1150 INSE
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#164119" title="Click to go to the Author Index">
             Marturi, Naresh
            </a>
           </td>
           <td class="r">
            University of Birmingham
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#117023" title="Click to go to the Author Index">
             Tamadazte, Brahim
            </a>
           </td>
           <td class="r">
            CNRS
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1501" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#surgical_robotics__planning" title="Click to go to the Keyword Index">
               Surgical Robotics: Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_medical_robotics" title="Click to go to the Keyword Index">
               Computer Vision for Medical Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Precise pedicle screw placement is critical for a variety of spine surgical procedures, demanding accurate geometric alignment. Failure to provide millimetric accuracy carries high risks. The literature reports a complication rate in the case of screw mispositioning of up to 18%. Robot-assisted navigation systems have been developed to assist the surgeon in finding optimal entry points and drilling directions. To bring the surgeon's tool in an optimal configuration, the navigation systems need to account for non-rigid movements of the spine, either caused by the patient's movement or interactions with the surgeon. However, this usually comes at the cost of repeated per-operative imaging radiation.
             <p>
              Relying on the functional map framework, we present a method to transfer drilling trajectories defined on a pre-operative CT scan of a spine to a partial, noisy observation of the same spine.
              <p>
               The method was assessed through simulations of increasingly complex deformation cases. Validation was performed by evaluating the registration error (translation and rotation) and verifying whether drilling trajectories remained clinically valid once transferred to the partially observed, noisy, and deformed spine model. Results demonstrated that the proposed method could provide a reliable method of transferring drilling trajectories, even in scenarios with noisy, partial views of non-rigidly deformed spine models.
              </p>
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t2_07">
             09:00-10:00, Paper ThPI4T2.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1748'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SURESTEP: An Uncertainty-Aware Trajectory Optimization Framework to Enhance Visual Tool Tracking for Robust Surgical Automation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#355515" title="Click to go to the Author Index">
             Shinde, Nikhil
            </a>
           </td>
           <td class="r">
            University of California San Diego
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#212249" title="Click to go to the Author Index">
             Chiu, Zih-Yun
            </a>
           </td>
           <td class="r">
            University of California, San Diego
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#234261" title="Click to go to the Author Index">
             Richter, Florian
            </a>
           </td>
           <td class="r">
            University of California, San Diego
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#279362" title="Click to go to the Author Index">
             Lim, Jason
            </a>
           </td>
           <td class="r">
            University of Nevada, Reno
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#285724" title="Click to go to the Author Index">
             Zhi, Yuheng
            </a>
           </td>
           <td class="r">
            University of California, San Diego
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#205556" title="Click to go to the Author Index">
             Herbert, Sylvia
            </a>
           </td>
           <td class="r">
            UC San Diego (UCSD)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#137547" title="Click to go to the Author Index">
             Yip, Michael C.
            </a>
           </td>
           <td class="r">
            University of California, San Diego
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1748" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#surgical_robotics__planning" title="Click to go to the Keyword Index">
               Surgical Robotics: Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#surgical_robotics__laparoscopy" title="Click to go to the Keyword Index">
               Surgical Robotics: Laparoscopy
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Inaccurate tool localization is one of the main rea- sons for failures in automating surgical tasks. Imprecise robot kinematics and noisy observations caused by the poor visual acuity of an endoscopic camera make tool tracking challenging. Previous works in surgical automation adopt environment specific setups or hard-coded strategies instead of explicitly considering motion and observation uncertainty of tool tracking in their policies. In this work, we present SURESTEP, an uncertainty-aware trajectory optimization framework for robust surgical automation. We model the uncertainty in tool tracking by considering noise sources that are typical in surgical environments. Using a Gaussian assumption to propagate our uncertainty models through a given tool trajectory, SURESTEP provides a general framework that minimizes the upper bound on the entropy of the final estimated tool distribution. We showcase our method by performing the first-ever, to our knowledge, needle regrasping with a moving endoscopic camera. We compare SURESTEP with a baseline method on a real-world suture needle regrasping task under challenging environmental conditions, such as poor lighting and a moving endoscopic camera. The results over 60 regrasps on the daVinci Research Kit (dVRK) demonstrate that our optimized trajectories significantly outperform the un-optimized baseline.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t2_08">
             09:00-10:00, Paper ThPI4T2.8
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2364'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SeeBelow: Sub-Dermal 3D Reconstruction of Tumors with Surgical Robotic Palpation and Tactile Exploration
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397995" title="Click to go to the Author Index">
             Uppuluri, Raghava
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397924" title="Click to go to the Author Index">
             Bhattacharjee, Abhinaba
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#309804" title="Click to go to the Author Index">
             Anwar, Sohel
            </a>
           </td>
           <td class="r">
            Indiana University Purdue University Indianapolis
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#184771" title="Click to go to the Author Index">
             She, Yu
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2364" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#surgical_robotics__planning" title="Click to go to the Keyword Index">
               Surgical Robotics: Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#force_and_tactile_sensing" title="Click to go to the Keyword Index">
               Force and Tactile Sensing
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#surgical_robotics__laparoscopy" title="Click to go to the Keyword Index">
               Surgical Robotics: Laparoscopy
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             AbstractSurgical scene understanding in Robot-assisted Minimally Invasive Surgery (RMIS) is highly reliant on visual cues and lacks tactile perception. Force-modulated surgical palpation with tactile feedback is necessary for localization, geometry/depth estimation, and dexterous exploration of abnormal stiff inclusions in subsurface tissue layers. Prior works explored surface-level tissue abnormalities or single layered tissue-tumor embeddings with more than 300 palpations for dense 2D stiffness mapping. Our approach focuses on 3D reconstructions of sub-dermal tumor surface profiles in multilayered tissue (skin-fat-muscle) using a visually-guided novel tactile navigation policy. A robotic palpation probe with triaxial force sensing was leveraged for tactile exploration of the phantom. From a surface mesh of the surgical region initialized from a depth camera, the policy explores a surgeons region of interest through palpation, sampled from bayesian optimization. Each palpation includes contour following using a contact-safe impedance controller to trace the sub-dermal tumor geometry, until the underlying tumor-tissue boundary is reached. Projections of these contour following palpation trajectories allows 3D reconstruction of the subdermal tumor surface profile in less than 100 palpations. Our approach generates high-fidelity 3D surface reconstructions of rigid tumor embeddings in tissue layers with isotropic elasticities, although soft tumor geometries are yet to be explored.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t2_09">
             09:00-10:00, Paper ThPI4T2.9
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3146'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SuFIA: Language-Guided Augmented Dexterity for Robotic Surgical Assistants
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#192566" title="Click to go to the Author Index">
             Moghani, Masoud
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#390992" title="Click to go to the Author Index">
             Doorenbos, Lars
            </a>
           </td>
           <td class="r">
            University of Bern
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#328828" title="Click to go to the Author Index">
             Panitch, William
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#390917" title="Click to go to the Author Index">
             Huver, Sean
            </a>
           </td>
           <td class="r">
            NVIDIA
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#103503" title="Click to go to the Author Index">
             Azizian, Mahdi
            </a>
           </td>
           <td class="r">
            Intuitive Surgical
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#107102" title="Click to go to the Author Index">
             Goldberg, Ken
            </a>
           </td>
           <td class="r">
            UC Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#155114" title="Click to go to the Author Index">
             Garg, Animesh
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3146" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#surgical_robotics__planning" title="Click to go to the Keyword Index">
               Surgical Robotics: Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this work, we present SuFIA, the first framework for natural language-guided augmented dexterity for robotic surgical assistants. SuFIA incorporates the strong reasoning capabilities of large language models (LLMs) with perception modules to implement high-level planning and low-level control of a robot for surgical sub-task execution. This enables a learning-free approach to surgical augmented dexterity without any in-context examples or motion primitives. SuFIA uses a human-in-the-loop paradigm by restoring control to the surgeon in the case of insufficient information, mitigating unexpected errors for mission-critical tasks. We evaluate SuFIA on four surgical sub-tasks in a simulation environment and two sub-tasks on a physical surgical robotic platform in the lab, demonstrating its ability to perform common surgical sub-tasks through supervised autonomous operation under challenging physical and workspace conditions. Project website: orbit-surgical.github.io/sufia
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t2_10">
             09:00-10:00, Paper ThPI4T2.10
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('125'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              FBG-Based Shape-Sensing to Enable Lateral Deflection Methods of Autonomous Needle Insertion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#305811" title="Click to go to the Author Index">
             Lezcano, Dimitri A.
            </a>
           </td>
           <td class="r">
            Johns Hopkins University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#108549" title="Click to go to the Author Index">
             Iordachita, Ioan Iulian
            </a>
           </td>
           <td class="r">
            Johns Hopkins University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#202088" title="Click to go to the Author Index">
             Kim, Jin Seob
            </a>
           </td>
           <td class="r">
            Johns Hopkins University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab125" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#surgical_robotics__steerable_catheters_needles" title="Click to go to the Keyword Index">
               Surgical Robotics: Steerable Catheters/Needles
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#surgical_robotics__planning" title="Click to go to the Keyword Index">
               Surgical Robotics: Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In diagnosing and treating prostate cancer the flexible bevel tip needle insertion surgical technique is commonly used. Bevel tip needles experience asymmetric loading on the needle's tip, inducing natural bending of the needle and enabling control mechanisms for precise placement of the needle during surgery. Several methods leverage the needles natural bending to provide autonomous control of needle insertion for accurate needle placement in an effort to reduce excess tissue damage and improve patient outcomes from needle insertion intraventions. Moreover, control methods using lateral deflection of the needle intra-operatively to steer the needle during insertion have been studied and have shown promising results. Thus, to enable these autonomous control methods, real-time, intra-operative shape-sensing feedback is pivotal for optimal performance of the needle insertion control. In this work, we demonstrate an extension of our proven Lie-group theoretic shape-sensing model to handle lateral deflection of the needle during needle insertion and validate this extension with robotic needle insertions in phantom tissue using stereo vision as a ground truth. Furthermore, we illustrate our system configuration for real-time shape-sensing using ROS 2, demonstrating average feedback frequency of 15 +/- 8 Hz. We realized average needle shape errors from this extension under 1 mm, validating the shape-sensing models' extension.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t2_11">
             09:00-10:00, Paper ThPI4T2.11
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2026'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              DESectBot Design and Validation of a Novel Two-Segment Decoupled Continuum Robotic System for Endoscopic Submucosal Dissection
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#330691" title="Click to go to the Author Index">
             Liu, Wenjie
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#377331" title="Click to go to the Author Index">
             Shao, Yuancheng
            </a>
           </td>
           <td class="r">
            City University of Macau
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#286018" title="Click to go to the Author Index">
             Zhang, Yao
            </a>
           </td>
           <td class="r">
            KU Leuven
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#326569" title="Click to go to the Author Index">
             Chen, Zixi
            </a>
           </td>
           <td class="r">
            Scuola Superiore Sant'Anna
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#256953" title="Click to go to the Author Index">
             Wu, Di
            </a>
           </td>
           <td class="r">
            KU Leuven
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#393051" title="Click to go to the Author Index">
             Chen, Yuqiao
            </a>
           </td>
           <td class="r">
            Zhuhai Institute of Advanced Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#101894" title="Click to go to the Author Index">
             Stefanini, Cesare
            </a>
           </td>
           <td class="r">
            Scuola Superiore Sant'Anna
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#377680" title="Click to go to the Author Index">
             Ling, Li
            </a>
           </td>
           <td class="r">
            Suzhou Ultimage Health Technology Co., Ltd
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#165194" title="Click to go to the Author Index">
             Qi, Peng
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2026" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#surgical_robotics__steerable_catheters_needles" title="Click to go to the Keyword Index">
               Surgical Robotics: Steerable Catheters/Needles
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Endoscopic Submucosal Dissection (ESD) is a minimally invasive procedure designed to remove precancerous and cancerous lesions from the gastrointestinal (GI) tract. Given the GI tract's tortuous and narrow shape, along with the need for varied movements during dissection, this requires highly flexible and compact instruments, making flexible continuum robots suitable candidates. In this paper, we propose a novel two-segment continuum robot system named DESectBot, featuring a diameter of 5.5 mm and a total length of the active bending module of 48 mm, while the robot's total length exceeds 1 m. We designed a novel joint combination structure called the spatial cross-curved disk skeleton for the robot, which addresses the mechanical coupling problem between flexible robot actuators. The DESectBot boasts six degrees of freedom, and its kinematic modeling has been derived and utilized in the closed-loop control of the DESectBot. The validation of the DESectBot was conducted through a two-stage test: first, the decoupling performance of the DESectBot was validated. The results show that when one active bending segment bends, the other segment remains almost uninfluenced, with a maximum variation of 1.15 degrees, demonstrating the robot's effective decoupling capability. Secondly, the accuracy of DESectBot was validated through trajectory-following experiments. The results reveal that the average tracking error for both trajectories is less than 2 mm, and the maximum tracking error is below 2.5 mm. Taking marking, one of the ESD procedures with a 5mm tolerance, as an example, the DESectBot has the potential to be utilized for ESD procedure.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t2_12">
             09:00-10:00, Paper ThPI4T2.12
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2729'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Bifurcation Identification for Ultrasound-Driven Robotic Cannulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#322555" title="Click to go to the Author Index">
             Morales, Cecilia
            </a>
           </td>
           <td class="r">
            Carnege Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#379985" title="Click to go to the Author Index">
             Srikanth, Dhruv
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398588" title="Click to go to the Author Index">
             Good, Jack
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#380307" title="Click to go to the Author Index">
             Dufendach, Keith
            </a>
           </td>
           <td class="r">
            University of Pittsburgh Medical Center
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#342365" title="Click to go to the Author Index">
             Dubrawski, Artur
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2729" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#surgical_robotics__steerable_catheters_needles" title="Click to go to the Keyword Index">
               Surgical Robotics: Steerable Catheters/Needles
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_medical_robotics" title="Click to go to the Keyword Index">
               Computer Vision for Medical Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In trauma and critical care settings, rapid and precise intravascular access is key to a patient's survival. Our research aims at ensuring this access, even when skilled medical personnel are not readily available. Vessel bifurcations are anatomical landmarks that can guide the safe placement of catheters or needles during medical procedures. Accurate identification of these bifurcations enables precise needle insertion. Although ultrasound is advantageous in navigating anatomical landmarks in emergency scenarios due to its portability and safety, to our knowledge, no existing algorithm can autonomously extract vessel bifurcations using ultrasound images. This is primarily due to the limited availability of ground truth data, in particular data from live subjects, needed for training and validating reliable models. Researchers often resort to using data from anatomical phantoms or simulations. We introduce BIFURC, Bifurcation Identification For Ultrasound-driven Robot Cannulation, a novel algorithm that identifies vessel bifurcations and provides optimal needle insertion sites for an autonomous robotic cannulation system. BIFURC integrates expert knowledge with deep learning techniques to efficiently detect vessel bifurcations within the femoral region and can be trained on a limited amount of in-vivo data. We evaluated our algorithm using a medical phantom as well as real-world experiments involving live pigs. In all cases, BIFURC consistently identified bifurcation points and needle insertion locations in alignment with those identified by expert clinicians.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t2_13">
             09:00-10:00, Paper ThPI4T2.13
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3031'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A 6-DOF Double-Layer Programmable Remote Center of Motion Robot for Vitreoretinal Surgery
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#394470" title="Click to go to the Author Index">
             Wang, Chenyu
            </a>
           </td>
           <td class="r">
            Chonnam National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#118341" title="Click to go to the Author Index">
             Ko, Seong Young
            </a>
           </td>
           <td class="r">
            Chonnam National University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3031" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#surgical_robotics__steerable_catheters_needles" title="Click to go to the Keyword Index">
               Surgical Robotics: Steerable Catheters/Needles
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mechanism_design" title="Click to go to the Keyword Index">
               Mechanism Design
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             During vitreoretinal surgery, surgeons are required to precisely manipulate surgical tools within a confined workspace of an eye, which is roughly 2.5 cm spherical in shape. Because the surgical view can only be obtained by a microscope placed above the eyeball through the pupil, the eyeball needs to be moved or rotated during the operation to see a larger portion of the retina. At this point, general Remote Center of Motion (RCM) mechanisms require additional actuators or manual modification. On the other hand, a programmable RCM mechanism can reduce surgery time without a physical alignment procedure. This study introduces a novel six-degree-of-freedom (DoF) programmable RCM mechanism capable of generating the RCM at random positions in 3D space. Our approach combines two planar 5-bar linkage mechanisms placed in parallel, creating a double-layered configuration to establish the programmable RCM mechanism. We optimized the workspace of each planar mechanism to a customized workspace for a general eyeball model using genetic algorithms, focusing on maximizing the manipulability of the target workspace. The Phantom Omni device was utilized as a remote controller to remotely control the proposed mechanism in a transparent eyeball model with a diameter of 4 cm. Evaluation of the functionality of the programmable RCM mechanism at various RCM points showed that the overall error was less than 1 millimeter. The repeatability of the mechanism was tested and showed an accuracy of about 127 micrometers.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t2_14">
             09:00-10:00, Paper ThPI4T2.14
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2860'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Miniaturisation and Evaluation of the SoftSCREEN System in Colon Phantoms
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#325643" title="Click to go to the Author Index">
             Consumi, Vanni
            </a>
           </td>
           <td class="r">
            UCL University College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398584" title="Click to go to the Author Index">
             Dei, Neri Niccol
            </a>
           </td>
           <td class="r">
            Scuola Superiore SantAnna
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#250675" title="Click to go to the Author Index">
             Ciuti, Gastone
            </a>
           </td>
           <td class="r">
            Scuola Superiore Sant'Anna
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#128031" title="Click to go to the Author Index">
             Stoyanov, Danail
            </a>
           </td>
           <td class="r">
            University College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#172565" title="Click to go to the Author Index">
             Stilli, Agostino
            </a>
           </td>
           <td class="r">
            University College London
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2860" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#soft_robot_materials_and_design" title="Click to go to the Keyword Index">
               Soft Robot Materials and Design
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#soft_sensors_and_actuators" title="Click to go to the Keyword Index">
               Soft Sensors and Actuators
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Screening of the lower gastrointestinal (GI) tract is of paramount importance for the early detection of precancerous lesions in the intestine, with an impact on reducing the high death rate of patients affected by cancer worldwide. Colonoscopy, i.e. standard procedure for screening the colon, is effective in reducing the incidence of colorectal cancer worldwide, nonetheless, this procedure remains an invasive method of screening, that typically causes discomfort and requires sedation for the patient. The SoftSCREEN system, a tethered robotic capsule designed for colonoscopy, aims to enable minimally invasive diagnosis of intestinal diseases through its innovative design that incorporates elastic tracks for locomotion and inflatable toroidal chambers for adaptable geometry to match the local lumen of the GI tract. After demonstrating the viability of the proposed design in a large-scale proof of concept in our previous work, the authors present here a miniaturised version of the SoftSCREEN system. We assess its performance in multiple phantom tests and evaluate the effect of pressure regulation on its locomotion. The conducted extensive tests demonstrate the capability of the soft robot to move inside intricate passages, capture internal images, and adjust its geometry to optimise traction. The results underscore the potential of the proposed design, offering promising advancements in the development of a robotic platform for efficient front-wheel locomotion and accurate intestinal screening.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thpi4t3">
             <b>
              ThPI4T3
             </b>
            </a>
           </td>
           <td class="r">
            Room 3
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thpi4t3" title="Click to go to the Program at a Glance">
             <b>
              Human-Robot Interaction (HRI) I
             </b>
            </a>
           </td>
           <td class="r">
            Teaser Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#372922" title="Click to go to the Author Index">
             Zhang, Yunbo
            </a>
           </td>
           <td class="r">
            Rochester Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#132634" title="Click to go to the Author Index">
             Li, Xiang
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t3_01">
             09:00-10:00, Paper ThPI4T3.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('880'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Driving Animatronic Robot Facial Expression from Speech
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#163585" title="Click to go to the Author Index">
             Li, Boren
            </a>
           </td>
           <td class="r">
            BIGAI
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#351396" title="Click to go to the Author Index">
             Li, Hang
            </a>
           </td>
           <td class="r">
            Beijing Institute for General Artificial Intelligence
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#196468" title="Click to go to the Author Index">
             Liu, Hangxin
            </a>
           </td>
           <td class="r">
            Beijing Institute for General Artificial Intelligence (BIGAI)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab880" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#art_and_entertainment_robotics" title="Click to go to the Keyword Index">
               Art and Entertainment Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#natural_machine_motion" title="Click to go to the Keyword Index">
               Natural Machine Motion
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Animatronic robots hold the promise of enabling natural human-robot interaction through lifelike facial expressions. However, generating realistic, speech-synchronized robot expressions poses significant challenges due to the complexities of facial biomechanics and the need for responsive motion synthesis. This paper introduces a novel, skinning-centric approach to drive animatronic robot facial expressions from speech input. At its core, the proposed approach employs linear blend skinning (LBS) as a unifying representation, guiding innovations in both embodiment design and motion synthesis. LBS informs the actuation topology, facilitates human expression retargeting, and enables efficient speech-driven facial motion generation. This approach demonstrates the capability to produce highly realistic facial expressions on an animatronic face in real-time at over 4000 fps on a single Nvidia RTX 4090, significantly advancing robots' ability to replicate nuanced human expressions for natural interaction. To foster further research and development in this field, the code has been made publicly available at: url{https://github.com/library87/OpenRoboExp}.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t3_02">
             09:00-10:00, Paper ThPI4T3.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1738'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Pseudo-Domain Adversarial Networks with Electrical Impedance Tomography for Electrode Offset Error
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396568" title="Click to go to the Author Index">
             Xu, Gengchen
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#285048" title="Click to go to the Author Index">
             Chen, Haofeng
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#371376" title="Click to go to the Author Index">
             Yang, Xuanxuan
            </a>
           </td>
           <td class="r">
            Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#285047" title="Click to go to the Author Index">
             Ma, Gang
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#188717" title="Click to go to the Author Index">
             Wang, Xiaojie
            </a>
           </td>
           <td class="r">
            Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1738" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#gesture__posture_and_facial_expressions" title="Click to go to the Keyword Index">
               Gesture, Posture and Facial Expressions
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#transfer_learning" title="Click to go to the Keyword Index">
               Transfer Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#wearable_robotics" title="Click to go to the Keyword Index">
               Wearable Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper propose a novel transfer learning approach, Pseudo-Domain Adversarial Network (PDAN), to tackle the issue of electrode displacement in Electrical Impedance Tomography (EIT). Electrode displacement, caused by human movement or improper operation, significantly affects the accuracy of EIT by introducing data errors. Existing solutions either modify the electrode assembly at a high cost or employ recognition algorithms that require retraining from scratch. To overcome these limitations, our work leverages the power of transfer learning to enhance model performance in the target domain by utilizing knowledge from a related task in the source domain. PDAN extends the capabilities of deep adversarial learning by incorporating noisy images to simulate post-electrode rotation scenarios, aiding in the reduction of negative impacts caused by minor electrode displacements. Our method demonstrates superior performance in classifying leg posture data, achieving around 90% accuracy, and proving robust against sensor electrode offset. Experimental results across various datasets validate the effectiveness of PDAN, indicating its potential in addressing complex real-world situations with improved generalization capabilities.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t3_03">
             09:00-10:00, Paper ThPI4T3.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2718'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Voxel-Enabled Robotic Assistant for Omnidirectional Conveyance
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#358316" title="Click to go to the Author Index">
             Carvajal, Michael Angelo
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#308101" title="Click to go to the Author Index">
             Mabulu, Katiso
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#378895" title="Click to go to the Author Index">
             Lalji, Muneer
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#378881" title="Click to go to the Author Index">
             Flanagan, James
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#355694" title="Click to go to the Author Index">
             Hibbard, Sam
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#246859" title="Click to go to the Author Index">
             Luo, Rui
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#378999" title="Click to go to the Author Index">
             Chinthapatla, Tanav
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#379011" title="Click to go to the Author Index">
             Bettadpur, Rohan
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#164870" title="Click to go to the Author Index">
             Bazzi, Salah
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#226883" title="Click to go to the Author Index">
             Zolotas, Mark
            </a>
           </td>
           <td class="r">
            Toyota Research Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#328026" title="Click to go to the Author Index">
             Kloeckl, Kristian
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#129456" title="Click to go to the Author Index">
             Padir, Taskin
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2718" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#human_centered_automation" title="Click to go to the Keyword Index">
               Human-Centered Automation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#human_centered_robotics" title="Click to go to the Keyword Index">
               Human-Centered Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#engineering_for_robotic_systems" title="Click to go to the Keyword Index">
               Engineering for Robotic Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Conventional bidirectional conveyance platforms use a flat translating belt or a series of spinning wheels or rollers to apply a shear force to payloads to move them. Wheel/roller-based conveyors in particular cannot double as a worktop when idle, do not support collision-free multi-object manipulation by default, and are not optimized to move objects that are either slippery or pliable--let alone both. This paper introduces a Voxel-Enabled Robotic Assistant (VERA), a network of intelligent table ``partitions'' whose topologically dynamic worktops enable omnidirectional conveyance; each partition is composed of a 2D array of ``quadrants,'' axisymmetric modules that can be hot-swapped for maintenance or repairs; each quadrant contains a 2D array of ``cells,'' unitary robotic submodules; each cell houses an independently controllable ``voxel,'' the motorized rotary element that conveys an overhead object. The efficacy of a VERA prototype was determined by evaluating waypoint error as a range of payloads were maneuvered between trajectory waypoints. By conveying both pliable and rigid payloads having slippery textures, the faceted voxels outperformed those with augmented profiles to mimic the circular wheels/rollers of competitor systems. VERA also successfully performed collision-free multi-object planar manipulations planned by its pathfinding algorithm. In light of these results, VERA emerges as a promising material handling platform for use in ``Future of Work'' settings as the need for versatile, multi-purpose, industrial robots continues to grow.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t3_04">
             09:00-10:00, Paper ThPI4T3.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('464'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              RobotGraffiti: An AR Tool for Semi-Automated Construction of Workcell Models to Optimize Robot Deployment
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#257450" title="Click to go to the Author Index">
             Zielinski, Krzysztof
            </a>
           </td>
           <td class="r">
            University of Southern Denmark / Universal Robots A/S
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#133024" title="Click to go to the Author Index">
             Penning, Ryan
            </a>
           </td>
           <td class="r">
            University of Wisconsin-Madison
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#378908" title="Click to go to the Author Index">
             Blumberg, Bruce
            </a>
           </td>
           <td class="r">
            Universal Robots A/S
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#109958" title="Click to go to the Author Index">
             Schlette, Christian
            </a>
           </td>
           <td class="r">
            University of Southern Denmark (SDU)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#296610" title="Click to go to the Author Index">
             Mikkel, Kjrgaard
            </a>
           </td>
           <td class="r">
            University of Southern Denmark
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab464" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#human_centered_robotics" title="Click to go to the Keyword Index">
               Human-Centered Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#virtual_reality_and_interfaces" title="Click to go to the Keyword Index">
               Virtual Reality and Interfaces
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#industrial_robots" title="Click to go to the Keyword Index">
               Industrial Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Improving robot deployment is a central step towards speeding up robot-based automation in manufacturing. A main challenge in robot deployment is how to best place the robot within the workcell. To tackle this challenge, we combine two knowledge sources: robotic knowledge of the system and workcell context awareness of the user, and intersect them with an Augmented Reality interface. RobotGraffiti is a unique tool that empowers the user in robot deployment tasks. One simply takes a 3D scan of the workcell with their mobile device, adds contextual data points that otherwise would be difficult to infer from the system, and receives a robot base position that satisfies the automation task. The proposed approach is an alternative to expensive and time-consuming digital twins, with a fast and easy-to-use tool that focuses on selected workcell features needed to run the placement optimization algorithm. The main contributions of this paper are the novel user interface for robot base placement data collection and a study comparing the traditional offline simulation with our proposed method. We showcase the method with a robot base placement solution and obtain up to 16 times reduction in time.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t3_05">
             09:00-10:00, Paper ThPI4T3.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1424'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Bridging the Gap to Natural Language-Based Grasp Predictions through Semantic Information Extraction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#319314" title="Click to go to the Author Index">
             Kleer, Niko
            </a>
           </td>
           <td class="r">
            DFKI, Saarland Informatics Campus
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#319402" title="Click to go to the Author Index">
             Feick, Martin
            </a>
           </td>
           <td class="r">
            DFKI, Saarland Informatics Campus
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#373656" title="Click to go to the Author Index">
             Gomaa, Amr
            </a>
           </td>
           <td class="r">
            DFKI, Saarland Informatics Campus
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#319400" title="Click to go to the Author Index">
             Feld, Michael
            </a>
           </td>
           <td class="r">
            German Research Center for Artificial Intelligence (DFKI), Saarb
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#373699" title="Click to go to the Author Index">
             Krger, Antonio
            </a>
           </td>
           <td class="r">
            DFKI, Saarland Informatics Campus
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1424" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#human_centered_robotics" title="Click to go to the Keyword Index">
               Human-Centered Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#multifingered_hands" title="Click to go to the Keyword Index">
               Multifingered Hands
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#natural_dialog_for_hri" title="Click to go to the Keyword Index">
               Natural Dialog for HRI
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Enabling multi-fingered robots to choose an appropriate grasp on an object from natural language instructions poses great difficulties for such systems. The diversity, imprecision, and limited information contained in the language make this task particularly challenging. However, speech serves humans as a natural communication interface that can aid robots in adapting to the environment more easily. Therefore, providing robots with relevant data about the objects they interact with is essential for them to understand how to carry out object manipulation tasks. By leveraging Named Entity Recognition (NER) to automatically extract semantic data, our work introduces a novel approach to text-based grasp predictions. Our methodology involves a multistage learning approach using a semantic information extractor that provides significant features to a grasp prediction model. To assess the effectiveness of our approach, we conducted experiments on an existing corpus and two corpora generated by ChatGPT. Our results demonstrate superior performance compared to similar grasp prediction models while overcoming limitations in the literature. Additionally, we open-source our training data for reproducibility and future research advancement.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t3_06">
             09:00-10:00, Paper ThPI4T3.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2333'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              REPeat: A Real2Sim2Real Approach for Pre-Acquisition of Soft Food Items in Robot-Assisted Feeding
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398700" title="Click to go to the Author Index">
             Ha, Nayoung
            </a>
           </td>
           <td class="r">
            Cornell University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#308838" title="Click to go to the Author Index">
             Ye, Ruolin
            </a>
           </td>
           <td class="r">
            Cornell University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#256550" title="Click to go to the Author Index">
             Liu, Ziang
            </a>
           </td>
           <td class="r">
            Cornell University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398282" title="Click to go to the Author Index">
             Sinha, Shubhangi
            </a>
           </td>
           <td class="r">
            Cornell University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#122308" title="Click to go to the Author Index">
             Bhattacharjee, Tapomayukh
            </a>
           </td>
           <td class="r">
            Cornell University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2333" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#human_centered_robotics" title="Click to go to the Keyword Index">
               Human-Centered Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The paper presents REPeat, a Real2Sim2Real framework designed to enhance bite acquisition in robot-assisted feeding for soft foods. It uses `pre-acquisition actions' such as pushing, cutting, and flipping to improve the success rate of bite acquisition actions such as skewering, scooping, and twirling. If the data-driven model predicts low success for direct bite acquisition, the system initiates a Real2Sim phase, reconstructing the food's geometry in a simulation. The robot explores various pre-acquisition actions in the simulation, then a Sim2Real step renders a photorealistic image to reassess success rates. If the success improves, the robot applies the action in the real world. We evaluate the system on 15 diverse plates with 10 types of food items for a soft food diet, showing improvement in bite-acquisition success rates by 27% on average across all plates. See our project website at https://emprise.cs.cornell.edu/repeat.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t3_07">
             09:00-10:00, Paper ThPI4T3.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('494'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              DiaGBT: An Explainable and Evolvable Robot Control Framework Using Dialogue Generative Behavior Trees
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#374464" title="Click to go to the Author Index">
             Liang, Jinde
            </a>
           </td>
           <td class="r">
            University of Electronic Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#254521" title="Click to go to the Author Index">
             Chang, Yuan
            </a>
           </td>
           <td class="r">
            National University of Defense Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#377187" title="Click to go to the Author Index">
             Wang, Qian
            </a>
           </td>
           <td class="r">
            National University of Defense Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#226691" title="Click to go to the Author Index">
             Wang, Yanzhen
            </a>
           </td>
           <td class="r">
            School of Computer, National University of Defense Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#232939" title="Click to go to the Author Index">
             Yi, Xiaodong
            </a>
           </td>
           <td class="r">
            National Innovation Institute of Defense Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab494" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#natural_dialog_for_hri" title="Click to go to the Keyword Index">
               Natural Dialog for HRI
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#software_architecture_for_robotic_and_automation" title="Click to go to the Keyword Index">
               Software Architecture for Robotic and Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Manipulating robots using natural language is the preferred way for non-technical specialists. The challenge lies in reliability and adaptability especially when robots operate in unstructured surroundings. In this paper, we propose a novel framework called Dialogue Generative Behavior Trees (DiaGBT). Natural language instructions from human operators are transformed into behavior trees (BTs) and further executed by robots. Compared to the emerging Large Language Models (LLMs), DiaGBT is comparable in terms of semantic understanding but more lightweight, since the parsing rules are produced by LLM but tailored for task-correlated instructions. Besides, DiaGBT allows multi-round human-robot interaction, where robots learn reusable skills in real time. For evaluation, we generate a dataset with 4k instruction-BT pairs covering 4 different scenarios. On average, DiaGBT reaches over 90% parsability and 80% plausibility. Similar results on the VEIL-500 dataset outperform the current state of the art.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t3_08">
             09:00-10:00, Paper ThPI4T3.8
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1096'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              RADAR: Robotics Assembly by Demonstration Via Augmented Reality
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#372456" title="Click to go to the Author Index">
             Yang, Wenhao
            </a>
           </td>
           <td class="r">
            Lamar University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#148801" title="Click to go to the Author Index">
             Bai, Shi
            </a>
           </td>
           <td class="r">
            IServe Robotics
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#372922" title="Click to go to the Author Index">
             Zhang, Yunbo
            </a>
           </td>
           <td class="r">
            Rochester Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1096" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#virtual_reality_and_interfaces" title="Click to go to the Keyword Index">
               Virtual Reality and Interfaces
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#manufacturing__maintenance_and_supply_chains" title="Click to go to the Keyword Index">
               Manufacturing, Maintenance and Supply Chains
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             With the widespread adoption of robots in high-mix, low-volume manufacturing, and the challenges posed by long-horizon assembly tasks, we introduce the RADAR systeman integrated human-robot collaboration system for Robotic Assembly by Demonstration via Augmented Reality. Existing frameworks lack a comprehensive, cross-task framework for effective assembly collaboration, limiting their applicability in complex tasks. We designed the RADAR systems conceptual model, detailing its workflow and components. The system integrates human input into robotic metal beam assembly through augmented reality interactions and interfaces. We also developed a task planner that dynamically adjusts human-robot assembly tasks at coarse-fine resolutions. Validating through practical scenarios, particularly the RAMP assembly benchmark, showed that human involvement significantly enhances assembly precision and success rates, proving RADARs effectiveness and efficiency in human-robot collaborative assembly.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t3_09">
             09:00-10:00, Paper ThPI4T3.9
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1370'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Collaborative Conversation in Safe Multimodal Human-Robot Collaboration
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#299387" title="Click to go to the Author Index">
             Ferrari, Davide
            </a>
           </td>
           <td class="r">
            University of Modena and Reggio Emilia
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#277746" title="Click to go to the Author Index">
             Pupa, Andrea
            </a>
           </td>
           <td class="r">
            University of Modena and Reggio Emilia
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#110186" title="Click to go to the Author Index">
             Secchi, Cristian
            </a>
           </td>
           <td class="r">
            Univ. of Modena &amp; Reggio Emilia
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1370" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#natural_dialog_for_hri" title="Click to go to the Keyword Index">
               Natural Dialog for HRI
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#safety_in_hri" title="Click to go to the Keyword Index">
               Safety in HRI
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In the context of Human-Robot Collaboration (HRC), it is crucial that the two actors are able to communicate with each other in a natural and efficient manner. The absence of a communication interface is often a cause of undesired slowdowns. On one hand, this is because unforeseen events may occur, leading to errors. On the other hand, due to the close contact between humans and robots, the speed must be reduced significantly to comply with safety standard ISO/TS 15066. In this paper, we propose a novel architecture that enables operators and robots to communicate efficiently, emulating human-to-human dialogue, while addressing safety concerns. This approach aims to establish a communication framework that not only facilitates collaboration but also reduces undesired speed reduction. Through the use of a predictive simulator, we can anticipate safety-related limitations, ensuring smoother workflows, minimizing risks, and optimizing efficiency. The overall architecture has been validated with a UR10e and compared with a state of the art technique. The results show a significant improvement in user experience, with a corresponding 23% reduction in execution times and a 50% decrease in robot downtime.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t3_10">
             09:00-10:00, Paper ThPI4T3.10
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2229'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Visual Attention Based Cognitive HumanRobot Collaboration for Pedicle Screw Placement in Robot-Assisted Orthopedic Surgery
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#312191" title="Click to go to the Author Index">
             Chen, Chen
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392687" title="Click to go to the Author Index">
             Zou, Qikai
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392311" title="Click to go to the Author Index">
             Song, Yuhang
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#297309" title="Click to go to the Author Index">
             Yu, Mingrui
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#134442" title="Click to go to the Author Index">
             Zhu, Senqiang
            </a>
           </td>
           <td class="r">
            Midea Group
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#244251" title="Click to go to the Author Index">
             Song, Shiji
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#132634" title="Click to go to the Author Index">
             Li, Xiang
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2229" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#physical_human_robot_interaction" title="Click to go to the Keyword Index">
               Physical Human-Robot Interaction
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Current orthopedic robotic systems largely focus on navigation, aiding surgeons in positioning a guiding tube but still requiring manual drilling and screw placement. The automation of this task not only demands high precision and safety due to the intricate physical interactions between the surgical tool and bone but also poses significant risks when executed without adequate human oversight. As it involves continuous physical interaction, the robot should collaborate with the surgeon, understand the human intent, and always include the surgeon in the loop. To achieve this, this paper proposes a new cognitive humanrobot collaboration framework, including the intuitive AR-haptic humanrobot interface, the visual-attention-based surgeon model, and the shared interaction control scheme for the robot. User studies on a robotic platform for orthopedic surgery are presented to illustrate the performance of the proposed method. The results demonstrate that the proposed humanrobot collaboration framework outperforms full robot and full human control in terms of safety and ergonomics.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t3_11">
             09:00-10:00, Paper ThPI4T3.11
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2318'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              DECAF: A Discrete-Event Based Collaborative Human-Robot Framework for Furniture Assembly
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#356043" title="Click to go to the Author Index">
             Giacomuzzo, Giulio
            </a>
           </td>
           <td class="r">
            University of Padova
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#224747" title="Click to go to the Author Index">
             Terreran, Matteo
            </a>
           </td>
           <td class="r">
            University of Padova
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#135389" title="Click to go to the Author Index">
             Jain, Siddarth
            </a>
           </td>
           <td class="r">
            Mitsubishi Electric Research Laboratories (MERL)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#188794" title="Click to go to the Author Index">
             Romeres, Diego
            </a>
           </td>
           <td class="r">
            Mitsubishi Electric Research Laboratories
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2318" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#human_robot_teaming" title="Click to go to the Keyword Index">
               Human-Robot Teaming
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#planning__scheduling_and_coordination" title="Click to go to the Keyword Index">
               Planning, Scheduling and Coordination
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper proposes a task planning framework for collaborative Human-Robot scenarios, specifically focused on assembling complex systems such as furniture. The human is characterized as an uncontrollable agent, implying for example that the agent is not bound by a pre-established sequence of actions and instead acts according to its own preferences. Meanwhile, the task planner computes reactively the optimal actions for the collaborative robot to efficiently complete the entire assembly task in the least time possible. We formalize the problem as a Discrete Event Markov Decision Problem (DE-MDP), a comprehensive framework that incorporates a variety of asynchronous behaviors, human change of mind, and failure recovery as stochastic events. Although the problem could theoretically be addressed by constructing a graph of all possible actions, such an approach would be constrained by computational limitations. The proposed formulation offers an alternative solution utilizing Reinforcement Learning to derive an optimal policy for the robot. Experiments were conducted both in simulation and on a real system with human subjects assembling a chair in collaboration with a 7-DoF manipulator.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t3_12">
             09:00-10:00, Paper ThPI4T3.12
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2706'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Open Human-Robot Collaboration Using Decentralized Inverse Reinforcement Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#251614" title="Click to go to the Author Index">
             Sengadu Suresh, Prasanth
            </a>
           </td>
           <td class="r">
            University of Georgia
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#135389" title="Click to go to the Author Index">
             Jain, Siddarth
            </a>
           </td>
           <td class="r">
            Mitsubishi Electric Research Laboratories (MERL)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#147846" title="Click to go to the Author Index">
             Doshi, Prashant
            </a>
           </td>
           <td class="r">
            University of Georgia
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#188794" title="Click to go to the Author Index">
             Romeres, Diego
            </a>
           </td>
           <td class="r">
            Mitsubishi Electric Research Laboratories
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2706" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#human_robot_teaming" title="Click to go to the Keyword Index">
               Human-Robot Teaming
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#factory_automation" title="Click to go to the Keyword Index">
               Factory Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The growing interest in human-robot collabora- tion (HRC), where humans and robots cooperate towards shared goals, has seen significant advancements over the past decade. While previous research has addressed various challenges, several key issues remain unresolved. Many domains within HRC involve activities that do not necessarily require human presence throughout the entire task. Existing literature typically models HRC as a closed system, where all agents are present for the entire duration of the task. In contrast, an open model offers flexibility by allowing an agent to enter and exit the collaboration as needed, enabling them to concurrently manage other tasks. In this paper, we introduce a novel multiagent framework called oDec-MDP, designed specifically to model open HRC scenarios where agents can join or leave tasks flexibly during execution. We generalize a recent multiagent inverse reinforcement learning method - Dec-AIRL to learn from open systems modeled using the oDec-MDP. Our method is validated through experiments conducted in both a simplified toy firefighting domain and a realistic dyadic human-robot collaborative assembly. Results show that our framework and learning method improves upon its closed system counterpart.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t3_13">
             09:00-10:00, Paper ThPI4T3.13
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3139'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              GOMA: Proactive Embodied Cooperative Communication Via Goal-Oriented Mental Alignment
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395487" title="Click to go to the Author Index">
             Ying, Lance
            </a>
           </td>
           <td class="r">
            Harvard University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395857" title="Click to go to the Author Index">
             Jha, Kunal
            </a>
           </td>
           <td class="r">
            Dartmouth College
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#378685" title="Click to go to the Author Index">
             Aarya, Shivam
            </a>
           </td>
           <td class="r">
            Johns Hopkins University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#202511" title="Click to go to the Author Index">
             Tenenbaum, Joshua
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#124048" title="Click to go to the Author Index">
             Torralba, Antonio
            </a>
           </td>
           <td class="r">
            MIT
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#204417" title="Click to go to the Author Index">
             Shu, Tianmin
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3139" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#human_robot_teaming" title="Click to go to the Keyword Index">
               Human-Robot Teaming
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#cooperating_robots" title="Click to go to the Keyword Index">
               Cooperating Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Verbal communication plays a crucial role in human cooperation, particularly when the partners only have incomplete information about the task, environment, and each other's mental state. In this paper, we propose a novel cooperative communication framework, Goal-Oriented Mental Alignment (GOMA). GOMA formulates verbal communication as a planning problem that minimizes the misalignment between the parts of agents' mental states that are relevant to the goals. This approach enables a robot to reason about when and how to proactively initialize communication with humans verbally using natural language to help achieve better cooperation. We evaluate our approach against strong baselines in two challenging environments, Overcooked (a multiplayer game) and VirtualHome (a household simulator). Our experimental results demonstrate that large language models struggle with generating meaningful communication that is grounded in the social and physical context. In contrast, our approach can successfully generate concise verbal communication for the robot to effectively boost the performance of the cooperation as well as human users' perception of the robot.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t3_14">
             09:00-10:00, Paper ThPI4T3.14
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2517'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SiSCo: Signal Synthesis for Effective Human-Robot Communication Via Large Language Models
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#273794" title="Click to go to the Author Index">
             Sonawani, Shubham
            </a>
           </td>
           <td class="r">
            Arizona State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#353432" title="Click to go to the Author Index">
             Weigend, Fabian Clemens
            </a>
           </td>
           <td class="r">
            Arizona State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#130054" title="Click to go to the Author Index">
             Ben Amor, Heni
            </a>
           </td>
           <td class="r">
            Arizona State University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2517" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#human_robot_teaming" title="Click to go to the Keyword Index">
               Human-Robot Teaming
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#virtual_reality_and_interfaces" title="Click to go to the Keyword Index">
               Virtual Reality and Interfaces
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Effective human-robot collaboration hinges on robust communication channels, with visual signaling playing a pivotal role due to its intuitive appeal. Yet, the creation of visually intuitive cues often demands extensive resources and specialized knowledge. The emergence of Large Language Models (LLMs) offers promising avenues for enhancing human-robot interactions and revolutionizing the way we generate context-aware visual cues. To this end, we introduce SiSCo--a novel framework that combines the computational power of LLMs with mixed-reality technologies to streamline the creation of visual cues for human-robot collaboration. Our results show that SiSCo improves the efficiency of communication in human-robot teaming tasks, reducing task completion time by approximately 73% and increasing task success rates by 18% compared to baseline natural language signals. Additionally, SiSCo reduces cognitive load for participants by 46%, as measured by the NASA-TLX subscale, and receives above-average user ratings for on-the-fly signals generated for unseen objects. To encourage further development and broader community engagement, we provide full access to SiSCo's implementation and related materials on our GitHub repository.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t3_15">
             09:00-10:00, Paper ThPI4T3.15
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3449'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Inferring Belief States in Partially-Observable Human-Robot Teams
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#303196" title="Click to go to the Author Index">
             Kolb, Jack
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#197904" title="Click to go to the Author Index">
             Feigh, Karen
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3449" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#human_robot_teaming" title="Click to go to the Keyword Index">
               Human-Robot Teaming
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We investigate the real-time estimation of human situation awareness using observations from a robot teammate with limited visibility. In human factors and human-autonomy teaming, it is recognized that individuals navigate their environments using an internal mental simulation, or mental model. The mental model informs cognitive processes including situation awareness, contextual reasoning, and task planning. In teaming domains, the mental model includes a team model of each teammate's beliefs and capabilities, enabling fluent teamwork without the need for explicit communication. However, little work has applied team models to human-robot teaming. In this work we compare the performance of two models, logical predicates and large language models, at estimating user situation awareness over varying visibility conditions. Our results indicate that the methods are largely resilient to low-visibility conditions in our domain, however opportunities exist to improve their overall performance.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t3_16">
             09:00-10:00, Paper ThPI4T3.16
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2167'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Design and Development of a Work Cell with a One-Handed Soldering Tool for Enhanced Human-Robot Collaboration
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#359952" title="Click to go to the Author Index">
             Suppaadirek, Natchanon
            </a>
           </td>
           <td class="r">
            Kyushu Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397310" title="Click to go to the Author Index">
             Sonnic, Maximilien
            </a>
           </td>
           <td class="r">
            Kyutech
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397801" title="Click to go to the Author Index">
             Duran Jimenez, Raul Ariel
            </a>
           </td>
           <td class="r">
            Kyutech
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#106955" title="Click to go to the Author Index">
             Shibata, Tomohiro
            </a>
           </td>
           <td class="r">
            Kyushu Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2167" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#intention_recognition" title="Click to go to the Keyword Index">
               Intention Recognition
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#machine_learning_for_robot_control" title="Click to go to the Keyword Index">
               Machine Learning for Robot Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The challenge of human-robot collaboration, particularly in the context of enhancing the productivity of work processes, has been a pivotal area of research and development for many years. Despite significant advancements, there remains a substantial gap in the design of these systems to cater specifically to individuals with disabilities. This paper presents an innovative approach in assistive robotics, focusing on the development of a work cell designed to facilitate individuals with single-arm functionality. Through the integration of depth camera technology, machine learning algorithms, and Mediapipe human tracking, our system is capable of interpreting human intentions, thereby making interactions with robots more intuitive and effective. Central to our research is the design of a specialized workspace that assists in object handling and incorporates a fully functional One-Handed Soldering Tool, integrated within a robotic arm setup. This work cell is tailored for users with limited arm functionality, demonstrating the system's versatility and providing invaluable insights into the practical implementation of applied robotics to bridge the theoretical and practical aspects of assistive technology.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t3_17">
             09:00-10:00, Paper ThPI4T3.17
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2996'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              CoBOS: Constraint-Based Online Scheduler for Human-Robot Collaboration
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398479" title="Click to go to the Author Index">
             Ionova, Marina
            </a>
           </td>
           <td class="r">
            Czech Technical University in Prague
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#214555" title="Click to go to the Author Index">
             Behrens, Jan Kristof
            </a>
           </td>
           <td class="r">
            Czech Technical University in Prague, CIIRC
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2996" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#planning__scheduling_and_coordination" title="Click to go to the Keyword Index">
               Planning, Scheduling and Coordination
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#intelligent_and_flexible_manufacturing" title="Click to go to the Keyword Index">
               Intelligent and Flexible Manufacturing
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#human_factors_and_human_in_the_loop" title="Click to go to the Keyword Index">
               Human Factors and Human-in-the-Loop
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Assembly processes involving humans and robots are challenging scenarios because the individual activities and access to shared workspace have to be coordinated. Fixed robot programs leave no room to diverge from a fixed protocol. Working on such a process can be stressful for the user and lead to ineffective behavior or failure. We propose a novel approach of online constraint-based scheduling in a reactive execution control framework facilitating behavior trees called CoBOS. This allows the robot to adapt to uncertain events such as delayed activity completions and activity selection (by the human). The user will experience less stress as the robotic coworkers adapt their behavior to best complement the human-selected activities to complete the common task. In addition to the improved working conditions, our algorithm leads to increased efficiency, even in highly uncertain scenarios. We evaluate our algorithm using a probabilistic simulation study with 56000 experiments. We outperform all other compared methods by a margin of 4-10%. Initial real robot experiments using a Franka Emika Panda robot and human tracking based on HTC Vive VR gloves look promising.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thpi4t4">
             <b>
              ThPI4T4
             </b>
            </a>
           </td>
           <td class="r">
            Room 4
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thpi4t4" title="Click to go to the Program at a Glance">
             <b>
              Robot Vision II
             </b>
            </a>
           </td>
           <td class="r">
            Teaser Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#214687" title="Click to go to the Author Index">
             Zeng, Long
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#112375" title="Click to go to the Author Index">
             Oishi, Takeshi
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t4_01">
             09:00-10:00, Paper ThPI4T4.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3059'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Coarse-To-Fine Detection of Multiple Seams for Robotic Welding
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396982" title="Click to go to the Author Index">
             Wei, Pengkun
            </a>
           </td>
           <td class="r">
            Shandong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398341" title="Click to go to the Author Index">
             Cheng, Shuo
            </a>
           </td>
           <td class="r">
            Shandong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#384300" title="Click to go to the Author Index">
             Li, Dayou
            </a>
           </td>
           <td class="r">
            School of Control Science and Engineering, Shandong Universisty
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#268891" title="Click to go to the Author Index">
             Song, Ran
            </a>
           </td>
           <td class="r">
            Shandong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398456" title="Click to go to the Author Index">
             Zhang, Yipeng
            </a>
           </td>
           <td class="r">
            University of California Los Angeles
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#203913" title="Click to go to the Author Index">
             Zhang, Wei
            </a>
           </td>
           <td class="r">
            Shandong University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3059" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#factory_automation" title="Click to go to the Keyword Index">
               Factory Automation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Efficiently detecting target weld seams while ensuring sub-millimeter accuracy has always been an important challenge in autonomous welding, which has significant application in industrial practice. Previous works mostly focused on recognizing and localizing welding seams one by one, leading to inferior efficiency in modeling the workpiece. This paper proposes a novel framework capable of multiple weld seams extraction using both RGB images and 3D point clouds. The RGB image is used to obtain the region of interest by approximately localizing the weld seams, and the point cloud is used to achieve the fine-edge extraction of the weld seams within the region of interest using region growth. Our method is further accelerated by using a pre-trained deep learning model to ensure both efficiency and generalization ability. The performance of the proposed method has been comprehensively tested on various workpieces featuring both linear and curved weld seams and in physical experiment systems. The results showcase considerable potential for real-world industrial applications, emphasizing the method's efficiency and effectiveness.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t4_02">
             09:00-10:00, Paper ThPI4T4.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3217'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Finetuning Pre-Trained Model with Limited Data for LiDAR-Based 3D Object Detection by Bridging Domain Gaps
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#399008" title="Click to go to the Author Index">
             Jang, Jiyun
            </a>
           </td>
           <td class="r">
            Korea University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#351743" title="Click to go to the Author Index">
             Chang, Mincheol
            </a>
           </td>
           <td class="r">
            Korea University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#416658" title="Click to go to the Author Index">
             Park, Jongwon
            </a>
           </td>
           <td class="r">
            Hyundai Motor Company
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#266953" title="Click to go to the Author Index">
             Kim, Jinkyu
            </a>
           </td>
           <td class="r">
            Korea University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3217" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             LiDAR-based 3D object detectors have been largely utilized in various applications, including autonomous vehicles or mobile robots. However, LiDAR-based detectors often fail to adapt well to target domains with different sensor configurations (e.g., types of sensors, spatial resolution, or FOVs) and location shifts. Collecting and annotating datasets in a new setup is commonly required to reduce such gaps, but it is often expensive and time-consuming. Recent studies suggest that pre-trained backbones can be learned in a self-supervised manner with large-scale unlabeled LiDAR frames. However, despite their expressive representations, they remain challenging to generalize well without substantial amounts of data from the target domain. Thus, we propose a novel method, called Domain Adaptive Distill-Tuning (DADT), to adapt a pre-trained model with limited target data (e.g., less than 100 LiDAR frames), retaining its representation power and preventing it from overfitting. Specifically, we use regularizers to align object-level and context-level representations between the pre-trained and finetuned models in a teacher-student architecture. Our experiments with driving benchmarks, i.e., Waymo Open dataset and KITTI, confirm that our method effectively finetunes a pre-trained model, achieving significant gains in accuracy.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t4_03">
             09:00-10:00, Paper ThPI4T4.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3271'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Active Neural Mapping at Scale
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#394694" title="Click to go to the Author Index">
             Kuang, Zijia
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#269669" title="Click to go to the Author Index">
             Yan, Zike
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#223795" title="Click to go to the Author Index">
             Zhao, Hao
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#165639" title="Click to go to the Author Index">
             Zhou, Guyue
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#103250" title="Click to go to the Author Index">
             Zha, Hongbin
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3271" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We introduce a NeRF-based active mapping system that enables efficient and robust exploration of large-scale indoor environments. The key to our approach is the extraction of a generalized Voronoi graph (GVG) from the continually updated neural map, leading to the synergistic integration of scene geometry, appearance, topology, and uncertainty. Anchoring uncertain areas induced by the neural map to the vertices of GVG allows the exploration to undergo adaptive granularity along a safe path that traverses unknown areas efficiently. Harnessing a modern hybrid NeRF representation, the proposed system achieves competitive results in terms of reconstruction accuracy, coverage completeness, and exploration efficiency even when scaling up to large indoor environments. Extensive results at different scales validate the efficacy of the proposed system.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t4_04">
             09:00-10:00, Paper ThPI4T4.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3321'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Self-Supervised Monocular Depth Estimation with Effective Feature Fusion and Self Distillation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392641" title="Click to go to the Author Index">
             Liu, ZhenFei
            </a>
           </td>
           <td class="r">
            Shenzhen Institute of Advanced Technology, Chinese Academy of Sc
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#285159" title="Click to go to the Author Index">
             Song, Chengqun
            </a>
           </td>
           <td class="r">
            Shenzhen Institutes of Advanced Technology, Chinese Academy of S
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#103711" title="Click to go to the Author Index">
             Cheng, Jun
            </a>
           </td>
           <td class="r">
            Shenzhen Institutes of Advanced Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392618" title="Click to go to the Author Index">
             Luo, Jiefu
            </a>
           </td>
           <td class="r">
            Shenzhen Institute of Advanced Technology, Chinese Academy of Sc
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#399035" title="Click to go to the Author Index">
             Wang, Xiaoyang
            </a>
           </td>
           <td class="r">
            Shenzhen Institute of Advanced Technology Chinese Academy of Sci
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3321" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#data_sets_for_robotic_vision" title="Click to go to the Keyword Index">
               Data Sets for Robotic Vision
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Monocular depth estimation obtaining scene depth information from a single image is an important task in the field of computer vision. Constrained by the limitations of convolutional networks in conducting long-distance modeling and the underutilization of datasets, the generalization of existing models is not satisfactory. In this paper, we propose an adaptive backbone named Internal Fusion Transformer to improve generalization ability compared to convolutional backbone, like HRNet, and a Bilateral Attention module which pays more attention to low-level semantic features compared to previous fuse methods. Meanwhile, we introduce three data augmentation methods, namely cropping-resizing (cr), cropping-shuffling (cs), and mirroring (mi), for self distillation, as well as discuss their contributions to model performance improvement. Our model is trained on the KITTI dataset, and without fine-tuning, tested on NYUv2 and Make3D datasets to confirm the generalization. The experimental results illustrate the effectiveness of our design. Our model also demonstrates better performance compared to other models on the KITTI dataset.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t4_05">
             09:00-10:00, Paper ThPI4T4.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             UW-SDF: Exploiting Hybrid Geometric Priors for Neural SDF Reconstruction from Underwater Multi-View Monocular Images
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#363466" title="Click to go to the Author Index">
             Chen, Zeyu
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#296219" title="Click to go to the Author Index">
             Tang, Jingyi
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#280334" title="Click to go to the Author Index">
             Wang, Gu
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#278878" title="Click to go to the Author Index">
             Li, Shengquan
            </a>
           </td>
           <td class="r">
            Pengcheng Lab
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#399079" title="Click to go to the Author Index">
             Li, Xinghui
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#268638" title="Click to go to the Author Index">
             Ji, Xiangyang
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#300070" title="Click to go to the Author Index">
             Li, Xiu
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t4_06">
             09:00-10:00, Paper ThPI4T4.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('211'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              PS6D: Point Cloud Based Symmetry-Aware 6D Object Pose Estimation in Robot Bin-Picking
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#390809" title="Click to go to the Author Index">
             Yang, Yifan
            </a>
           </td>
           <td class="r">
            Nankai University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#371750" title="Click to go to the Author Index">
             Cui, Zhihao
            </a>
           </td>
           <td class="r">
            Mech-Mind Robotics
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#280381" title="Click to go to the Author Index">
             Zhang, Qianyi
            </a>
           </td>
           <td class="r">
            Nankai University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#106992" title="Click to go to the Author Index">
             Liu, Jingtai
            </a>
           </td>
           <td class="r">
            Nankai University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab211" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_manufacturing" title="Click to go to the Keyword Index">
               Computer Vision for Manufacturing
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             6D object pose estimation holds essential roles in various fields, particularly in the grasping of industrial workpieces. Given challenges like rust, high reflectivity, and absent textures, this paper introduces a point cloud based pose estimation framework (PS6D). PS6D centers on slender and multi-symmetric objects. It extracts multi-scale features through an attention-guided feature extraction module, designs a symmetry-aware rotation loss and a center distance sensitive translation loss to regress the pose of each point to the centroid of the instance, and then uses a two-stage clustering method to complete instance segmentation and pose estimation. Objects from the Silane and IPA datasets and typical workpieces from industrial practice are used to generate data and evaluate the algorithm. In comparison to the state-of-the-art approach, PS6D demonstrates an 11.5% improvement in F_1_inst and a 14.8% improvement in Recall. The main part of PS6D has been deployed to the software of Mech-Mind, and achieves a 91.7% success rate in bin-picking experiments, marking its application in industrial pose estimation tasks.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t4_07">
             09:00-10:00, Paper ThPI4T4.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1352'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              DuCAS: A Knowledge-Enhanced Dual-Hand Compositional Action Segmentation Method for Human-Robot Collaborative Assembly
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#358300" title="Click to go to the Author Index">
             Zheng, Hao
            </a>
           </td>
           <td class="r">
            The University of Auckland
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#388776" title="Click to go to the Author Index">
             Lee, Regina
            </a>
           </td>
           <td class="r">
            The University of Auckland
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395994" title="Click to go to the Author Index">
             Liang, Huachang
            </a>
           </td>
           <td class="r">
            The University of Auckland
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#248554" title="Click to go to the Author Index">
             Lu, Yuqian
            </a>
           </td>
           <td class="r">
            The University of Auckland
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#116199" title="Click to go to the Author Index">
             Xu, Xun
            </a>
           </td>
           <td class="r">
            University of Auckland
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1352" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_manufacturing" title="Click to go to the Keyword Index">
               Computer Vision for Manufacturing
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#human_centered_automation" title="Click to go to the Keyword Index">
               Human-Centered Automation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#human_and_humanoid_motion_analysis_and_synthesis" title="Click to go to the Keyword Index">
               Human and Humanoid Motion Analysis and Synthesis
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Recognising and tracking human actions from videos is crucial for human-robot collaborative assembly (HRCA). However, traditional action segmentation methods suffer from limited scene adaptability, partly because they conceptualise actions as unified verb-object entities with complete semantics. To overcome this, we propose a compositional action segmentation method. Following the human-robot shared assembly taxonomy, we deconstruct an assembly action into four elements: action verb, manipulated object, target object and tool. Our approach employs individual segmentation models for each action element, and then integrates general knowledge from large language models and domain-specific knowledge from predefined rules to form semantic-complete actions. Our method's emphasis on general action elements and a modular design endows it with greater flexibility and adaptability than traditional approaches. Another attribute of our method is its capability to segment actions of each hand concurrently, facilitating more nuanced HRCA. Comparative experiments validate the superiority of our method over traditional action segmentation methods.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t4_08">
             09:00-10:00, Paper ThPI4T4.8
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1923'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              ParametricNet+: A 6DoF Pose Estimation Network with Sparse Keypoint Recovery for Parametric Shapes in Stacked Scenarios
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#330033" title="Click to go to the Author Index">
             Xie, Yihan
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#290792" title="Click to go to the Author Index">
             Lv, Weijie
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#290793" title="Click to go to the Author Index">
             Zhang, Xinyu
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#330038" title="Click to go to the Author Index">
             Chen, YiHong
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#214687" title="Click to go to the Author Index">
             Zeng, Long
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1923" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_manufacturing" title="Click to go to the Keyword Index">
               Computer Vision for Manufacturing
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Most industrial parts are designed from parametric shapes with the properties of diversity and uncertainty. We propose a 6DoF pose estimation network, ParametricNet++, based on pointwise regression and sparse keypoint recovery, which is extended from ParametricNet to include optimizations of keypoint selection and prediction. Keypoint selection optimization selects geometrically unique keypoints and keypoint groups to reduce the difficulty of scene keypoint prediction and template keypoint recovery. Keypoint prediction optimization predicts keypoints from rough to precise, which improves the accuracy of scene keypoint prediction and template keypoint recovery. Compared with other state-of-the-art methods, the average of APs of ParametricNet++ is improved by over 15% on the public Sil'eane dataset, and the average of mAPs is improved by 12% and 14% on L-dataset and G-dataset from Parametric dataset, respectively. In particular, ParametricNet++ outperforms our original ParametricNet by 5% for both learning and generalization ability evaluation on the Parametric dataset. The experimental results demonstrate that ParametricNet++ lays a solid foundation for robot grasping in industrial scenarios.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t4_09">
             09:00-10:00, Paper ThPI4T4.9
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2540'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Towards Intelligent Robotic Sole Deburring: From Burrs Identification to Path Planning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#309618" title="Click to go to the Author Index">
             Tafuro, Alessandra
            </a>
           </td>
           <td class="r">
            Politecnico Di MIlano
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398314" title="Click to go to the Author Index">
             Cacciani, Luigi
            </a>
           </td>
           <td class="r">
            Politecnico Di Milano
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#129139" title="Click to go to the Author Index">
             Zanchettin, Andrea Maria
            </a>
           </td>
           <td class="r">
            Politecnico Di Milano
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#10019" title="Click to go to the Author Index">
             Rocco, Paolo
            </a>
           </td>
           <td class="r">
            Politecnico Di Milano
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2540" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_manufacturing" title="Click to go to the Keyword Index">
               Computer Vision for Manufacturing
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Today, intelligent robotic manufacturing systems are reshaping the production industry. Using robots as actuators, multi-source sensors for perception, and Artificial Intelligence (AI) as decision-making systems, they can perform routine manufacturing tasks, surpassing the capabilities of traditional hard-programmed Computer Numerical Control (CNC) machinery. One specific challenge in footwear manufacturing is sole deburring, traditionally done manually by skilled workers. This paper focuses on developing a robust path planning pipeline, comprising vision-based and Learning from Demonstrations (LfD) modules for autonomous deburring of soles. The vision-based module exploits Deep Learning (DL) techniques to handle key challenges such as precise segmentation of different soles types across diverse scenarios despite potential occlusions. Additionally, a novel method for burrs identification has been developed leveraging image processing and optimization techniques. Determining the optimal cutting tool orientation during sole deburring relies on human experience. The LfD module aims to impart this knowledge to the robot from videos of expert demonstrations, requiring adaptability to every new incoming sole that needs deburring. Experimental results showcase the methods performance and flexibility, underlining the potential to advance the field of the proposed approach.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t4_10">
             09:00-10:00, Paper ThPI4T4.10
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3193'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              REF^2-NeRF: Reflection and Refraction Aware Neural Radiance Field
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395752" title="Click to go to the Author Index">
             Kim, Wooseok
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395755" title="Click to go to the Author Index">
             Fukiage, Taiki
            </a>
           </td>
           <td class="r">
            NTT Communication Science Laboratories
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#112375" title="Click to go to the Author Index">
             Oishi, Takeshi
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3193" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_manufacturing" title="Click to go to the Keyword Index">
               Computer Vision for Manufacturing
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#range_sensing" title="Click to go to the Keyword Index">
               Range Sensing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Recently, significant progress has been made in the study of methods for 3D reconstruction from multiple images using implicit neural representations, exemplified by the neural radiance field (NeRF) method. Such methods, which are based on volume rendering, can model various light phenomena, and various extended methods have been proposed to accommodate different scenes and situations. However, when handling scenes with multiple glass objects, e.g., objects in a glass showcase, modeling the target scene accurately has been challenging due to the presence of multiple reflection and refraction effects. Thus, this paper proposes a NeRF-based modeling method for scenes containing a glass case. In the proposed method, refraction and reflection are modeled using elements that are dependent and independent of the viewer's perspective. This approach allows us to estimate the surfaces where refraction occurs, i.e., glass surfaces, and enables the separation and modeling of both direct and reflected light components. The proposed method requires predetermined camera poses, but accurately estimating these poses in scenes with glass objects is difficult. Therefore, we used a robotic arm with an attached camera to acquire images with known poses. Compared to existing methods, the proposed method enables more accurate modeling of both glass refraction and the overall scene.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t4_11">
             09:00-10:00, Paper ThPI4T4.11
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('427'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Bidirectional Partial-To-Full Non-Rigid Point Set Registration with Non-Overlapping Filtering
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#391357" title="Click to go to the Author Index">
             Yu, Hao
            </a>
           </td>
           <td class="r">
            Shandong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#391757" title="Click to go to the Author Index">
             Liu, Mingyang
            </a>
           </td>
           <td class="r">
            Shandong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#225131" title="Click to go to the Author Index">
             Song, Rui
            </a>
           </td>
           <td class="r">
            Shandong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#163802" title="Click to go to the Author Index">
             Li, Yibin
            </a>
           </td>
           <td class="r">
            Shandong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#100160" title="Click to go to the Author Index">
             Meng, Max Q.-H.
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#206118" title="Click to go to the Author Index">
             Min, Zhe
            </a>
           </td>
           <td class="r">
            University College London
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab427" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_medical_robotics" title="Click to go to the Keyword Index">
               Computer Vision for Medical Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper, we introduce Bidirectional Non-Overlapping Filtering Network (Bi-NOFNet), which registers the partial intraoperative point set with full preoperative point set for computer-assisted interventions (CAI). Our contributions are three-folds. First, Bi-NOFNet adopts customised feature extractor to extract distinctive features from both point sets, with which the per-point overlap mask is predicted and the overlapping region is segmented for the preoperative point set. Furthermore, we propose two methods to filter out the non-overlapping regions, at feature-level (i.e., Bi-NOFNet(Feature)) and point-level (i.e., Bi-NOFNet (Point)). For these two methods, we develop supervised registration strategy where the ground-truth overlap mask and displacement vectors are employed, and weakly-supervised registration strategies where only the ground-truth overlap mask is available. Additionally, to fully utilise the information in both space, we propose a bidirectional registration mechanism, which predicts the displacement vectors associated with the intraoperative point set (i.e., the forward way) and those warpping the preoperative point set (i.e., the backward way). Experiments have been conducted on the proposed DeformMedShapeNet dataset that contains 615 different liver shapes. Extensive results demonstrate that Bi-NOFNet performs well for partial-to-full registration tasks under various scenarios of noise, overlap ratios and deformation levels, outperforming existing non-rigid registration approaches.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t4_12">
             09:00-10:00, Paper ThPI4T4.12
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1156'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Vertebrea-Based Global X-Ray to CT Registration for Thoracic Surgeries
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#290942" title="Click to go to the Author Index">
             Liu, Lilu
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#245555" title="Click to go to the Author Index">
             Jiao, Yanmei
            </a>
           </td>
           <td class="r">
            Hangzhou Normal University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#363145" title="Click to go to the Author Index">
             An, Zhou
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#305676" title="Click to go to the Author Index">
             Ma, Honghai
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#115010" title="Click to go to the Author Index">
             Zhou, Chunlin
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#184494" title="Click to go to the Author Index">
             Lu, Haojian
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#217288" title="Click to go to the Author Index">
             Hu, Jian
            </a>
           </td>
           <td class="r">
            The First Affiliated Hospital, College of Medicine, Zhejiang Uni
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#113216" title="Click to go to the Author Index">
             Xiong, Rong
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#156231" title="Click to go to the Author Index">
             Wang, Yue
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1156" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_medical_robotics" title="Click to go to the Keyword Index">
               Computer Vision for Medical Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             X-ray to CT registration is an essential technique to provide on-site guidance for clinicians and medical robots by aligning preoperative information with intraoperative images. Current methods focus on local registration with small capture ranges and necessitate a manual initial alignment before precise registration. Some existing global methods are likely to fail in thoracic surgeries because of the respiratory motion and the nearly colinear nature of vertebrae landmarks. In this study, we propose an vertebrae-based global X-ray to CT registration method with the assist of clinical setups for thoracic surgeries. Firstly, vertebrae centroids are automatically localized by CNN-based networks in CT and X-ray for establishing 2-D/3-D correspondences. Then, inspired by clinical setup, we address the degradation of colinear landmarks of 6-DoF pose estimation by introducing a 4-DoF solver. Considering the inaccurate priori and landmark mislocalization, the solver is embedded into the Adaptive Error-Aware Estimator (AE2) to simultaneously estimate weights and aggregate candidate poses. Finally, the whole method is trained in an end-to-end manner for better performance. Evaluations on both the public LIDC-IDRI dataset and clinical dataset demonstrate that our method outperforms existing optimization-based and learning-based approaches in terms of registration accuracy and success rate. Our code: https://github.com/LiuLiluZJU/2P-AE2
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t4_13">
             09:00-10:00, Paper ThPI4T4.13
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1368'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Shadow Maintenance for Automatic Light-Probe Control in Ophthalmic Surgeries Using Only 2D Information
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#310783" title="Click to go to the Author Index">
             Yang, Junjie
            </a>
           </td>
           <td class="r">
            TUM
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#325327" title="Click to go to the Author Index">
             Inagaki, Satoshi
            </a>
           </td>
           <td class="r">
            NSK.Ltd
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#368476" title="Click to go to the Author Index">
             Zhao, Zhihao
            </a>
           </td>
           <td class="r">
            Technische Universitt Mnchen
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#160087" title="Click to go to the Author Index">
             Zapp, Daniel
            </a>
           </td>
           <td class="r">
            Klinikum Rechts Der Isar Der TU Mnchen
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#160084" title="Click to go to the Author Index">
             Maier, Mathias
            </a>
           </td>
           <td class="r">
            Klinikum Rechts Der Isar Der TU Mnchen
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#193739" title="Click to go to the Author Index">
             Huang, Kai
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#107647" title="Click to go to the Author Index">
             Navab, Nassir
            </a>
           </td>
           <td class="r">
            TU Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#160079" title="Click to go to the Author Index">
             Nasseri, M. Ali
            </a>
           </td>
           <td class="r">
            Technische Universitaet Muenchen
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1368" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_medical_robotics" title="Click to go to the Keyword Index">
               Computer Vision for Medical Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In ophthalmic surgeries, the light probe is responsible for providing safe intraocular illumination and ensuring the visibility of the instrument and its shadow as the only available reference for qualitative depth estimation and landing point prediction in fundus microscopic images. To achieve sustainable shadow-based estimation during surgeries, we propose controlling the light probe automatically to limit the shadow position around the instrument tip using only 2D information from the microscope. We also integrate an intensity balancing sub-module to guarantee the normal intensity distribution and the safe depth of light-tip placement. Without motor-based pose coordination between the light probe and the instrument, experiments analyze the performance of our image-based shadow maintenance with only image information under the constraints of RCM and discuss the working-volume limitation using simulation and real robots.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t4_14">
             09:00-10:00, Paper ThPI4T4.14
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2190'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Tracking Tumors under Deformation from Partial Point Clouds Using Occupancy Networks
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395995" title="Click to go to the Author Index">
             Henrich, Pit
            </a>
           </td>
           <td class="r">
            FAU Erlangen-Nrnberg, Germany
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#355058" title="Click to go to the Author Index">
             Liu, Jiawei
            </a>
           </td>
           <td class="r">
            Johns Hopkins University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#240309" title="Click to go to the Author Index">
             Ge, Jiawei
            </a>
           </td>
           <td class="r">
            Johns Hopkins University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#375819" title="Click to go to the Author Index">
             Schmidgall, Samuel
            </a>
           </td>
           <td class="r">
            Johns Hopkins University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396855" title="Click to go to the Author Index">
             Shepard, Lauren
            </a>
           </td>
           <td class="r">
            Department of Urology, Johns Hopkins University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#345769" title="Click to go to the Author Index">
             Ghazi, Ahmed
            </a>
           </td>
           <td class="r">
            University of Rochester Medical Center
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#141354" title="Click to go to the Author Index">
             Mathis-Ullrich, Franziska
            </a>
           </td>
           <td class="r">
            Friedrich-Alexander-University Erlangen-Nurnberg (FAU)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#119227" title="Click to go to the Author Index">
             Krieger, Axel
            </a>
           </td>
           <td class="r">
            Johns Hopkins University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2190" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_medical_robotics" title="Click to go to the Keyword Index">
               Computer Vision for Medical Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             To track tumors during surgery, information from preoperative CT scans is used to determine their position. However, as the surgeon operates, the tumor may be deformed which presents a major hurdle for accurately resecting the tumor, and can lead to surgical inaccuracy, increased operation time, and excessive margins. This issue is particularly pronounced in robot-assisted partial nephrectomy (RAPN), where the kidney undergoes significant deformations during operation. Toward addressing this, we introduce a occupancy network-based method for the localization of tumors within kidney phantoms undergoing deformations at interactive speeds. We validate our method by introducing a 3D hydrogel kidney phantom embedded with exophytic and endophytic renal tumors. It closely mimics real tissue mechanics to simulate kidney deformation during in vivo surgery, providing excellent contrast and clear delineation of tumor margins to enable automatic threshold-based segmentation. Our findings indicate that the proposed method can localize tumors in moderately deforming kidneys with a margin of 6mm to 10mm, while providing essential volumetric 3D information at over 60Hz. This capability directly enables downstream tasks such as robotic resection.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t4_15">
             09:00-10:00, Paper ThPI4T4.15
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3134'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              OBHMR: Robust Partial-To-Full Generalized Point Set Registration with Overlap-Guided Bidirectional Hybrid Mixture Model
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#393596" title="Click to go to the Author Index">
             Du, Xinzhe
            </a>
           </td>
           <td class="r">
            Shandong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#305117" title="Click to go to the Author Index">
             Zhang, Zhengyan
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology, Shenzhen
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#245651" title="Click to go to the Author Index">
             Zhang, Ang
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#225131" title="Click to go to the Author Index">
             Song, Rui
            </a>
           </td>
           <td class="r">
            Shandong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#163802" title="Click to go to the Author Index">
             Li, Yibin
            </a>
           </td>
           <td class="r">
            Shandong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#100160" title="Click to go to the Author Index">
             Meng, Max Q.-H.
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#206118" title="Click to go to the Author Index">
             Min, Zhe
            </a>
           </td>
           <td class="r">
            University College London
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3134" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_medical_robotics" title="Click to go to the Keyword Index">
               Computer Vision for Medical Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#probability_and_statistical_methods" title="Click to go to the Keyword Index">
               Probability and Statistical Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper, we introduce a novel overlap-based bidirectional point set registration approach, i.e., Overlap Bidirectional Hybrid Mixture Registration (OBHMR), which incorporates geometric information (i.e., normal vectors) in both the correspondence and transformation stages and formulates the optimization objective of registration in a bidirectional manner. More importantly, to address the issue of partial-to-full registration, OBHMR utilises the predicted point-wise overlap score using networks to formulate the overlap-guided Hybrid Mixture Model consisting of the Gaussian Mixture Model (GMM) and Fisher Mixture Model (FMM). OBHMR contains four components: (1) the correspondence network that estimates the correspondence probabilities; (2) the overlap prediction network that calculates the point-wise overlap score; (3) the posterior module that estimates the overlap-guided HMM parameters; (4) the bidirectional transformation module that computes the rigid transformation by formulating the optimisation objective in a bidirectional registration way, given correspondences and overlap-guided HMM parameters. Experiments using 291 human femur and 260 human hip models demonstrate significant improvements in partial-to-full registration performance (p&lt;0.01) under different overlapping ratios, compared to state-of-the-art registration approaches. Furthermore, individual contributions of three modules (i.e., additional normal vectors, overlap score estimation module, and the bidirectional mechanism) in OBHMR have been validated in ablation studies. The results demonstrate OBHMR's capability of tackling the challenging partial-to-full registration problems in computer-assisted orthopedic surgery. The codes are available at https://github.com/Dxinz/DeepOBHMR.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t4_16">
             09:00-10:00, Paper ThPI4T4.16
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1943'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Visuo-Tactile Zero-Shot Object Recognition with Vision-Language Model
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#356711" title="Click to go to the Author Index">
             Ueda, Shiori
            </a>
           </td>
           <td class="r">
            Keio University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#311036" title="Click to go to the Author Index">
             Hashimoto, Atsushi
            </a>
           </td>
           <td class="r">
            Omron Sinic X
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#178939" title="Click to go to the Author Index">
             Hamaya, Masashi
            </a>
           </td>
           <td class="r">
            OMRON SINIC X Corporation
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#158337" title="Click to go to the Author Index">
             Tanaka, Kazutoshi
            </a>
           </td>
           <td class="r">
            OMRON SINIC X Corporation
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#107622" title="Click to go to the Author Index">
             Saito, Hideo
            </a>
           </td>
           <td class="r">
            Keio University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1943" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#recognition" title="Click to go to the Keyword Index">
               Recognition
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#force_and_tactile_sensing" title="Click to go to the Keyword Index">
               Force and Tactile Sensing
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#ai_based_methods" title="Click to go to the Keyword Index">
               AI-Based Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Tactile perception is vital, especially when distinguishing visually similar objects. We propose an approach to incorporate tactile data into a Vision-Language Model (VLM) for visuo-tactile zero-shot object recognition. Our approach leverages the zero-shot capability of VLMs to infer tactile properties from the names of tactilely similar objects. The proposed method translates tactile data into a textual description solely by annotating object names for each tactile sequence during training, making it adaptable to various contexts with low training costs. The proposed method was evaluated on the FoodReplica and Cube datasets, demonstrating its effectiveness in recognizing objects that are difficult to distinguish by vision alone.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thpi4t5">
             <b>
              ThPI4T5
             </b>
            </a>
           </td>
           <td class="r">
            Room 5
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thpi4t5" title="Click to go to the Program at a Glance">
             <b>
              Deep Learning IV
             </b>
            </a>
           </td>
           <td class="r">
            Teaser Session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#102317" title="Click to go to the Author Index">
             Triebel, Rudolph
            </a>
           </td>
           <td class="r">
            German Aerospace Center (DLR)
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t5_01">
             09:00-10:00, Paper ThPI4T5.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('190'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Depth Helps: Improving Pre-Trained RGB-Based Policy with Depth Information Injection
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#376530" title="Click to go to the Author Index">
             Pang, Xincheng
            </a>
           </td>
           <td class="r">
            Renmin University of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#370939" title="Click to go to the Author Index">
             Xia, Wenke
            </a>
           </td>
           <td class="r">
            Renmin University of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#376465" title="Click to go to the Author Index">
             Wang, Zhigang
            </a>
           </td>
           <td class="r">
            Shanghai AI Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#372118" title="Click to go to the Author Index">
             Zhao, Bin
            </a>
           </td>
           <td class="r">
            Northwestern Polytechnical University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#376462" title="Click to go to the Author Index">
             Hu, Di
            </a>
           </td>
           <td class="r">
            Renmin University of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#345464" title="Click to go to the Author Index">
             Wang, Dong
            </a>
           </td>
           <td class="r">
            Shanghai Artificial Intelligence Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#372120" title="Click to go to the Author Index">
             Li, Xuelong
            </a>
           </td>
           <td class="r">
            Northwestern Polytechnical University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab190" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#ai_based_methods" title="Click to go to the Keyword Index">
               AI-Based Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             3D perception ability is crucial for generalizable robotic manipulation. While recent foundation models have made significant strides in perception and decision-making with RGB-based input, their lack of 3D perception limits their effectiveness in fine-grained robotic manipulation tasks. To address these limitations, we propose a Depth Information Injection
             <i>
              DI
             </i>
             <sup>
              2
             </sup>
             framework that leverages the RGB-Depth modality for policy fine-tuning, while relying solely on RGB images for robust and efficient deployment. Concretely, we introduce the Depth Completion Module (DCM) to extract the spatial prior knowledge related to depth information and generate virtual depth information from RGB inputs to aid policy deployment. Further, we propose the Depth-Aware Codebook (DAC) to eliminate noise and reduce the cumulative error from the depth prediction. In the inference phase, this framework employs RGB inputs and accurately predicted depth data to generate the manipulation action. We conduct experiments on simulated LIBERO environments and real-world scenarios, and the experiment results prove that our method could effectively enhance the pre-trained RGB-based policy with 3D perception ability for robotic manipulation. The website is released at https://gewu-lab.github.io/DepthHelps-IROS2024.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t5_02">
             09:00-10:00, Paper ThPI4T5.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('199'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              OPG-Policy: Occluded Push-Grasp Policy Learning with Amodal Segmentation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#376993" title="Click to go to the Author Index">
             Ding, Hao
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#376991" title="Click to go to the Author Index">
             Zeng, Yiming
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#313957" title="Click to go to the Author Index">
             Wan, Zhaoliang
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#169114" title="Click to go to the Author Index">
             Cheng, Hui
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab199" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#dexterous_manipulation" title="Click to go to the Keyword Index">
               Dexterous Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#grasping" title="Click to go to the Keyword Index">
               Grasping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Goal-oriented grasping in dense clutter, a fundamental challenge in robotics, demands an adaptive policy to handle occluded target objects and diverse configurations. Previous methods typically learn policies based on partially observable segments of the occluded target to generate motions. However, these policies often struggle to generate optimal motions due to uncertainties regarding the invisible portions of different occluded target objects across various scenes, resulting in low motion efficiency. To this end, we propose OPG-Policy, a novel framework that leverages amodal segmentation to predict occluded portions of the target and develop an adaptive push-grasp policy for cluttered scenarios where the target object is partially observed. Specifically, our approach trains a dedicated amodal segmentation module for diverse target objects to generate amodal masks. These masks and scene observations are mapped to the future rewards of grasp and push motion primitives via deep Q-learning to learn the motion critic. Afterward, the push and grasp motion candidates predicted by the critic, along with the relevant domain knowledge, are fed into the coordinator to generate the optimal motion implemented by the robot. Extensive experiments conducted in both simulated and real-world environments demonstrate the effectiveness of our approach in generating motion sequences for retrieving occluded targets, outperforming other baseline methods in success rate and motion efficiency.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t5_03">
             09:00-10:00, Paper ThPI4T5.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('261'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Behavior-Actor: Behavioral Decomposition and Efficient-Training for Robotic Manipulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#371682" title="Click to go to the Author Index">
             Jiang, Wenyi
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#371737" title="Click to go to the Author Index">
             Xv, Baowei
            </a>
           </td>
           <td class="r">
            Mech-Mind Robotics Technologies Ltd
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#371750" title="Click to go to the Author Index">
             Cui, Zhihao
            </a>
           </td>
           <td class="r">
            Mech-Mind Robotics
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab261" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Language-conditioned Robotic manipulation demonstrates great potential in tackling various tasks. However, the generalization ability of this technique to unseen commands remains a challenge. Moreover, existing methods suffer from the burdensome overhead of data collection costs. Nowadays, Large language models (LLMs) have demonstrated impressive natural language understanding capabilities. In this work, we propose a novel scheme called Behavior-Actor(BehAct), which leverages the power of LLM to decompose language commands into executable behaviors in Retrieval-Augmented Generation(RAG) manner. A Transformer-based actor is then trained to execute these identified behaviors. BehActs LLM acts as a brain, while the actor acts as a hand. The end-to-end actor employs behavior cloning to facilitate multitask learning. By digging the behaviors between tasks and adopting the HG-DAgger style data collection, we effectively address the data-intensive demands typically associated with real-world robot manipulation. A single actor model is trained from scratch on 11 real-world tasks, 40 behaviors using 276 demonstrations, only 7 for each behavior in average. We achieve a 68% average success rate on seen commands, which aligns comparably with recent works. Moreover, BehAct exhibits an impressive 45% average success rate on unseen commands, doubling the performance of the baseline approach. In the BehAct system, LLM-agnostic design enables flexibility in leveraging advanced LLMs without necessitating fine-tuning. Our code has been made publicly available here.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t5_04">
             09:00-10:00, Paper ThPI4T5.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('714'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              RPMArt: Towards Robust Perception and Manipulation for Articulated Objects
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#372071" title="Click to go to the Author Index">
             Wang, Junbo
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#237220" title="Click to go to the Author Index">
             Liu, Wenhai
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#290263" title="Click to go to the Author Index">
             Yu, Qiaojun
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#390197" title="Click to go to the Author Index">
             You, Yang
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#371357" title="Click to go to the Author Index">
             Liu, Liu
            </a>
           </td>
           <td class="r">
            Hefei University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#237324" title="Click to go to the Author Index">
             Wang, Weiming
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#224610" title="Click to go to the Author Index">
             Lu, Cewu
            </a>
           </td>
           <td class="r">
            ShangHai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab714" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#learning_categories_and_concepts" title="Click to go to the Keyword Index">
               Learning Categories and Concepts
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Articulated objects are commonly found in daily life. It is essential that robots can exhibit robust perception and manipulation skills for articulated objects in real-world robotic applications. However, existing methods for articulated objects insufficiently address noise in point clouds and struggle to bridge the gap between simulation and reality, thus limiting the practical deployment in real-world scenarios. To tackle these challenges, we propose a framework towards Robust Perception and Manipulation for Articulated Objects (RPMArt), which learns to estimate the articulation parameters and manipulate the articulation part from the noisy point cloud. Our primary contribution is a Robust Articulation Network (RoArtNet) that is able to predict both joint parameters and affordable points robustly by local feature learning and point tuple voting. Moreover, we introduce an articulation-aware classification scheme to enhance its ability for sim-to-real transfer. Finally, with the estimated affordable point and articulation joint constraint, the robot can generate robust actions to manipulate articulated objects. After learning only from synthetic data, RPMArt is able to transfer zero-shot to real-world articulated objects. Experimental results confirm our approach's effectiveness, with our framework achieving state-of-the-art performance in both noise-added simulation and real-world environments. Code, data and more results can be found on the project website at https://r-pmart.github.io.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t5_05">
             09:00-10:00, Paper ThPI4T5.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('969'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              PreAfford: Universal Affordance-Based Pre-Grasping for Diverse Objects and Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#389770" title="Click to go to the Author Index">
             Ding, Kairui
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397006" title="Click to go to the Author Index">
             Chen, Boyuan
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#299203" title="Click to go to the Author Index">
             Wu, Ruihai
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#337940" title="Click to go to the Author Index">
             Li, Yuyang
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397002" title="Click to go to the Author Index">
             Zhang, Zongzheng
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#339018" title="Click to go to the Author Index">
             Gao, Huan-ang
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#368329" title="Click to go to the Author Index">
             Li, Siqi
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#194821" title="Click to go to the Author Index">
             Zhu, Yixin
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#165639" title="Click to go to the Author Index">
             Zhou, Guyue
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#280114" title="Click to go to the Author Index">
             Dong, Hao
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#223795" title="Click to go to the Author Index">
             Zhao, Hao
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab969" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#manipulation_planning" title="Click to go to the Keyword Index">
               Manipulation Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robotic manipulation with two-finger grippers is challenged by objects lacking distinct graspable features. Traditional pre-grasping methods, which typically involve repositioning objects or utilizing external aids like table edges, are limited in their adaptability across different object categories and environments. To overcome these limitations, we introduce PreAfford, a novel pre-grasping planning framework incorporating a point-level affordance representation and a relay training approach. Our method significantly improves adaptability, allowing effective manipulation across a wide range of environments and object types. When evaluated on the ShapeNet-v2 dataset, PreAfford not only enhances grasping success rates by 69% but also demonstrates its practicality through successful real-world experiments. These improvements highlight PreAfford's potential to redefine standards for robotic handling of complex manipulation tasks in diverse settings.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t5_06">
             09:00-10:00, Paper ThPI4T5.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1102'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Contact Model Based on Denoising Diffusion to Learn Variable Impedance Control for Contact-Rich Manipulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#219260" title="Click to go to the Author Index">
             Okada, Masashi
            </a>
           </td>
           <td class="r">
            Panasonic Holdings Corporation
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#207711" title="Click to go to the Author Index">
             Komatsu, Mayumi
            </a>
           </td>
           <td class="r">
            Panasonic Corp
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#107751" title="Click to go to the Author Index">
             Taniguchi, Tadahiro
            </a>
           </td>
           <td class="r">
            Ritsumeikan University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1102" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#compliance_and_impedance_control" title="Click to go to the Keyword Index">
               Compliance and Impedance Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper, a novel approach is proposed for learning robot control in contact-rich tasks such as wiping, by developing Diffusion Contact Model (DCM). Previous methods of learning such tasks relied on impedance control with time-varying stiffness tuning by performing Bayesian optimization by trial-and-error with robots. The proposed approach aims to reduce the cost of robot operation by predicting the robot contact trajectories from the variable stiffness inputs and using neural models. However, contact dynamics are inherently highly nonlinear, and their simulation requires iterative computations such as convex optimization. Moreover, approximating such computations by using finite-layer neural models is difficult. To overcome these limitations, the proposed DCM used the denoising diffusion models that could simulate the complex dynamics via iterative computations of multi-step denoising, thus improving the prediction accuracy. Stiffness tuning experiments conducted in simulated and real environments showed that the DCM achieved comparable performance to a conventional robot-based optimization method while reducing the number of robot trials.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t5_07">
             09:00-10:00, Paper ThPI4T5.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1159'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              GraspContrast: Self-Supervised Contrastive Learning with False Negative Elimination for 6-DoF Grasp Detection
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#367135" title="Click to go to the Author Index">
             Wang, Wenshuo
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#173622" title="Click to go to the Author Index">
             Zhu, Haiyue
            </a>
           </td>
           <td class="r">
            Agency for Science, Technology and Research (A*STAR)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#100760" title="Click to go to the Author Index">
             Ang Jr, Marcelo H
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1159" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#transfer_learning" title="Click to go to the Keyword Index">
               Transfer Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robotic manipulation is a grand domain that primarily involves the use of robotic arms to interact with objects in the environment. While proposed methods have achieved advancements in grasping objects, they rely heavily on extensive training data that presents a significant challenge due to the labor-intensive process of human annotation. To address the issue, we propose GraspContrast, a self-supervised contrastive learning framework leveraging unlabeled RGB-D images to enhance point-wise feature representations for 6-DoF grasp detection. Our method designs a dual-branch network architecture to learn transformations that embed positive point pairs nearby, while pushing negative point pairs far apart. Specifically, we discuss a false negative elimination strategy to explicitly detect and remove the false negative samples that undesirably repel the point instances from the geometrically similar samples. Our method exhibits consistent improvements over existing learning-based grasp detection methods on both the GraspNet-1B benchmark and physical UR10e platform. These significant performance gains demonstrate the effectiveness of our proposed framework.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t5_08">
             09:00-10:00, Paper ThPI4T5.8
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1830'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Reinforcement Learning for Active Search and Grasp in Clutter
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397322" title="Click to go to the Author Index">
             Pitcher, Thomas
            </a>
           </td>
           <td class="r">
            University of Queensland
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#223618" title="Click to go to the Author Index">
             Frster, Julian
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#150492" title="Click to go to the Author Index">
             Chung, Jen Jen
            </a>
           </td>
           <td class="r">
            The University of Queensland
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1830" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#perception_action_coupling" title="Click to go to the Keyword Index">
               Perception-Action Coupling
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents an Active Search policy that balances between moving the camera and removing occluding objects to search for and retrieve a target object in clutter. While both types of action can reveal unobserved parts of a scene, they typically vary in execution complexity and time. Our proposed method explicitly reasons about the occluded spaces in the scene where the target object may be hidden, and uses reinforcement learning to compute the value of each action with the ultimate goal of finding and retrieving the target object in minimal time. Results in simulation and real-world experiments demonstrate a significant improvement in both task execution speed and success rate compared to baseline grasping strategies.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t5_09">
             09:00-10:00, Paper ThPI4T5.9
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2099'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SculptDiff: Learning Robotic Clay Sculpting from Humans with Goal Conditioned Diffusion Policy
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#336687" title="Click to go to the Author Index">
             Bartsch, Alison
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395948" title="Click to go to the Author Index">
             Car, Arvind
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#373945" title="Click to go to the Author Index">
             Avra, Charlotte
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#254226" title="Click to go to the Author Index">
             Barati Farimani, Amir
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2099" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#art_and_entertainment_robotics" title="Click to go to the Keyword Index">
               Art and Entertainment Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Manipulating deformable objects remains a challenge within robotics due to the difficulties of state estimation, long-horizon planning, and predicting how the object will deform given an interaction. These challenges are the most pronounced with 3D deformable objects. We propose SculptDiff, a goal-conditioned diffusion-based imitation learning framework that works with point cloud state observations to directly learn clay sculpting policies for a variety of target shapes. To the best of our knowledge this is the first real-world method that successfully learns manipulation policies for 3D deformable objects. For sculpting videos and access to our dataset and hardware CAD models, see the project website: https://sites.google.com/andrew.cmu.edu/imitationsculpting/ home
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t5_10">
             09:00-10:00, Paper ThPI4T5.10
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2138'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Inverse Kinematics for Neuro-Robotic Grasping with Humanoid Embodied Agents
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396696" title="Click to go to the Author Index">
             Habekost, Jan-Gerrit
            </a>
           </td>
           <td class="r">
            University of Hamburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396738" title="Click to go to the Author Index">
             Gde, Connor
            </a>
           </td>
           <td class="r">
            University of Hamburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#170321" title="Click to go to the Author Index">
             Allgeuer, Philipp
            </a>
           </td>
           <td class="r">
            University of Hamburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#110222" title="Click to go to the Author Index">
             Wermter, Stefan
            </a>
           </td>
           <td class="r">
            University of Hamburg
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2138" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#task_and_motion_planning" title="Click to go to the Keyword Index">
               Task and Motion Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#humanoid_robot_systems" title="Click to go to the Keyword Index">
               Humanoid Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper introduces a novel zero-shot motion planning method that allows users to quickly design smooth robot motions in Cartesian space. A Bzier curve-based Cartesian plan is transformed into a joint space trajectory by our neuro-inspired inverse kinematics (IK) method CycleIK, for which we enable platform independence by scaling it to arbitrary robot designs. The motion planner is evaluated on the physical hardware of the two humanoid robots NICO and NICOL in a human-in-the-loop grasping scenario. Our method is deployed with an embodied agent that is a large language model (LLM) at its core. We generalize the embodied agent, that was introduced for NICOL, to also be embodied by NICO. The agent can execute a discrete set of physical actions and allows the user to verbally instruct various different robots. We contribute a grasping primitive to its action space that allows for precise manipulation of household objects. The new CycleIK method is compared to popular numerical IK solvers and state-of-the-art neural IK methods in simulation and is shown to be competitive with or outperform all evaluated methods when the algorithm runtime is very short. The grasping primitive is evaluated on both NICOL and NICO robots with a reported grasp success of 72% to 82% for each robot, respectively.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t5_11">
             09:00-10:00, Paper ThPI4T5.11
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2539'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              RT-Grasp: Reasoning Tuning Robotic Grasping Via Multi-Modal Large Language Model
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#375851" title="Click to go to the Author Index">
             Xu, Jinxuan
            </a>
           </td>
           <td class="r">
            Rutgers University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#246081" title="Click to go to the Author Index">
             Jin, Shiyu
            </a>
           </td>
           <td class="r">
            Baidu
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#378373" title="Click to go to the Author Index">
             Lei, Yutian
            </a>
           </td>
           <td class="r">
            Baidu
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#378602" title="Click to go to the Author Index">
             Zhang, Yuqian
            </a>
           </td>
           <td class="r">
            Rutgers University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#101913" title="Click to go to the Author Index">
             Zhang, Liangjun
            </a>
           </td>
           <td class="r">
            Baidu
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2539" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Recent advances in Large Language Models (LLMs) have showcased their remarkable reasoning capabilities, making them influential across various fields. However, in robotics, their use has primarily been limited to manipulation planning tasks due to their inherent textual output. This paper addresses this limitation by investigating the potential of adopting the reasoning ability of LLMs for generating numerical predictions in robotics tasks, specifically for robotic grasping. We propose Reasoning Tuning, a novel method that integrates a reasoning phase before prediction during training, leveraging the extensive prior knowledge and advanced reasoning abilities of LLMs. This approach enables LLMs, notably with multi-modal capabilities, to generate accurate numerical outputs like grasp poses that are context-aware and adaptable through conversations. Additionally, we present the Reasoning Tuning VLM Grasp dataset, carefully curated to facilitate the adaptation of LLMs to robotic grasping. Extensive validation on both grasping datasets and real-world experiments underscores the adaptability of multi-modal LLMs for numerical prediction tasks in robotics. This not only expands their applicability but also bridges the gap between text-based planning and direct robot control, thereby maximizing the potential of LLMs in robotics. Our dataset will be released. More details and videos of this work are available on our project page: https://sites.google.com/view/rt-grasp.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t5_12">
             09:00-10:00, Paper ThPI4T5.12
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2663'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Learning-Based Controller for Multi-Contact Grasps on Unknown Objects with a Dexterous Hand
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#286526" title="Click to go to the Author Index">
             Winkelbauer, Dominik
            </a>
           </td>
           <td class="r">
            DLR
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#102317" title="Click to go to the Author Index">
             Triebel, Rudolph
            </a>
           </td>
           <td class="r">
            German Aerospace Center (DLR)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#105517" title="Click to go to the Author Index">
             Buml, Berthold
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2663" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#dexterous_manipulation" title="Click to go to the Keyword Index">
               Dexterous Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#force_control" title="Click to go to the Keyword Index">
               Force Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Existing grasp controllers usually either only support finger-tip grasps or need explicit configuration of the inner forces. We propose a novel grasp controller that supports arbitrary grasp types, including power grasps with multi-contacts, while operating self-contained on before unseen objects. No detailed contact information is needed, but only a rough 3D model, e.g., reconstructed from a single depth image. First, the external wrench being applied to the object is estimated by using the measured torques at the joints. Then, the torques necessary to counteract the estimated wrench while keeping the object at its initial pose are predicted. The torques are commanded via desired joint angles to an underlying joint-level impedance controller. To reach real-time performance, we propose a learning-based approach that is based on a wrench estimator- and a torque predictor-neural network. Both networks are trained in a supervised fashion using data generated via the analytical formulation of the controller. In an extensive simulation-based evaluation, we show that our controller is able to keep 83.1% of the tested grasps stable when applying external wrenches with up to SI{10}{newton}. At the same time, we outperform the two tested baselines by being more efficient and inducing less involuntary object movement. Finally, we show that the controller also works on the real DLR-Hand II reaching a cycle time of 6ms.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t5_13">
             09:00-10:00, Paper ThPI4T5.13
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2906'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Gradual Receptive Expansion Using Vision Transformer for Online 3D Bin Packing
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#244440" title="Click to go to the Author Index">
             Kang, Minjae
            </a>
           </td>
           <td class="r">
            Seoul National University (SNU)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#266254" title="Click to go to the Author Index">
             Kee, Hogun
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395155" title="Click to go to the Author Index">
             Park, Yoseph
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#325779" title="Click to go to the Author Index">
             Kim, Junseok
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#299146" title="Click to go to the Author Index">
             Jeong, Jaeyeon
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396157" title="Click to go to the Author Index">
             Cheon, Geunje
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395356" title="Click to go to the Author Index">
             Lee, Jaewon
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#119971" title="Click to go to the Author Index">
             Oh, Songhwai
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2906" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#manipulation_planning" title="Click to go to the Keyword Index">
               Manipulation Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#factory_automation" title="Click to go to the Keyword Index">
               Factory Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The bin packing problem (BPP) is a challenging combinatorial optimization problem with a number of practical applications. This paper focuses on online 3D-BPP, where the packer makes immediate decisions for a loading position as items continually arrive. We propose a novel reinforcement learning algorithm, GREViT, which utilizes a vision transformer to tackle online 3D-BPP for the first time. By introducing the gradual receptive expansion technique, GREViT overcomes the limitations inherent in learning-based methods that only excel in their trained bins. As a result, GREViT surpasses existing BPP algorithms in packing ratio across various bin sizes. The effectiveness of GREViT in real-world scenarios is validated by its successful demonstrations using a real robot for solving 3D-BPP. The attached video demonstrates GREViT undertaking 3D-BPP in both simulated and real-world environments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t5_14">
             09:00-10:00, Paper ThPI4T5.14
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3018'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Constrained 6-DoF Grasp Generation on Complex Shapes for Improved Dual-Arm Manipulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#341638" title="Click to go to the Author Index">
             Singh, Gaurav
            </a>
           </td>
           <td class="r">
            IIIT Hyderabad
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#339069" title="Click to go to the Author Index">
             Kalwar, Sanket
            </a>
           </td>
           <td class="r">
            International Institute of Information Technology, Hyderabad
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#391554" title="Click to go to the Author Index">
             Karim, Md Faizal
            </a>
           </td>
           <td class="r">
            IIIT Hyderabad
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#341633" title="Click to go to the Author Index">
             Sen, Bipasha
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#216911" title="Click to go to the Author Index">
             Govindan, Nagamanikandan
            </a>
           </td>
           <td class="r">
            International Institute of Information Technology Hyderabad
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#173768" title="Click to go to the Author Index">
             Sridhar, Srinath
            </a>
           </td>
           <td class="r">
            Brown University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#102906" title="Click to go to the Author Index">
             Krishna, Madhava
            </a>
           </td>
           <td class="r">
            IIIT Hyderabad
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3018" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#dual_arm_manipulation" title="Click to go to the Keyword Index">
               Dual Arm Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Efficiently generating grasp poses tailored to specific regions of an object is vital for various robotic manipulation tasks, especially in a dual-arm setup. This scenario presents a significant challenge due to the complex geometries involved, requiring a deep understanding of the local geometry to generate grasps efficiently on the specified constrained regions. Existing methods only explore settings involving table-top/small objects and require augmented datasets to train, limiting their performance on complex objects. We propose CGDF: Constrained Grasp Diffusion Fields, a diffusion-based grasp generative model that generalizes to objects with arbitrary geometries, as well as generates dense grasps on the target regions. CGDF uses a part-guided diffusion approach that enables it to get high sample efficiency in constrained grasping without explicitly training on massive constraint-augmented datasets. We provide qualitative and quantitative comparisons using analytical metrics and in simulation, in both unconstrained and constrained settings to show that our method can generalize to generate stable grasps on complex objects, especially useful for dual-arm manipulation settings while existing methods struggle to do so.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t5_15">
             09:00-10:00, Paper ThPI4T5.15
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3101'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Leveraging Simulation-Based Model Preconditions for Fast Action Parameter Optimization with Multiple Models
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#241028" title="Click to go to the Author Index">
             Seker, Muhammet Yunus
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#124019" title="Click to go to the Author Index">
             Kroemer, Oliver
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3101" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#model_learning_for_control" title="Click to go to the Keyword Index">
               Model Learning for Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Optimizing robotic action parameters is a significant challenge for manipulation tasks that demand high levels of precision and generalization. Using a model-based approach, the robot must quickly reason about the outcomes of different actions using a predictive model to find a set of parameters that will have the desired effect. The model may need to capture the behaviors of rigid and deformable objects, as well as objects of various shapes and sizes. Predictive models often need to trade-off speed for prediction accuracy and generalization. This paper proposes a framework that leverages the strengths of multiple predictive models, including analytical, learned, and simulation-based models, to enhance the efficiency and accuracy of action parameter optimization. Our approach uses Model Deviation Estimators (MDEs) to determine the most suitable predictive model for any given state-action parameters, allowing the robot to select models to make fast and precise predictions. We extend the MDE framework by not only learning sim-to-real MDEs, but also sim-to-sim MDEs. Our experiments show that these sim-to-sim MDEs provide significantly faster parameter optimization as well as a basis for efficiently learning sim-to-real MDEs through finetuning. The ease of collecting sim-to-sim training data also allows the robot to learn MDEs based directly on visual inputs and local material properties.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t5_16">
             09:00-10:00, Paper ThPI4T5.16
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3374'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Open6DOR: Benchmarking Open-Instruction 6-DoF Object Rearrangement and a VLM-Based Baseline
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397447" title="Click to go to the Author Index">
             Ding, Yufei
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#341712" title="Click to go to the Author Index">
             Geng, Haoran
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#377082" title="Click to go to the Author Index">
             Xu, Chaoyi
            </a>
           </td>
           <td class="r">
            BAAI
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#377089" title="Click to go to the Author Index">
             Fang, Xiaomeng
            </a>
           </td>
           <td class="r">
            Beijing Academy of Artificial Intelligence
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#331922" title="Click to go to the Author Index">
             Zhang, Jiazhao
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#306414" title="Click to go to the Author Index">
             Wei, Songlin
            </a>
           </td>
           <td class="r">
            Soochow University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#209075" title="Click to go to the Author Index">
             Zhang, Zhizheng
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#155911" title="Click to go to the Author Index">
             Wang, He
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3374" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#manipulation_planning" title="Click to go to the Keyword Index">
               Manipulation Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The integration of large-scale Vision-Language Models (VLMs) with embodied AI can greatly enhance the generalizability and the capacity to follow open instructions for robots. However, existing studies on object manipulation are not up to full consideration of the 6-DoF requirements, let alone establishing a comprehensive benchmark. In this paper, we propel the pioneer construction of the benchmark and approach for Open-instruction 6-DoF Object Rearrangement (Open6DOR). Specifically, we collect a synthetic dataset of 200+ objects and carefully design 5400+ Open6DOR tasks. These tasks are divided into the Position-track, Rotation-track, and 6-DoF-track for evaluating different embodied agents in predicting the positions and rotations of target objects. Besides, we also propose a VLM-based approach for Open6DOR, named Open6DOR-GPT, which empowers GPT-4V with 3D-awareness and simulation-assistance while exploiting its strengths in generalizability and instruction-following. We compare the existing embodied agents with our Open6DOR-GPT on the proposed Open6DOR benchmark and find that Open6DOR-GPT achieves state-of-the-art performance. We further show the impressive performance of Open6DOR-GPT in diverse real-world experiments.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thpi4t6">
             <b>
              ThPI4T6
             </b>
            </a>
           </td>
           <td class="r">
            Room 6
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thpi4t6" title="Click to go to the Program at a Glance">
             <b>
              Learning III
             </b>
            </a>
           </td>
           <td class="r">
            Teaser Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#251633" title="Click to go to the Author Index">
             Wang, Peng
            </a>
           </td>
           <td class="r">
            Manchester Metropolitan University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t6_01">
             09:00-10:00, Paper ThPI4T6.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('868'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Proposal and Demonstration of a Robot Behavior Planning System Utilizing Video with Open Source Models in Real-World Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#394454" title="Click to go to the Author Index">
             Akutsu, Yuki
            </a>
           </td>
           <td class="r">
            Osaka University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#390578" title="Click to go to the Author Index">
             Yoshida, Takahiro
            </a>
           </td>
           <td class="r">
            Osaka University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#371123" title="Click to go to the Author Index">
             Kato, Yuki
            </a>
           </td>
           <td class="r">
            Osaka University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#141486" title="Click to go to the Author Index">
             Sueoka, Yuichiro
            </a>
           </td>
           <td class="r">
            Osaka Univ
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#10031" title="Click to go to the Author Index">
             Osuka, Koichi
            </a>
           </td>
           <td class="r">
            Osaka University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab868" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#planning__scheduling_and_coordination" title="Click to go to the Keyword Index">
               Planning, Scheduling and Coordination
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#ai_based_methods" title="Click to go to the Keyword Index">
               AI-Based Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In the field of robotics, researches have sought to control robots capable of dealing with a variety of envi- ronments and tasks generically, through the use of foundation models. Among these, the systems for robot behavior planning utilizing video have also been proposed. The system enables the generation of robot behaviors that are not dependent on specific environments or tasks. This is achieved by generating videos based on text input, which utilizes the vast knowledge inherent in the foundation models. Also, by using a visual interface such as video, it is possible to confirm the behavioral indicators on which the robot is operating. Although, there are few examples of research on robot behavior planning utilizing video. Previous studies have emphasized the verification of behavior generation utilizing video, with simplified object manipulation for testing on simulations. This is not enough to demonstrate the usefulness of robot behavior planning utilizing video in real- world environments. In addition, the systems from previous studies are not open, and such systems have not been sufficiently discussed. This paper attempts to construct robot behavior planning utilizing video as an open system, and to verify the validity of the behavior planning using actual machines. In this paper, we first focus on using Robotiss TURTLEBOT3 Waffle Pi and Mobile Manipulator(referred to as Waffle) to construct robot behavior planning system utilizing video. Second, we create planning videos targeting the pick-and- place motion using the proposed system, and control the arm part of Waffle in the actual machine verification. Finally, by comparing the target coordinates from the planning video with the coordinates observed from the actual machine, we can confirm whether it is possible to control Waffle as planned. Errors are calculated from the coordinate comparison, and the control is performed again. Based on the results, we verify whether the proposed system is useful for controlling robots in real-world environments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t6_02">
             09:00-10:00, Paper ThPI4T6.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2082'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Robot Shape and Location Retention in Video Generation Using Diffusion Models
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#251633" title="Click to go to the Author Index">
             Wang, Peng
            </a>
           </td>
           <td class="r">
            Manchester Metropolitan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397497" title="Click to go to the Author Index">
             Guo, Zhihao
            </a>
           </td>
           <td class="r">
            Manchester Metropolitan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397802" title="Click to go to the Author Index">
             Sait, Abdul Latheef
            </a>
           </td>
           <td class="r">
            JD Sports Fashion PLC
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397638" title="Click to go to the Author Index">
             Pham, Minh Huy
            </a>
           </td>
           <td class="r">
            Manchester Metropolitan University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2082" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#robot_safety" title="Click to go to the Keyword Index">
               Robot Safety
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Diffusion models have marked a significant milestone in the enhancement of image and video generation technologies. However, generating videos that precisely retain the shape and location of moving objects such as robots remains a challenge. This paper presents diffusion models specifically tailored to generate videos that accurately maintain the shape and location of mobile robots. The proposed models incorporate techniques such as embedding accessible robot pose information and applying semantic mask regulation within the scalable and efficient ConvNext backbone network. These techniques are designed to refine intermediate outputs, therefore improving the retention performance of shape and location. Through extensive experimentation, our models have demonstrated notable improvements in maintaining the shape and location of different robots, as well as enhancing overall video generation quality, compared to the benchmark diffusion model. Codes will be open-sourced at: https://github.com/PengPaulWang/diffusion-robots.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t6_03">
             09:00-10:00, Paper ThPI4T6.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2523'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SceneSense: Diffusion Models for 3D Occupancy Synthesis from Partial Observation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#343297" title="Click to go to the Author Index">
             Reed, Alec
            </a>
           </td>
           <td class="r">
            University of Colorado Boulder
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#291635" title="Click to go to the Author Index">
             Crowe, Brendan
            </a>
           </td>
           <td class="r">
            University of Colorado Boulder
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#354498" title="Click to go to the Author Index">
             Albin, Doncey
            </a>
           </td>
           <td class="r">
            University of Colorado - Boulder
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398196" title="Click to go to the Author Index">
             Achey, Lorin
            </a>
           </td>
           <td class="r">
            University of Colorado Boulder
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#156445" title="Click to go to the Author Index">
             Hayes, Bradley
            </a>
           </td>
           <td class="r">
            University of Colorado Boulder
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#180637" title="Click to go to the Author Index">
             Heckman, Christoffer
            </a>
           </td>
           <td class="r">
            University of Colorado at Boulder
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2523" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             When exploring new areas, robotic systems generally exclusively plan and execute controls over geometry that has been directly measured. This planning paradigm can lead to unintuitive exploration or replanning latency when entering areas that were previous obstructed from view. To address this we present SceneSense, a real-time 3D diffusion model for synthesizing 3D occupancy information from partial observations that effectively predicts these occluded or out of view geometries for use in future planning and control frameworks. SceneSense uses a running occupancy map and a single RGB-D camera to generate predicted geometry around the platform at runtime, even when the geometry is occluded or out of view. Our architecture ensures that SceneSense never overwrites observed free or occupied space. By preserving the integrity of the observed map, SceneSense mitigates the risk of corrupting the observed space with generative predictions. While SceneSense is shown to operate well using a single RGB-D camera, the framework is flexible enough to extend to additional modalities. Unlike existing models that necessitate multiple views and offline scene synthesis, or are focused on filling gaps in observed data, our findings demonstrate that SceneSense is an effective approach to estimating unobserved local occupancy information at runtime. Local occupancy predictions from SceneSense are shown to better represent the ground truth occupancy distribution during the test exploration trajectories than the running occupancy map. The source code can be found on our website: https://arpg.github.io/scenesense/
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t6_04">
             09:00-10:00, Paper ThPI4T6.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('642'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              How Physics and Background Attributes Impact Video Transformers in Robotic Manipulation: A Case Study on Planar Pushing
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#323184" title="Click to go to the Author Index">
             Jin, Shutong
            </a>
           </td>
           <td class="r">
            KTH Royal Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#375361" title="Click to go to the Author Index">
             Wang, Ruiyu
            </a>
           </td>
           <td class="r">
            KTH Royal Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#377722" title="Click to go to the Author Index">
             Zahid, Muhammad
            </a>
           </td>
           <td class="r">
            KTH Royal Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#159924" title="Click to go to the Author Index">
             Pokorny, Florian T.
            </a>
           </td>
           <td class="r">
            KTH Royal Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab642" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#data_sets_for_robot_learning" title="Click to go to the Keyword Index">
               Data Sets for Robot Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             As model and dataset sizes continue to scale in robot learning, the need to understand how the composition and properties of a dataset affect model performance becomes increasingly urgent to ensure cost-effective data collection and model performance. In this work, we empirically investigate how physics attributes (color, friction coefficient, shape) and scene background characteristics, such as the complexity and dynamics of interactions with background objects, influence the performance of Video Transformers in predicting planar pushing trajectories. We investigate three primary questions: How do physics attributes and background scene characteristics influence model performance? What kind of changes in attributes are most detrimental to model generalization? What proportion of fine-tuning data is required to adapt models to novel scenarios? To facilitate this research, we present CloudGripper-Push-1K, a large real-world vision-based robot pushing dataset comprising 1278 hours and 460,000 videos of planar pushing interactions with objects with different physics and background attributes. We also propose Video Occlusion Transformer (VOT), a generic modular video-transformer-based trajectory prediction framework which features 3 choices of 2D-spatial encoders as the subject of our case study. The dataset and source code are available at https://cloudgripper.org.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t6_05">
             09:00-10:00, Paper ThPI4T6.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('254'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Lightweight Fisheye Object Detection Network with Transformer-Based Feature Enhancement for Autonomous Driving
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#273074" title="Click to go to the Author Index">
             Cao, Hu
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#391163" title="Click to go to the Author Index">
             Li, Yanpeng
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#276997" title="Click to go to the Author Index">
             Liu, Yinlong
            </a>
           </td>
           <td class="r">
            University of Macau
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#347206" title="Click to go to the Author Index">
             Li, Xinyi
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#157845" title="Click to go to the Author Index">
             Chen, Guang
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#105608" title="Click to go to the Author Index">
             Knoll, Alois
            </a>
           </td>
           <td class="r">
            Tech. Univ. Muenchen TUM
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab254" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#recognition" title="Click to go to the Keyword Index">
               Recognition
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Fisheye cameras, offering a wide field of view (FOV) of 360^{circ}, are extensively employed for surround-view perception in autonomous driving. Compared with the object detection on the standard images, it lacks studies for fisheye images. Moreover, efficient perception is crucial for autonomous vehicles with limited computational capability. In this work, we introduce a lightweight fisheye object detection network with transformer-based feature enhancement for autonomous driving. Specifically, we leverage ShuffleNet V2 as a feature extraction network to reduce computation complexity and develop a transformer-based feature enhancement module (TFEM) to integrate multi-level features. Notably, we observe that data augmentation methods like mix-up and mosaic, effective on standard images, do not yield positive results on fisheye images. The results on the WoodScape dataset demonstrate that our method can achieve better performance with fewer parameters and floating-point operations per second (FLOPs). Extending our evaluation to the Microsoft Common Objects in Context (MS COCO) dataset shows that the proposed method has excellent generalization capability.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t6_06">
             09:00-10:00, Paper ThPI4T6.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('329'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              DiffusionNOCS: Managing Symmetry and Uncertainty in Sim2Real Multi-Modal Category-Level Pose Estimation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#172403" title="Click to go to the Author Index">
             Ikeda, Takuya
            </a>
           </td>
           <td class="r">
            Woven by Toyota, Inc
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#211158" title="Click to go to the Author Index">
             Zakharov, Sergey
            </a>
           </td>
           <td class="r">
            Toyota Research Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#177032" title="Click to go to the Author Index">
             Ko, Tianyi
            </a>
           </td>
           <td class="r">
            Woven by Toyota, Inc
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#237975" title="Click to go to the Author Index">
             Irshad, Muhammad Zubair
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#238317" title="Click to go to the Author Index">
             Lee, Robert
            </a>
           </td>
           <td class="r">
            Australian Centre for Robotic Vision
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#217166" title="Click to go to the Author Index">
             Liu, Katherine
            </a>
           </td>
           <td class="r">
            Toyota Research Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#117786" title="Click to go to the Author Index">
             Ambrus, Rares
            </a>
           </td>
           <td class="r">
            Toyota Research Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#104350" title="Click to go to the Author Index">
             Nishiwaki, Koichi
            </a>
           </td>
           <td class="r">
            Woven by Toyota
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab329" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#probabilistic_inference" title="Click to go to the Keyword Index">
               Probabilistic Inference
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper addresses the challenging problem of category-level pose estimation. Current state-of-the-art methods for this task face challenges when dealing with symmetric objects and when attempting to generalize to new environments solely through synthetic data training. In this work, we address these challenges by proposing a probabilistic model that relies on diffusion to estimate dense canonical maps crucial for recovering partial object shapes as well as establishing correspondences essential for pose estimation. Furthermore, we introduce critical components to enhance performance by leveraging the strength of the diffusion models with multi-modal input representations. We demonstrate the effectiveness of our method by testing it on a range of real datasets. Despite being trained solely on our generated synthetic data, our approach achieves state-of-the-art performance and unprecedented generalization qualities, outperforming baselines, even those specifically trained on the target domain. Our code and data for the generalization benchmark can be found at https://woven-planet.github.io/DiffusionNOCS/.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t6_07">
             09:00-10:00, Paper ThPI4T6.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('419'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              3DR-DIFF: Blind Diffusion Inpainting for 3D Point Cloud Reconstruction and Segmentation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392434" title="Click to go to the Author Index">
             Kariyawasam Thanthrige, Yasas Mahima
            </a>
           </td>
           <td class="r">
            University of New South Wales
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#327761" title="Click to go to the Author Index">
             Perera, Asanka
            </a>
           </td>
           <td class="r">
            University of Southern Queensland
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#216759" title="Click to go to the Author Index">
             Anavatti, Sreenatha
            </a>
           </td>
           <td class="r">
            University of New South Wales
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#124297" title="Click to go to the Author Index">
             Garratt, Matthew
            </a>
           </td>
           <td class="r">
            UNSW Australia, Canberra
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab419" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             LiDAR-based 3D perception is a focal point in autonomous vehicle research due to its efficacy in real-world environments and falling costs. However, recent research reveals challenges with LiDAR sensing under corruptions that occur due to adverse weather conditions and sensor-level errors, known as common corruptions. In particular, the majority of these corruptions lead to sparsity or noise in LiDAR point clouds, degrading the performance of downstream perception tasks. To address this, we propose a blind inpainting method named 3DR-DIFF, utilizing diffusion networks to reconstruct and segment corrupted point clouds. 3DR-DIFF comprises two key components: a corrupted region prediction network, acting as a binary mask predictor, and a conditional diffusion network. The evaluation results demonstrate that the 3DR-DIFF is able to reconstruct the LiDAR samples with a depth error of less than 0.56 mean absolute error (MAE) and an intensity error of 0.02 MAE, along with an average segmentation performance of 0.43 mean intersection over union. Furthermore, benchmarking results highlight that 3DR-DIFF outperforms state-of-the-art methods in reconstructing LiDAR beam-missing scenarios, exhibiting an approximately 9.2% lower error for a degradation of 1 MAE.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t6_08">
             09:00-10:00, Paper ThPI4T6.8
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1607'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Lightweight De-Confounding Transformer for Image Captioning in Wearable Assistive Navigation Device
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#111763" title="Click to go to the Author Index">
             Cao, Zhengcai
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#376190" title="Click to go to the Author Index">
             Xia, Ji
            </a>
           </td>
           <td class="r">
            Beijing University of Chemical Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#376203" title="Click to go to the Author Index">
             Shi, Yinbin
            </a>
           </td>
           <td class="r">
            Beijing University of Chemical Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#112540" title="Click to go to the Author Index">
             Zhou, MengChu
            </a>
           </td>
           <td class="r">
            New Jersey Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1607" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#wearable_robotics" title="Click to go to the Keyword Index">
               Wearable Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Image captioning is a multi-modal task that enables the transformation from scene images to natural language, providing valuable insights for visually impaired individuals to understand their environment. Therefore, its application to wearable navigation devices for visually impaired individuals holds immense potential. However, in practical applications, confusion between scene visuals and semantics, coupled with model complexity, often leads to performance degradation, resulting in inaccurate environmental interpretation. In light of this, we introduce a Lightweight De-confounding Transformer Network (LDTNet) for image captioning equipped with a Causal Adjustment module to eliminate confounders. Moreover, we design a Suppression Gate Unit that efficiently integrates fine-grained information from shallow features, while reducing the number of network layers to have a lightweight model. Experimental results demonstrate that our approach not only addresses the visual-semantic confusion issue effectively but also improves the response speed of wearable devices in comparison with the state of the art. Twenty volunteers are recruited to evaluate LDTNets efficacy in real-world settings in terms of both response speed and generated outputs by wearing the resulting assistive navigation devices. The outcomes well show its outstanding performance and great potential for visualy impaired individuals to use.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t6_09">
             09:00-10:00, Paper ThPI4T6.9
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1742'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Temporal Attention for Cross-View Sequential Image Localization
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#394081" title="Click to go to the Author Index">
             Yuan, Dong
            </a>
           </td>
           <td class="r">
            QUT Centre for Robotics, Queensland University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#104800" title="Click to go to the Author Index">
             Maire, Frederic
            </a>
           </td>
           <td class="r">
            Queensland University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#116733" title="Click to go to the Author Index">
             Dayoub, Feras
            </a>
           </td>
           <td class="r">
            The University of Adelaide
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1742" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper introduces a novel approach to enhancing cross-view localization, focusing on the fine-grained, sequential localization of street-view images within a single known satellite image patch, a significant departure from traditional one-to-one image retrieval methods. By expanding to sequential image fine-grained localization, our model, equipped with a novel Temporal Attention Module (TAM), leverages contextual information to significantly improve sequential image localization accuracy. Our method shows substantial reductions in both mean and median localization errors on the Cross-View Image Sequence (CVIS) dataset, outperforming current state-of-the-art single-image localization techniques. Additionally, by adapting the KITTI-CVL dataset into sequential image sets, we not only offer a more realistic dataset for future research but also demonstrate our model's robust generalization capabilities across varying times and areas, evidenced by a 75.3% reduction in mean distance error in cross-view sequential image localization.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t6_10">
             09:00-10:00, Paper ThPI4T6.10
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1829'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Fine-Tuning the Diffusion Model and Distilling Informative Priors for Sparse-View 3D Reconstruction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#348454" title="Click to go to the Author Index">
             Tang, Jiadong
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#324050" title="Click to go to the Author Index">
             Gao, Yu
            </a>
           </td>
           <td class="r">
            Beijing Institude of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#341674" title="Click to go to the Author Index">
             Jiang, Tianji
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#128868" title="Click to go to the Author Index">
             Yang, Yi
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#128869" title="Click to go to the Author Index">
             Fu, Mengyin
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1829" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#ai_based_methods" title="Click to go to the Keyword Index">
               AI-Based Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             3D reconstruction methods such as Neural Radiance Fields (NeRFs) are capable of optimizing high-quality 3D scene representation from images. However, NeRF is limited by the requirement for a large number of multi-view images, making its application to real-world scenarios challenging. In this work, we propose a method that can reconstruct real-world scenes from a few input images and a simple text prompt. Specifically, we fine-tune a pretrained diffusion model to constrain its powerful priors to the visual inputs and generate 3D-aware images, leveraging the coarse renderings obtained from input images as the image condition, along with the text prompt as the text condition. Our fine-tuning method saves a significant amount of training time and GPU memory usage while also generating credible results. Moreover, to enable our method to have self-evaluation capabilities, we design a semantic switch to filter out generated images that do not match real scenes, ensuring that only informative priors from the fine-tuned diffusion model are distilled into the 3D model. The semantic switch we designed can be used as a plug-in and improve performance by 13%. We perform our approach on a real-world dataset and demonstrate competitive results compared to existing sparse-view 3D reconstruction methods.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t6_11">
             09:00-10:00, Paper ThPI4T6.11
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2246'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Multi-Fingered Dragging of Unknown Objects and Orientations Using Distributed Tactile Information through Vision-Transformer and LSTM
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#365589" title="Click to go to the Author Index">
             Ueno, Takahisa
            </a>
           </td>
           <td class="r">
            Waseda University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#184706" title="Click to go to the Author Index">
             Funabashi, Satoshi
            </a>
           </td>
           <td class="r">
            Waseda University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#243493" title="Click to go to the Author Index">
             Ito, Hiroshi
            </a>
           </td>
           <td class="r">
            Hitachi, Ltd
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#108304" title="Click to go to the Author Index">
             Schmitz, Alexander
            </a>
           </td>
           <td class="r">
            Waseda University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#275238" title="Click to go to the Author Index">
             Kulkarni, Shardul
            </a>
           </td>
           <td class="r">
            Waseda University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#101828" title="Click to go to the Author Index">
             Ogata, Tetsuya
            </a>
           </td>
           <td class="r">
            Waseda University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#100151" title="Click to go to the Author Index">
             Sugano, Shigeki
            </a>
           </td>
           <td class="r">
            Waseda University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2246" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#multifingered_hands" title="Click to go to the Keyword Index">
               Multifingered Hands
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#force_and_tactile_sensing" title="Click to go to the Keyword Index">
               Force and Tactile Sensing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Multi-fingered hands can be suitable for stable object manipulation. Furthermore, abundant tactile information can be acquired with multi-fingered hands, useful to recognize the objects properties, which is beneficial to adapt the motion to the object. However, generating dexterous manipulation motions with multi-fingered hands with high density tactile sensors is challenging due to complex touch states. Hence, tasks that conventionally require a high level of active tactile sensing simultaneously with motion generation, such as pulling in the hand while recognizing the posture of an object are difficult to accomplish. In this letter, we propose a novel deep predictive learning approach using Vision-Transformer (ViT) and Long-Short Term Memory (LSTM). The ViT's attention mechanism can spatially focus on specific fingers represented by distributed 3-axis tactile sensors (uSkin). The LSTM can preserve long time-series information of the manipulation which can realize changing the desired motion according to the initial touching position and orientation for the target object. Results showed that the ViT-LSTM is effective in performing adaptive finger movements according to the properties of the object, i.e. its hardness and relative posture.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t6_12">
             09:00-10:00, Paper ThPI4T6.12
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2637'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Diff-Control: A Stateful Diffusion-Based Policy for Imitation Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#223863" title="Click to go to the Author Index">
             Liu, Xiao
            </a>
           </td>
           <td class="r">
            Arizona State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#324106" title="Click to go to the Author Index">
             Zhou, Yifan
            </a>
           </td>
           <td class="r">
            Arizona State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#353432" title="Click to go to the Author Index">
             Weigend, Fabian Clemens
            </a>
           </td>
           <td class="r">
            Arizona State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#273794" title="Click to go to the Author Index">
             Sonawani, Shubham
            </a>
           </td>
           <td class="r">
            Arizona State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#105255" title="Click to go to the Author Index">
             Ikemoto, Shuhei
            </a>
           </td>
           <td class="r">
            Kyushu Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#130054" title="Click to go to the Author Index">
             Ben Amor, Heni
            </a>
           </td>
           <td class="r">
            Arizona State University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2637" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             While imitation learning provides a simple and effective framework for policy learning, acquiring consistent action during robot execution remains a challenging task. Existing approaches primarily focus on either modifying the action representation at data curation stage or altering the model itself, both of which do not fully address the scalability of consistent action generation. To overcome this limitation, we introduce the Diff-Control policy, which utilizes a diffusion-based model to learn action representation from a state-space modeling viewpoint. We demonstrate that diffusion-based policies can acquire statefulness through a Bayesian formulation facilitated by ControlNet, leading to improved robustness and success rates. Our experimental results demonstrate the significance of incorporating action statefulness in policy learning, where Diff-Control shows improved performance across various tasks. Specifically, Diff-Control achieves an average success rate of 72% and 84% on stateful and dynamic tasks, respectively. Notably, Diff-Control also shows consistent performance in the presence of perturbations, outperforming other state-of-the-art methods that falter under similar conditions. Project page: https://diff-control.github.io/
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t6_13">
             09:00-10:00, Paper ThPI4T6.13
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('598'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Off-Dynamics Conditional Diffusion Planners
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#361860" title="Click to go to the Author Index">
             Ng, Wen Zheng Terence
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#361861" title="Click to go to the Author Index">
             Chen, Jianda
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#256104" title="Click to go to the Author Index">
             Zhang, Tianwei
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab598" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Offline Reinforcement Learning (RL) offers an attractive alternative to interactive data acquisition by leveraging pre-existing datasets. However, its effectiveness hinges on the quantity and quality of the data samples. This work explores the use of more readily available, albeit off-dynamics datasets, to address the challenge of data scarcity in Offline RL. We propose a novel approach using conditional Diffusion Probabilistic Models (DPMs) to learn the joint distribution of the large-scale off-dynamics dataset and the limited target dataset. To enable the model to capture the underlying dynamics structure, we introduce two contexts for the conditional model: (1) a continuous dynamics score allows for partial overlap between trajectories from both datasets, providing the model with richer information; (2) an inverse-dynamics context guides the model to generate trajectories that adhere to the target environment's dynamic constraints. Empirical results demonstrate that our method significantly outperforms several strong baselines. Ablation studies further reveal the critical role of each dynamics context. Additionally, our model demonstrates that by modifying the context, we can interpolate between source and target dynamics, making it more robust to subtle shifts in the environment.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t6_14">
             09:00-10:00, Paper ThPI4T6.14
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1326'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Predictive Coding for Decision Transformer
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#294545" title="Click to go to the Author Index">
             Luu, Tung
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395314" title="Click to go to the Author Index">
             Lee, Donghoon
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science &amp; Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#253386" title="Click to go to the Author Index">
             Yoo, Chang D.
            </a>
           </td>
           <td class="r">
            KAIST
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1326" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#learning_from_experience" title="Click to go to the Keyword Index">
               Learning from Experience
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Recent work in offline reinforcement learning (RL) has demonstrated the effectiveness of formulating decision-making as return-conditioned supervised learning. Notably, the decision transformer (DT) architecture has shown promise across various domains. However, despite its initial success, DTs have underperformed on several challenging datasets in goal-conditioned RL. This limitation stems from the inefficiency of return conditioning for guiding policy learning, particularly in unstructured and suboptimal datasets, resulting in DTs failing to effectively learn temporal compositionality. Moreover, this problem might be further exacerbated in long-horizon sparse-reward tasks. To address this challenge, we propose the Predictive Coding for Decision Transformer (PCDT) framework, which leverages generalized future conditioning to enhance DT methods. PCDT utilizes an architecture that extends the DT framework, conditioned on predictive codings, enabling decision-making based on both present and future factors, thereby improving generalization. Through extensive experiments on eight datasets from the AntMaze and FrankaKitchen environments, our proposed method achieves performance on par with or surpassing existing popular value-based and transformer-based methods in offline goal-conditioned RL. Furthermore, we also evaluate our method on a goal-reaching task with a physical robot.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t6_15">
             09:00-10:00, Paper ThPI4T6.15
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2749'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Steering Decision Transformers Via Temporal Difference Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#356422" title="Click to go to the Author Index">
             Hsu, Hao-Lun
            </a>
           </td>
           <td class="r">
            Duke University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#269194" title="Click to go to the Author Index">
             Bozkurt, Alper Kamil
            </a>
           </td>
           <td class="r">
            Duke University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#379312" title="Click to go to the Author Index">
             Dong, Juncheng
            </a>
           </td>
           <td class="r">
            Duke University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#268630" title="Click to go to the Author Index">
             Gao, Qitong
            </a>
           </td>
           <td class="r">
            Duke University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#414115" title="Click to go to the Author Index">
             Tarokh, Vahid
            </a>
           </td>
           <td class="r">
            Duke University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#169117" title="Click to go to the Author Index">
             Pajic, Miroslav
            </a>
           </td>
           <td class="r">
            Duke University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2749" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#manipulation_planning" title="Click to go to the Keyword Index">
               Manipulation Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Decision Transformers (DTs) have been highly effective for offline reinforcement learning (RL) tasks, successfully modeling the sequences of actions in a given set of demonstrations. However, DTs may perform poorly in stochastic environments, which are prevalent in robotics scenarios. In this paper, we identify that the root cause of this performance degradation is the growing variance of returns-to-go, the signal used by DTs to predict actions, accumulated over the horizon. Building upon this insight, we propose an extension to DTs that allows them to be steered toward high-reward regions, where the expected returns are estimated using temporal difference learning. This way, we not only mitigate the growing variance problem but also eliminate the need for DTs to have access to returns-to-go during evaluation and deployment phases. We show that our method outperforms state-of-the-art offline RL methods in both simulated and real-world robotic arm environments.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thpi4t7">
             <b>
              ThPI4T7
             </b>
            </a>
           </td>
           <td class="r">
            Room 7
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thpi4t7" title="Click to go to the Program at a Glance">
             <b>
              Perception II (Grasping and Manipulation)
             </b>
            </a>
           </td>
           <td class="r">
            Teaser Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#246632" title="Click to go to the Author Index">
             Lee, Chun-Yi
            </a>
           </td>
           <td class="r">
            National Taiwan University
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#103866" title="Click to go to the Author Index">
             Doulgeri, Zoe
            </a>
           </td>
           <td class="r">
            Aristotle University of Thessaloniki
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t7_01">
             09:00-10:00, Paper ThPI4T7.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('187'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Precise Pick-And-Place Using Score-Based Diffusion Networks
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#389263" title="Click to go to the Author Index">
             Guo, Shih-Wei
            </a>
           </td>
           <td class="r">
            National Tsing Hua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#322568" title="Click to go to the Author Index">
             Hsiao, Tsu-Ching
            </a>
           </td>
           <td class="r">
            National Tsing Hua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#389479" title="Click to go to the Author Index">
             Liu, Yu-Lun
            </a>
           </td>
           <td class="r">
            National Yang Ming Chiao Tung University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#246632" title="Click to go to the Author Index">
             Lee, Chun-Yi
            </a>
           </td>
           <td class="r">
            National Tsing Hua University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab187" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper, we propose a novel coarse-to-fine continuous pose diffusion method to enhance the precision of pick-and-place operations within robotic manipulation tasks. Leveraging the capabilities of diffusion networks, we facilitate the accurate perception of object poses. This accurate perception enhances both pick-and-place success rates and overall manipulation precision. Our methodology utilizes a top-down RGB image projected from an RGB-D camera and adopts a coarse-to-fine architecture. This architecture enables efficient learning of coarse and fine models. A distinguishing feature of our approach is its focus on continuous pose estimation, which enables more precise object manipulation, particularly concerning rotational angles. In addition, we employ pose and color augmentation techniques to enable effective training with limited data. Through extensive experiments in simulated and real-world scenarios, as well as an ablation study, we comprehensively evaluate our proposed methodology. Taken together, the findings validate its effectiveness in achieving high-precision pick-and-place tasks.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t7_02">
             09:00-10:00, Paper ThPI4T7.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('273'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Language-Driven Grasp Detection with Mask-Guided Attention
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#372973" title="Click to go to the Author Index">
             Van Vo, Tuan
            </a>
           </td>
           <td class="r">
            FPT Software
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#196876" title="Click to go to the Author Index">
             Vu, Minh Nhat
            </a>
           </td>
           <td class="r">
            TU Wien, Austria
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#234039" title="Click to go to the Author Index">
             Huang, Baoru
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#350994" title="Click to go to the Author Index">
             Vuong, An Dinh
            </a>
           </td>
           <td class="r">
            MBZUAI
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#351637" title="Click to go to the Author Index">
             Le, Ngan
            </a>
           </td>
           <td class="r">
            University of Arkansas
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#351899" title="Click to go to the Author Index">
             Vo, Thieu
            </a>
           </td>
           <td class="r">
            Ton Duc Thang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#171078" title="Click to go to the Author Index">
             Nguyen, Anh
            </a>
           </td>
           <td class="r">
            University of Liverpool
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab273" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Grasp detection is an essential task in robotics with various industrial applications. However, traditional methods often struggle with occlusions and do not utilize language for grasping. Incorporating natural language into grasp detection remains a challenging task and largely unexplored. To address this gap, we propose a new method for language-driven grasp detection with mask-guided attention by utilizing the transformer attention mechanism with semantic segmentation features. Our approach integrates visual data, segmentation mask features, and natural language instructions, significantly improving grasp detection accuracy. Our work introduces a new framework for language-driven grasp detection, paving the way for language-driven robotic applications. Intensive experiments show that our method outperforms other recent baselines by a clear margin, with a 10.0% success score improvement. We further validate our method in real-world robotic experiments, confirming the effectiveness of our approach.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t7_03">
             09:00-10:00, Paper ThPI4T7.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('811'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              HyperTaxel: Hyper-Resolution for Taxel-Based Tactile Signals through Contrastive Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#324814" title="Click to go to the Author Index">
             Li, Hongyu
            </a>
           </td>
           <td class="r">
            Brown University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#300098" title="Click to go to the Author Index">
             Dikhale, Snehal
            </a>
           </td>
           <td class="r">
            Honda Research Institute USA
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#236529" title="Click to go to the Author Index">
             Cui, Jinda
            </a>
           </td>
           <td class="r">
            Honda Research Institute USA, Inc
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#124170" title="Click to go to the Author Index">
             Iba, Soshi
            </a>
           </td>
           <td class="r">
            Honda Research Institute USA
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#119438" title="Click to go to the Author Index">
             Jamali, Nawid
            </a>
           </td>
           <td class="r">
            Honda Research Institute USA, Inc
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab811" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#force_and_tactile_sensing" title="Click to go to the Keyword Index">
               Force and Tactile Sensing
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#representation_learning" title="Click to go to the Keyword Index">
               Representation Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             To achieve dexterity comparable to that of humans, robots must intelligently process tactile sensor data. Taxel-based tactile signals often have low spatial-resolution, with non-standardized representations. In this paper, we propose a novel framework, HyperTaxel, for learning a geometrically-informed representation of taxel-based tactile signals to address challenges associated with their spatial resolution. We use this representation and a contrastive learning objective to encode and map sparse low-resolution taxel signals to high-resolution contact surfaces. To address the uncertainty inherent in these signals, we leverage joint probability distributions across multiple simultaneous contacts to improve taxel hyper-resolution. We evaluate our representation by comparing it with two baselines and present results that suggest our representation outperforms the baselines. Furthermore, we present qualitative results that demonstrate the learned representation captures the geometric features of the contact surface, such as flatness, curvature, and edges, and generalizes across different objects and sensor configurations. Moreover, we present results that suggest our representation improves the performance of various downstream tasks, such as surface classification, 6D in-hand pose estimation, and sim-to-real transfer.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t7_04">
             09:00-10:00, Paper ThPI4T7.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('850'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              OVGNet: An Unified Visual-Linguistic Framework for Open-Vocabulary Robotic Grasping
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#394226" title="Click to go to the Author Index">
             Li, Meng
            </a>
           </td>
           <td class="r">
            Beihang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#394246" title="Click to go to the Author Index">
             Zhao, Qi
            </a>
           </td>
           <td class="r">
            Beihang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#394239" title="Click to go to the Author Index">
             Lyu, Shuchang
            </a>
           </td>
           <td class="r">
            Beihang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#394244" title="Click to go to the Author Index">
             Wang, Chunlei
            </a>
           </td>
           <td class="r">
            School of Electronic and Information Engineering, Beihang Univer
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#394236" title="Click to go to the Author Index">
             Ma, Yujing
            </a>
           </td>
           <td class="r">
            SenseTime
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#394250" title="Click to go to the Author Index">
             Cheng, Guangliang
            </a>
           </td>
           <td class="r">
            University of Liverpool
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#107047" title="Click to go to the Author Index">
             Yang, Chenguang
            </a>
           </td>
           <td class="r">
            University of Liverpool
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab850" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#data_sets_for_robotic_vision" title="Click to go to the Keyword Index">
               Data Sets for Robotic Vision
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Recognizing and grasping novel-category objects remains a crucial yet challenging problem in real-world robotic applications. Despite its significance, limited research has been conducted in this specific domain. To address this, we seamlessly propose a novel framework that integrates open-vocabulary learning into the domain of robotic grasping, empowering robots with the capability to adeptly handle novel objects. Our contributions are threefold. Firstly, we present a large-scale benchmark dataset specifically tailored for evaluating the performance of open-vocabulary grasping tasks. Secondly, we propose a unified visual-linguistic framework that serves as a guide for robots in successfully grasping both base and novel objects. Thirdly, we introduce two alignment modules designed to enhance visual-linguistic perception in the robotic grasping process. Extensive experiments validate the efficacy and utility of our approach. Notably, our framework achieves an average accuracy of 71.2% and 64.4% on base and novel categories in our new dataset, respectively. Our code and dataset are available at https://github.com/cv516Buaa/OVGNet.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t7_05">
             09:00-10:00, Paper ThPI4T7.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1479'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Raising Body Ownership in End-To-End Visuomotor Policy Learning Via Robot-Centric Pooling
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#218333" title="Click to go to the Author Index">
             Zhuang, Zheyu
            </a>
           </td>
           <td class="r">
            KTH Royal Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#105044" title="Click to go to the Author Index">
             Kyrki, Ville
            </a>
           </td>
           <td class="r">
            Aalto University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#101716" title="Click to go to the Author Index">
             Kragic, Danica
            </a>
           </td>
           <td class="r">
            KTH
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1479" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#visual_servoing" title="Click to go to the Keyword Index">
               Visual Servoing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present Robot-centric Pooling (RcP), a novel pooling method designed to enhance end-to-end visuomotor policies by enabling differentiation between the robots and similar entities or their surroundings. Given an image-proprioception pair, RcP guides the aggregation of image features by highlighting image regions correlating with the robot's proprioceptive states, thereby extracting robot-centric image representations for policy learning. Leveraging contrastive learning techniques, RcP integrates seamlessly with existing visuomotor policy learning frameworks and is trained jointly with the policy using the same dataset, requiring no extra data collection involving self-distractors. We evaluate the proposed method with reaching tasks in both simulated and real-world settings. The results demonstrate that RcP significantly enhances the policies' robustness against various unseen distractors, including self-distractors, positioned at different locations. Additionally, the inherent robot-centric characteristic of RcP enables the learnt policy to be far more resilient to pixel shifts compared to the baselines.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t7_06">
             09:00-10:00, Paper ThPI4T7.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1603'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Direct Semi-Exhaustive Search Method for Robust, Partial-To-Full Point Cloud Registration
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#182088" title="Click to go to the Author Index">
             Cheng, Richard
            </a>
           </td>
           <td class="r">
            California Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#128000" title="Click to go to the Author Index">
             Papazov, Chavdar
            </a>
           </td>
           <td class="r">
            Technische Universitaet Muenchen
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#104550" title="Click to go to the Author Index">
             Helmick, Daniel
            </a>
           </td>
           <td class="r">
            Toyota Research Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#309838" title="Click to go to the Author Index">
             Tjersland, Mark
            </a>
           </td>
           <td class="r">
            Toyota Research Institute
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1603" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#visual_tracking" title="Click to go to the Keyword Index">
               Visual Tracking
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Point cloud registration refers to the problem of finding the rigid transformation that aligns two given point clouds, and is crucial for many applications in robotics and computer vision. The main insight of this paper is that we can directly optimize the point cloud registration problem without correspondences by utilizing an algorithmically simple, yet computationally complex, semi-exhaustive search approach that is very well-suited for parallelization on modern GPUs. Our proposed algorithm, Direct Semi-Exhaustive Search (DSES), iterates over potential rotation matrices and efficiently computes the inlier-maximizing translation associated with each rotation. It then computes the optimal rigid transformation based on any desired distance metric by directly computing the error associated with each transformation candidate. By leveraging the parallelism of modern GPUs, DSES outperforms state-of-the-art methods for partial-to-full point cloud registration on the simulated ModelNet40 benchmark and demonstrates high performance and robustness for pose estimation on a real-world robotics problem.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t7_07">
             09:00-10:00, Paper ThPI4T7.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1721'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              3D Affordance Keypoint Detection for Robotic Manipulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#321397" title="Click to go to the Author Index">
             Liu, Zhiyang
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#367101" title="Click to go to the Author Index">
             Zhao, Ruiteng
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#310091" title="Click to go to the Author Index">
             Zhou, Lei
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#355146" title="Click to go to the Author Index">
             Yuan, Chengran
            </a>
           </td>
           <td class="r">
            National Universtiy of Singapore
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#364556" title="Click to go to the Author Index">
             Wu, Yuwei
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#289094" title="Click to go to the Author Index">
             Guo, Sheng
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#367044" title="Click to go to the Author Index">
             Zhang, Zhengshen
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#387280" title="Click to go to the Author Index">
             Liu, Chenchen
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#100760" title="Click to go to the Author Index">
             Ang Jr, Marcelo H
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#125362" title="Click to go to the Author Index">
             Tay, Francis
            </a>
           </td>
           <td class="r">
            NUS
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1721" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a novel approach for affordance-informed robotic manipulation by introducing 3D keypoints to enhance the understanding of object parts' functionality. The proposed approach provides direct information about textit{what} the potential use of objects is, as well as guidance on textit{where} and textit{how} a manipulator should engage, whereas conventional methods treat affordance detection as a semantic segmentation task, focusing solely on answering the textit{what} question. To address this gap, we propose a Fusion-based Affordance Keypoint Network (FAKP-Net) by introducing 3D keypoint quadruplet that harnesses the synergistic potential of RGB and Depth image to provide information on execution position, direction, and extent. Benchmark testing demonstrates that FAKP-Net outperforms existing models by significant margins in affordance segmentation task and keypoint detection task. Real-world experiments also showcase the reliability of our method in accomplishing manipulation tasks with previously unseen objects. Our source code and video demo will be public.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t7_08">
             09:00-10:00, Paper ThPI4T7.8
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1773'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SAID-NeRF: Segmentation-AIDed NeRF for Depth Completion of Transparent Objects
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#277693" title="Click to go to the Author Index">
             Ummadisingu, Avinash
            </a>
           </td>
           <td class="r">
            Preferred Networks, Inc
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#372793" title="Click to go to the Author Index">
             Choi, Jongkeum
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#347163" title="Click to go to the Author Index">
             Yamane, Koki
            </a>
           </td>
           <td class="r">
            University of Tsukuba
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#232615" title="Click to go to the Author Index">
             Masuda, Shimpei
            </a>
           </td>
           <td class="r">
            Preferred Networks, Inc / University of Tsukuba
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#137744" title="Click to go to the Author Index">
             Fukaya, Naoki
            </a>
           </td>
           <td class="r">
            Preferred Networks, Inc
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#173446" title="Click to go to the Author Index">
             Takahashi, Kuniyuki
            </a>
           </td>
           <td class="r">
            Preferred Networks, Inc
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1773" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Acquiring accurate depth information of transparent objects using off-the-shelf RGB-D cameras is a well-known challenge in Computer Vision and Robotics. Depth estimation/completion methods are typically employed and trained on datasets with quality depth labels acquired from either simulation, additional sensors or specialized data collection setups and known 3d models. However, acquiring reliable depth information for datasets at scale is not straightforward, limiting training scalability and generalization. Neural Radiance Fields (NeRFs) are learning-free approaches and have demonstrated wide success in novel view synthesis and shape recovery. However, heuristics and controlled environments (lights, backgrounds, etc) are often required to accurately capture specular surfaces. In this paper, we propose using Visual Foundation Models (VFMs) for segmentation in a zero-shot, label-free way to guide the NeRF reconstruction process for these objects via the simultaneous reconstruction of semantic fields and extensions to increase robustness. Our proposed method Segmentation-AIDed NeRF (SAID-NeRF) shows significant performance on depth completion datasets for transparent objects and robotic grasping.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t7_09">
             09:00-10:00, Paper ThPI4T7.9
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1866'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Learning a Pre-Grasp Manipulation Policy to Effectively Retrieve a Target in Dense Clutter
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#236802" title="Click to go to the Author Index">
             Kiatos, Marios
            </a>
           </td>
           <td class="r">
            Aristotle University of Thessaloniki
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#237997" title="Click to go to the Author Index">
             Koutras, Leonidas
            </a>
           </td>
           <td class="r">
            Aristotle University of Thessaloniki
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#211057" title="Click to go to the Author Index">
             Sarantopoulos, Iason
            </a>
           </td>
           <td class="r">
            Aristotle University of Thessaloniki
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#103866" title="Click to go to the Author Index">
             Doulgeri, Zoe
            </a>
           </td>
           <td class="r">
            Aristotle University of Thessaloniki
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1866" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#grasping" title="Click to go to the Keyword Index">
               Grasping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robotic grasping of a target object in cluttered environments poses considerable challenges, often due to limited collision-free grasp affordances caused by the close proximity of other objects. To overcome this limitation, non-prehensile actions like pushing can be strategically employed to manipulate the environment and improve the chances of successful grasps. In this paper, we introduce a novel pre-grasp manipulation policy designed to efficiently retrieve a target object from dense clutter by leveraging pushing actions and considering the gripper's kinematic capabilities to strategically position the target object within the gripper's closing region for a secure grasp. Unlike conventional approaches, our policy incorporates sequential pushing, allowing the robot to make decisions while within the camera's field of view without retracting to a home position, leading to significantly reduced execution time per action. Our policy, trained in simulation, seamlessly transfers to real-world scenarios. Extensive experimental evaluation demonstrates superior performance, faster completion times, and robust generalization to unseen objects compared to existing baselines.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t7_10">
             09:00-10:00, Paper ThPI4T7.10
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2090'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Active Pose Refinement for Textureless Shiny Objects Using the Structured Light Camera
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#286296" title="Click to go to the Author Index">
             Yang, Jun
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#368161" title="Click to go to the Author Index">
             Yao, Jian
            </a>
           </td>
           <td class="r">
            Epson Canada
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#115221" title="Click to go to the Author Index">
             Waslander, Steven Lake
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2090" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             6D pose estimation of textureless shiny objects has become an essential problem in many robotic applications. Many pose estimators require high-quality depth data, often measured by structured light cameras. However, when objects have shiny surfaces (e.g., metal parts), these cameras fail to sense complete depths from a single viewpoint due to the specular reflection, resulting in a significant drop in the final pose accuracy. To mitigate this issue, we present a complete active vision framework for 6D object pose refinement and next-best-view prediction. Specifically, we first develop an optimization-based pose refinement module for the structured light camera. Our system then selects the next best camera viewpoint to collect depth measurements by minimizing the predicted uncertainty of the object pose. Compared to previous approaches, we additionally predict measurement uncertainties of future viewpoints by online rendering, which significantly improves the next-best-view prediction performance. We test our method on the real-world ROBI dataset. The results show that our pose refinement module outperforms the traditional ICP-based approach when given the same input depth data, and our next-best-view strategy can achieve high object pose accuracy with significantly fewer viewpoints than the heuristic-based policies.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t7_11">
             09:00-10:00, Paper ThPI4T7.11
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2235'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Potential Field-Based Online Path Planning for Robust Cable Routing
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#291027" title="Click to go to the Author Index">
             Monguzzi, Andrea
            </a>
           </td>
           <td class="r">
            Leonardo, Innovation Labs
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392171" title="Click to go to the Author Index">
             Mantegna, Niccol
            </a>
           </td>
           <td class="r">
            Politecnico Di Milano
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#129139" title="Click to go to the Author Index">
             Zanchettin, Andrea Maria
            </a>
           </td>
           <td class="r">
            Politecnico Di Milano
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#10019" title="Click to go to the Author Index">
             Rocco, Paolo
            </a>
           </td>
           <td class="r">
            Politecnico Di Milano
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2235" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#dexterous_manipulation" title="Click to go to the Keyword Index">
               Dexterous Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#force_and_tactile_sensing" title="Click to go to the Keyword Index">
               Force and Tactile Sensing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper tackles the complex task of routing elastic deformable linear objects (DLOs) characterized by considerable stiffness, such as cables or hoses, which are already constrained at both ends. Specifically, a single arm robot is controlled to slide along the unknown contour of the cable, performing collision-free contour following, and to insert specific DLO segments into intermediate known clips. The contour following motion is executed avoiding both collisions with static obstacles and excessive deformation of the manipulated DLO. In particular, the path is defined considering an artificial potential field that is updated after each sliding motion along the DLO. This field accounts for static obstacles, the local cable shape (reconstructed using tactile sensors on the gripper fingertips) and the estimation of the global DLO shape obtained from a dynamic model of the DLO, accounting for the constraints imposed by the clips and the gripper. The proposed method is experimentally validated on an industrial robot executing cable routing in several DLO configurations.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t7_12">
             09:00-10:00, Paper ThPI4T7.12
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2412'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              DITTO: Demonstration Imitation by Trajectory Transformation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#324871" title="Click to go to the Author Index">
             Heppert, Nick
            </a>
           </td>
           <td class="r">
            University of Freiburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#150366" title="Click to go to the Author Index">
             Argus, Maximilian
            </a>
           </td>
           <td class="r">
            University of Freiburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#177085" title="Click to go to the Author Index">
             Welschehold, Tim
            </a>
           </td>
           <td class="r">
            Albert-Ludwigs-Universitt Freiburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#151845" title="Click to go to the Author Index">
             Brox, Thomas
            </a>
           </td>
           <td class="r">
            University of Freiburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#160427" title="Click to go to the Author Index">
             Valada, Abhinav
            </a>
           </td>
           <td class="r">
            University of Freiburg
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2412" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#manipulation_planning" title="Click to go to the Keyword Index">
               Manipulation Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Teaching robots new skills quickly and conveniently is crucial for the broader adoption of robotic systems. In this work, we address the problem of one-shot imitation from a single human demonstration, given by an RGB-D video recording. We propose a two-stage process. In the first stage we extract the demonstration trajectory offline. This entails segmenting manipulated objects and determining their relative motion in relation to secondary objects such as containers. In the online trajectory generation stage, we first re-detect all objects, then warp the demonstration trajectory to the current scene and execute it on the robot. To complete these steps, our method leverages several ancillary models, including those for segmentation, relative object pose estimation, and grasp prediction. We systematically evaluate different combinations of correspondence and re-detection methods to validate our design decision across a diverse range of tasks. Specifically, we collect and quantitatively test on demonstrations of ten different tasks including pick-and-place tasks as well as articulated object manipulation. Finally, we perform extensive evaluations on a real robot system to demonstrate the effectiveness and utility of our approach in real-world scenarios. We make the code publicly available at http://ditto.cs.uni-freiburg.de.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t7_13">
             09:00-10:00, Paper ThPI4T7.13
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2518'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Estimating Perceptual Uncertainty to Predict Robust Motion Plans
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#336693" title="Click to go to the Author Index">
             Gupta, Arjun
            </a>
           </td>
           <td class="r">
            UIUC
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#378704" title="Click to go to the Author Index">
             Zhang, Michelle
            </a>
           </td>
           <td class="r">
            University of Illinois Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#192737" title="Click to go to the Author Index">
             Gupta, Saurabh
            </a>
           </td>
           <td class="r">
            UIUC
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2518" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mobile_manipulation" title="Click to go to the Keyword Index">
               Mobile Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             A typical sense-plan-act robotics pipeline is brittle due to the inherent inaccuracies in the output of the sensing module and the lack of awareness of the planning module to those inaccuracies. This paper develops a framework to predict uncertainty estimates for neural network-based vision models used for state estimation in robotics pipelines. Our uncertainty estimates are based directly on the image observation data and are explicitly trained to model the error distribution on a held-out calibration set. We also demonstrate how predicted uncertainties can be used to select robust control strategies. We conduct experiments on the mobile manipulation problem of articulating everyday objects (e.g. opening a cupboard) and demonstrate the quality of estimated uncertainty and its downstream impact on robustness of inferred control strategies.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t7_14">
             09:00-10:00, Paper ThPI4T7.14
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3302'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              ManipVQA: Injecting Robotic Affordance and Physically Grounded Information into Multi-Modal Large Language Models
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#373130" title="Click to go to the Author Index">
             Huang, Siyuan
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#372860" title="Click to go to the Author Index">
             Ponomarenko, Iaroslav
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#335119" title="Click to go to the Author Index">
             Jiang, Zhengkai
            </a>
           </td>
           <td class="r">
            Tencent
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#372289" title="Click to go to the Author Index">
             Li, Xiaoqi
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397844" title="Click to go to the Author Index">
             Hu, Xiaobin
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#339574" title="Click to go to the Author Index">
             Gao, Peng
            </a>
           </td>
           <td class="r">
            Shanghai AI Lab
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#221079" title="Click to go to the Author Index">
             Li, Hongsheng
            </a>
           </td>
           <td class="r">
            Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#280114" title="Click to go to the Author Index">
             Dong, Hao
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3302" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             While the integration of Multi-modal Large Lan- guage Models (MLLMs) with robotic systems has significantly improved robots ability to understand and execute natural language instructions, their performance in manipulation tasks remains limited due to a lack of robotics-specific knowledge. Conventional MLLMs are typically trained on generic image- text pairs, leaving them deficient in understanding affordances and physical concepts crucial for manipulation. To address this gap, we propose ManipVQA, a novel framework that infuses MLLMs with manipulation-centric knowledge through a Visual Question-Answering (VQA) format. This approach encompasses tool detection, affordance recognition, and a broader understanding of physical concepts. We curated a diverse dataset of images depicting interactive objects, to challenge robotic understanding in tool detection, affordance prediction, and physical concept comprehension. To effectively integrate this robotics-specific knowledge with the inherent vision-reasoning capabilities of MLLMs, we leverage a uni- fied VQA format and devise a fine-tuning strategy. This strategy preserves the original vision-reasoning abilities while incorporating the newly acquired robotic insights. Empirical evaluations conducted in robotic simulators and across various vision task benchmarks demonstrate the robust performance of ManipVQA. The code and dataset are publicly available at https://github.com/SiyuanHuang95/ManipVQA.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t7_15">
             09:00-10:00, Paper ThPI4T7.15
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3387'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              MV-ROPE: Multi-View Constraints for Robust Category-Level Object Pose and Size Estimation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#286411" title="Click to go to the Author Index">
             Yang, Jiaqi
            </a>
           </td>
           <td class="r">
            ShanghaiTech University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397890" title="Click to go to the Author Index">
             Chen, Yucong
            </a>
           </td>
           <td class="r">
            ShanghaiTech University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397384" title="Click to go to the Author Index">
             Meng, Xiangting
            </a>
           </td>
           <td class="r">
            ShanghaiTech University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398167" title="Click to go to the Author Index">
             Yan, Chenxin
            </a>
           </td>
           <td class="r">
            Shanghaitech University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#326075" title="Click to go to the Author Index">
             Li, Min
            </a>
           </td>
           <td class="r">
            ShanghaiTech University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#311188" title="Click to go to the Author Index">
             Cheng, Ran
            </a>
           </td>
           <td class="r">
            Midea Robozone
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#311183" title="Click to go to the Author Index">
             Lige, Liu
            </a>
           </td>
           <td class="r">
            Midea Group
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#311185" title="Click to go to the Author Index">
             Sun, Tao
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#123271" title="Click to go to the Author Index">
             Kneip, Laurent
            </a>
           </td>
           <td class="r">
            ShanghaiTech University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3387" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Recently there has been a growing interest in category-level object pose and size estimation, and prevailing methods commonly rely on single view RGB-D images. However, one disadvantage of such methods is that they require accurate depth maps which cannot be produced by consumer-grade sensors. Furthermore, many practical real-world situations involve a moving camera that continuously observes its surroundings, and the temporal information of the input video streams is simply overlooked by single-view methods. We propose a novel solution that makes use of RGB video streams. Our framework consists of three modules: a scale-aware monocular dense SLAM solution, a lightweight object pose predictor, and an object-level pose graph optimizer. The SLAM module utilizes a video stream and additional scale-sensitive readings to estimate camera poses and metric depth. The object pose predictor then generates canonical object representations from RGB images. The object pose is estimated through geometric registration of these canonical object representations with estimated object depth points. All per-view estimates finally undergo optimization within a pose graph, culminating in the output of robust and accurate canonical object poses. Our experimental results demonstrate that when utilizing public dataset sequences with high-quality depth information, the proposed method exhibits comparable performance to state-of-the-art RGB-D methods. We also collect and evaluate on new datasets containing depth maps of varying quality to further quantitatively benchmark the proposed method alongside previous RGB-D based methods. We demonstrate a significant advantage in scenarios where depth input is absent or the quality of depth sensing is limited.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t7_16">
             09:00-10:00, Paper ThPI4T7.16
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2903'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Interactive Learning of Physical Object Properties through Robot Manipulation and Database of Object Measurements
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396291" title="Click to go to the Author Index">
             Kruliak, Andrej
            </a>
           </td>
           <td class="r">
            Czech Technical University in Prague
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#386970" title="Click to go to the Author Index">
             Hartvich, Jiri
            </a>
           </td>
           <td class="r">
            Czech Technical University in Prague
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#296661" title="Click to go to the Author Index">
             Patni, Shubhan
            </a>
           </td>
           <td class="r">
            Ceske Vysoke Uceni Technicke V Praze, FEL
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#284752" title="Click to go to the Author Index">
             Rustler, Lukas
            </a>
           </td>
           <td class="r">
            Ceske Vysoke Uceni Technicke V Praze, FEL
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#214555" title="Click to go to the Author Index">
             Behrens, Jan Kristof
            </a>
           </td>
           <td class="r">
            Czech Technical University in Prague, CIIRC
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#143489" title="Click to go to the Author Index">
             Abu-Dakka, Fares
            </a>
           </td>
           <td class="r">
            New York University Abu Dhabi
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#111738" title="Click to go to the Author Index">
             Mikolajczyk, Krystian
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#105044" title="Click to go to the Author Index">
             Kyrki, Ville
            </a>
           </td>
           <td class="r">
            Aalto University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#114104" title="Click to go to the Author Index">
             Hoffmann, Matej
            </a>
           </td>
           <td class="r">
            Czech Technical University in Prague, Faculty of Electrical Engi
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2903" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#probabilistic_inference" title="Click to go to the Keyword Index">
               Probabilistic Inference
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#data_sets_for_robot_learning" title="Click to go to the Keyword Index">
               Data Sets for Robot Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This work presents a framework for automatically extracting physical object properties, such as material composition, mass, volume, and stiffness, through robot manipulation and a database of object measurements. The framework involves exploratory action selection to maximize learning about objects on a table. A Bayesian network models conditional dependencies between object properties, incorporating prior probability distributions and uncertainty associated with measurement actions. The algorithm selects optimal exploratory actions based on expected information gain and updates object properties through Bayesian inference. Experimental evaluation demonstrates effective action selection compared to a baseline and correct termination of the experiments if there is nothing more to be learned. The algorithm proved to behave intelligently when presented with trick objects with material properties in conflict with their appearance. The robot pipeline integrates with a logging module and an online database of objects, containing over 24000 measurements of 63 objects with different grippers. All code and data are publicly available, facilitating automatic digitization of objects and their physical properties through exploratory manipulations.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thpi4t8">
             <b>
              ThPI4T8
             </b>
            </a>
           </td>
           <td class="r">
            Room 8
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thpi4t8" title="Click to go to the Program at a Glance">
             <b>
              Robot Motion Planning III
             </b>
            </a>
           </td>
           <td class="r">
            Teaser Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#104011" title="Click to go to the Author Index">
             Choset, Howie
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t8_01">
             09:00-10:00, Paper ThPI4T8.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2616'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Semantic Belief Behavior Graph: Enabling Autonomous Robot Inspection in Unknown Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#278868" title="Click to go to the Author Index">
             Ginting, Muhammad Fadhil
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#221454" title="Click to go to the Author Index">
             Fan, David D
            </a>
           </td>
           <td class="r">
            NASA Jet Propulsion Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#118338" title="Click to go to the Author Index">
             Kim, Sung-Kyun
            </a>
           </td>
           <td class="r">
            NASA Jet Propulsion Laboratory, Caltech
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#170846" title="Click to go to the Author Index">
             Kochenderfer, Mykel
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#114852" title="Click to go to the Author Index">
             Agha-mohammadi, Ali-akbar
            </a>
           </td>
           <td class="r">
            NASA-JPL, Caltech
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2616" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#surveillance_robotic_systems" title="Click to go to the Keyword Index">
               Surveillance Robotic Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper addresses the problem of autonomous robotic inspection in complex and unknown environments. This capability is crucial for efficient and precise inspections in various real-world scenarios, even when faced with perceptual uncertainty and lack of prior knowledge of the environment. Existing methods for real-world autonomous inspections typically rely on predefined targets and waypoints and often fail to adapt to dynamic or unknown settings. In this paper, we introduce the Semantic Belief Behavior Graph (SB2G) framework as a new approach to semantic-aware autonomous robot inspection. SB2G generates a control policy for the robot, using behavior nodes that encapsulate various semantic-based policies designed for inspecting different classes of objects. We design an active semantic search behavior to guide the robot in locating objects for inspection while reducing semantic information uncertainty. The edges in the SB2G encode transitions between these behaviors. We validate our approach through simulation and real-world urban inspections using a legged robotic platform. Our results show that SB2G enables a more efficient object inspection policy, exhibiting similar behaviors comparable to human-operated inspections.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t8_02">
             09:00-10:00, Paper ThPI4T8.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2696'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              GESCE: Graph-Based Ergodic Search in Cluttered Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397989" title="Click to go to the Author Index">
             Shirose, Burhanuddin
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397992" title="Click to go to the Author Index">
             Johnson, Adam
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#239286" title="Click to go to the Author Index">
             Vundurthy, Bhaskar
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#104011" title="Click to go to the Author Index">
             Choset, Howie
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#119727" title="Click to go to the Author Index">
             Travers, Matthew
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2696" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#autonomous_vehicle_navigation" title="Click to go to the Keyword Index">
               Autonomous Vehicle Navigation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper, we present a novel motion planning algorithm that inherits the strengths of both optimization and search-based planners. Optimization-based planners use the gradient of an objective function to generate a desired path, whereas search-based planners operate on a graph capturing the salient topology of a robots free space. A class of optimization-based planners leverages prior information, modeled as a probability distribution of target locations in an environment, to guide path generation. We embrace one specific measure, referred to as ergodicity, which encourages a robot to spend a proportion of its time, weighted by the distribution, where it is likely to find targets of interest. Methods that minimize ergodicity were not designed to handle obstacles in the environment, and augmented approaches that add soft constraints for obstacles to the cost function may still yield a path that collides with an obstacle. In this work, we present a hybrid approach that first generates a graph of the environments free space, followed by searching the graph with ergodicity as a heuristic. Our approach not only restricts the search to the free space, thereby avoiding obstacles by design, but also generates trajectories with low ergodicity values. Extensive testing on 125 test scenarios with varying degrees of clutter, information distribution, and robot start locations illustrate the efficacy of our algorithm.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t8_03">
             09:00-10:00, Paper ThPI4T8.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2712'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              PAAMP: Polytopic Action-Set and Motion Planning for Long Horizon Dynamic Motion Planning Via Mixed Integer Linear Programming
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398553" title="Click to go to the Author Index">
             Jaitly, Akshay
            </a>
           </td>
           <td class="r">
            Worcester Polytechnic Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#160893" title="Click to go to the Author Index">
             Farzan, Siavash
            </a>
           </td>
           <td class="r">
            California Polytechnic State University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2712" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#task_and_motion_planning" title="Click to go to the Keyword Index">
               Task and Motion Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Optimization methods for long-horizon, dynamically feasible motion planning in robotics tackle challenging non-convex and discontinuous optimization problems. Traditional methods often falter due to the nonlinear characteristics of these problems. We introduce a technique that utilizes learned representations of the system, known as Polytopic Action Sets, to efficiently compute long-horizon trajectories. By employing a suitable sequence of Polytopic Action Sets, we transform the long-horizon dynamically feasible motion planning problem into a Linear Program. This reformulation enables us to address motion planning as a Mixed Integer Linear Program (MILP). We demonstrate the effectiveness of a Polytopic Action-Set and Motion Planning (PAAMP) approach by identifying swing-up motions for a torque-constrained pendulum as fast as 0.75 milliseconds. This approach is well-suited for solving complex motion planning and long-horizon Constraint Satisfaction Problems (CSPs) in dynamic and underactuated systems such as legged and aerial robots.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t8_04">
             09:00-10:00, Paper ThPI4T8.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2732'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              VLPG-Nav: Object Navigation Using Visual Language Pose Graph and Object Localization Probability Maps
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#243362" title="Click to go to the Author Index">
             Arul, Senthil Hariharan
            </a>
           </td>
           <td class="r">
            University of Maryland, College Park
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#337702" title="Click to go to the Author Index">
             Kumar, Dhruva
            </a>
           </td>
           <td class="r">
            Amazon Lab126
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#394872" title="Click to go to the Author Index">
             Sugirtharaj, Vivek
            </a>
           </td>
           <td class="r">
            Amazon
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#338092" title="Click to go to the Author Index">
             Kim, Richard
            </a>
           </td>
           <td class="r">
            Amazon, Lab126
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#319519" title="Click to go to the Author Index">
             Qi, Xuewei
            </a>
           </td>
           <td class="r">
            Toyota Research Labs
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#336692" title="Click to go to the Author Index">
             Madhivanan, Rajasimman
            </a>
           </td>
           <td class="r">
            Amazon.com
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#336589" title="Click to go to the Author Index">
             Sen, Arnab
            </a>
           </td>
           <td class="r">
            Amazon
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#106235" title="Click to go to the Author Index">
             Manocha, Dinesh
            </a>
           </td>
           <td class="r">
            University of Maryland
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2732" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#reactive_and_sensor_based_planning" title="Click to go to the Keyword Index">
               Reactive and Sensor-Based Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present VLPG-Nav, a visual language navigation method for guiding robots to specified objects within household scenes. Unlike existing methods primarily focused on navigating the robot toward objects, our approach considers the additional challenge of centering the object within the robot's camera view. Our method builds a visual language pose graph (VLPG) that functions as a spatial map of VL embeddings. Given an open-vocabulary object query, we plan a viewpoint for object navigation using the VLPG. Despite navigating to the viewpoint, real-world challenges such as object occlusion, displacement, and the robot's localization errors can prevent visibility. We build an object localization probability map that leverages the robot's current observations and prior VLPG. When the object is not visible, the probability map is updated, and an alternate viewpoint is computed. In addition, we propose an object-centering formulation that locally adjusts the robot's pose to center the object in the camera view. We evaluate the effectiveness of our approach through simulations and real-world experiments, evaluating its ability to successfully view and center the object within the camera's field of view. VLPG-Nav demonstrates improved performance in locating the object, navigating around occlusions, and centering the object within the robot's camera view, outperforming selected baselines in the evaluation settings.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t8_05">
             09:00-10:00, Paper ThPI4T8.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2794'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Valuing Attrition in a Fleet of Robots Used As Path-Based Sensors for Gathering Information in a Communications Restricted Environment
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#252693" title="Click to go to the Author Index">
             McGuire, Loy
            </a>
           </td>
           <td class="r">
            U.S. Naval Research Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#109575" title="Click to go to the Author Index">
             Otte, Michael W.
            </a>
           </td>
           <td class="r">
            University of Maryland
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#107961" title="Click to go to the Author Index">
             Sofge, Donald
            </a>
           </td>
           <td class="r">
            Naval Research Laboratory
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2794" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper we propose a new algorithm for robots searching a hazardous, communications-denied area to gather information using a robot fleet that has a limited number of agents. The centralized algorithm uses robot survival along search paths as a sensor event for a distributed sensor network. As agents are lost to hazards, the search behavior adjusts to prioritize agent longevity in order to maximize information gain. In the past, related work solving this problem has assumed an infinite number of agents. In contrast, we assume that the number of agents is finite. We use Bayesian inference to update target and hazard belief maps of an area using data from the probability of survival of prior agents' paths as well as sensor readings from the agents along those paths. Using those belief maps, the algorithm can construct paths that maximize information gain, in expectation, while taking into account the predicted decrease in future information collected when losing an agent. This behavior increases the likelihood that agents survive longer, allowing them to collect more data.
             <p>
              Using simulations with various fleet sizes and probabilities for hazards disabling agents, we compare our algorithm to work that does not account for attrition. The results show an increase in the longevity of the fleet when hazards are more effective at disabling agents. In nearly all cases, this contributes to an increased rate in information gain when the fleet size is small. Small sized fleets, in our case 10 or less agents, do not meet a threshold of collected information necessary to direct agents away from hazards. Large fleets, over 200 agents in our scenario, collect most of the information before Our algorithm causes a noticeable change in agent behavior (as compared to existing techniques). We find that the proposed method provides the greatest advantage for mid-sized fleets, between 20 and 100 agents, and when hazards have an increased probability of immobilizing agents.
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t8_06">
             09:00-10:00, Paper ThPI4T8.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2831'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              An Efficient Coverage Method for Irregularly Shaped Terrains
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397985" title="Click to go to the Author Index">
             Tang, Yuxuan
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#368274" title="Click to go to the Author Index">
             Wu, Qizhen
            </a>
           </td>
           <td class="r">
            Beihang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398616" title="Click to go to the Author Index">
             Zhu, Chunli
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#372145" title="Click to go to the Author Index">
             Chen, Lei
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2831" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In mobile robotics, effectively covering known terrains is essential. While grid-based methods surpass exact cell decomposition in path length and multi-robot scalability, they face challenges in irregular areas. Here we develop a model for shortening coverage paths in arbitrary environments using grid-based methods, which redefines the path optimization problem as finding the largest Hamiltonian sub-graph of a given grid graph. Additionally, we present a Hamiltonian cycle expansion strategy to simplify the resolution process and propose a low-repetitive coverage path planner based on the strategy. Our path planner enables the quick finding of an efficient full coverage path in any region. Simulation results show that our algorithm consistently produces efficient coverage paths across diverse settings and demonstrates its adaptability in multi-robot systems.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t8_07">
             09:00-10:00, Paper ThPI4T8.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2899'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              HP3: Hierarchical Prediction-Pretrained Planning for Unprotected Left Turn
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#372205" title="Click to go to the Author Index">
             Ou, Zhihao
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#371787" title="Click to go to the Author Index">
             Wang, Zhibo
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#372266" title="Click to go to the Author Index">
             Hua, Yue
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#372991" title="Click to go to the Author Index">
             Dou, Jinsheng
            </a>
           </td>
           <td class="r">
            Mogo Auto Intelligence and Telematics Information Technology Co
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#373066" title="Click to go to the Author Index">
             Feng, Di
            </a>
           </td>
           <td class="r">
            Mogo Auto Intelligence and Telematics Information Technology Co
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#276122" title="Click to go to the Author Index">
             Pu, Jian
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2899" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Trajectory planning for unprotected left turns poses a significant challenge in autonomous driving. Reinforcement learning (RL) offers potential, but existing methods often rely on scenario-specific state representations, limiting their adaptability. This paper introduces Hierarchical Prediction-Pretrained Planning (HP^textbf{3}), a generalizable hierarchical RL framework designed for unprotected left turns. HP^textbf{3} leverages historical trajectories of all vehicles and complete map information to achieve versatile state representation and generalizable scene understanding. Its two-layer architecture predicts semantic behavior (upper layer) and generates corresponding trajectories (lower layer). A scene encoder comprehends trajectories and roads, while a trajectory decoder outputs sequential points. To accelerate convergence, we pretrain the main network on a modified trajectory prediction dataset. Evaluation on a CARLA-based map with complex, unprotected left-turn intersections demonstrates HP^textbf{3}'s superiority over rule-based and simple RL-based methods, highlighting the effectiveness of our pretraining approach for this critical autonomous driving task.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t8_08">
             09:00-10:00, Paper ThPI4T8.8
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2910'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Geometry-Based Approach for Support-Free Additive Manufacturing of Structures with Large Overhang Angles and Closed Features
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#374484" title="Click to go to the Author Index">
             Liu, Jitian
            </a>
           </td>
           <td class="r">
            Johns Hopkins University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#378356" title="Click to go to the Author Index">
             Cohen, Zachary
            </a>
           </td>
           <td class="r">
            United States Naval Academy
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#202088" title="Click to go to the Author Index">
             Kim, Jin Seob
            </a>
           </td>
           <td class="r">
            Johns Hopkins University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#131802" title="Click to go to the Author Index">
             Armand, Mehran
            </a>
           </td>
           <td class="r">
            Johns Hopkins University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#131780" title="Click to go to the Author Index">
             Kutzer, Michael Dennis Mays
            </a>
           </td>
           <td class="r">
            United States Naval Academy
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2910" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#additive_manufacturing" title="Click to go to the Keyword Index">
               Additive Manufacturing
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#computational_geometry" title="Click to go to the Keyword Index">
               Computational Geometry
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Architected materials derive performance characteristics from material properties and internal geometry. These materials are increasingly prevalent across a wide variety of domains. Many intricate feature geometries associated with architected materials can be explored using additive manufacturing (AM) processes. However, current AM methods generally cannot fabricate geometries with completely closed voids without introducing a support structure. This paper describes a new, support-free approach to AM capable of creating structures with closed voids. This work limits part geometry to three-dimensional (3D) geometries defined by a revolution about a single axis. This limitation enables planar analysis within a three-degree-of-freedom (3-DoF) task space. Part geometry in 3-DoF task space is constrained to a convex arch. Task space geometry is divided into an ordered set of sub-regions, considering feasible deposition orientations and collision constraints. The use of 3-DoF task space provides planar translation and rotation of the component during fabrication. The introduction of this rotational DoF addresses AM overhang constraints imposed by gravity. Methods for generating, ordering, and layering sub-regions suitable for printing a part with a closed hole are presented. Layers derived in the 3-DoF task space analysis are then extended to 3D deposition paths using the axis of revolution defined by the original part. The method of hole closure relies on the concept of a "keystone" which requires a 45 nozzle offset for collision-free deposition within keystone-adjacent sub-regions. The feasibility of deposition using a 45 nozzle offset is explored experimentally, and results demonstrate feasibility.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t8_09">
             09:00-10:00, Paper ThPI4T8.9
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2953'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Adaptive Planning with Generative Models under Uncertainty
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398766" title="Click to go to the Author Index">
             Jutras-Dube, Pascal
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398789" title="Click to go to the Author Index">
             Zhang, Ruqi
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#167990" title="Click to go to the Author Index">
             Bera, Aniket
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2953" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Planning with generative models has emerged as an effective decision-making paradigm across a wide range of domains, including reinforcement learning and autonomous navigation. While continuous replanning at each timestep might seem intuitive because it allows decisions to be made based on the most recent environmental observations, it results in substantial computational challenges, primarily due to the complexity of the generative model's underlying deep learning architecture. Our work addresses this challenge by introducing a simple adaptive planning policy that leverages the generative model's ability to predict long-horizon state trajectories, enabling the execution of multiple actions consecutively without the need for immediate replanning. We propose to use the predictive uncertainty derived from a Deep Ensemble of inverse dynamics models to dynamically adjust the intervals between planning sessions. In our experiments conducted on locomotion tasks within the OpenAI Gym framework, we demonstrate that our adaptive planning policy allows for a reduction in replanning frequency to only about 10% of the steps without compromising the performance. Our results underscore the potential of generative modeling as an efficient and effective tool for decision-making.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t8_10">
             09:00-10:00, Paper ThPI4T8.10
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3037'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Model Predictive Trees: Sample-Efficient Receding Horizon Planning with Reusable Tree Search
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#271784" title="Click to go to the Author Index">
             Lathrop, John
            </a>
           </td>
           <td class="r">
            California Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#275618" title="Click to go to the Author Index">
             Riviere, Benjamin
            </a>
           </td>
           <td class="r">
            California Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#386764" title="Click to go to the Author Index">
             Alindogan, Jedidiah
            </a>
           </td>
           <td class="r">
            California Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#118282" title="Click to go to the Author Index">
             Chung, Soon-Jo
            </a>
           </td>
           <td class="r">
            Caltech
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3037" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#wheeled_robots" title="Click to go to the Keyword Index">
               Wheeled Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#manipulation_planning" title="Click to go to the Keyword Index">
               Manipulation Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present Model Predictive Trees (MPT), a receding horizon tree search algorithm that improves its performance by reusing information efficiently. Whereas existing solvers reuse only the highest-quality trajectory from the previous iteration as a hotstart, our method reuses the entire optimal subtree, enabling the search to be simultaneously guided away from the low-quality areas and towards the high-quality areas. We characterize the restrictions on tree reuse by analyzing the induced tracking error under time-varying dynamics, revealing a tradeoff between the search depth and the timescale of the changing dynamics. In numerical studies, our algorithm outperforms state-of-the-art sampling-based cross- entropy methods with hotstarting. We demonstrate our planner on an autonomous vehicle testbed performing a nonprehensile manipulation task: pushing a target object through an obstacle field. Code associated with this work will be made available at https://github.com/jplathrop/mpt.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t8_11">
             09:00-10:00, Paper ThPI4T8.11
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3066'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Practical Framework for Path Representation and Following Control in Mobile Industrial Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#348889" title="Click to go to the Author Index">
             Koh, Youngil
            </a>
           </td>
           <td class="r">
            Robot Center, Samsung Research, Samsung Electronics
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#390642" title="Click to go to the Author Index">
             Kim, WooJeong
            </a>
           </td>
           <td class="r">
            Samsung Electronics
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#354950" title="Click to go to the Author Index">
             Choi, MidEum
            </a>
           </td>
           <td class="r">
            Samsung Electronics
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3066" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#motion_control" title="Click to go to the Keyword Index">
               Motion Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#industrial_robots" title="Click to go to the Keyword Index">
               Industrial Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a practical framework for path representation and following control for mobile industrial robots. Mobile industrial robots perform various missions, such as navigating predefined routes and transporting cargo in industrial environments. Unlike traditional AGVs that rely on physical tapes installed densely along routes, our framework supports the generation of drivable paths by connecting waypoints specified by users. The proposed framework generates drivable work paths while considering the permissible deviation between waypoints and the path, as well as the maximum curvature, without nonlinear optimization techniques. Additionally, we introduce a "stopover attribute for each waypoint to enhance usability in narrow workspaces. Furthermore, we have developed a practical path following control system that takes into account real-world challenges such as actuation delay and computational efficiency. The proposed speed planner incorporates predictive information without relying on optimization techniques, and supports robust following performance by braking intervention if path deviation increases. The effectiveness of the proposed framework has been validated through robot tests, demonstrating accurate path following performance and efficient computation.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t8_12">
             09:00-10:00, Paper ThPI4T8.12
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3109'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              AMCO: Adaptive Multimodal Coupling of Vision and Proprioception for Quadruped Robot Navigation in Outdoor Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#367201" title="Click to go to the Author Index">
             Elnoor, Mohamed
            </a>
           </td>
           <td class="r">
            University of Maryland
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#308861" title="Click to go to the Author Index">
             Kulathun Mudiyanselage, Kasun Weerakoon
            </a>
           </td>
           <td class="r">
            University of Maryland, College Park
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#243363" title="Click to go to the Author Index">
             Sathyamoorthy, Adarsh Jagan
            </a>
           </td>
           <td class="r">
            University of Maryland
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#269556" title="Click to go to the Author Index">
             Guan, Tianrui
            </a>
           </td>
           <td class="r">
            University of Maryland
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396015" title="Click to go to the Author Index">
             Rajagopal, Vignesh
            </a>
           </td>
           <td class="r">
            University of Maryland, College Park
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#106235" title="Click to go to the Author Index">
             Manocha, Dinesh
            </a>
           </td>
           <td class="r">
            University of Maryland
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3109" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#perception_action_coupling" title="Click to go to the Keyword Index">
               Perception-Action Coupling
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present AMCO, a novel navigation method for quadruped robots that adaptively combines vision-based and proprioception-based perception capabilities. Our approach uses three cost maps: general knowledge map; traversability history map; and current proprioception map; which are derived from a robots vision and proprioception data, and couples them to obtain a traversability cost map for navigation. The general knowledge map encodes terrains semantically segmented from visual sensing, and represents a terrains typically expected traversability. The traversability history map encodes the robots recent proprioceptive measurements on a terrain and its semantic segmentation as a cost map. Further, the robots present proprioceptive measurement is encoded as a cost map in the current proprioception map. As the general knowledge map and traversability history map rely on semantic segmentation, we evaluate the reliability of the visual sensory data by estimating the brightness and motion blur of input RGB images and accordingly combine the three cost maps to obtain the coupled traversability cost map used for navigation. Leveraging this adaptive coupling, the robot can depend on the most reliable input modality available. Finally, we present a novel planner that selects appropriate gaits and velocities for traversing challenging outdoor environments using the coupled traversability cost map. We demonstrate AMCOs navigation performance in different real-world outdoor environments and observe 10.8%-34.9% reduction w.r.t. two stability metrics, and up to 50% improvement in terms of success rate compared to current navigation methods.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t8_13">
             09:00-10:00, Paper ThPI4T8.13
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3165'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              When, What, and with Whom to Communicate: Enhancing RL-Based Multi-Robot Navigation through Selective Communication
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#243362" title="Click to go to the Author Index">
             Arul, Senthil Hariharan
            </a>
           </td>
           <td class="r">
            University of Maryland, College Park
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#274898" title="Click to go to the Author Index">
             Bedi, Amrit Singh
            </a>
           </td>
           <td class="r">
            University of Maryland, College Park
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#106235" title="Click to go to the Author Index">
             Manocha, Dinesh
            </a>
           </td>
           <td class="r">
            University of Maryland
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3165" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#collision_avoidance" title="Click to go to the Keyword Index">
               Collision Avoidance
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#path_planning_for_multiple_mobile_robots_or_agents" title="Click to go to the Keyword Index">
               Path Planning for Multiple Mobile Robots or Agents
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Decentralized navigation methods rely primarily on local observations, lacking the global awareness needed to coordinate effectively within a multi-agent system. Exchanging relevant messages between agents can promote cooperation and improve navigation efficiency. We present an RL-based decentralized navigation approach that learns 'when,' 'what,' and 'with whom' to communicate for safe and cooperative navigation. Our method leverages a visual transformer and self-attention mechanism to encode the local occupancy map and the state information of neighbors into fixed-length encodings, thereby being able to handle an arbitrary number of neighbors for collision-free navigation. In addition, the network encodes the agent's state information and observations of neighboring agents into a concise message vector by learning what information is crucial to communicate, which is shared with neighboring agents upon request. Moreover, to avoid indiscriminate broadcasting, the network learns when and with whom to communicate and request message vectors. Subsequently, the messages communicated alongside the local information are used to guide navigation decisions. We evaluate our method against state-of-the-art baselines in complex scenarios, including narrow corridors and environments with multiple agents. We observe considerable improvements in terms of navigation performance, showing up to ~2x improvement in navigation success rates and a reduction of upto ~20% in path length.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t8_14">
             09:00-10:00, Paper ThPI4T8.14
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3241'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Context-Generative Default Policy for Bounded Rational Agent
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#245868" title="Click to go to the Author Index">
             Pushp, Durgakant
            </a>
           </td>
           <td class="r">
            Indiana University Bloomington
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#241274" title="Click to go to the Author Index">
             Xu, Junhong
            </a>
           </td>
           <td class="r">
            Indiana University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#246551" title="Click to go to the Author Index">
             Chen, Zheng
            </a>
           </td>
           <td class="r">
            Indiana University Bloomington
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#141677" title="Click to go to the Author Index">
             Liu, Lantao
            </a>
           </td>
           <td class="r">
            Indiana University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3241" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#motion_control" title="Click to go to the Keyword Index">
               Motion Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#cognitive_modeling" title="Click to go to the Keyword Index">
               Cognitive Modeling
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Bounded rational agents often make decisions by evaluating a finite selection of choices, typically derived from a reference point termed the default policy, based on previous experience. However, the inherent rigidity of the static default policy presents significant challenges for agents when operating in unknown environment, that are not included in agents prior knowledge. In this work, we introduce a context-generative default policy that leverages the region observed by the robot to predict unobserved map, thereby enabling the robot to adaptively adjust its default policy based on both the actual observed map and the imagined unobserved map. Furthermore, the adaptive nature of the bounded rationality framework enables the robot to manage unreliable or incorrect imaginations by selectively sampling a few trajectories in the vicinity of the default policy. Our approach utilizes a diffusion model for map prediction and a sampling-based planning with B-spline trajectory optimization to generate the default policy. Extensive evaluations reveal that the context-generative policy outperforms the baseline methods in identifying and avoiding unseen obstacles. Additionally, real-world experiments conducted with the Crazyflie drones demonstrate the adaptability of our proposed method, even when acting in environments outside the domain of the training distribution.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t8_15">
             09:00-10:00, Paper ThPI4T8.15
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3409'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Multi-Fidelity Reinforcement Learning for Minimum Energy Trajectory Planning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#399145" title="Click to go to the Author Index">
             de Castro, Luke
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#217953" title="Click to go to the Author Index">
             Ryou, Gilhyun
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#399178" title="Click to go to the Author Index">
             Ohn, Hyungseuk
            </a>
           </td>
           <td class="r">
            Hyundai Motor Company
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#124153" title="Click to go to the Author Index">
             Karaman, Sertac
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3409" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Modeling the energy consumption of a quadrotor involves complex electrical and physical dynamics, making it difficult to optimize. To address this challenge, this paper presents a multi-fidelity Gaussian process (MFGP) method that efficiently learns an accurate energy prediction model by combining many low-fidelity samples from a simple motor model with a few computationally expensive samples from a numerical battery simulation. We present extensive sample-efficiency experiments, demonstrating that a single-fidelity model often needs 10 times more high-fidelity data to match the accuracy achieved by the MFGP. The energy prediction model is then applied to a reinforcement learning (RL) agent, providing a reward signal to a minimum energy planning policy. The RL policy generates more energy efficient trajectories than those found by the minimum snap baseline method, achieving an average 3.6% energy reduction.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thpi4t9">
             <b>
              ThPI4T9
             </b>
            </a>
           </td>
           <td class="r">
            Room 9
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thpi4t9" title="Click to go to the Program at a Glance">
             <b>
              Navigation III
             </b>
            </a>
           </td>
           <td class="r">
            Teaser Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#107751" title="Click to go to the Author Index">
             Taniguchi, Tadahiro
            </a>
           </td>
           <td class="r">
            Ritsumeikan University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t9_01">
             09:00-10:00, Paper ThPI4T9.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2971'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              PathFormer: A Transformer-Based Framework for Vision-Centric Autonomous Navigation in Off-Road Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#333626" title="Click to go to the Author Index">
             Hassan, Bilal
            </a>
           </td>
           <td class="r">
            Khalifa University, Abu Dhabi
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#367945" title="Click to go to the Author Index">
             Abdel Madjid, Nadya
            </a>
           </td>
           <td class="r">
            Khalifa University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#367855" title="Click to go to the Author Index">
             Kashwani, Fatima
            </a>
           </td>
           <td class="r">
            Khalifa University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#355169" title="Click to go to the Author Index">
             Alansari, Mohamad
            </a>
           </td>
           <td class="r">
            Khalifa University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#267359" title="Click to go to the Author Index">
             Khonji, Majid
            </a>
           </td>
           <td class="r">
            Khalifa University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#101631" title="Click to go to the Author Index">
             Dias, Jorge
            </a>
           </td>
           <td class="r">
            Khalifa University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2971" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The efficient navigation of autonomous vehicles across rugged and unstructured terrains remains a significant challenge. Most existing research in this area emphasizes the need for complex mappings or intricate multi-step methodologies. However, these traditional approaches often struggle to adapt to dynamic changes in environmental conditions. In this paper, we introduce PathFormer, an end-to-end framework designed specifically to address these challenges. PathFormer utilizes transformers to decode free-space semantics and configurations directly from camera images, enabling efficient path planning without the reliance on detailed, pre-existing maps. The performance of PathFormer was rigorously evaluated across diverse datasets, where it demonstrated superior capabilities, outperforming other state-of-the-art methods by 3.68% in precisely segmenting free-space regions and showing a 13.65% improvement in correctly predicting traversable paths.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t9_02">
             09:00-10:00, Paper ThPI4T9.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('335'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Enhanced Language-Guided Robot Navigation with Panoramic Semantic Depth Perception and Cross-Modal Fusion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#331467" title="Click to go to the Author Index">
             Wang, Liuyi
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#372507" title="Click to go to the Author Index">
             Tang, Jiagui
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#331262" title="Click to go to the Author Index">
             He, Zongtao
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#372508" title="Click to go to the Author Index">
             Dang, Ronghao
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#134932" title="Click to go to the Author Index">
             Liu, Chengju
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#118155" title="Click to go to the Author Index">
             Chen, Qijun
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab335" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#multi_modal_perception_for_hri" title="Click to go to the Keyword Index">
               Multi-Modal Perception for HRI
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Integrating visual observation with linguistic instruction holds significant promise for enhancing robot navigation across unstructured environments and enriches the human-robot interaction experience. However, while panoramic RGB views furnish robots with extensive environmental visuals, current methods significantly overlook crucial semantic and depth cues. This incomplete representation may lead to misinterpretation or inadequate execution of language instructions, thereby impeding navigation performance and adaptability. In this paper, we introduce SEAT, a semantic-depth aware cross-modal transformer model. Our approach incorporates an efficient panoramic multi-type visual encoder to capture comprehensive environmental details. To mitigate the rigidity of feature mapping stemming from the freezing of pre-training encoders, we propose a novel region query pre-training task. Additionally, we leverage an improved dual-scale cross-modal transformer to facilitate the integration of instructions, topological memory, and action prediction. Extensive experiments on three language-guided robot navigation datasets demonstrate the efficacy of our model, achieving competitive navigation success rates with fewer parameters and computational load. Furthermore, we validate SEAT's effectiveness in real-world scenarios by deploying it on a mobile robot across various environments. The code is available at https://github.com/CrystalSixone/SEAT.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t9_03">
             09:00-10:00, Paper ThPI4T9.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('407'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SWIFT: Strategic Weather-Informed Image-Based Forecasting for Trajectories
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#234066" title="Click to go to the Author Index">
             Xia, Youya
            </a>
           </td>
           <td class="r">
            Cornell University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#313303" title="Click to go to the Author Index">
             Nino, Jose
            </a>
           </td>
           <td class="r">
            Cornell University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#234056" title="Click to go to the Author Index">
             Han, Yutao
            </a>
           </td>
           <td class="r">
            Cornell University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#102926" title="Click to go to the Author Index">
             Campbell, Mark
            </a>
           </td>
           <td class="r">
            Cornell University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab407" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#autonomous_agents" title="Click to go to the Keyword Index">
               Autonomous Agents
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Predicting agents trajectories in complex environ- ments is critical for achieving safe autonomous robot naviga- tion. Empirically, agents (e.g., vehicles, cyclists, pedestrians) decisions and preferences are susceptible to changes in envi- ronmental factors (e.g., interactions with other agents, weather conditions, traffic rules). State-of-the-art methods rely on High- Definition (HD) or semantic maps to model the environment, even when there are unpredictable factors such as complex weather conditions. However, since HD maps are nontrivial to obtain, those methods are limited in the scope of environments they can be applied in. We propose a more flexible graph based trajectory prediction model that uses only images to model the environment, without requiring expensive map information. We experimentally validate our proposed model, demonstrating robust performances in trajectory prediction compared to state of the art methods, and outperform in complex environments that cannot be modeled with purely map based methods, such as diverse weather conditions.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t9_04">
             09:00-10:00, Paper ThPI4T9.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('636'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Multiple Visual Features in Topological Map for Vision-And-Language Navigation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#354136" title="Click to go to the Author Index">
             Liu, Ruonan
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#354037" title="Click to go to the Author Index">
             Kong, Ping
            </a>
           </td>
           <td class="r">
            Tianjin University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#252706" title="Click to go to the Author Index">
             Zhang, Weidong
            </a>
           </td>
           <td class="r">
            Shanghai JiaoTong University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab636" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#agent_based_systems" title="Click to go to the Keyword Index">
               Agent-Based Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Vision-and-Language Navigation (VLN) in continuous environments aims to navigate robot agents in unseen environments following natural language instructions. The majority of existing approaches rely on constructing semantic maps or topological maps to record information. However, semantic maps overlook the detailed information of objects and the correspondence among views during navigation, while topological maps lack the spatial representation between entities. To address these limitations, we propose a novel visual feature representation method for continuous VLN, called Multiple Visual Features in Topological Map (MV-Topo). MV-Topo utilizes three distinct visual encoders to extract visual features, which are integrated in the dynamically generated topological map. These fused features actively participate in the subsequent cross-modal planning to derive a long-term path towards a subgoal, effectively guiding the agent to reach the final location. We experimentally demonstrate the effectiveness of our approach and achieve competitive results on the full VLN-CE test splits. Notably, our method outperforms the state-of-the-art by 3.5% in terms of the Navigation Error (NE) metric, indicating that the utilization of multiple visual features significantly enhances the agent's perception of semantic targets.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t9_05">
             09:00-10:00, Paper ThPI4T9.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('872'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              MG-VLN: Benchmarking Multi-Goal and Long-Horizon Vision-Language Navigation with Language Enhanced Memory Map
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#367646" title="Click to go to the Author Index">
             Zhang, Junbo
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#311337" title="Click to go to the Author Index">
             Ma, Kaisheng
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab872" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#ai_based_methods" title="Click to go to the Keyword Index">
               AI-Based Methods
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Vision-Language Navigation (VLN) with high-level language instructions is a crucial task in robotics. Existing VLN benchmarks, such as the REVERIE challenge which has single-goal instructions and limited navigation steps, do not fully encapsulate the complexity of real-world navigation that often require multi-objective and long-horizon navigation. To address this, we propose a new benchmark task: Multi-Goal and Long-Horizon Vision-Language Navigation (MG-VLN), extending the REVERIE benchmark to encompass multi-objective and long-horizon navigation scenarios with sequences of high-level instructions. This task aims to provide a simulation benchmark to guide the design of lifelong and long-horizon navigation robots. To initiate the exploration in this newly proposed task, we first investigate the role of long-term memory in improving navigation performance by leveraging environmental information gathered during previous sub-goals. Additionally, we examine the types of knowledge that most effectively enrich this long-term memory. Specifically, we integrate the visual contents with linguistic knowledge such as object categories, visual captions, and object attributes/relationships. Our findings indicate that: 1) the explicit long-term memory map significantly enhances navigation performance in multi-goal and long-horizon scenarios; 2) incorporating object attributes and relationships information is the most advantageous for aligning environmental cues with high-level instructions.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t9_06">
             09:00-10:00, Paper ThPI4T9.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1033'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Real-Time Birds-Eye-View Panoptic Segmentation for Monocular-Based Indoor Navigation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#242796" title="Click to go to the Author Index">
             Kim, Dawit
            </a>
           </td>
           <td class="r">
            NAVER LABS
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#189651" title="Click to go to the Author Index">
             Koo, Jungmo
            </a>
           </td>
           <td class="r">
            NAVER LABS
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#300636" title="Click to go to the Author Index">
             Yun, Jongseob
            </a>
           </td>
           <td class="r">
            NAVER LABS
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#115733" title="Click to go to the Author Index">
             Park, Soonyong
            </a>
           </td>
           <td class="r">
            NAVER LABS
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1033" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#data_sets_for_robotic_vision" title="Click to go to the Keyword Index">
               Data Sets for Robotic Vision
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Birds-Eye-View (BEV) segmentation is a essential technology for safe and efficient navigation. This is even more necessary in indoor driving, where there are dynamic and unstructured objects such as people and robot. However, since there is no way to generate training data, most of the researches have been conducted mainly in outdoor environments. In this paper, we propose an innovative approach to address this challenge. We automatically generate BEV training data for indoor environments based on the physics engine of a simulator. This eliminates the needs for tons of real data and virtual environments. We also propose a lightweight network architecture capable of BEV panoptic segmentation in real-time. Based on this network and simple image processing, our framework shows fast but also robust performance in real-world environments with no gap between simulation and reality. The network can inference at a speed of about 61 FPS on AGX Orin in FP16 mode. Furthermore, it outperforms existing algorithms in the semantic segmentation task, achieving 14% higher mIoU.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t9_07">
             09:00-10:00, Paper ThPI4T9.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1068'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Perception-Aware Full Body Trajectory Planning for Autonomous Systems Using Motion Primitives
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#344167" title="Click to go to the Author Index">
             Kuhne, Moritz
            </a>
           </td>
           <td class="r">
            German Aerospace Center (DLR)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#225197" title="Click to go to the Author Index">
             Giubilato, Riccardo
            </a>
           </td>
           <td class="r">
            German Aerospace Center (DLR)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#136979" title="Click to go to the Author Index">
             Schuster, Martin J.
            </a>
           </td>
           <td class="r">
            German Aerospace Center (DLR)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#104075" title="Click to go to the Author Index">
             Roa, Maximo A.
            </a>
           </td>
           <td class="r">
            German Aerospace Center (DLR)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1068" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#reactive_and_sensor_based_planning" title="Click to go to the Keyword Index">
               Reactive and Sensor-Based Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#space_robotics_and_automation" title="Click to go to the Keyword Index">
               Space Robotics and Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Many robotic systems rely on visual sensing to accomplish simultaneously the tasks of state estimation, mapping, and path planning. One one hand, the usage of camera sensors represents a power-efficient and lightweight option for solving this problem. On the other hand, these tasks pose requirements on the quality of the visual input (e.g. number of tracked features for Visual Odometry) that are often in contrast to the optimal viewpoint planning for local mapping and obstacle avoidance. Dealing with this constraint is actively researched in the field of perception-aware planning. The approaches delivered by this field mostly concern Micro air vehicles (MAVs), but could be applied to a larger group of robotic systems. We propose a perception-aware trajectory planner for a class of robotic systems that can orient their cameras independently from their direction of travel. By using motion primitives, our planner does not require differentiable models for motion and perception objectives. We evaluate our method in simulation, showing increased capabilities in localization-aware motions around obstacles, and demonstrate its run-time capability on a real planetary rover. The code is released publicly under https://github.com/DLR-RM/palp.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t9_08">
             09:00-10:00, Paper ThPI4T9.8
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1316'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Asynchronous Event-Inertial Odometry Using a Unified Gaussian Process Regression Framework
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#375561" title="Click to go to the Author Index">
             Li, Xudong
            </a>
           </td>
           <td class="r">
            Northwestern Polytechnical University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#367335" title="Click to go to the Author Index">
             Wang, Zhixiang
            </a>
           </td>
           <td class="r">
            Northwestern Polytechnical University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#377374" title="Click to go to the Author Index">
             Liu, Zihao
            </a>
           </td>
           <td class="r">
            Northwestern Polytechnical University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#135120" title="Click to go to the Author Index">
             Zhang, Yizhai
            </a>
           </td>
           <td class="r">
            Northwestern Polytechnical University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#234355" title="Click to go to the Author Index">
             Zhang, Fan
            </a>
           </td>
           <td class="r">
            Northwestern Polytechnical Univeristy
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#414234" title="Click to go to the Author Index">
             Yao, Xiuming
            </a>
           </td>
           <td class="r">
            Beijing Jiaotong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#102722" title="Click to go to the Author Index">
             Huang, Panfeng
            </a>
           </td>
           <td class="r">
            Northwestern Polytechnical University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1316" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#visual_inertial_slam" title="Click to go to the Keyword Index">
               Visual-Inertial SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#sensor_fusion" title="Click to go to the Keyword Index">
               Sensor Fusion
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Recent works have combined the monocular event camera and inertial measurement unit to estimate the SE(3) trajectory. However, the asynchronicity of event cameras brings great challenge to conventional fusion algorithms. In this paper, we present an asynchronous event-inertial odometry under a unified Gaussian Process (GP) regression framework to naturally fuse asynchronous data associations and inertial measurements. A GP latent variable model is leveraged to build data-driven motion prior and acquire the analytical integration capacity. Then, asynchronous event-based feature associations and integral pseudo measurements are tightly coupled using the same GP framework. Subsequently, this fusion estimation problem is solved by underlying factor graph in a sliding window manner. With consideration of sparsity, those history states are marginalized orderly. A twin system is also designed for comparison, where the traditional inertial preintegration scheme is embedded in the GP-based framework to replace the GP latent variable model. Evaluations on public event-inertial datasets demonstrate the validity of both systems. Comparison experiments show competitive precision compared to the state of-the-art synchronous scheme.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t9_09">
             09:00-10:00, Paper ThPI4T9.9
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1320'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Environmental and Behavioral Imitation for Autonomous Navigation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#271392" title="Click to go to the Author Index">
             Aoki, Junki
            </a>
           </td>
           <td class="r">
            Ricoh Company, Ltd
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#276564" title="Click to go to the Author Index">
             Sasaki, Fumihiro
            </a>
           </td>
           <td class="r">
            Ricoh Company, LTD
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#217773" title="Click to go to the Author Index">
             Matsumoto, Kohei
            </a>
           </td>
           <td class="r">
            Kyushu University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#143776" title="Click to go to the Author Index">
             Yamashina, Ryota
            </a>
           </td>
           <td class="r">
            Ricoh Company, Ltd
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#103231" title="Click to go to the Author Index">
             Kurazume, Ryo
            </a>
           </td>
           <td class="r">
            Kyushu University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1320" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper, we introduce a framework for imitation learning in navigation that enables policy learning from one-shot images without a physical robot and facilitates the transfer of this policy from simulation to reality. Utilizing Neural Radiance Fields (NeRF), our approach generates a simulated environment and simultaneously models expert behavior. This removes the necessity for a physical robot during both the expert teaching phase and the agent's learning process, allowing for the application of policies learned within the NeRF simulation to real-world robots. We validate our method by demonstrating the navigation with an actual robot using the policy learned by our approach. Moreover, we present a method for adapting to changes in the robot configuration, such as camera parameters and robot dimensions, by simulating adjustments in the robot configuration throughout the learning and assessing its generalizability.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t9_10">
             09:00-10:00, Paper ThPI4T9.10
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1557'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              DiPPeST: Diffusion-Based Path Planner for Synthesizing Trajectories Applied on Quadruped Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#366407" title="Click to go to the Author Index">
             Stamatopoulou, Maria
            </a>
           </td>
           <td class="r">
            University College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#365173" title="Click to go to the Author Index">
             Liu, Jianwei
            </a>
           </td>
           <td class="r">
            University College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#147896" title="Click to go to the Author Index">
             Kanoulas, Dimitrios
            </a>
           </td>
           <td class="r">
            University College London
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1557" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#visual_servoing" title="Click to go to the Keyword Index">
               Visual Servoing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present DiPPeST, a novel image and goal conditioned diffusion-based trajectory generator for quadrupedal robot path planning. DiPPeST is a zero-shot adaptation of our previously introduced diffusion-based 2D global trajectory generator (DiPPeR). The introduced system incorporates a novel strategy for local real-time path refinements, that is reactive to camera input, without requiring any further training, image processing, or environment interpretation techniques. DiPPeST achieves 92% success rate in obstacle avoidance for nominal environments and an average of 88% success rate when tested in environments that are up to 3.5 times more complex in pixel variation than DiPPeR. A visual-servoing framework is developed to allow for real-world execution, tested on the quadruped robot, achieving 80% success rate in different environments and showcasing improved behavior than complex state-of-the-art local planners, in narrow environments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t9_11">
             09:00-10:00, Paper ThPI4T9.11
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1573'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              IN-Sight: Interactive Navigation through Sight
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396831" title="Click to go to the Author Index">
             Schoch, Philipp
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#287076" title="Click to go to the Author Index">
             Yang, Fan
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#314985" title="Click to go to the Author Index">
             Ma, Yuntao
            </a>
           </td>
           <td class="r">
            ETH
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#151104" title="Click to go to the Author Index">
             Leutenegger, Stefan
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#114045" title="Click to go to the Author Index">
             Hutter, Marco
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#202323" title="Click to go to the Author Index">
             Leboutet, Quentin
            </a>
           </td>
           <td class="r">
            Intel Labs
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1573" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Current visual navigation systems often treat the environment as static, lacking the ability to adaptively interact with obstacles. This limitation leads to navigation failure when encountering unavoidable obstructions. In response, we introduce IN-Sight, a novel approach to self-supervised path planning, enabling more effective navigation strategies through interaction with obstacles. Utilizing RGB-D observations, IN-Sight calculates traversability scores and incorporates them into a semantic map, facilitating long-range path planning in complex, maze-like environments. To precisely navigate around obstacles, IN-Sight employs a local planner, trained imperatively on a differentiable costmap using representation learning techniques. The entire framework undergoes end-to-end training within the state-of-the-art photorealistic Intel SPEAR Simulator. We validate the effectiveness of IN-Sight through extensive benchmarking in a variety of simulated scenarios and ablation studies. Moreover, we demonstrate the system's real-world applicability with zero-shot sim-to-real transfer, deploying our planner on the legged robot platform ANYmal, showcasing its practical potential for interactive navigation in real environments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t9_12">
             09:00-10:00, Paper ThPI4T9.12
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1645'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Active Human Pose Estimation Via an Autonomous UAV Agent
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#291759" title="Click to go to the Author Index">
             Chen, Jingxi
            </a>
           </td>
           <td class="r">
            University of Maryland
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#288413" title="Click to go to the Author Index">
             He, Botao
            </a>
           </td>
           <td class="r">
            University of Maryland
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#223112" title="Click to go to the Author Index">
             Singh, Chahat Deep
            </a>
           </td>
           <td class="r">
            University of Maryland, College Park
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#136632" title="Click to go to the Author Index">
             Fermuller, Cornelia
            </a>
           </td>
           <td class="r">
            University of Maryland
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#118557" title="Click to go to the Author Index">
             Aloimonos, Yiannis
            </a>
           </td>
           <td class="r">
            University of Maryland
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1645" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#perception_action_coupling" title="Click to go to the Keyword Index">
               Perception-Action Coupling
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__perception_and_autonomy" title="Click to go to the Keyword Index">
               Aerial Systems: Perception and Autonomy
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             One of the core activities of an active observer involves moving to secure a "better" view of the scene, where the definition of "better" is task-dependent. This paper focuses on the task of human pose estimation from videos capturing a person's activity. Self-occlusions within the scene can complicate or even prevent accurate human pose estimation. To address this, relocating the camera to a new vantage point is necessary to clarify the view, thereby improving 2D human pose estimation. This paper formalizes the process of achieving an improved viewpoint. Our proposed solution to this challenge comprises three main components: a NeRF-based Drone-View Data Generation Framework, an On-Drone Network for Camera View Error Estimation, and a Combined Planner for devising a feasible motion plan to reposition the camera based on the predicted errors for camera views. The Data Generation Framework utilizes NeRF-based methods to generate a comprehensive dataset of human poses and activities, enhancing the drone's adaptability in various scenarios. The Camera View Error Estimation Network is designed to evaluate the current human pose and identify the most promising next viewing angles for the drone, ensuring a reliable and precise pose estimation from those angles. Finally, the combined planner incorporates these angles while considering the drone's physical and environmental limitations, employing efficient algorithms to navigate safe and effective flight paths. This system represents a significant advancement in active 2D human pose estimation for an autonomous UAV agent, offering substantial potential for applications in aerial cinematography by improving the performance of autonomous human pose estimation and maintaining the operational safety and efficiency of UAVs.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t9_13">
             09:00-10:00, Paper ThPI4T9.13
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2001'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Embodiment: Self-Supervised Depth Estimation Based on Camera Models
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#379578" title="Click to go to the Author Index">
             Zhang, Jinchang
            </a>
           </td>
           <td class="r">
            University of Georgia
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#379577" title="Click to go to the Author Index">
             Kamsani, Praveen Kumar Reddy
            </a>
           </td>
           <td class="r">
            University of Georgia
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#211246" title="Click to go to the Author Index">
             Wong, Xue Iuan
            </a>
           </td>
           <td class="r">
            University at Buffalo
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#118557" title="Click to go to the Author Index">
             Aloimonos, Yiannis
            </a>
           </td>
           <td class="r">
            University of Maryland
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#235923" title="Click to go to the Author Index">
             Lu, Guoyu
            </a>
           </td>
           <td class="r">
            University of Georgia
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2001" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#range_sensing" title="Click to go to the Keyword Index">
               Range Sensing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Depth estimationn is a critical topic for robotics and vision-related tasks. In monocular depth estimation, in comparison with supervised learning that requires expensive ground truth labeling, self-supervised methods possess great potential due to no labeling cost. However, self-supervised learning still has a large gap with supervised learning in 3D reconstruction and depth estimation performance. Meanwhile, scaling is also a major issue for monocular unsupervised depth estimation, which commonly still needs ground truth scale from GPS, LiDAR, or existing maps to correct. In the era of deep learning, existing methods primarily rely on exploring image relationships to train unsupervised neural networks, while the physical properties of the camera itselfsuch as intrinsics and extrinsicsare often overlooked. These physical properties are not just mathematical parameters; they are embodiments of the camera's interaction with the physical world. By embedding these physical properties into the depth learning model, we can calculate depth priors for ground regions and regions connected to the ground based on physical principles, providing free supervision signals without the need for additional sensors. This approach is not only easy to implement but also enhances the effects of all unsupervised methods by embedding the camera's physical properties into the model, thereby achieving an embodied understanding of the real world.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t9_14">
             09:00-10:00, Paper ThPI4T9.14
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2251'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Object Instance Retrieval in Assistive Robotics: Leveraging Fine-Tuned SimSiam with Multi-View Images Based on 3D Semantic Map
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#327539" title="Click to go to the Author Index">
             Sakaguchi, Taichi
            </a>
           </td>
           <td class="r">
            Ritsumeikan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#182186" title="Click to go to the Author Index">
             Taniguchi, Akira
            </a>
           </td>
           <td class="r">
            Ritsumeikan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#167872" title="Click to go to the Author Index">
             Hagiwara, Yoshinobu
            </a>
           </td>
           <td class="r">
            Ritsumeikan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#191035" title="Click to go to the Author Index">
             El Hafi, Lotfi
            </a>
           </td>
           <td class="r">
            Ritsumeikan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#333905" title="Click to go to the Author Index">
             Hasegawa, Shoichi
            </a>
           </td>
           <td class="r">
            Ritsumeikan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#107751" title="Click to go to the Author Index">
             Taniguchi, Tadahiro
            </a>
           </td>
           <td class="r">
            Ritsumeikan University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2251" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robots that assist humans in their daily lives should be able to locate specific instances of objects in an environment that match a user's desired objects. This task is known as instance-specific image goal navigation (InstanceImageNav), which requires a model that can distinguish different instances of an object within the same class. A significant challenge in robotics is that when a robot observes the same object from various 3D viewpoints, its appearance may differ significantly, making it difficult to recognize and locate accurately. In this paper, we introduce a method called SimView, which leverages multi-view images based on a 3D semantic map of an environment and self-supervised learning using SimSiam to train an instance-identification model on-site. The effectiveness of our approach was validated using a photorealistic simulator, Habitat Matterport 3D, created by scanning actual home environments. Our results demonstrate a 1.7-fold improvement in task accuracy compared with contrastive language-image pre-training (CLIP), a pre-trained multimodal contrastive learning method for object searching. This improvement highlights the benefits of our proposed fine-tuning method in enhancing the performance of assistive robots in InstanceImageNav tasks. The project website is https://emergentsystemlabstudent.github.io/MultiViewRetrieve/.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t9_15">
             09:00-10:00, Paper ThPI4T9.15
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('893'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Enhancing Reinforcement Learning in Sensor Fusion: A Comparative Analysis of Cubature and Sampling-Based Integration Methods for Rover Search Planning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#394543" title="Click to go to the Author Index">
             Ewers, Jan-Hendrik
            </a>
           </td>
           <td class="r">
            University of Glasgow
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#355955" title="Click to go to the Author Index">
             Swinton, Sarah
            </a>
           </td>
           <td class="r">
            University of Glasgow
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#151859" title="Click to go to the Author Index">
             Anderson, Dave
            </a>
           </td>
           <td class="r">
            University of Glasgow
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#156584" title="Click to go to the Author Index">
             McGookin, Euan William
            </a>
           </td>
           <td class="r">
            University of Glasgow
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#389732" title="Click to go to the Author Index">
             Thomson, Douglas
            </a>
           </td>
           <td class="r">
            University of Glasgow
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab893" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#performance_evaluation_and_benchmarking" title="Click to go to the Keyword Index">
               Performance Evaluation and Benchmarking
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#ai_based_methods" title="Click to go to the Keyword Index">
               AI-Based Methods
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#path_planning_for_multiple_mobile_robots_or_agents" title="Click to go to the Keyword Index">
               Path Planning for Multiple Mobile Robots or Agents
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This study investigates the computational speed and accuracy of two numerical integration methods, cubature and sampling-based, for integrating an integrand over a 2D polygon. Using a group of rovers searching the Martian surface with a limited sensor footprint as a test bed, the relative error and computational time are compared as the area was sub- divided to improve accuracy in the sampling-based approach. The results show that the sampling-based approach exhibits a 14.75% deviation in relative error compared to cubature when it matches the computational performance at 100%. Furthermore, achieving a relative error below 1% necessitates a 10000% increase in relative time to calculate due to the O(N^2) complexity of the sampling-based method. It is concluded that for enhancing reinforcement learning capabilities and other high iteration algorithms, the cubature method is preferred over the sampling-based method.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thpi4t10">
             <b>
              ThPI4T10
             </b>
            </a>
           </td>
           <td class="r">
            Room 10
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thpi4t10" title="Click to go to the Program at a Glance">
             <b>
              Simultaneous Localization and Mapping (SLAM) IV
             </b>
            </a>
           </td>
           <td class="r">
            Teaser Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#114414" title="Click to go to the Author Index">
             Maragos, Petros
            </a>
           </td>
           <td class="r">
            National Technical University of Athens
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#320655" title="Click to go to the Author Index">
             Fu, Chunyun
            </a>
           </td>
           <td class="r">
            Chongqing University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t10_01">
             09:00-10:00, Paper ThPI4T10.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('143'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              RaNDT SLAM: Radar SLAM Based on Intensity-Augmented Normal Distributions Transform
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#368127" title="Click to go to the Author Index">
             Hilger, Maximilian
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#250733" title="Click to go to the Author Index">
             Mandischer, Nils
            </a>
           </td>
           <td class="r">
            University of Augsburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#111599" title="Click to go to the Author Index">
             Corves, Burkhard
            </a>
           </td>
           <td class="r">
            RWTH Aachen University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab143" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#search_and_rescue_robots" title="Click to go to the Keyword Index">
               Search and Rescue Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Rescue robotics sets high requirements to perception algorithms due to the unstructured and potentially vision-denied environments. Pivoting Frequency-Modulated Continuous Wave radars are an emerging sensing modality for SLAM in this kind of environment. However, the complex noise characteristics of radar SLAM makes, particularly indoor, applications computationally demanding and slow. In this work, we introduce a novel radar SLAM framework, RaNDT SLAM, that operates fast and generates accurate robot trajectories. The method is based on the Normal Distributions Transform augmented by radar intensity measures. Motion estimation is based on fusion of motion model, IMU data, and registration of the intensity-augmented Normal Distributions Transform. We evaluate RaNDT SLAM in a new benchmark dataset and the Oxford Radar RobotCar dataset. The new dataset contains indoor and outdoor environments besides multiple sensing modalities (LiDAR, radar, and IMU).
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t10_02">
             09:00-10:00, Paper ThPI4T10.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('210'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              DNS-SLAM: Dense Neural Semantic-Informed SLAM
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#390846" title="Click to go to the Author Index">
             Li, Kunyi
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#371881" title="Click to go to the Author Index">
             Niemeyer, Michael
            </a>
           </td>
           <td class="r">
            Google
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#107647" title="Click to go to the Author Index">
             Navab, Nassir
            </a>
           </td>
           <td class="r">
            TU Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#123306" title="Click to go to the Author Index">
             Tombari, Federico
            </a>
           </td>
           <td class="r">
            Technische Universitt Mnchen
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab210" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In recent years, coordinate-based neural implicit representations have shown promising results for the task of Simultaneous Localization and Mapping (SLAM). While achieving impressive performance on small synthetic scenes, these methods often suffer from losing details, especially for complex real-world scenes. In this work, we introduce DNS SLAM, a novel neural RGB-D semantic SLAM approach featuring a hybrid representation. Relying only on 2D semantic priors, we propose the first semantic neural SLAM method that trains class-wise scene representations while providing stable camera tracking at the same time. Our method integrates multi-view geometry constraints with image-based feature extraction to improve appearance details and to output color, occupancy, and semantic class information, enabling many downstream applications. To further enable fast tracking, we introduce a lightweight coarse scene representation which is trained in a self-supervised manner in latent space. Our experimental results achieve state-of-the-art performance on both synthetic data and real-world data tracking while maintaining a commendable operational speed on off-the-shelf hardware. Further, our method outputs class-wise decomposed reconstructions with better texture, capturing appearance and geometric details.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t10_03">
             09:00-10:00, Paper ThPI4T10.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('246'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              V3D-SLAM: Robust RGB-D SLAM in Dynamic Environments with 3D Semantic Geometry Voting
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#351067" title="Click to go to the Author Index">
             Dang, Tuan
            </a>
           </td>
           <td class="r">
            University Taxes at Arlington
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#350237" title="Click to go to the Author Index">
             Nguyen, Khang
            </a>
           </td>
           <td class="r">
            University of Texas at Arlington
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#101285" title="Click to go to the Author Index">
             Huber, Manfred
            </a>
           </td>
           <td class="r">
            University of Texas at Arlington
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab246" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Simultaneous localization and mapping (SLAM) in highly dynamic environments is challenging due to the correlation complexity between moving objects and the camera pose. Many methods have been proposed to deal with this problem; however, the moving properties of dynamic objects with a moving camera remain unclear. Therefore, to improve SLAM's performance, minimizing disruptive events of moving objects with a physical understanding of 3D shapes and dynamics of objects is needed. In this paper, we propose a robust method, V3D-SLAM, to remove moving objects via two lightweight re-evaluation stages, including identifying potentially moving and static objects using a spatial-reasoned Hough voting mechanism and refining static objects by detecting dynamic noise caused by intra-object motions using Chamfer distances as similarity measurements. Through our experiment on the TUM RGB-D benchmark on dynamic sequences with ground-truth camera trajectories, the results show that our methods outperform most other recent state-of-the-art SLAM methods. Our source code is available at https://github.com/tuantdang/v3d-slam.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t10_04">
             09:00-10:00, Paper ThPI4T10.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('309'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              NDT-Map-Code: A 3D Global Descriptor for Real-Time Loop Closure Detection in Lidar SLAM
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#320732" title="Click to go to the Author Index">
             Liao, Lizhou
            </a>
           </td>
           <td class="r">
            Chongqing University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#391131" title="Click to go to the Author Index">
             Yan, Wenlei
            </a>
           </td>
           <td class="r">
            ChongQing University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#172346" title="Click to go to the Author Index">
             Sun, Li
            </a>
           </td>
           <td class="r">
            University of Sheffield
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#346517" title="Click to go to the Author Index">
             Bai, Xinhui
            </a>
           </td>
           <td class="r">
            NIO
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#346568" title="Click to go to the Author Index">
             You, Zhenxing
            </a>
           </td>
           <td class="r">
            Autonomous Driving Division of NIO Inc
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#342355" title="Click to go to the Author Index">
             Yuan, Hongyuan
            </a>
           </td>
           <td class="r">
            Autonomous Driving Division of NIO Inc
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#320655" title="Click to go to the Author Index">
             Fu, Chunyun
            </a>
           </td>
           <td class="r">
            Chongqing University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab309" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Loop-closure detection, also known as place recognition, aiming to identify previously visited locations, is an essential component of a SLAM system. Existing research on lidar-based loop closure heavily relies on dense point cloud and 360 FOV lidars. This paper proposes an out-of-the-box NDT (Normal Distribution Transform) based global descriptor, NDT-Map-Code, designed for both on-road driving and underground valet parking scenarios. NDT-Map-Code can be directly extracted from the NDT map without the need for a dense point cloud, resulting in excellent scalability and low maintenance cost. The NDT representation is leveraged to identify representative patterns, which are further encoded according to their spatial location (bearing, range, and height). Experimental results on the NIO underground parking lot dataset and the KITTI dataset demonstrate that our method achieves significantly better performance compared to the state-of-the-art.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t10_05">
             09:00-10:00, Paper ThPI4T10.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('438'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SR-LIO: LiDAR-Inertial Odometry with Sweep Reconstruction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#307947" title="Click to go to the Author Index">
             Yuan, Zikang
            </a>
           </td>
           <td class="r">
            Huazhong University, Wuhan, 430073, China
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#344165" title="Click to go to the Author Index">
             Lang, Fengtian
            </a>
           </td>
           <td class="r">
            Huazhong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#308417" title="Click to go to the Author Index">
             Xu, Tianle
            </a>
           </td>
           <td class="r">
            Huazhong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#201893" title="Click to go to the Author Index">
             Yang, Xin
            </a>
           </td>
           <td class="r">
            Huazhong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab438" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#sensor_fusion" title="Click to go to the Keyword Index">
               Sensor Fusion
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper proposes a novel LiDAR-Inertial odometry (LIO), named SR-LIO, based on an error state iterated Kalman filter (ESIKF) framework. We adapt the sweep reconstruction method, which segments and reconstructs raw input sweeps from spinning LiDAR to obtain reconstructed sweeps with higher frequency. We found that such method can effectively reduce the time interval for each iterated state update, improving the state estimation accuracy and enabling the usage of ESIKF framework for fusing high-frequency IMU and low-frequency LiDAR. To prevent inaccurate trajectory caused by multiple distortion correction to a particular point, we further propose to perform distortion correction for each segment. Experimental results on four public datasets demonstrate that our SR-LIO outperforms all existing state-of-the-art methods on accuracy, and reducing the time interval of iterated state update via the proposed sweep reconstruction can improve the accuracy and frequency of estimated states. The source code of SR-LIO is publicly available for the development of the community.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t10_06">
             09:00-10:00, Paper ThPI4T10.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('520'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SMORE-SLAM: Semantic Monocular SLAM with Scale Correction and Reverse Loop Utilization in Outdoor Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#372467" title="Click to go to the Author Index">
             Chen, Yushi
            </a>
           </td>
           <td class="r">
            Beijing University of Posts and Telecommunications
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#297541" title="Click to go to the Author Index">
             Zhao, Fang
            </a>
           </td>
           <td class="r">
            Beijing University of Posts and Telecommunications
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#372491" title="Click to go to the Author Index">
             Zhuge, Yue
            </a>
           </td>
           <td class="r">
            Institute of Computing Technology, Chinese Academy of Sciences;
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#372486" title="Click to go to the Author Index">
             Liu, Junxiong
            </a>
           </td>
           <td class="r">
            Beijing University of Posts and Telecommunications
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#297529" title="Click to go to the Author Index">
             Yan, Jiaquan
            </a>
           </td>
           <td class="r">
            Beijing University of Posts and Telecommunications
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#297539" title="Click to go to the Author Index">
             Luo, Haiyong
            </a>
           </td>
           <td class="r">
            Institute of Computing Technology, Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab520" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In large-scale outdoor environments, vehicles often encounter situations like retracing their path or turning around, resulting in many reverse loop closures where the vehicles traverse previously covered paths from opposite viewpoints. Existing monocular SLAM methods, due to insufficient utilization of semantic information and neglect of leveraging reverse loop closures, result in significant scale and pose drift when confronted with such scenarios. In this paper, we introduce SMORE-SLAM, a semantic monocular SLAM with scale correction and reverse loop closure module. We constrain scale drift by harnessing semantics across a wide spatial extent and further detect and correct reverse loop closures using semantic point cloud to reduce pose drift. Experimental results on the KITTI odometry dataset and the Oxford RobotCar dataset demonstrate the capability of our research in scale correction and reverse loop closure detection, enabling a reduction in trajectory errors of monocular SLAM.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t10_07">
             09:00-10:00, Paper ThPI4T10.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('534'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Sharing Attention Mechanism in V-SLAM: Relative Pose Estimation with Messenger Tokens on Small Datasets
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#376763" title="Click to go to the Author Index">
             Dai, Dun
            </a>
           </td>
           <td class="r">
            Beihang Univeristy
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#194601" title="Click to go to the Author Index">
             Quan, Quan
            </a>
           </td>
           <td class="r">
            Beihang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#143783" title="Click to go to the Author Index">
             Cai, Kai-Yuan
            </a>
           </td>
           <td class="r">
            Beijing University of Aeronautics and Astronautics
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab534" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In V-SLAM, the estimation of relative camera pose is crucial to determine the spatial relationship between consecutive camera images, helping to accurately track the movement of the camera in its environment. In small indoor scenes, when the training set is limited, which is very common in robot SLAM, learning-based methods may fail to converge, especially the Transformer architecture, which requires a more substantial dataset to match the performance of the CNN architecture model. This work addresses this problem with the sharing attention mechanism, building on recent improvements in solving visual Transformer architectures on small datasets while incorporating messenger tokens. Besides, double-embedding is introduced to capture the spatial of images and order of images. In summary, we introduce an intuitive end-to-end relative pose estimation solution and prove its accuracy on the two smallest sub-datasets of 7Scenes. The proposed method is tested with a set of comparison experiments conducted across CNN-based, Transformer-based end-to-end relative pose estimation models, and the robust feature-matching non-learning method. Our model outperforms in all comparisons. Furthermore, ablation studies clearly illustrate that these innovations are crucial for the accuracy of relative pose estimation on small datasets.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t10_08">
             09:00-10:00, Paper ThPI4T10.8
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('572'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              MCGMapper: Light-Weight Incremental Structure from Motion and Visual Localization with Planar Markers and Camera Groups
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#315725" title="Click to go to the Author Index">
             Xie, Yusen
            </a>
           </td>
           <td class="r">
            The Hong Kong University of Science and Technology (Guangzhou)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#352788" title="Click to go to the Author Index">
             Huang, Zhenmin
            </a>
           </td>
           <td class="r">
            The Hong Kong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#285922" title="Click to go to the Author Index">
             Chen, Kai
            </a>
           </td>
           <td class="r">
            The Hong Kong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#341869" title="Click to go to the Author Index">
             Zhu, Lei
            </a>
           </td>
           <td class="r">
            The Hong Kong University of Science and Technology (Guangzhou)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#182083" title="Click to go to the Author Index">
             Ma, Jun
            </a>
           </td>
           <td class="r">
            The Hong Kong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab572" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Structure from Motion (SfM) and visual localization in indoor texture-less scenes and industrial scenarios present prevalent yet challenging research topics. Existing SfM methods designed for natural scenes typically yield low accuracy or map-building failures due to insufficient robust feature extraction in such settings. Visual markers, with their artificially designed features, can effectively address these issues. Nonetheless, existing marker-assisted SfM methods encounter problems like slow running speed and difficulties in convergence; and also, they are governed by the strong assumption of unique marker size. In this paper, we propose a novel SfM framework that utilizes planar markers and multiple cameras with known extrinsics to capture the surrounding environment and reconstruct the marker map. In our algorithm, the initial poses of markers and cameras are calculated with Perspective-n-Points (PnP) in the front-end, while bundle adjustment methods customized for markers and camera groups are designed in the back-end to optimize the 6-DOF pose directly. Our algorithm facilitates the reconstruction of large scenes with different marker sizes, and its accuracy and speed of map building are shown to surpass existing methods. Our approach is suitable for a wide range of scenarios, including laboratories, basements, warehouses, and other industrial settings. Furthermore, we incorporate representative scenarios into simulations and also supply our datasets with pose labels to address the scarcity of quantitative ground-truth datasets in this research field. The datasets and source code are available on GitHub.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t10_09">
             09:00-10:00, Paper ThPI4T10.9
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('941'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SDPL-SLAM: Introducing Lines in Dynamic Visual SLAM and Multi-Object Tracking
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#394746" title="Click to go to the Author Index">
             Manetas, Argyris
            </a>
           </td>
           <td class="r">
            National Technical University of Athens
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#288786" title="Click to go to the Author Index">
             Mermigkas, Panagiotis
            </a>
           </td>
           <td class="r">
            National Technical University of Athens
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#114414" title="Click to go to the Author Index">
             Maragos, Petros
            </a>
           </td>
           <td class="r">
            National Technical University of Athens
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab941" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#visual_inertial_slam" title="Click to go to the Keyword Index">
               Visual-Inertial SLAM
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The need for a robust visual SLAM system operating in real human environments has led to the gradual abandonment of the static world assumption and to the creation of many dynamic SLAM algorithms. Even though there have been many dynamic SLAM proposals, the vast majority of them relied on point features. However, research in static SLAM systems has demonstrated that the use of more complex geometric shapes such as lines can improve performance. Motivated by this we have created a new dynamic SLAM system that estimates the camera poses and the motion of rigid objects, by exploiting both static and dynamic points and lines. Line segments have been incorporated in a novel way in every aspect of our algorithm, by improving their correspondences through optical flow refinement, and by introducing line error terms in both camera and object motion, and in batch optimization. Our proposal has been tested extensively in indoor and outdoor datasets and has achieved a great improvement compared to other state-of-the-art dynamic SLAM systems. Our research demonstrated that line segments enhanced the robustness, thus contributing towards a fully operational SLAM system.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t10_10">
             09:00-10:00, Paper ThPI4T10.10
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('961'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Semantic SLAM Fusing Moving Constraint for Dynamic Objects under Indoor Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#376373" title="Click to go to the Author Index">
             Yang, Zhenyuan
            </a>
           </td>
           <td class="r">
            Singapore University of Technology and Design
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#367142" title="Click to go to the Author Index">
             Rishan Sachinthana, Wijenayaka Kankanamge
            </a>
           </td>
           <td class="r">
            Singapore University of Technology and Design
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#219480" title="Click to go to the Author Index">
             Samarakoon Mudiyanselage, Bhagya Prasangi Samarakoon
            </a>
           </td>
           <td class="r">
            Singapore University of Technology and Design
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#107677" title="Click to go to the Author Index">
             Elara, Mohan Rajesh
            </a>
           </td>
           <td class="r">
            Singapore University of Technology and Design
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab961" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Simultaneous Localization and Mapping (SLAM) technology is a rapidly developing field in robotics. Most existing SLAM algorithms lack robustness in dynamic environments because moving objects can influence mapping and localization accuracy, making it challenging for robots to identify moving objects and understand their surroundings. Though some works proposed semantic SLAM methods, they rely on point-based feature extraction and matching algorithms with semantic information and simply exclude dynamic objects. In this work, we proposed real-time RGB-D SLAM to combine point, line, and plane features with object detection to increase the robustness in dynamic environments. The proposed method combines object detection, feature points, and lines to identify moving objects and uses feature planes and semantic information to identify constrained moving objects. Thus, the localization accuracy can be improved under dynamic environments by excluding or using dynamic objects. Experiments were conducted on a public TUM dataset and in a real-world environment. The result shows that the proposed SLAM algorithm can increase the dynamic object detection speed and the robustness of SLAM performance compared to state-of-the-art in dynamic environments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t10_11">
             09:00-10:00, Paper ThPI4T10.11
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('962'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              C3P-VoxelMap: Compact, Cumulative and Coalescible Probabilistic Voxel Mapping
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392506" title="Click to go to the Author Index">
             Yang, Xu
            </a>
           </td>
           <td class="r">
            Deptrum
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392513" title="Click to go to the Author Index">
             Li, Wenhao
            </a>
           </td>
           <td class="r">
            Deptrum
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392543" title="Click to go to the Author Index">
             Ge, Qijie
            </a>
           </td>
           <td class="r">
            Deptrum
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392761" title="Click to go to the Author Index">
             Suo, Lulu
            </a>
           </td>
           <td class="r">
            Deptrum
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392548" title="Click to go to the Author Index">
             Tang, Weijie
            </a>
           </td>
           <td class="r">
            Deptrum
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392537" title="Click to go to the Author Index">
             Wei, Zhengyu
            </a>
           </td>
           <td class="r">
            Deptrum
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#146323" title="Click to go to the Author Index">
             Huang, Longxiang
            </a>
           </td>
           <td class="r">
            Deptrum
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392516" title="Click to go to the Author Index">
             Wang, Bo
            </a>
           </td>
           <td class="r">
            Deptrum
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab962" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This work presents a compact, cumulative, and coalescible probabilistic voxel mapping method to enhance performance, accuracy, and memory efficiency in LiDAR odometry. Probabilistic voxel mapping requires storing past point clouds and re-iterating them to update the uncertainty at every iteration, which consumes large memory space and CPU cycles. To solve this problem, we propose a two-fold strategy. First, we introduce a compact point-free representation for probabilistic voxels and derive a cumulative update of the planar uncertainty without caching original point clouds. Our voxel structure only keeps track of a predetermined set of statistics for points that lie inside it. This method reduces the runtime complexity from O(MN) to O(N) and the space complexity from O(N) to O(1) where M is the number of iterations and N is the number of points. Second, to further minimize memory usage and enhance mapping accuracy, we provide a strategy to dynamically merge voxels associated with the same physical planes by taking advantage of the geometric features in the real world. Rather than constantly scanning for these coalescible voxels at every iteration, our merging strategy accumulates voxels in a locality-sensitive hash and triggers merging lazily. On-demand merging reduces memory footprint with minimal computational overhead and improves localization accuracy thanks to cross-voxel denoising. Experiments exhibit 20% higher accuracy, 20% faster performance, and 70% lower memory consumption than the state-of-the-art.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t10_12">
             09:00-10:00, Paper ThPI4T10.12
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1247'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              CurricularVPR: Curricular Contrastive Loss for Visual Place Recognition
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#339679" title="Click to go to the Author Index">
             Zhang, Dongshuo
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396076" title="Click to go to the Author Index">
             Chen, Nanhua
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#339710" title="Click to go to the Author Index">
             Wu, Meiqing
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#140031" title="Click to go to the Author Index">
             Lam, Siew Kei
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1247" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Visual Place Recognition (VPR) techniques commonly utilize Contrastive Losses (CL) to train models that generate compact and discriminative global descriptors for images. These models often result in poor performance due to one of the following reasons during training: 1) loss functions that focus primarily on easier samples, 2) reliance on time-consuming hard sample mining methods to identify informative supervisory samples, which hinders effective learning from large-scale datasets. To enhance both learning efficiency and effectiveness, we propose a Curricular Contrastive Loss (CCL) and use graded similarity labels as a measure of sample difficulty. Inspired by human learning that begin with easier concepts and progressively tackle more challenging ones, our CCL dynamically emphasizes easier samples during the initial training stages to achieve rapid convergence. The learning gradually focuses on harder samples in later training stages to bolster robustness of the models under challenging conditions. Our proposed method has been extensively evaluated on popular datasets, and the results demonstrate its superior performance compared to the CL and Generalized CL functions.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t10_13">
             09:00-10:00, Paper ThPI4T10.13
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1073'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              GV-Bench: Benchmarking Local Feature Matching for Geometric Verification of Long-Term Loop Closure Detection
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#309443" title="Click to go to the Author Index">
             Yu, Jingwen
            </a>
           </td>
           <td class="r">
            The Hong Kong University of Science and Technology, Southern Uni
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#320141" title="Click to go to the Author Index">
             Ye, Hanjing
            </a>
           </td>
           <td class="r">
            Southern University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#212673" title="Click to go to the Author Index">
             Jiao, Jianhao
            </a>
           </td>
           <td class="r">
            University College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#219198" title="Click to go to the Author Index">
             Tan, Ping
            </a>
           </td>
           <td class="r">
            Simon Fraser University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#100155" title="Click to go to the Author Index">
             Zhang, Hong
            </a>
           </td>
           <td class="r">
            SUSTech
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1073" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Visual loop closure detection is an important module in visual simultaneous localization and mapping (SLAM), which associates current camera observation with previously visited places. Loop closures correct drifts in trajectory estimation to build a globally consistent map. However, a false loop closure can be fatal, so verification is required as an additional step to ensure robustness by rejecting the false positive loops. Geometric verification has been a well-acknowledged solution that leverages spatial clues provided by local feature matching to find true positives. Existing feature matching methods focus on homography and pose estimation in long-term visual localization, lacking references for geometric verification. To fill the gap, this paper proposes a unified benchmark targeting geometric verification of loop closure detection under long-term conditional variations. Furthermore, we evaluate six representative local feature matching methods (handcrafted and learning-based) under the benchmark, with in-depth analysis for limitations and future directions. The proposed benchmark is open-sourced at https://github.com/jarvisyjw/GV-Bench.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t10_14">
             09:00-10:00, Paper ThPI4T10.14
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1178'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Ternary-Type Opacity and Hybrid Odometry for RGB NeRF-SLAM
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395486" title="Click to go to the Author Index">
             Lin, Junru
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395598" title="Click to go to the Author Index">
             Nachkov, Asen
            </a>
           </td>
           <td class="r">
            INSAIT, Sofia University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#326858" title="Click to go to the Author Index">
             Peng, Songyou
            </a>
           </td>
           <td class="r">
            ETH Zurich and Max Planck Institute for Intelligent Systems
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#107640" title="Click to go to the Author Index">
             Van Gool, Luc
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#171278" title="Click to go to the Author Index">
             Paudel, Danda Pani
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1178" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#representation_learning" title="Click to go to the Keyword Index">
               Representation Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this work, we address the challenge of deploying Neural Radiance Field (NeRFs) in Simultaneous Localization and Mapping (SLAM) under the condition of lacking depth information, relying solely on RGB inputs. The key to unlocking the full potential of NeRF in such a challenging context lies in the integration of real-world priors. A crucial prior we explore is the binary opacity prior of 3D space with opaque objects. To effectively incorporate this prior into the NeRF framework, we introduce a ternary-type opacity (TT) model instead, which categorizes points on a ray intersecting a surface into three regions: before, on, and behind the surface. This enables a more accurate rendering of depth, subsequently improving the performance of image warping techniques. Therefore, we further propose a novel hybrid odometry (HO) scheme that merges bundle adjustment and warping-based localization. Our integrated approach of TT and HO achieves state-of-the-art performance on synthetic and real-world datasets, in terms of both speed and accuracy. This breakthrough underscores the potential of NeRF-SLAM in navigating complex environments with high fidelity.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t10_15">
             09:00-10:00, Paper ThPI4T10.15
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1200'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              AVM-SLAM: Semantic Visual SLAM with Multi-Sensor Fusion in a Bird's Eye View for Automated Valet Parking
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#371669" title="Click to go to the Author Index">
             Li, Ye
            </a>
           </td>
           <td class="r">
            University of Chinese Academy of Sciences, Beijing 100049, China
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#371842" title="Click to go to the Author Index">
             Yang, Wenchao
            </a>
           </td>
           <td class="r">
            GWM
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392030" title="Click to go to the Author Index">
             Lin, Dekun
            </a>
           </td>
           <td class="r">
            Chengdu Institute of Computer Applications, Chinese Academy of S
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#368278" title="Click to go to the Author Index">
             Wang, Qianlei
            </a>
           </td>
           <td class="r">
            University of Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#373352" title="Click to go to the Author Index">
             Cui, Zhe
            </a>
           </td>
           <td class="r">
            Chengdu Information Technology of Chinese Academy of Sciences Co
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#373922" title="Click to go to the Author Index">
             Qin, Xiaolin
            </a>
           </td>
           <td class="r">
            Chengdu Institute of Computer Applications, Chinese Academy of S
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1200" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#data_sets_for_slam" title="Click to go to the Keyword Index">
               Data Sets for SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#autonomous_vehicle_navigation" title="Click to go to the Keyword Index">
               Autonomous Vehicle Navigation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Accurate localization in challenging garage environmentsmarked by poor lighting, sparse textures, repetitive structures, dynamic scenes, and the absence of GPSis crucial for automated valet parking (AVP) tasks. Addressing these challenges, our research introduces AVM-SLAM, a cutting-edge semantic visual SLAM architecture with multi-sensor fusion in a bird's eye view (BEV). This novel framework synergizes the capabilities of four fisheye cameras, wheel encoders, and an inertial measurement unit (IMU) to construct a robust SLAM system. Unique to our approach is the implementation of a flare removal technique within the BEV imagery, significantly enhancing road marking detection and semantic feature extraction by convolutional neural networks for superior mapping and localization. Our work also pioneers a semantic pre-qualification (SPQ) module, designed to adeptly handle the challenges posed by environments with repetitive textures, thereby enhancing loop detection and system robustness. To demonstrate the effectiveness and resilience of AVM-SLAM, we have released a specialized multi-sensor and high-resolution dataset of an underground garage, accessible at https://yale-cv.github.io/avm-slam_dataset, encouraging further exploration and validation of our approach within similar settings.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thpi4t11">
             <b>
              ThPI4T11
             </b>
            </a>
           </td>
           <td class="r">
            Room 11
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thpi4t11" title="Click to go to the Program at a Glance">
             <b>
              Multi-Robot Systems and Swarms III
             </b>
            </a>
           </td>
           <td class="r">
            Teaser Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#133879" title="Click to go to the Author Index">
             Bezzo, Nicola
            </a>
           </td>
           <td class="r">
            University of Virginia
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#105894" title="Click to go to the Author Index">
             Akesson, Knut
            </a>
           </td>
           <td class="r">
            Chalmers University of Technology
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t11_01">
             09:00-10:00, Paper ThPI4T11.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('713'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Collaborative Stereo Camera with Two UAVs for Long-Distance Mapping of Urban Buildings
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#296768" title="Click to go to the Author Index">
             Wang, Zhaoying
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#253549" title="Click to go to the Author Index">
             Dong, Wei
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab713" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#cooperating_robots" title="Click to go to the Keyword Index">
               Cooperating Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__perception_and_autonomy" title="Click to go to the Keyword Index">
               Aerial Systems: Perception and Autonomy
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             For a swarm of Unmanned Aerial Vehicle (UAV), long-distance visual mapping is advantageous for pre-planning navigation paths in unknown urban building environments. This work leverages two cameras on two UAVs to build a wide-baseline collaborative stereo camera, which can construct a navigable mesh map for remote building obstacles. We present a complete framework of the collaborative stereo camera for long-distance mapping, including online extrinsic parameter estimation of the stereo camera, real-time cross-camera feature association, and semantic mesh map generation of remote buildings. Extensive simulations and real-world experiments verify the effectiveness of the collaborative stereo camera. With a 3m baseline, the collaborative stereo camera achieves long-distance mapping of buildings (20m - 50m) away with a relative error of approximately 10%. The constructed remote map enables UAVs to pre-detect large obstacles and pre-plan navigation paths in large-scale building environments. Hopefully, this work can provide a novel and practical approach for collaborative visual tasks of UAV swarm.
             <p>
              Video - https://youtu.be/a0kj-1zb6KI
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t11_02">
             09:00-10:00, Paper ThPI4T11.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1608'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Cooperative Recovery Framework for Resilient Multi-Robot Swarm Operations under Loss of Localization in Unknown Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#251777" title="Click to go to the Author Index">
             Bonczek, Paul
            </a>
           </td>
           <td class="r">
            University of Virginia
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#133879" title="Click to go to the Author Index">
             Bezzo, Nicola
            </a>
           </td>
           <td class="r">
            University of Virginia
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1608" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#cooperating_robots" title="Click to go to the Keyword Index">
               Cooperating Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#failure_detection_and_recovery" title="Click to go to the Keyword Index">
               Failure Detection and Recovery
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#swarm_robotics" title="Click to go to the Keyword Index">
               Swarm Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Localization is one of the most important tasks for mobile robot operations. Without such capability, a robot may wander toward unsafe states and never complete a desired task. Such capability is even more important in multi-robot system (MRS) operations in which their motion is coordinated based on consensus schemes that leverage information from surrounding neighbors. Thus, in the event of compromised or malfunctioning on-board positioning sensing (e.g., due to cyber attacks or faults) on individual robots, the entire robotic system may be hijacked toward undesired states. In this work, we target this problem by proposing a decentralized framework where: i) robots with loss of localization capabilities detect the anomalous behavior then generate a notification signal within information exchanges to alert neighboring robots, and ii) neighboring robots leverage their mobility to aid in recovery allowing compromised robots to re-localize. Our framework is validated in simulations and lab experiments on proximity-based formations of homogeneous unmanned multi-robot swarms.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t11_03">
             09:00-10:00, Paper ThPI4T11.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2756'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Towards the New Generation of Smart Home-Care with Cloud-Based Internet of Humans and Robotic Things
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#191798" title="Click to go to the Author Index">
             Zhang, Dandan
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#345559" title="Click to go to the Author Index">
             Zheng, Jin
            </a>
           </td>
           <td class="r">
            University of Bristol
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2756" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#hardware_software_integration_in_robotics" title="Click to go to the Keyword Index">
               Hardware-Software Integration in Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#software_hardware_integration_for_robot_systems" title="Click to go to the Keyword Index">
               Software-Hardware Integration for Robot Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The burgeoning demand for home-care services, driven by a rapidly aging global population, necessitates innovative solutions to alleviate the burden on caregivers and enhance care quality. This paper introduces the development of an Internet of Human and Robotic Things (IoHRT) framework, which synergizes cloud computing and the Internet of Robotic Things (IoRT) with human-robot collaborative control mechanisms for home-care applications. The IoHRT framework is designed to enable the seamless integration of customizable robotic platforms with modular, scalable, and compatible features, thereby creating a dynamic and adaptable home-care ecosystem. By leveraging the scalability and computational power of cloud computing, the framework facilitates real-time data analysis and remote monitoring, thus enhancing the efficiency and effectiveness of home-care. We present an in-depth analysis of the key characteristics of IoHRT, supported by evidence embedded in our design, and conduct user studies to evaluate the framework from users' perspectives. We demonstrate the performance and utility of our proposed framework for the future of home-care applications.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t11_04">
             09:00-10:00, Paper ThPI4T11.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('496'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              CollabLoc: Collaborative Information Sharing for Real-Time Multiuser Visual Localization System
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#375777" title="Click to go to the Author Index">
             Yu, Teng-Te
            </a>
           </td>
           <td class="r">
            National Yang Ming Chiao Tung University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#309585" title="Click to go to the Author Index">
             Lau, Yo-Chung
            </a>
           </td>
           <td class="r">
            National Taiwan University, Taipei, Taiwan
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#391027" title="Click to go to the Author Index">
             Wang, Kai-Li
            </a>
           </td>
           <td class="r">
            National Yang Ming Chiao Tung University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#218673" title="Click to go to the Author Index">
             Chen, Kuan-Wen
            </a>
           </td>
           <td class="r">
            National Yang Ming Chiao Tung University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab496" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents CollabLoc, a novel approach for real-time multi-user visual localization. Typically, localization systems employ a client-server design for locating cameras. In these systems, lightweight simultaneous localization and mapping computations are performed on the client side, while the server handles intensive localization tasks. This approach harnesses the complementary capabilities of the client and server, resulting in accurate, real-time localization results. However, existing architectures primarily operate on a one-to-one client-server structure, limiting their scalability and multi-user capabilities. Therefore, CollabLoc is designed to accommodate multiple clients through collaborative information sharing to considerably reduce computational overhead and enhance overall efficiency and accuracy. We propose a tracking confidence module that evaluates the tracking quality of individual clients and plays a pivotal role in prioritizing client requests by the server-side algorithm. On the server, we utilize fused poses to accelerate image retrieval. Moreover, we enhance the efficiency of optical flow estimation by employing a simplified feature extraction module and leveraging spatial similarities among neighboring clients to improve its performance. Finally, via the Pose Fusion Module, the server can periodically adjust fused poses to mitigate accumulated errors. Experimental results indicate that compared with a baseline method, CollabLoc improves positioning efficiency by nearly twice and achieves higher accuracy in multi-user scenarios.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t11_05">
             09:00-10:00, Paper ThPI4T11.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('954'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Blending Distributed NeRFs with Tri-Stage Robust Pose Optimization
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#370200" title="Click to go to the Author Index">
             Ye, Baijun
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#371411" title="Click to go to the Author Index">
             Liu, Caiyun
            </a>
           </td>
           <td class="r">
            Institute for AI Industry Research, Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#374828" title="Click to go to the Author Index">
             Xiaoyu, Ye
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology, AIR Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#338991" title="Click to go to the Author Index">
             Chen, Yuantao
            </a>
           </td>
           <td class="r">
            Xi'an University of Architecture and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#291669" title="Click to go to the Author Index">
             Wang, Yuhai
            </a>
           </td>
           <td class="r">
            University of Southern California
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#269669" title="Click to go to the Author Index">
             Yan, Zike
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#338957" title="Click to go to the Author Index">
             Shi, Yongliang
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#223795" title="Click to go to the Author Index">
             Zhao, Hao
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#165639" title="Click to go to the Author Index">
             Zhou, Guyue
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab954" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#distributed_robot_systems" title="Click to go to the Keyword Index">
               Distributed Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Due to the limited model capacity, leveraging distributed Neural Radiance Fields (NeRFs) for modeling extensive urban environments has become a necessity. However, current distributed NeRF registration approaches encounter aliasing artifacts, arising from discrepancies in rendering resolutions and suboptimal pose precision. These factors collectively deteriorate the fidelity of pose estimation within NeRF frameworks, resulting in occlusion artifacts during the NeRF blending stage. In this paper, we present a distributed NeRF system with tri-stage pose optimization. In the first stage, precise poses of images are achieved by bundle adjusting Mip-NeRF 360 with a coarse-to-fine strategy. In the second stage, we incorporate the inverting Mip-NeRF 360, coupled with the truncated dynamic low-pass filter, to enable the achievement of robust and precise poses, termed Frame2Model optimization. On top of this, we obtain a coarse transformation between NeRFs in different coordinate systems. In the third stage, we fine-tune the transformation between NeRFs by Model2Model pose optimization. After obtaining precise transformation parameters, we proceed to implement NeRF blending, showcasing superior performance metrics in both real-world and simulation scenarios. Codes and data will be publicly available at https://github.com/boilcy/Distributed-NeRF.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t11_06">
             09:00-10:00, Paper ThPI4T11.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('330'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              P4: Pruning and Prediction-Based Priority Planning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392011" title="Click to go to the Author Index">
             Yang, Rui
            </a>
           </td>
           <td class="r">
            University of California, Riverside
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392017" title="Click to go to the Author Index">
             Gupta, Rajiv
            </a>
           </td>
           <td class="r">
            University of California Riverside
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab330" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#path_planning_for_multiple_mobile_robots_or_agents" title="Click to go to the Keyword Index">
               Path Planning for Multiple Mobile Robots or Agents
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#planning__scheduling_and_coordination" title="Click to go to the Keyword Index">
               Planning, Scheduling and Coordination
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In recent years, multi-agent path finding (MAPF) has attracted widespread attention in the fields of artificial intelligence and robotics. Its main goal is to find paths for multiple agents, each having specified start and end locations on a grid, without collisions, while minimizing the total travel time. In this work, we present a new algorithm called P4 (Pruning and Prediction-based Priority Planning), designed to accommodate large numbers of agents with enhanced scalability. The P4 method combines three key components: Point-to-Point (PnP) algorithm, dynamic window approach, and path direction prediction. In this way we reduce the search space and increase the speed of the computation. Our experiments show that P4 consistently achieves shorter execution times and produces solutions that are close to optimal. For example, for 200 agents and real map orz900d, P4 is 4 faster than optimal algorithm CBSH while the sum of delays is within 15% of optimal. The P4 method outperforms other existing suboptimal methods in both performance and solution quality. We also show that our approach exceeds existing methods in success rate under time constraints. As time limit is increased from 0.1 to 100 seconds, success rate of P4 increases from 50% to 100%. On the other hand, the success rate for alternative sub-optimal methods is less than that of P4.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t11_07">
             09:00-10:00, Paper ThPI4T11.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('341'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Multi-Agent Teamwise Cooperative Path Finding and Traffic Intersection Coordination
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#211685" title="Click to go to the Author Index">
             Ren, Zhongqiang
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#286637" title="Click to go to the Author Index">
             Cai, Yilin
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#103003" title="Click to go to the Author Index">
             Wang, Hesheng
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab341" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#path_planning_for_multiple_mobile_robots_or_agents" title="Click to go to the Keyword Index">
               Path Planning for Multiple Mobile Robots or Agents
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#planning__scheduling_and_coordination" title="Click to go to the Keyword Index">
               Planning, Scheduling and Coordination
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             When coordinating the motion of connected autonomous vehicles at a signal-free intersection, the vehicles from each direction naturally forms a team and each team seeks to minimize their own traversal time through the intersection, without concerning the traversal times of other teams. Since the intersection is shared by all teams and agent-agent collision must be avoided, the coordination has to trade the traversal time of one team for the other. This paper thus investigates a problem called Multi-Agent Teamwise Cooperative Path Finding (TCPF), which seeks a set of collision-free paths for the agents from their respective start to goal locations, and agents are grouped into multiple teams with each team having its own objective function to optimize. In general, there are more than one teams and hence multiple objectives. TCPF thus seeks the Pareto-optimal front that represents possible trade-offs among the teams. We develop a centralized planner for TCPF by leveraging the Multi-Agent Path Finding techniques to resolve agent-agent collision, and Multi-Objective Optimization to find Pareto-optimal solutions. We analyze the completeness and optimality of the planner, which is then tested in various settings with up to 40 agents to verify the runtime efficiency and showcase the usage in intersection coordination.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t11_08">
             09:00-10:00, Paper ThPI4T11.8
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('656'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Optimal and Bounded Suboptimal Any-Angle Multi-Agent Pathfinding
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#226181" title="Click to go to the Author Index">
             Yakovlev, Konstantin
            </a>
           </td>
           <td class="r">
            Federal Research Center for Computer Science and Control of Russ
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#226184" title="Click to go to the Author Index">
             Andreychuk, Anton
            </a>
           </td>
           <td class="r">
            Peoples' Friendship University of Russia (RUDN University)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#305571" title="Click to go to the Author Index">
             Stern, Roni
            </a>
           </td>
           <td class="r">
            Ben Gurion University of the Negev, Palo Alto Research Center (P
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab656" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#path_planning_for_multiple_mobile_robots_or_agents" title="Click to go to the Keyword Index">
               Path Planning for Multiple Mobile Robots or Agents
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Multi-agent pathfinding (MAPF) is the problem of finding a set of conflict-free paths for a set of agents. Typically, the agents' moves are limited to a pre-defined graph of possible locations and allowed transitions between them, e.g. a 4-neighborhood grid. We explore how to solve MAPF problems when each agent can move between any pair of possible locations as long as traversing the line segment connecting them does not lead to a collision with the obstacles. This is known as any-angle pathfinding. We present the first optimal any-angle multi-agent pathfinding algorithm. Our planner is based on the Continuous Conflict-based Search (CCBS) algorithm and an optimal any-angle variant of the Safe Interval Path Planning (TO-AA-SIPP). The straightforward combination of those, however, scales poorly since any-angle path finding induces search trees with a very large branching factor. To mitigate this, we adapt two techniques from classical MAPF to the any-angle setting, namely Disjoint Splitting and Multi-Constraints. Experimental results on different combinations of these techniques show they enable solving over 30% more problems than the vanilla combination of CCBS and TO-AA-SIPP. In addition, we present a bounded-suboptimal variant of our algorithm, that enables trading runtime for solution cost in a controlled manner.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t11_09">
             09:00-10:00, Paper ThPI4T11.9
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('670'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Bird's-Eye-View Trajectory Planning of Multiple Robots Using Continuous Deep Reinforcement Learning and Model Predictive Control
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#356548" title="Click to go to the Author Index">
             Ceder, Kristian
            </a>
           </td>
           <td class="r">
            Chalmers University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#300751" title="Click to go to the Author Index">
             Zhang, Ze
            </a>
           </td>
           <td class="r">
            Chalmers University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#390091" title="Click to go to the Author Index">
             Burman, Adam
            </a>
           </td>
           <td class="r">
            Chalmers University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#390549" title="Click to go to the Author Index">
             Kuangaliyev, Ilya
            </a>
           </td>
           <td class="r">
            Chalmers University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#390146" title="Click to go to the Author Index">
             Mattsson, Krister
            </a>
           </td>
           <td class="r">
            Chalmers University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#390090" title="Click to go to the Author Index">
             Nyman, Gabriel
            </a>
           </td>
           <td class="r">
            Chalmers University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#391798" title="Click to go to the Author Index">
             Petersn, Arvid
            </a>
           </td>
           <td class="r">
            Chalmers University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#390707" title="Click to go to the Author Index">
             Wisell, Lukas
            </a>
           </td>
           <td class="r">
            Chalmers University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#105894" title="Click to go to the Author Index">
             Akesson, Knut
            </a>
           </td>
           <td class="r">
            Chalmers University of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab670" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#path_planning_for_multiple_mobile_robots_or_agents" title="Click to go to the Keyword Index">
               Path Planning for Multiple Mobile Robots or Agents
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#collision_avoidance" title="Click to go to the Keyword Index">
               Collision Avoidance
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Efficient motion planning and control for multiple mobile robots in industrial automation and indoor logistics face challenges such as trajectory generation and collision avoidance in complex environments. We propose a hybrid, sequential method combining Bird's-Eye-View vision-based continuous Deep Reinforcement Learning (DRL) with Model Predictive Control (MPC). DRL generates candidate trajectories in complex environments, while MPC refines these trajectories to ensure adherence to kinematic and dynamic constraints of the robot, as well as constraints modeling humans' current and predicted future positions. In this study, the DRL utilizes a Deep Deterministic Policy Gradient model for trajectory generation, demonstrating its capability to navigate non-convex obstacles, a task that might pose challenges for MPC. We demonstrate that the proposed hybrid DRL-MPC model performs favorably in handling new scenarios, computational efficiency, time to destination, and adaptability to complex multi-robot situations when compared to pure DRL or pure MPC approaches.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t11_10">
             09:00-10:00, Paper ThPI4T11.10
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('711'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Dual-Process Optimization for Multi-Vehicle Route Planning and Parts Collection Sequencing
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#276396" title="Click to go to the Author Index">
             Higa, Ryota
            </a>
           </td>
           <td class="r">
            NEC Corporation, National Institute of Advanced Industrial Scien
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#231274" title="Click to go to the Author Index">
             Kato, Takuro
            </a>
           </td>
           <td class="r">
            National Institute of Advanced Industrial Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#308416" title="Click to go to the Author Index">
             Ho, Florence
            </a>
           </td>
           <td class="r">
            NEC Corporation, National Institute of Advanced Industrial Scien
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab711" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#path_planning_for_multiple_mobile_robots_or_agents" title="Click to go to the Keyword Index">
               Path Planning for Multiple Mobile Robots or Agents
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#task_planning" title="Click to go to the Keyword Index">
               Task Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#autonomous_vehicle_navigation" title="Click to go to the Keyword Index">
               Autonomous Vehicle Navigation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We proposed a novel dual-process optimization approach for parts collection order and route planning in parts warehouses. Conventional multi-agent parts collection typically uses the vehicle routing problem (VRP), which focuses on minimizing the number of agents and costs. However, the model does not fully leverage the vehicle's potential. Moreover, multi-agent path finding (MAPF) focuses on route planning and avoiding path conflicts, ignoring the order of part collection. The proposed approach integrates algorithms from the traveling salesman problem (TSP) and path planning, and modifies them to suit the dynamic and complex environment of parts warehouses. This integration streamlines the collection process and considerably reduces the operational time. Thus, the study can improve automation and efficiency in parts warehouse management and improve optimization techniques. The proposed method achieved more than tenfold acceleration compared with the ideal centralized optimization, without cost increments. As the number of agents and part collections increases, centralized optimization requires a metaheuristic approach, which results in solution degradation. However, the proposed approach maintains over tenfold acceleration and produces solutions with shorter operational times. Furthermore, we conducted an ablation study comparing six methods, from entirely independent to centralized optimization, demonstrating that the proposed approach effectively balances computational time and solution accuracy.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t11_11">
             09:00-10:00, Paper ThPI4T11.11
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('793'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Path Re-Planning with Stochastic Obstacle Modeling: A Monte Carlo Tree Search Approach
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#354679" title="Click to go to the Author Index">
             Trotti, Francesco
            </a>
           </td>
           <td class="r">
            University of Verona
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#106097" title="Click to go to the Author Index">
             Farinelli, Alessandro
            </a>
           </td>
           <td class="r">
            University of Verona
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#100471" title="Click to go to the Author Index">
             Muradore, Riccardo
            </a>
           </td>
           <td class="r">
            University of Verona
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab793" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#path_planning_for_multiple_mobile_robots_or_agents" title="Click to go to the Keyword Index">
               Path Planning for Multiple Mobile Robots or Agents
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#probability_and_statistical_methods" title="Click to go to the Keyword Index">
               Probability and Statistical Methods
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#planning__scheduling_and_coordination" title="Click to go to the Keyword Index">
               Planning, Scheduling and Coordination
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Path re-planning and repairing are key topics for robust planning and navigation in open dynamic environments, finding applications in various domains such as fleet control of Unmanned Ground Vehicles (UGVs) in warehouses. The use of UGVs in open and dynamic environments requires flexible cooperation between human operators and the UGV fleet within a shared environment. In this paper, we propose a local strategy to re-plan the path of robots encountering unexpected and dynamic obstacles. Specifically, starting from a given Multi-Agent path, we model the re-planning problem as a Markov Decision Process (MDP) considering a stochastic obstacle lifespan, and we propose two local approaches based on Monte-Carlo Tree Search to re-plan the path of the robots that encounter obstacles. We compare these approaches with traditional Multi-Agent Path Finding (MAPF) algorithms to obtain new collision-free paths when an obstacle is detected. The evaluation is performed in simulation using benchmarking instances of warehouses and experimentally in a research facility with a scaled-down Industry 4.0 production line.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t11_12">
             09:00-10:00, Paper ThPI4T11.12
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('933'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Decentralized Partially Observable Markov Decision Process for Dynamic Obstacle Avoidance and Complete Area Coverage Using Multiple Reconfigurable Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#324043" title="Click to go to the Author Index">
             Pey, Javier Jia Jie
            </a>
           </td>
           <td class="r">
            Singapore University of Technology and Design
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#219480" title="Click to go to the Author Index">
             Samarakoon Mudiyanselage, Bhagya Prasangi Samarakoon
            </a>
           </td>
           <td class="r">
            Singapore University of Technology and Design
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#191819" title="Click to go to the Author Index">
             Muthugala Arachchige, Viraj Jagathpriya Muthugala
            </a>
           </td>
           <td class="r">
            Singapore University of Technology and Design
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#107677" title="Click to go to the Author Index">
             Elara, Mohan Rajesh
            </a>
           </td>
           <td class="r">
            Singapore University of Technology and Design
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab933" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#path_planning_for_multiple_mobile_robots_or_agents" title="Click to go to the Keyword Index">
               Path Planning for Multiple Mobile Robots or Agents
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Achieving complete area coverage in robotics is an essential aspect for applications such as cleaning and patrolling. While multi-agent frameworks have been implemented to address the challenge of complete coverage, the area coverage performances are hindered by physical constraints and dynamic obstacles that cause inaccessibility to certain areas of the environment. Reconfigurable robots have been adopted to mitigate this issue as the independent alteration of the morphologies during deployments enables overcoming tight spaces to access obstructed areas. Hence, this paper proposes a Multi-Agent Reinforcement Learning (MARL) framework leveraging the Decentralized Partially Observable Markov Decision Process (Dec-POMDP) to enable a team of reconfigurable robots to achieve complete coverage under the presence of dynamic obstacles. The framework is modelled to allow the robots to coordinate and plan their motions effectively while using shape adaptability to access narrow spaces while avoiding dynamic obstacles. Experimental results demonstrated the framework's ability to be generalised even when scaled up to a different number of agents across larger environments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t11_13">
             09:00-10:00, Paper ThPI4T11.13
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('956'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              CASRL: Collision Avoidance with Spiking Reinforcement Learning among Dynamic, Decision-Making Agents
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#355257" title="Click to go to the Author Index">
             Zhang, Chengjun
            </a>
           </td>
           <td class="r">
            ZhejiangLab
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#375458" title="Click to go to the Author Index">
             Yip, Ka-Wa
            </a>
           </td>
           <td class="r">
            Zhejiang Lab
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#351649" title="Click to go to the Author Index">
             Yang, Bo
            </a>
           </td>
           <td class="r">
            Zhejiang Lab
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#373976" title="Click to go to the Author Index">
             Zhang, Zhiyong
            </a>
           </td>
           <td class="r">
            Zhejiang Lab
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#355274" title="Click to go to the Author Index">
             Yuan, Mengwen
            </a>
           </td>
           <td class="r">
            Zhejianglab
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#405598" title="Click to go to the Author Index">
             Yan, Rui
            </a>
           </td>
           <td class="r">
            Zhejiang University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#150562" title="Click to go to the Author Index">
             Tang, Huajin
            </a>
           </td>
           <td class="r">
            Zhejiang University, China
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab956" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#path_planning_for_multiple_mobile_robots_or_agents" title="Click to go to the Keyword Index">
               Path Planning for Multiple Mobile Robots or Agents
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#collision_avoidance" title="Click to go to the Keyword Index">
               Collision Avoidance
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#bioinspired_robot_learning" title="Click to go to the Keyword Index">
               Bioinspired Robot Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
              Developing an efficient collision avoidance policy with Spiking Reinforcement Learning for dynamic, decisionmaking agents remains challenging. Moreover, the implementation of energy-efficient collision avoidance is important for mobile robots that operate with limited on-board computing resources. Most existing energy-efficient methods via spiking reinforcement learning are predominately concerned with the navigational capabilities of a single agent, and are unable to handle a large, and possibly varying number of agents. To overcome these limitations, we propose a model called collision avoidance with spiking reinforcement learning (CASRL), based on proximal policy optimization algorithms. This proposed model consists of an actor with spiking neural networks (SNNs) and a critic with deep neural networks (DNNs). Our spiking reinforcement learning algorithm is advantageous to handle an arbitrary number of other agents by virtue of a spiking-gated transformer (SpikeGTr) architecture and an accumulate-tofire (ATF) module. Extensive experimental results demonstrate that CASRL obtains a competitive success rate of navigation and exhibits higher time-efficiency for navigation in crowded scenarios compared to traditional DNN-based methods.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t11_14">
             09:00-10:00, Paper ThPI4T11.14
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1045'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Mastering Scene Rearrangement with Expert-Assisted Curriculum Learning and Adaptive Trade-Off Tree-Search
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#350947" title="Click to go to the Author Index">
             Wang, Zan
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395239" title="Click to go to the Author Index">
             Wang, Hanqing
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#350943" title="Click to go to the Author Index">
             Liang, Wei
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1045" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#path_planning_for_multiple_mobile_robots_or_agents" title="Click to go to the Keyword Index">
               Path Planning for Multiple Mobile Robots or Agents
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Scene Rearrangement Planning (SRP) has recently emerged as a crucial interior scene task; however, current approaches still face two primary issues. First, prior works define the action space of SRP using handcrafted coarse-grained actions, which are inflexible for scene arrangement transition and impractical for real-world deployment. Secondly, the scarcity of realistic indoor scene rearrangement data hinders popular data-hungry learning approaches and quantitative evaluation. To tackle these issues, we propose a fine-grained action space definition and curate a large-scale scene rearrangement dataset to facilitate the training of learning approaches and comprehensive benchmarking. Building upon this dataset, we introduce a novel framework, PLATO, designed for efficient agent training and inference. Our approach features an exPert-assisted curriculum Learning (PL) paradigm that possesses a Behavior Cloning (BC) and an offline Reinforcement Learning (RL) curriculum for agent training, along with an advanced tree-search-based planner enhanced by an Adaptive Trade-Off (ATO) strategy to improve expert agent performance further. We demonstrate the superior performance of our method over baseline agents through extensive experiments and provide a detailed analysis to elucidate its rationale. Our project website can be accessed at pl-ato.github.io.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t11_15">
             09:00-10:00, Paper ThPI4T11.15
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1052'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Ensembling Prioritized Hybrid Policies for Multi-Agent Pathfinding
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395297" title="Click to go to the Author Index">
             Tang, Huijie
            </a>
           </td>
           <td class="r">
            KAIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#302289" title="Click to go to the Author Index">
             Berto, Federico
            </a>
           </td>
           <td class="r">
            KAIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#325410" title="Click to go to the Author Index">
             Park, Jinkyoo
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1052" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#path_planning_for_multiple_mobile_robots_or_agents" title="Click to go to the Keyword Index">
               Path Planning for Multiple Mobile Robots or Agents
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#machine_learning_for_robot_control" title="Click to go to the Keyword Index">
               Machine Learning for Robot Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Multi-Agent Reinforcement Learning (MARL) based Multi-Agent Path Finding (MAPF) has recently gained attention due to its efficiency and scalability. Several MARL-MAPF methods choose to use communication to enrich the information one agent can perceive. However, existing works still struggle in structured environments with high obstacle density and a high number of agents. To further improve the performance of the communication-based MARL-MAPF solvers, we propose a new method, Ensembling Prioritized Hybrid Policies (EPH). We first propose a selective communication block to gather richer information for better agent coordination within multi-agent environments and train the model with a Q-learning-based algorithm. We further introduce three advanced inference strategies aimed at bolstering performance during the execution phase. First, we hybridize the neural policy with single-agent expert guidance for navigating conflict-free zones. Secondly, we propose Q value-based methods for prioritized resolution of conflicts as well as deadlock situations. Finally, we introduce a robust ensemble method that can efficiently collect the best out of multiple possible solutions. We empirically evaluate EPH in complex multi-agent environments and demonstrate competitive performance against state-of-the-art neural methods for MAPF.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t11_16">
             09:00-10:00, Paper ThPI4T11.16
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1812'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Hierarchical Search-Based Cooperative Motion Planning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#362245" title="Click to go to the Author Index">
             Wu, Yuchen
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#346415" title="Click to go to the Author Index">
             Yang, Yifan
            </a>
           </td>
           <td class="r">
            ZheJiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#314116" title="Click to go to the Author Index">
             Xu, Gang
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#272511" title="Click to go to the Author Index">
             Cao, Junjie
            </a>
           </td>
           <td class="r">
            Institute of Cyber Systems and Control, Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#318208" title="Click to go to the Author Index">
             Chen, Yansong
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#272739" title="Click to go to the Author Index">
             Wen, Licheng
            </a>
           </td>
           <td class="r">
            Shanghai AI Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#122966" title="Click to go to the Author Index">
             Liu, Yong
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1812" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#path_planning_for_multiple_mobile_robots_or_agents" title="Click to go to the Keyword Index">
               Path Planning for Multiple Mobile Robots or Agents
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Cooperative path planning, a crucial aspect of multi-agent systems research, serves a variety of sectors, including military, agriculture, and industry. Many existing algorithms, however, come with certain limitations, such as simplified kinematic models and inadequate support for multiple group scenarios. Focusing on the planning problem associated with a nonholonomic Ackermann model for Unmanned Ground Vehicles (UGV), we propose a leaderless, hierarchical Search-Based Cooperative Motion Planning (SCMP) method. The high-level utilizes a binary conflict search tree to minimize runtime, while the low-level fabricates kinematically feasible, collision-free paths that are shape-constrained. Our algorithm can adapt to scenarios featuring multiple groups with different shapes, outlier agents, and elaborate obstacles. We conduct algorithm comparisons, performance testing, simulation, and real-world testing, verifying the effectiveness and applicability of our algorithm. The implementation of our method will be open-sourced at https://github.com/WYCUniverStar/SCMP.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thpi4t12">
             <b>
              ThPI4T12
             </b>
            </a>
           </td>
           <td class="r">
            Room 12
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thpi4t12" title="Click to go to the Program at a Glance">
             <b>
              Aerial Systems I
             </b>
            </a>
           </td>
           <td class="r">
            Teaser Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#285160" title="Click to go to the Author Index">
             Liu, Song
            </a>
           </td>
           <td class="r">
            ShanghaiTech University
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#190182" title="Click to go to the Author Index">
             Hamaza, Salua
            </a>
           </td>
           <td class="r">
            TU Delft
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t12_01">
             09:00-10:00, Paper ThPI4T12.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3123'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Streamlining Forest Wildfire Surveillance: AI-Enhanced UAVs Utilizing the FLAME Aerial Video Dataset for Lightweight and Efficient Monitoring
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398859" title="Click to go to the Author Index">
             Zhao, Lemeng
            </a>
           </td>
           <td class="r">
            Center for Applied Statistics and School of Statistics, Renmin U
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#285190" title="Click to go to the Author Index">
             Hu, Junjie
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong, Shenzhen
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398905" title="Click to go to the Author Index">
             Bi, Jianchao
            </a>
           </td>
           <td class="r">
            Gaoling School of Artificial Intelligence, Renmin University Of
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397659" title="Click to go to the Author Index">
             Bai, Yanbing
            </a>
           </td>
           <td class="r">
            Center for Applied Statistics, School of Statistics, Renmin Univ
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398862" title="Click to go to the Author Index">
             Erick, Mas
            </a>
           </td>
           <td class="r">
            International Research Institute of Disaster Science(IRIDeS), To
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397668" title="Click to go to the Author Index">
             Koshimura, Shunichi
            </a>
           </td>
           <td class="r">
            International Research Institute of Disaster Science, Tohoku Uni
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3123" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#environment_monitoring_and_management" title="Click to go to the Keyword Index">
               Environment Monitoring and Management
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#robotics_and_automation_in_agriculture_and_forestry" title="Click to go to the Keyword Index">
               Robotics and Automation in Agriculture and Forestry
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#energy_and_environment_aware_automation" title="Click to go to the Keyword Index">
               Energy and Environment-Aware Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In recent years, unmanned aerial vehicles (UAVs) have played an increasingly crucial role in supporting disaster emergency response efforts by analyzing aerial images. While current deep-learning models focus on improving accuracy, they often overlook the limited computing resources of UAVs. This study recognizes the imperative for real-time data processing in disaster response scenarios and introduces a lightweight and efficient approach for aerial video understanding. Our methodology identifies redundant portions within the video through policy networks and eliminates this excess information using frame compression techniques. Additionally, we introduced the concept of a station point, which leverages future information in the sequential policy network, thereby enhancing accuracy. To validate our method, we employed the wildfire FLAME dataset. Compared to the baseline, our approach reduces computation costs by more than 10 times while improving accuracy by 3%. Moreover, our method can intelligently select salient frames from the video, refining the dataset. This feature enables sophisticated models to be effectively trained on a smaller dataset, significantly reducing the time spent during the training process.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t12_02">
             09:00-10:00, Paper ThPI4T12.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('538'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Reconfigurable Multi-Rotor for High-Precision Physical Interaction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392840" title="Click to go to the Author Index">
             Taylor, Joshua
            </a>
           </td>
           <td class="r">
            National University of Singapore (NUS)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#227936" title="Click to go to the Author Index">
             Nursultan, Imanberdiyev
            </a>
           </td>
           <td class="r">
            Agency for Science, Technology and Research (A*STAR)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#155676" title="Click to go to the Author Index">
             Chuah, Meng Yee (Michael)
            </a>
           </td>
           <td class="r">
            Agency for Science, Technology and Research (A*STAR)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#196715" title="Click to go to the Author Index">
             Yau, Wei-Yun
            </a>
           </td>
           <td class="r">
            I2R
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#180884" title="Click to go to the Author Index">
             Sartoretti, Guillaume Adrien
            </a>
           </td>
           <td class="r">
            National University of Singapore (NUS)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#203750" title="Click to go to the Author Index">
             Camci, Efe
            </a>
           </td>
           <td class="r">
            Institute for Infocomm Research
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab538" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#grippers_and_other_end_effectors" title="Click to go to the Keyword Index">
               Grippers and Other End-Effectors
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Unmanned aerial vehicles (UAVs) for contact-based tasks at height can greatly improve the safety of the human workers involved. However, performing contact-based tasks with typical under-actuated UAVs is non-trivial. Due to their coupled translational and rotational dynamics and their limited station-keeping performance under physical disturbances, it is difficult to maintain precise and consistent contact. We address these problems in the context of physical interaction with vertical, cylindrical target objects, such as trees. We present a novel UAV design with a pair of tilt-rotors and a landing gear that can reconfigure into a front-mounted, two-fingered gripper. While the tilt-rotors provide horizontal force toward the target object without pitching the UAV forward, the reconfigurable landing gear enables the UAV to obtain support from the target object. Such support results in an approximately 80% improvement in position- and heading-keeping performance. Moreover, the landing gear is designed as a cable-driven under-actuated system, which requires only one actuator to control both the reconfiguration and the grasping (i.e., five degrees of freedom in total). Such a minimalist design helps keep the UAV power consumption for interactions low. This marks progress towards safe, high-precision physical interaction against vertical, cylindrical target objects. Our UAV in action: https://youtu.be/D-65vldox_A.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t12_03">
             09:00-10:00, Paper ThPI4T12.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1116'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Time-Varying Control Barrier Function for Safe and Precise Landing of a UAV on a Moving Target
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#277573" title="Click to go to the Author Index">
             Sankaranarayanan, Viswa Narayanan
            </a>
           </td>
           <td class="r">
            Lulea University of Techonology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#341005" title="Click to go to the Author Index">
             Saradagi, Akshit
            </a>
           </td>
           <td class="r">
            Lule University of Technology, Lule, Sweden
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#305363" title="Click to go to the Author Index">
             Satpute, Sumeet
            </a>
           </td>
           <td class="r">
            Lule University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#105882" title="Click to go to the Author Index">
             Nikolakopoulos, George
            </a>
           </td>
           <td class="r">
            Lule University of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1116" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#robot_safety" title="Click to go to the Keyword Index">
               Robot Safety
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this article, we present a control barrier function (CBF)-based control strategy for the safe and precise landing of an unmanned aerial vehicle (UAV) on a moving target. The CBF is time-varying, as it depends on the velocity of the landing platform and captures three crucial safety constraints: (a) collision avoidance with the landing platform, (b) precise vertical descent on a narrow landing platform, and (c) ground clearance throughout the landing maneuver. The proposed CBF's parameters can be adjusted to set the desired width and height of the descending cone. A quadratic program-based CBF safety filter is designed, which takes a nominal position tracking control input and yields a minimally invasive control input that enforces the safety constraints throughout the landing maneuver. The controller's feasibility is analyzed and its performance is validated through multiple experiments using a quadrotor UAV and an unmanned ground vehicle.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t12_04">
             09:00-10:00, Paper ThPI4T12.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1340'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              VRExplorer: An Efficient View-Region Based Autonomous Exploration Method in Unknown Environments for UAV
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#380629" title="Click to go to the Author Index">
             Xu, Kai
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#310721" title="Click to go to the Author Index">
             Zheng, Lanxiang
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#291886" title="Click to go to the Author Index">
             Wei, Mingxin
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#169114" title="Click to go to the Author Index">
             Cheng, Hui
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1340" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#search_and_rescue_robots" title="Click to go to the Keyword Index">
               Search and Rescue Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Autonomous exploration plays a crucial role in robotics applications like rescue and scene reconstruction. This work addresses the challenges of autonomous exploration in intricate unknown environments by presenting a novel UAV autonomous exploration method based on a new concept of the view-region. Our proposed approach leverages the view-region to replace the conventional viewpoint generation and selection process, streamlining the planning process for exploration. Simultaneously, we model the problem of maximizing frontier coverage within the field of view during exploration, and jointly optimize it with the exploration path optimization problem. This approach ensures exploration path safety and effectiveness while being aggressive. Additionally, a gimbal is incorporated beneath the camera, with an associated optimization problem designed to minimize UAV self-rotation and enhance exploration efficiency. Simulations and real-world experiments demonstrate that the proposed method outperforms existing state-of-the-art methods in terms of runtime and distance traveled.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t12_05">
             09:00-10:00, Paper ThPI4T12.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1551'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Autonomous Power Line Tracking with mmWave Radar
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#327943" title="Click to go to the Author Index">
             Malle, Nicolaj
            </a>
           </td>
           <td class="r">
            University of Southern Denmark
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#268678" title="Click to go to the Author Index">
             Ebeid, Emad
            </a>
           </td>
           <td class="r">
            University of Southern Denmark
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1551" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This work proposes a novel drone system designed to autonomously track and follow power lines and reconstruct them in 3D with a point cloud representation based on mmWave radar measurements. The system is composed of a GNSS-enabled quadrotor UAV equipped with a combined mmWave radar sensor and onboard compute module payload and has been designed to be small, lightweight, and low-cost. MmWave radar sensors offer great range and sensitivity in the task of power line detection with a high level of sparsity in the produced data when compared to traditional sensors such as LiDARs. The proposed system overcomes the radar sensor's shortcomings by building up a point cloud representing the power line environment as the drone moves around in it. The built-up point cloud is analyzed using the onboard computer to detect the cables in the power line environment and to produce pose-estimates of each line. The system has been tested in a variety of scenarios and has been shown to be able to accurately detect and track power lines in varying weather conditions.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t12_06">
             09:00-10:00, Paper ThPI4T12.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2400'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Data-Driven System Identification of Quadrotors Subject to Motor Delays
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#266338" title="Click to go to the Author Index">
             Eschmann, Jonas
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#210977" title="Click to go to the Author Index">
             Albani, Dario
            </a>
           </td>
           <td class="r">
            Technology Innovation Institure
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#142369" title="Click to go to the Author Index">
             Loianno, Giuseppe
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2400" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Recently non-linear control methods like Model Predictive Control (MPC) and Reinforcement Learning (RL) have attracted increased interest in the quadrotor control community. In contrast to classic control methods like cascaded PID controllers, MPC and RL heavily rely on an accurate model of the system dynamics. The process of quadrotor system identification is notoriously tedious and is often pursued with additional equipment like a thrust stand. Furthermore, low-level details like motor delays which are crucial for accurate end-to-end control are often neglected. In this work, we introduce a data-driven method to identify a quadrotors inertia parameters, thrust curves, torque coefficients, and first-order motor delay purely based on proprioceptive data. The estimation of the motor delay is particularly challenging as usually, the RPMs can not be measured. We derive a Maximum A Posteriori (MAP)-based method to estimate the latent time constant. Our approach only requires about a minute of flying data that can be collected without any additional equipment and usually consists of three simple maneuvers. Experimental results demonstrate the ability of our method to accurately recover the parameters of multiple quadrotors. It also facilitates the deployment of RL-based, end-to-end quadrotor control of a large quadrotor under harsh, outdoor conditions.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t12_07">
             09:00-10:00, Paper ThPI4T12.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2423'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Tactile Odometry in Aerial Physical Interaction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#320311" title="Click to go to the Author Index">
             Schuster, Micha
            </a>
           </td>
           <td class="r">
            TU Dresden
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#283082" title="Click to go to the Author Index">
             Bredenbeck, Anton
            </a>
           </td>
           <td class="r">
            TU Delft
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#323173" title="Click to go to the Author Index">
             Beitelschmidt, Michael
            </a>
           </td>
           <td class="r">
            TU Dresden, Institute of Solid Mechanics
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#190182" title="Click to go to the Author Index">
             Hamaza, Salua
            </a>
           </td>
           <td class="r">
            TU Delft
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2423" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__perception_and_autonomy" title="Click to go to the Keyword Index">
               Aerial Systems: Perception and Autonomy
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Aerial robots are well-established technologies in environments characterized by reliable GNSS signals and favorable conditions for navigation based on cameras or LiDARs. However, their robustness is significantly challenged whenever ambient lighting is insufficient, GNSS signals are blocked, and range measurements are corrupted, for example, in underground, dark, or foggy environments. There, conventional navigation methods solely based on computer vision are very limited. This work proposes a completely novel approach to Aerial Tactile Odometry for pose estimation of aerial robots exploiting contact to precisely determine the system's pose. By employing a compliant end-effector design with onboard tactile information by means of a trackball, we infer the complete UAV's pose with respect to the environment, and the path traveled during contact. Through a large set of experiments, the proposed method shows centimeter-level accuracy for various relative orientations between the environment and the robot as well as for different trajectories. Akin to conventional dead-reckoning odometry methods in wheeled robotics, this method provides a valuable additional source of pose estimation, increasing the robustness of aerial robots -- especially aerial manipulators -- in the real world.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t12_08">
             09:00-10:00, Paper ThPI4T12.8
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2930'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Online Rotor Fault Detection and Isolation for Vertical Takeoff and Landing Vehicles
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#394913" title="Click to go to the Author Index">
             Lian, Jiaqi
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#205616" title="Click to go to the Author Index">
             Gandhi, Neeraj
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398748" title="Click to go to the Author Index">
             Wang, Yifan
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#257352" title="Click to go to the Author Index">
             Phan, Linh Thi Xuan
            </a>
           </td>
           <td class="r">
            University of Pennsylvania / Roblox
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2930" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#failure_detection_and_recovery" title="Click to go to the Keyword Index">
               Failure Detection and Recovery
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Vertical take-off and landing (VTOL) vehicles are becoming increasingly popular for real-world transport; but, as with any vehicle, guaranteeing safety is both extremely critical and highly challenging due to issues like rotor faults. Existing fault detection and isolation (FDI) techniques usually focus on multirotor systems or fixed wing systems, rather than the hybrid VTOLs. Since VTOLs have both rotors and ailerons, a fault in a rotor may be masked by the (correctly working) ailerons, making it much more difficult to detect faults. However, this masking only works when ailersons are used (e.g., during cruising), leaving the takeoff and landing vulnerable to crashes. This paper presents an online rotor fault detection and isolation (FDI) method for VTOLs. The approach uses pose analysis and aileron command data to quickly and accurately identify the faulty rotor and to compute the severity of the fault. Our method works for hard-to-detect fault scenarios, such as small-severity faults that are masked during cruise flight but not during vertical motion. We evaluated our technique in a SITL PX4 simulation of a modified Deltaquad QuadPlane. The results show that our FDI technique can quickly detect and isolate faults in real time (within 1s-2.5s) and achieve high isolation success rate (91.67%) across six rotors, and that it can estimate the severity of faults to within 2%. When applying a simple recovery process post-isolation, the system consistently achieved safe landing.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t12_09">
             09:00-10:00, Paper ThPI4T12.9
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2967'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              ROG-Map: An Efficient Robocentric Occupancy Grid Map for Large-Scene and High-Resolution LiDAR-Based Motion Planning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#292033" title="Click to go to the Author Index">
             Ren, Yunfan
            </a>
           </td>
           <td class="r">
            The University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#283472" title="Click to go to the Author Index">
             Cai, Yixi
            </a>
           </td>
           <td class="r">
            University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#320284" title="Click to go to the Author Index">
             Zhu, Fangcheng
            </a>
           </td>
           <td class="r">
            The University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#338715" title="Click to go to the Author Index">
             Liang, Siqi
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology, Shenzhen
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#204381" title="Click to go to the Author Index">
             Zhang, Fu
            </a>
           </td>
           <td class="r">
            University of Hong Kong
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2967" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__perception_and_autonomy" title="Click to go to the Keyword Index">
               Aerial Systems: Perception and Autonomy
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Recent advances in LiDAR technology have opened up new possibilities for robotic navigation. Given the widespread use of occupancy grid maps (OGMs) in robotic motion planning, this paper aims to address the challenges of integrating LiDAR with OGMs. To this end, we propose ROG-Map, a uniform grid-based OGM that maintains a local map moving along with the robot to enable efficient map operation and reduce memory costs for large-scene autonomous flight. Moreover, we present a novel incremental obstacle inflation method that significantly reduces the computational cost of inflation. The proposed method outperforms state-of-the-art (SOTA) methods on various public datasets. To demonstrate the effectiveness and efficiency of ROG-Map, we integrate it into a complete quadrotor system and perform autonomous flights against both small obstacles and large-scale scenes. During real-world flight tests with a 0.05m resolution local map and 30m x 30m x 6m local map size, ROG-Map takes only 29.8% of frame time on average to update the map at a frame rate of 50Hz (i.e., 5.96ms in 20ms), including 0.33% (i.e., 0.66ms) to perform obstacle inflation, which represents only half of the total map updating time when compared to the state-of-the-art baseline. We release ROG-Map as an open-source ROS package to promote the development of LiDAR-based motion planning.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t12_10">
             09:00-10:00, Paper ThPI4T12.10
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2060'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              DOB-Based Wind Estimation of a UAV Using Its Onboard Sensor
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#391309" title="Click to go to the Author Index">
             Yu, Haowen
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#391930" title="Click to go to the Author Index">
             Liang, Xianqi
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#204460" title="Click to go to the Author Index">
             Lyu, Ximin
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2060" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__mechanics_and_control" title="Click to go to the Keyword Index">
               Aerial Systems: Mechanics and Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Unmanned Aerial Vehicles (UAVs) play a crucial role in meteorological research, particularly in environmental wind field measurements. However, several challenges exist in current wind measurement methods using UAVs that need to be addressed. Firstly, the accuracy of measurement is low, and the measurement range is limited. Secondly, the algorithms employed lack robustness and adaptability across different UAV platforms. Thirdly, there are limited approaches available for wind estimation during dynamic flight. Finally, while horizontal plane measurements are feasible, vertical direction estimation is often missing. To tackle these challenges, we present and implement a comprehensive wind estimation algorithm. Our algorithm offers several key features, including the capability to estimate the 3-D wind vector, enabling wind estimation even during dynamic flight of the UAV. Furthermore, our algorithm exhibits adaptability across various UAV platforms. Experimental results in the wind tunnel validate the effectiveness of our algorithm, showcasing improvements such as wind speed accuracy of 0.11 m/s and wind direction errors of less than 2.8 deg. Additionally, our approach extends the measurement range to 10 m/s.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t12_11">
             09:00-10:00, Paper ThPI4T12.11
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2468'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SwiftEagle: An Advanced Open-Source, Miniaturized FPGA UAS Platform with Dual DVS/Frame Camera for Cutting-Edge Low-Latency Autonomous Algorithms
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398276" title="Click to go to the Author Index">
             Vogt, Christian
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398279" title="Click to go to the Author Index">
             Jost, Michael
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#335799" title="Click to go to the Author Index">
             Magno, Michele
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2468" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#embedded_systems_for_robotic_and_automation" title="Click to go to the Keyword Index">
               Embedded Systems for Robotic and Automation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#hardware_software_integration_in_robotics" title="Click to go to the Keyword Index">
               Hardware-Software Integration in Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Low-latency sensing and decision-making processing are critical requirements for the highly dynamic control and perception applications often found in Unmanned Areal Systems (UASs). Novel sensors such as Dynamic Vision Sensors (DVSs) are enhancing the pure performance of the perception component with orders of magnitude lower latency. However, they are typically not optimally integrated with the computing hardware, which effectively reduces the potential in both latency and power consumption. In addition to the non-optimal integration, low latency processing of such high data rate generating sensors is often challenging on resource-constrained platforms. Here, Field Programmable Gate Arrays (FPGAs) platforms offer a promising set of features, including low-level access to hardware and memories, as well as high-speed interfaces. On the other hand, FPGA platforms are not as popular on UASs due to their complex programming and development environments, high initial learning curve, and challenges in achieving optimal performance. To accelerate future development, this paper presents SwiftEagle, an open-source, cutting-edge 720 g FPGA based UAS based on a custom hardware design including a dual camera interface RGB/DVSs on a multi-sensors subsystem and an initial software and firmware stack designed for high precision recording of machine learning datasets in-flight with sub-micro-second time resolution and on-FPGA rendering of DVS event frames. Utilizing this developed platform, as proof-of-concept the end-to-end latency of a novel, just released DVS is shown to be below 210 s as a worst-case scenario, enabling future cutting-edge autonomous algorithms.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t12_12">
             09:00-10:00, Paper ThPI4T12.12
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3478'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Hardware-Software Co-Design for Path Planning by Drones
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398496" title="Click to go to the Author Index">
             Dube, Ayushi
            </a>
           </td>
           <td class="r">
            Arizona State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397757" title="Click to go to the Author Index">
             Patil, Omkar Deepak
            </a>
           </td>
           <td class="r">
            Arizona State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398511" title="Click to go to the Author Index">
             Singh, Gian
            </a>
           </td>
           <td class="r">
            Arizona State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#160888" title="Click to go to the Author Index">
             Gopalan, Nakul
            </a>
           </td>
           <td class="r">
            Arizona State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398494" title="Click to go to the Author Index">
             Vrudhula, Sarma
            </a>
           </td>
           <td class="r">
            Arizona State University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3478" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#motion_control" title="Click to go to the Keyword Index">
               Motion Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#service_robotics" title="Click to go to the Keyword Index">
               Service Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This work consists of two main components: designing a hardware-software co-design, MT+, for adapting the Mikami-Tabuchi algorithm for on-board path planning by drones in a 3D environment; and development of a specialized custom hardware accelerator CDU, as a part of MT+, for parallel collision detection. Collision detection is a performance bottleneck in path planning. MT+ reduces the delay in path planning without using any heuristic. A comparative analysis between the state-of-the-art path planning algorithm A* and Mikami-Tabuchi is performed to show that Mikami-Tabuchi is faster than A* in typical real-world environments. In custom-generated environments, path planning using Mikami-Tabuchi shows a latency improvement of	1.7X across varying average sizes of obstacles and 2.7X across varying obstacle density over state-of-the-art path planning algorithm, A*. Further, the experiments show that the co-design achieves speedups over a full software implementation on CPU, averaging between 10% to 60% across different densities and sizes of obstacles. CDU area and power overheads are negligible against a conventional single-core processor.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t12_13">
             09:00-10:00, Paper ThPI4T12.13
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1456'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Attainable Force Approximation and Full-Pose Tracking Control of an Over-Actuated Thrust-Vectoring Modular Team UAV
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#372812" title="Click to go to the Author Index">
             Chu, Yen-Cheng
            </a>
           </td>
           <td class="r">
            National Taiwan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#412644" title="Click to go to the Author Index">
             Fang, Kai-Cheng
            </a>
           </td>
           <td class="r">
            National Taiwan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#102410" title="Click to go to the Author Index">
             Lian, Feng-Li
            </a>
           </td>
           <td class="r">
            National Taiwan University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1456" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__mechanics_and_control" title="Click to go to the Keyword Index">
               Aerial Systems: Mechanics and Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#cellular_and_modular_robots" title="Click to go to the Keyword Index">
               Cellular and Modular Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Traditional vertical take-off and landing (VTOL) aircraft can not achieve optimal efficiency for various payload weights and has limited mobility due to its under-actuation. With the thrust-vectoring mechanism, the proposed modular team UAV is fully actuated at certain attitudes. However, the attainable force space (AFS) differs according to the team configuration, which makes the controller design difficult. We propose an approximation to the AFS and a full-pose tracking controller with an attitude planner and a force projection, which guarantees the control force is feasible. The proposed approach can be applied to UAVs having multiple thrust-vectoring effectors with homogeneous agents. The simulation and experiment demonstrate a tilting motion during hovering for a 4-agent team.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t12_14">
             09:00-10:00, Paper ThPI4T12.14
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2091'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Intention-Aware Planner for Robust and Safe Aerial Tracking
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#373958" title="Click to go to the Author Index">
             Ren, Qiuyu
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#317106" title="Click to go to the Author Index">
             Yu, Huan
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#374843" title="Click to go to the Author Index">
             Dai, Jiajun
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#317120" title="Click to go to the Author Index">
             Zheng, Zhi
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#284182" title="Click to go to the Author Index">
             Meng, Jun
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#142169" title="Click to go to the Author Index">
             Xu, Li
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#213352" title="Click to go to the Author Index">
             Xu, Chao
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#200893" title="Click to go to the Author Index">
             Gao, Fei
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#178926" title="Click to go to the Author Index">
             Cao, Yanjun
            </a>
           </td>
           <td class="r">
            Zhejiang University, Huzhou Institute of Zhejiang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2091" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__perception_and_autonomy" title="Click to go to the Keyword Index">
               Aerial Systems: Perception and Autonomy
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#human_aware_motion_planning" title="Click to go to the Keyword Index">
               Human-Aware Motion Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Autonomous target tracking with quadrotors has wide applications in many scenarios, such as cinematographic follow-up shooting or suspect chasing.Target motion prediction is necessary when designing the tracking planner.However, the widely used constant velocity or constant rotation assumption can not fully capture the dynamics of the target. The tracker may fail when the target happens to move aggressively, such as sudden turn or deceleration.In this paper, we propose an intention-aware planner by additionally considering the intention of the target to enhance safety and robustness in aerial tracking applications. Firstly, a designated intention prediction method is proposed, which combines a user-defined potential assessment function and a state observation function. A reachable region is generated to specifically evaluate the turning intentions. Then we design an intention-driven hybrid A* method to predict the future possible positions for the target. Finally, an intention-aware optimization approach is designed to generate a spatial-temporal optimal trajectory, allowing the tracker to perceive unexpected situations from the target. Benchmark comparisons and real-world experiments are conducted to validate the performance of our method.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t12_15">
             09:00-10:00, Paper ThPI4T12.15
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3211'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              AGL-NET: Aerial-Ground Cross-Modal Global Localization with Varying Scales
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#269556" title="Click to go to the Author Index">
             Guan, Tianrui
            </a>
           </td>
           <td class="r">
            University of Maryland
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#339326" title="Click to go to the Author Index">
             Xian, Ruiqi
            </a>
           </td>
           <td class="r">
            University of Maryland-College Park
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#325883" title="Click to go to the Author Index">
             Wang, Xijun
            </a>
           </td>
           <td class="r">
            University of Maryland, College Park
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#367351" title="Click to go to the Author Index">
             Wu, Xiyang
            </a>
           </td>
           <td class="r">
            University of Maryland
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#367201" title="Click to go to the Author Index">
             Elnoor, Mohamed
            </a>
           </td>
           <td class="r">
            University of Maryland
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#210737" title="Click to go to the Author Index">
             Song, Daeun
            </a>
           </td>
           <td class="r">
            University of Maryland
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#106235" title="Click to go to the Author Index">
             Manocha, Dinesh
            </a>
           </td>
           <td class="r">
            University of Maryland
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3211" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__perception_and_autonomy" title="Click to go to the Keyword Index">
               Aerial Systems: Perception and Autonomy
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#ai_based_methods" title="Click to go to the Keyword Index">
               AI-Based Methods
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present AGL-Net, a novel learning-based method for global localization using LiDAR point clouds and satellite maps. AGL-Net tackles two critical challenges: bridging the representation gap between image and points modalities for robust feature matching, and handling inherent scale discrepancies between global view and local view. To address these challenges, AGL-Net leverages a unified network architecture with a novel two-stage matching design. The first stage extracts informative neural features directly from raw sensor data and performs initial feature matching. The second stage refines this matching process by extracting informative skeleton features and incorporating a novel scale alignment step to rectify scale variations between LiDAR and map data. Furthermore, a novel scale and skeleton loss function guides the network toward learning scale-invariant feature representations, eliminating the need for pre-processing satellite maps. This significantly improves real-world applicability in scenarios with unknown map scales. To facilitate rigorous performance evaluation, we introduce a meticulously designed dataset within the CARLA simulator specifically tailored for metric localization training and assessment.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t12_16">
             09:00-10:00, Paper ThPI4T12.16
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('892'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Data-Driven Modeling of Ground Effect for UAV Landing on a Vertical Oscillating Platform
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#366834" title="Click to go to the Author Index">
             He, Binglin
            </a>
           </td>
           <td class="r">
            ShanghaiTech University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#376464" title="Click to go to the Author Index">
             Zhang, Heng
            </a>
           </td>
           <td class="r">
            ShanghaiTech University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#412393" title="Click to go to the Author Index">
             Lai, Baisheng
            </a>
           </td>
           <td class="r">
            Alibaba Group
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#285160" title="Click to go to the Author Index">
             Liu, Song
            </a>
           </td>
           <td class="r">
            ShanghaiTech University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#352058" title="Click to go to the Author Index">
             Wang, Yang
            </a>
           </td>
           <td class="r">
            Shanghaitech University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab892" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#model_learning_for_control" title="Click to go to the Keyword Index">
               Model Learning for Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#underactuated_robots" title="Click to go to the Keyword Index">
               Underactuated Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__mechanics_and_control" title="Click to go to the Keyword Index">
               Aerial Systems: Mechanics and Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Abstract Landing on a vertical oscillating platform poses a significant challenge for multi-rotor unmanned aerial vehicle (UAV) due to the time-varying ground effect (GE). In this work, we formulated a data-driven GE dynamic model that accurately describes the complex interactions between UAVs and both stationary and oscillating platforms. Integrating this model with a feedforward controller effectively compensates for GE, resulting in improved landing performance. The proposed GE model elucidates the relationship between GE and factors such as UAVs velocity, throttle magnitude, and the motion of the landing platform. It highlights that the GE experienced by the landing process of UAV is not only contingent on the current state but also related to past states. The resulting GE model is parsimonious and suitable to onboard computers with limited computational power, and its accuracy has been confirmed through a series of flight experiments. To demonstrate the effectiveness of the developed UAV landing scheme, we compared our approach with robust control and internal model control methods. Experimental results indicate that the proposed landing strategy achieves faster and smoother landings, with at least a 22% improvement in smoothness and a 25% reduction in landing time. Moreover, although the proposed control scheme is designed for UAV landing tasks, it is also applicable to scenarios where a UAV hovers over vertically oscillating platforms.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi4t12_17">
             09:00-10:00, Paper ThPI4T12.17
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1704'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Small Multi-Rotor UAV Oriented Direct Thrust Sensor Based on Lightweight Barometers
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#337833" title="Click to go to the Author Index">
             Jiang, Han
            </a>
           </td>
           <td class="r">
            The State Key Laboratory of Robotics, Shenyang Institute of Auto
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#357571" title="Click to go to the Author Index">
             Chang, Yanchun
            </a>
           </td>
           <td class="r">
            Shenyang Institute of Automation
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#322190" title="Click to go to the Author Index">
             Yang, Liying
            </a>
           </td>
           <td class="r">
            Shenyang Institute of Automation
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#130637" title="Click to go to the Author Index">
             He, Yuqing
            </a>
           </td>
           <td class="r">
            Shenyang Institute of Automation, Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1704" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#force_and_tactile_sensing" title="Click to go to the Keyword Index">
               Force and Tactile Sensing
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#force_control" title="Click to go to the Keyword Index">
               Force Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mechanism_design" title="Click to go to the Keyword Index">
               Mechanism Design
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The multirotor unmanned aerial vehicle (UAV) requires precise control over thrust output when operating in wind-disturbed environments or executing intricate flight missions. Although current commercial force sensors offer high sensitivity and accuracy, they are often heavy and costly. These characteristics restrict their applicability in weight-sensitive and cost-sensitive scenarios, such as thrust measurement in UAVs. To overcome this difficulty, we have developed an embedded barometric force sensor (BFS) that mounts between the UAVs airframe and motor, allowing direct measurement of the force exerted by the rotor on the UAVs rigid body. The BFS is designed using low-cost MEMS barometers as tactile force sensors, encased in polyurethane rubber. Subsequently, we established its parameter model and devised a stability improvement strategy to reduce the impact of temperature. Additionally, we designed a structure suitable for mounting the BFS on the UAV to safeguard the rubber module from damage and reconstructed the thrust model to account for the impact of weight and friction on thrust measurement. Finally, we assembled testing platforms to validate the performance of the BFS. Experimental results demonstrate the BFSs excellent linearity, wide range, adequate bandwidth to respond to UAV thrust variations, and confirm the feasibility of mounting the BFS on the UAV for thrust measurement and force feedback control.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that1">
             <b>
              ThAT1
             </b>
            </a>
           </td>
           <td class="r">
            Room 1
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#that1" title="Click to go to the Program at a Glance">
             <b>
              SLAM I
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#185927" title="Click to go to the Author Index">
             Yuan, Shenghai
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that1_01">
             10:00-10:15, Paper ThAT1.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('25'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              HI-SLAM: Monocular Real-Time Dense Mapping with Hybrid Implicit Fields
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#335248" title="Click to go to the Author Index">
             Zhang, Wei
            </a>
           </td>
           <td class="r">
            University of Stuttgart
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#302748" title="Click to go to the Author Index">
             Sun, Tiecheng
            </a>
           </td>
           <td class="r">
            UESTC
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#273212" title="Click to go to the Author Index">
             Wang, Sen
            </a>
           </td>
           <td class="r">
            Techinische Universitt Mnchen
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#311695" title="Click to go to the Author Index">
             Cheng, Qing
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#340052" title="Click to go to the Author Index">
             Haala, Norbert
            </a>
           </td>
           <td class="r">
            University of Stuttgart
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab25" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this letter, we present a neural field-based real-time monocular mapping framework for accurate and dense Simultaneous Localization and Mapping (SLAM). Recent neural mapping frameworks show promising results, but rely on RGB-D or pose inputs, or cannot run in real-time. To address these limitations, our approach integrates dense-SLAM with neural implicit fields. Specifically, our dense SLAM approach runs parallel tracking and global optimization, while a neural field-based map is constructed incrementally based on the latest SLAM estimates. For the efficient construction of neural fields, we employ multi-resolution grid encoding and signed distance function (SDF) representation. This allows us to keep the map always up-to-date and adapt instantly to global updates via loop closing. For global consistency, we propose an efficient Sim(3)-based pose graph bundle adjustment (PGBA) approach to run online loop closing and mitigate the pose and scale drift. To enhance depth accuracy further, we incorporate
             <p>
              learned monocular depth priors. We propose a novel joint depth and scale adjustment (JDSA) module to solve the scale ambiguity inherent in
              <p>
               depth priors. Extensive evaluations across synthetic and real-world datasets validate that our approach outperforms existing methods in accuracy and map completeness while preserving real-time performance.
              </p>
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that1_02">
             10:15-10:30, Paper ThAT1.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('140'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              ObVi-SLAM: Long-Term Object-Visual SLAM
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#310984" title="Click to go to the Author Index">
             Adkins, Amanda
            </a>
           </td>
           <td class="r">
            University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#325692" title="Click to go to the Author Index">
             Chen, Taijing
            </a>
           </td>
           <td class="r">
            University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#127043" title="Click to go to the Author Index">
             Biswas, Joydeep
            </a>
           </td>
           <td class="r">
            University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab140" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robots responsible for tasks over long time scales must be able to localize consistently and scalably amid geometric, viewpoint, and appearance changes. Existing visual SLAM approaches rely on low-level feature descriptors that are not robust to such environmental changes and result in large map sizes that scale poorly over long-term deployments. In contrast, object detections are robust to environmental variations and lead to more compact representations, but most object-based SLAM systems target short-term indoor deployments with close objects. In this paper, we introduce ObVi-SLAM to overcome these challenges by leveraging the best of both approaches. ObVi-SLAM uses low-level visual features for high-quality short-term visual odometry; and to ensure global, long-term consistency, ObVi-SLAM builds an uncertainty-aware long-term map of persistent objects and updates it after every deployment. By evaluating ObVi-SLAM on data from 16 deployment sessions spanning different weather and lighting conditions, we empirically show that ObVi-SLAM generates accurate localization estimates consistent over long time scales in spite of varying appearance conditions.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that1_03">
             10:30-10:45, Paper ThAT1.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('150'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Light-LOAM: A Lightweight LiDAR Odometry and Mapping Based on Graph-Matching
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#355178" title="Click to go to the Author Index">
             Yi, Shiquan
            </a>
           </td>
           <td class="r">
            Northwestern Polytechnical University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#198472" title="Click to go to the Author Index">
             Lyu, Yang
            </a>
           </td>
           <td class="r">
            Northwestern Polytechnical University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#355389" title="Click to go to the Author Index">
             Hua, Lin
            </a>
           </td>
           <td class="r">
            Northwestern Polytechnical University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#191452" title="Click to go to the Author Index">
             Pan, Quan
            </a>
           </td>
           <td class="r">
            Northwestern Polytechnical University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#251512" title="Click to go to the Author Index">
             Zhao, Chunhui
            </a>
           </td>
           <td class="r">
            Northwestern Polytechnical University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab150" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Simultaneous Localization and Mapping (SLAM) plays an important role in robot autonomy. Reliability and efficiency are the two most valued features for applying SLAM in robot applications. In this paper, we consider achieving a reliable LiDAR-based SLAM function in computation-limited platforms, such as quadrotor UAVs based on graph-based point cloud association. First, contrary to most works selecting salient features for point cloud registration, we propose a non-conspicuous feature selection strategy for reliability and robustness purposes. Then a two-stage correspondence selection method is used to register the point cloud, which includes a KD-tree-based coarse matching followed by a graph-based matching method that uses geometric consistency to vote out incorrect correspondences. Additionally, we propose an odometry approach where the weight optimizations are guided by vote results from the aforementioned geometric consistency graph. In this way, the optimization of LiDAR odometry rapidly converges and evaluates a fairly accurate transformation resulting in the back-end module efficiently finishing the mapping task. Finally, we evaluate our proposed framework on different datasets and real-world environments. Experiments show that our SLAM system achieves a comparative level or higher level of accuracy with more balanced computation efficiency compared with the mainstream LiDAR-based SLAM solutions.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that1_04">
             10:45-11:00, Paper ThAT1.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('151'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Light-LOAM: A Lightweight LiDAR Odometry and Mapping Based on Graph-Matching
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#355178" title="Click to go to the Author Index">
             Yi, Shiquan
            </a>
           </td>
           <td class="r">
            Northwestern Polytechnical University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#198472" title="Click to go to the Author Index">
             Lyu, Yang
            </a>
           </td>
           <td class="r">
            Northwestern Polytechnical University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#355389" title="Click to go to the Author Index">
             Hua, Lin
            </a>
           </td>
           <td class="r">
            Northwestern Polytechnical University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#191452" title="Click to go to the Author Index">
             Pan, Quan
            </a>
           </td>
           <td class="r">
            Northwestern Polytechnical University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#251512" title="Click to go to the Author Index">
             Zhao, Chunhui
            </a>
           </td>
           <td class="r">
            Northwestern Polytechnical University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab151" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Simultaneous Localization and Mapping (SLAM) plays an important role in robot autonomy. Reliability and efficiency are the two most valued features for applying SLAM in robot applications. In this paper, we consider achieving a reliable LiDAR-based SLAM function in computation-limited platforms, such as quadrotor UAVs based on graph-based point cloud association. First, contrary to most works selecting salient features for point cloud registration, we propose a non-conspicuous feature selection strategy for reliability and robustness purposes. Then a two-stage correspondence selection method is used to register the point cloud, which includes a KD-tree-based coarse matching followed by a graph-based matching method that uses geometric consistency to vote out incorrect correspondences. Additionally, we propose an odometry approach where the weight optimizations are guided by vote results from the aforementioned geometric consistency graph. In this way, the optimization of LiDAR odometry rapidly converges and evaluates a fairly accurate transformation resulting in the back-end module efficiently finishing the mapping task. Finally, we evaluate our proposed framework on different datasets and real-world environments. Experiments show that our SLAM system achieves a comparative level or higher level of accuracy with more balanced computation efficiency compared with the mainstream LiDAR-based SLAM solutions.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that2">
             <b>
              ThAT2
             </b>
            </a>
           </td>
           <td class="r">
            Room 2
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#that2" title="Click to go to the Program at a Glance">
             <b>
              Marine Robotics II
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#102952" title="Click to go to the Author Index">
             Yamashita, Atsushi
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that2_01">
             10:00-10:15, Paper ThAT2.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('67'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              TransCODNet: Underwater Transparently Camouflaged Object Detection Via RGB and Event Frames Collaboration
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#239623" title="Click to go to the Author Index">
             Luo, Cai
            </a>
           </td>
           <td class="r">
            UPC
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#362825" title="Click to go to the Author Index">
             Wu, Jihua
            </a>
           </td>
           <td class="r">
            China University of Petroleum(East China)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#370060" title="Click to go to the Author Index">
             Sun, Shixin
            </a>
           </td>
           <td class="r">
            China University of Petroleum (East China)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#309775" title="Click to go to the Author Index">
             Ren, Peng
            </a>
           </td>
           <td class="r">
            China University of PetroleumEast China
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab67" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#data_sets_for_robotic_vision" title="Click to go to the Keyword Index">
               Data Sets for Robotic Vision
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Underwater transparently camouflaged organisms can be perfectly invisible in the ocean to avoid the capture of predators. Due to the blurry contour boundaries of their bodies, obtaining their boundary features and determining their specific positions are challenging for detection tasks. To address this issue, first, we propose a large-scale underwater transparently camouflaged object dataset, termed Aqua-Eye, which is obtained from event data and contains five types of underwater transparent organisms, with a total of 6,497 annotated images. Second, to evaluate the effectiveness of this dataset, we propose a simple and effective detection network termed underwater Transparently Camouflaged Object Detection Network (TransCODNet), which can obtain local features and specific locations of targets, providing a better detection method for underwater transparently camouflaged organisms. In this letter, we performed ablation study and nine representative deep learning algorithms were evaluated based on the dataset. Finally, experiments show that the detection accuracy of this algorithm is 84.7%, which is superior to mainstream object detection algorithms, proving the effectiveness of the proposed method. The dataset is available at https://github.com/lunaWU628/Aqua-Eye-Dataset.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that2_02">
             10:15-10:30, Paper ThAT2.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3718'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Multi-Robot Multimodal Deep Sea Surveys for Detailed Estimation of Manganese Crust Distribution (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#230911" title="Click to go to the Author Index">
             Neettiyath, Umesh
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#367262" title="Click to go to the Author Index">
             Sugimatsu, Harumi
            </a>
           </td>
           <td class="r">
            University of Tokyo
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#367260" title="Click to go to the Author Index">
             Koike, Tetsu
            </a>
           </td>
           <td class="r">
            Kaiyo Engineering Co., Ltd
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#367261" title="Click to go to the Author Index">
             Kazunori, Nagano
            </a>
           </td>
           <td class="r">
            University of Tokyo
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#107620" title="Click to go to the Author Index">
             Ura, Tamaki
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#148636" title="Click to go to the Author Index">
             Thornton, Blair
            </a>
           </td>
           <td class="r">
            University of Southampton
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3718" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#marine_robotics" title="Click to go to the Keyword Index">
               Marine Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper describes a multi-year survey of Cobalt-rich manganese crust (Mn-crust) deposits using multiple underwater robots. Using two autonomous underwater vehicles and one remotely operated vehicle, mounted with various sensors, large areas were surveyed by incorporating the advantages of each robot to create a comprehensive database of Mn-crust distribution estimates. Since Mn-crust is projected to be a potential source of rare minerals including Cobalt, which is an essential ingredient in Li-ion batteries, estimating the distribution of these mineral resources in the deep seabed is an task of high importance. The results from this survey can be combined with ship base multibeam data for seamount scale estimates of Mn-crust volumetric distribution with high accuracy.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that2_03">
             10:30-10:45, Paper ThAT2.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3748'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Underwater Vibration Adhesion by Frequency-Controlled Rigid Disc for Underwater Robotics Grasping
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#137920" title="Click to go to the Author Index">
             Sun, Yi
            </a>
           </td>
           <td class="r">
            Shanghai University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#369057" title="Click to go to the Author Index">
             Hu, Yangyi
            </a>
           </td>
           <td class="r">
            Shanghai University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#232176" title="Click to go to the Author Index">
             Yang, Yi
            </a>
           </td>
           <td class="r">
            Shanghai University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#284586" title="Click to go to the Author Index">
             Wang, Min
            </a>
           </td>
           <td class="r">
            Shanghai University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#284585" title="Click to go to the Author Index">
             Ding, Jiheng
            </a>
           </td>
           <td class="r">
            Shanghai University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#154410" title="Click to go to the Author Index">
             Pu, Huayan
            </a>
           </td>
           <td class="r">
            Shanghai University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#136867" title="Click to go to the Author Index">
             Jia, Wenchuan
            </a>
           </td>
           <td class="r">
            Shanghai University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3748" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#marine_robotics" title="Click to go to the Keyword Index">
               Marine Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#grippers_and_other_end_effectors" title="Click to go to the Keyword Index">
               Grippers and Other End-Effectors
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             <p>
              Underwater adsorption is the key function of underwater robot operation, in order to complete the underwater wall fixation, underwater object capture and other operations. The underwater adhesion
              <p>
               mechanism based on vibration control has the advantage of using the liquid viscosity to pull the object surface at a certain distance and control the adhesion effect by frequency. At three vibration frequencies of 50Hz, 100Hz and 150Hz, a disk of 0.5mm thickness and 100mm diameter can accommodate the adhesion of different rough planes (50 Hz), a curved cup can be pulled and grabbed at a distance of 7mm (100 Hz), while at 150 Hz, a load mass of 1 Kg can be adhered, and the adhesion can be further enhanced by cooperating with the soft material.
               <p>
                In short, the underwater vibration adhesion of rigid body is a new adhesion mechanism, which is expected to provide new ideas and applications in the field of underwater wall climbing robot and underwater grasping.
               </p>
              </p>
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that2_04">
             10:45-11:00, Paper ThAT2.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('36'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              LodeStar: Maritime Radar Descriptor for Semi-Direct Radar Odometry
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#232046" title="Click to go to the Author Index">
             Jang, Hyesu
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#311476" title="Click to go to the Author Index">
             Jung, Minwoo
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#226880" title="Click to go to the Author Index">
             Jeon, Myung-Hwan
            </a>
           </td>
           <td class="r">
            SNU
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#126962" title="Click to go to the Author Index">
             Kim, Ayoung
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab36" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#range_sensing" title="Click to go to the Keyword Index">
               Range Sensing
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#marine_robotics" title="Click to go to the Keyword Index">
               Marine Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Maritime radars are prevalently adopted to capture the vessels omnidirectional data as imagery. Nevertheless, inherent challenges persist with marine radars, including limited frequency, suboptimal resolution, and indeterminate detections. Additionally, the scarcity of discernible landmarks in the vast marine expanses remains a challenge, resulting in consecutive scenes that often lack matching feature points. In this context, we introduce a resilient maritime radar scan representation LodeStar, and an enhanced feature extraction technique tailored for marine radar applications. Moreover, we embark on estimating marine radar odometry utilizing a semi-direct approach. LodeStar-based approach markedly attenuates the errors in odometry estimation, and our assertion is corroborated through meticulous experimental validation.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that3">
             <b>
              ThAT3
             </b>
            </a>
           </td>
           <td class="r">
            Room 3
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#that3" title="Click to go to the Program at a Glance">
             <b>
              Multifingered Hands
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#103658" title="Click to go to the Author Index">
             Watanabe, Tetsuyou
            </a>
           </td>
           <td class="r">
            Kanazawa University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that3_01">
             10:00-10:15, Paper ThAT3.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3629'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              RESPRECT: Speeding-Up Multi-Fingered Grasping with Residual Reinforcement Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#286387" title="Click to go to the Author Index">
             Ceola, Federico
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#160981" title="Click to go to the Author Index">
             Rosasco, Lorenzo
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia &amp; MassachusettsInstitute OfTechn
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#111056" title="Click to go to the Author Index">
             Natale, Lorenzo
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3629" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#dexterous_manipulation" title="Click to go to the Keyword Index">
               Dexterous Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#multifingered_hands" title="Click to go to the Keyword Index">
               Multifingered Hands
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Deep Reinforcement Learning (DRL) has proven effective in learning control policies using robotic grippers, but much less practical for solving the problem of grasping with dexterous hands  especially on real robotic platforms  due to the high dimensionality of the problem. In this work, we focus on the multi-fingered grasping task with the anthropomorphic hand of the iCub humanoid. We propose the RESidual learning with PREtrained CriTics (RESPRECT) method that, starting from a policy pre-trained on a large set of objects, can learn a residual policy to grasp a novel object in a fraction ( 5 faster) of the timesteps required to train a policy from scratch, without requiring any task demonstration. To our knowledge, this is the first Residual Reinforcement Learning (RRL) approach that learns a residual policy on top of another policy pre-trained with DRL. We exploit some components of the pre-trained policy during residual learning that further speed-up the training. We benchmark our results in the iCub simulated environment, and we show that RESPRECT can be effectively used to learn a multi-fingered grasping policy on the real iCub robot. The code to reproduce the experiments is released together with the paper with an open source license.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that3_02">
             10:15-10:30, Paper ThAT3.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3783'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Fast In-Hand Slip Control on Unfeatured Objects with Programmable Tactile Sensing
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#189514" title="Click to go to the Author Index">
             Gloumakov, Yuri
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#155998" title="Click to go to the Author Index">
             Huh, Tae Myung
            </a>
           </td>
           <td class="r">
            UC Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#168436" title="Click to go to the Author Index">
             Stuart, Hannah
            </a>
           </td>
           <td class="r">
            UC Berkeley
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3783" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#force_and_tactile_sensing" title="Click to go to the Keyword Index">
               Force and Tactile Sensing
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#in_hand_manipulation" title="Click to go to the Keyword Index">
               In-Hand Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Accurate dynamic object manipulation in a robotic hand remains a difficult task, especially when frictional slip is involved. Prior solutions involve extensive data collection to train complex models to control the hand that do not necessarily generalize to other slip circumstances. Our approach focuses on direct slip sensing using a tactile sensor with a capacitive array, coupled with a programmable system on a chip, capable of mode switching and sampling rate adjustment. We characterize the sensors capacity to sense slip features at higher speeds and introduce a novel methodology for estimating motions. Low-level sensor reprogramming that couples multiple taxels improves slip avoidance and reaction time during rapid slip onset events. The technology also tracks dominant surface vibration frequencies resulting from stick-slip cycles, estimating speed and acceleration of smooth flat surfaces. Using a parallel-jaw robotic gripper, we demonstrate dynamic repositioning of objects lacking trackable surface features within the hand. The goal of this investigation is to support faster reasoning and reflexes for dynamic dexterous robots that experience directional in-hand slip.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that3_03">
             10:30-10:45, Paper ThAT3.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('130'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              The Fingertip Manipulability Assessment of Tendon-Driven Multi-Fingered Hands
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#338803" title="Click to go to the Author Index">
             Li, Junnan
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#223070" title="Click to go to the Author Index">
             Ganguly, Amartya
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#156255" title="Click to go to the Author Index">
             Figueredo, Luis
            </a>
           </td>
           <td class="r">
            University of Nottingham (UoN)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#108317" title="Click to go to the Author Index">
             Haddadin, Sami
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab130" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#multifingered_hands" title="Click to go to the Keyword Index">
               Multifingered Hands
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#grasping" title="Click to go to the Keyword Index">
               Grasping
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#performance_evaluation_and_benchmarking" title="Click to go to the Keyword Index">
               Performance Evaluation and Benchmarking
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The ability of robotic fingers to exert force and exhibit motion is vital for achieving dexterity in manipulation tasks. To evaluate dexterous capabilities in terms of both features, i.e., quantifying finger performance that facilitates task planning and design optimization, we introduce the Fingertip Manipulability (FtM) metric. The FtM is a comprehensive assessment tool linked to finger parameters rather than specific task requirements, e.g., wrench information, contact-points, among others. It takes into account the entire voxelized fingertip workspace of all fingers, filling a gap in providing a global representation during the design and deployment phase of tendon-driven robotic hands. It composes the assessment map of a multi-fingered hand that enables real-time performance monitoring and planning for dexterous tendon-driven hands. To illustrate the practical application of this metric, we showcase its assessment of the Shadow Hand, demonstrating its characteristics in optimizing poses for a multi-finger grasping scenario.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that3_04">
             10:45-11:00, Paper ThAT3.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('149'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Tactile Object Property Recognition Using Geometrical Graph Edge Features and Multi-Thread Graph Convolutional Network
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#275238" title="Click to go to the Author Index">
             Kulkarni, Shardul
            </a>
           </td>
           <td class="r">
            Waseda University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#184706" title="Click to go to the Author Index">
             Funabashi, Satoshi
            </a>
           </td>
           <td class="r">
            Waseda University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#108304" title="Click to go to the Author Index">
             Schmitz, Alexander
            </a>
           </td>
           <td class="r">
            Waseda University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#101828" title="Click to go to the Author Index">
             Ogata, Tetsuya
            </a>
           </td>
           <td class="r">
            Waseda University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#100151" title="Click to go to the Author Index">
             Sugano, Shigeki
            </a>
           </td>
           <td class="r">
            Waseda University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab149" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#multifingered_hands" title="Click to go to the Keyword Index">
               Multifingered Hands
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#force_and_tactile_sensing" title="Click to go to the Keyword Index">
               Force and Tactile Sensing
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Performing dexterous tasks with a multi-fingered robotic hand remains challenging. Tactile sensors provide touch states and object features for multifingered tasks, yet the variety in shapes, sizes, textures, deformabilities and masses of everyday objects makes the task conditions diverse. Despite these challenges, humans accomplish these difficult tasks by producing a sensory-motor representation of their body. This combined tactile and proprioceptive representation enables humans to accommodate the diversity in daily objects. Referring to this concept, this paper presents a method for object property recognition using Graph Convolutional Networks (GCNs), leveraging robot hand proprioception and morphology with spatial embeddings derived from geometrical graph edge features acquired from real tactile sensor alignments on an Allegro Hand. Additionally, a Multi-Thread GCN (MT-GCN) architecture is introduced to process these edge features and basically multi-modal data in a graph. Training data was acquired using a data-glove, from tri-axial tactile sensors distributed across the fingertips, finger phalanges, and palm of an Allegro Hand, producing a total of 1152 tactile measurements. MT-GCN with proposed edge features, tactile features and joint angles achieved a high recognition rate, 86.08% for six classes of object property combinations from eight objects. The effect of variation in graph adjacency on MT-GCN was examined. The proposed network showed clusters following the robot hand configuration with t-SNE analysis. Furthermore, analysis of learned parameters in the edge-feature encoder demonstrated its ability to discern joint positions on the hand, acquiring proprioceptive features effectively. Consequently, we could confirm that the proposed method was effective for multi-fingered dexterous tasks.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that4">
             <b>
              ThAT4
             </b>
            </a>
           </td>
           <td class="r">
            Room 4
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#that4" title="Click to go to the Program at a Glance">
             <b>
              Soft Sensors and Actuators III
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#206601" title="Click to go to the Author Index">
             Yun, Dongwon
            </a>
           </td>
           <td class="r">
            Daegu Gyeongbuk Institute of Science and Technology (DGIST)
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that4_01">
             10:00-10:15, Paper ThAT4.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('977'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              FOCWS: A High Sensitive Flexible Optical Curvature Sensor Inspired by Arthropod Sensory Systems
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#377692" title="Click to go to the Author Index">
             Wei, Jiachen
            </a>
           </td>
           <td class="r">
            University of Science and Technology Beijing
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#222592" title="Click to go to the Author Index">
             Li, Zhengwei
            </a>
           </td>
           <td class="r">
            Institute of Automation, Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#351479" title="Click to go to the Author Index">
             Liu, Zeyu
            </a>
           </td>
           <td class="r">
            Institute of Automation, Chinese Acadamy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#158922" title="Click to go to the Author Index">
             He, Wei
            </a>
           </td>
           <td class="r">
            University of Science and Technology Beijing
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#113118" title="Click to go to the Author Index">
             Cheng, Long
            </a>
           </td>
           <td class="r">
            Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#267294" title="Click to go to the Author Index">
             Liu, Yanhong
            </a>
           </td>
           <td class="r">
            Zhengzhou University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab977" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#soft_sensors_and_actuators" title="Click to go to the Keyword Index">
               Soft Sensors and Actuators
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#wearable_robotics" title="Click to go to the Keyword Index">
               Wearable Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#physical_human_robot_interaction" title="Click to go to the Keyword Index">
               Physical Human-Robot Interaction
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Flexible sensors for joint angle measurement play a crucial role in various human-robot interaction applications. In previous studies, sensors with various sensing mechanisms have been developed. Among them, optical waveguide sensors exhibit high resistance to environmental factors (such as temperature and humidity) and low sensitivity to electromagnetic interference. Researchers have enhanced the sensitivity of optical waveguide sensors to tensile strain by doping other substances (such as graphite) into the optical core material of the optical waveguide. However, the sensitivity of measuring joint angles based on tensile strain principles remains relatively low. In nature, arthropods utilize crack-like structures near their leg joints to perceive minute mechanical stress changes. Here, we propose a curvature sensor based on a Flexible Optical Crack Waveguide Structure (FOCWS) inspired by the arthropod sensory systems. By cutting the optical core, we increase its light power loss during bending strain, thereby enhancing the sensors sensitivity to angle measurement. The characteristics of light propagation and geometric parameters were studied through simulation, and experiments were designed to validate the simulation results. The average sensitivity is 0.068 dB/, which is nearly 300 times higher compared to uncut optical waveguide.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that4_02">
             10:15-10:30, Paper ThAT4.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1680'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Embedded Valves for Distributed Control of Soft Pneumatic Actuators
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#288928" title="Click to go to the Author Index">
             Zuo, Runze
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397043" title="Click to go to the Author Index">
             Mehta, Mayank
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397049" title="Click to go to the Author Index">
             Han, Dong Heon
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#205650" title="Click to go to the Author Index">
             Bruder, Daniel
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1680" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#soft_sensors_and_actuators" title="Click to go to the Keyword Index">
               Soft Sensors and Actuators
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#soft_robot_applications" title="Click to go to the Keyword Index">
               Soft Robot Applications
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#modeling__control__and_learning_for_soft_robots" title="Click to go to the Keyword Index">
               Modeling, Control, and Learning for Soft Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Soft robotic systems are inherently compliant, giving them unique capabilities not possessed by traditional rigid-bodied robot systems. Many soft systems rely on soft pneumatic actuators. One of the biggest downsides of such actuators is the need for bulky pressure-regulating devices and individual pneumatic supply lines. In this work, a miniaturized pressure-regulating system is developed and embedded into the unused space inside of a soft pneumatic McKibben actuator, enabling the simultaneous pressure control of multiple actuators connected to a single pneumatic supply line. This ``valve-embedded" actuator is capable of regulating its internal pressure within 0.05 psi of a desired set point, even under external load. Compared to a conventional McKibben actuator driven by external valves, the valve-embedded actuator is experimentally shown to consistently achieve faster settling times. To showcase the practical application of the valve-embedded actuator on a robotic system, a 0.9m serial-linked robot driven by five independently controlled valve-embedded actuators was assembled, and was shown to achieve an average root mean square error of less than 1.5cm in a waypoint tracking experiment. The miniaturized pressure control system developed in this work is open source and could be embedded in any fluid-driven actuator, enabling more capable and densely actuated pneumatic soft robots.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that4_03">
             10:30-10:45, Paper ThAT4.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2253'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              An Origami-Inspired Pneumatic Continuum Module with Active Variable Stiffness
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397665" title="Click to go to the Author Index">
             Li, Zhuowen
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#252429" title="Click to go to the Author Index">
             Chen, Huaiyuan
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#252793" title="Click to go to the Author Index">
             Xu, Fan
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#103003" title="Click to go to the Author Index">
             Wang, Hesheng
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2253" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#soft_sensors_and_actuators" title="Click to go to the Keyword Index">
               Soft Sensors and Actuators
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#hydraulic_pneumatic_actuators" title="Click to go to the Keyword Index">
               Hydraulic/Pneumatic Actuators
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#compliant_joints_and_mechanisms" title="Click to go to the Keyword Index">
               Compliant Joints and Mechanisms
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a novel pneumatic continuum module featuring high contraction-ratio, bidirectional actuation, and active stiffness regulation. The module comprises four linear pneumatic actuators integrating rigid polygon origami frame into soft bellow. This integration not only helps to regulate motion but also enhances structural strength, facilitating contraction and bidirectional bending performance. The contraction resulting from vacuum pressure serves as a limiting layer for one opposing pair of actuators, while the other pair operates in a virtual antagonistic configuration, allowing for active regulation of joint stiffness through pressure control. The paper provides a detailed workflow covering the design, fabrication, and mathematical modeling of the pneumatic module. Furthermore, the paper presents verifications of the module's actuation capacity and active variable stiffness. The findings of this study can serve as valuable references for the design of manipulators.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that4_04">
             10:45-11:00, Paper ThAT4.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2366'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              High-Frequency Capacitive Sensing for Electrohydraulic Soft Actuators
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#377432" title="Click to go to the Author Index">
             Vogt, Michel Ryan
            </a>
           </td>
           <td class="r">
            ETH Zrich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397458" title="Click to go to the Author Index">
             Eberlein, Maximilian
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#377391" title="Click to go to the Author Index">
             Christoph, Clemens Claudio
            </a>
           </td>
           <td class="r">
            ETH Zrich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397546" title="Click to go to the Author Index">
             Baumann, Felix
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398111" title="Click to go to the Author Index">
             Bourquin, Fabrice
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397442" title="Click to go to the Author Index">
             Wende, Wim
            </a>
           </td>
           <td class="r">
            ETH Zrich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397440" title="Click to go to the Author Index">
             Schaub, Fabio
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#310038" title="Click to go to the Author Index">
             Kazemipour, Amirhossein
            </a>
           </td>
           <td class="r">
            ETH Zrich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#164300" title="Click to go to the Author Index">
             Katzschmann, Robert Kevin
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2366" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#soft_sensors_and_actuators" title="Click to go to the Keyword Index">
               Soft Sensors and Actuators
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#wearable_robotics" title="Click to go to the Keyword Index">
               Wearable Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#soft_robot_materials_and_design" title="Click to go to the Keyword Index">
               Soft Robot Materials and Design
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The need for compliant and proprioceptive actuators has grown more evident in pursuing more adaptable and versatile robotic systems. Hydraulically Amplified Self-Healing Electrostatic (HASEL) actuators offer distinctive advantages with their inherent softness and flexibility, making them promising candidates for various robotic tasks, including delicate interactions with humans and animals, biomimetic locomotion, prosthetics, and exoskeletons. This has resulted in a growing interest in the capacitive self-sensing capabilities of HASEL actuators to create miniature displacement estimation circuitry that does not require external sensors. However, achieving HASEL self-sensing for actuation frequencies above 1 Hz and with miniature high-voltage power supplies has remained limited. In this paper, we introduce the F-HASEL actuator, which adds an additional electrode pair used exclusively for capacitive sensing to a Peano-HASEL actuator. We demonstrate displacement estimation of the F-HASEL during high-frequency actuation up to 20 Hz and during external loading using miniaturized circuitry comprised of low-cost off-the-shelf components and a miniature high-voltage power supply. Finally, we propose circuitry to estimate the displacement of multiple F-HASELs and demonstrate it in a wearable application to track joint rotations of a virtual reality user in real-time.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that5">
             <b>
              ThAT5
             </b>
            </a>
           </td>
           <td class="r">
            Room 5
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#that5" title="Click to go to the Program at a Glance">
             <b>
              Calibration and Identification II
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#223070" title="Click to go to the Author Index">
             Ganguly, Amartya
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that5_01">
             10:00-10:15, Paper ThAT5.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1218'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Interactive Robot-Environment Self-Calibration Via Compliant Exploratory Actions
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#338108" title="Click to go to the Author Index">
             Chanrungmaneekul, Podshara
            </a>
           </td>
           <td class="r">
            Rice University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#236218" title="Click to go to the Author Index">
             Ren, Kejia
            </a>
           </td>
           <td class="r">
            Rice University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#351066" title="Click to go to the Author Index">
             Grace, Joshua
            </a>
           </td>
           <td class="r">
            Yale University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#103607" title="Click to go to the Author Index">
             Dollar, Aaron
            </a>
           </td>
           <td class="r">
            Yale University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#155700" title="Click to go to the Author Index">
             Hang, Kaiyu
            </a>
           </td>
           <td class="r">
            Rice University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1218" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#calibration_and_identification" title="Click to go to the Keyword Index">
               Calibration and Identification
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Calibrating robots into their workspaces is crucial for manipulation tasks. Existing calibration techniques often rely on sensors external to the robot (cameras, laser scanners, etc.) or specialized tools. This reliance complicates the calibration process and increases the costs and time requirements. Furthermore, the associated setup and measurement procedures require significant human intervention, which makes them more challenging to operate. Using the built-in force-torque sensors, which are nowadays a default component in collaborative robots, this work proposes a self-calibration framework where robot-environmental spatial relations are automatically estimated through compliant exploratory actions by the robot itself. The self-calibration approach converges, verifies its own accuracy, and terminates upon completion, autonomously purely through interactive exploration of the environments geometries. Extensive experiments validate the effectiveness of our self-calibration approach in accurately establishing the robot-environment spatial relationships without the need for additional sensing equipment or any human intervention.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that5_02">
             10:15-10:30, Paper ThAT5.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2226'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              3DGS-Calib: 3D Gaussian Splatting for Multimodal SpatioTemporal Calibration
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#354193" title="Click to go to the Author Index">
             Herau, Quentin
            </a>
           </td>
           <td class="r">
            Huawei, University of Burgundy
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#165947" title="Click to go to the Author Index">
             Bennehar, Moussab
            </a>
           </td>
           <td class="r">
            Lirmm - Umr 5506
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#298141" title="Click to go to the Author Index">
             Moreau, Arthur
            </a>
           </td>
           <td class="r">
            Huawei Noah's Ark Lab
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#191038" title="Click to go to the Author Index">
             Piasco, Nathan
            </a>
           </td>
           <td class="r">
            Huawei Technologies France
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#179510" title="Click to go to the Author Index">
             Roldao, Luis
            </a>
           </td>
           <td class="r">
            Huawei
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#116705" title="Click to go to the Author Index">
             Tsishkou, Dzmitry
            </a>
           </td>
           <td class="r">
            Huawei Technologies
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#342889" title="Click to go to the Author Index">
             Migniot, Cyrille
            </a>
           </td>
           <td class="r">
            U Bourgogne
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#106237" title="Click to go to the Author Index">
             Vasseur, Pascal
            </a>
           </td>
           <td class="r">
            Universit De Picardie Jules Verne
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#106253" title="Click to go to the Author Index">
             Demonceaux, Cdric
            </a>
           </td>
           <td class="r">
            Universit De Bourgogne
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2226" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#calibration_and_identification" title="Click to go to the Keyword Index">
               Calibration and Identification
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#sensor_fusion" title="Click to go to the Keyword Index">
               Sensor Fusion
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_transportation" title="Click to go to the Keyword Index">
               Computer Vision for Transportation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Reliable multimodal sensor fusion algorithms require accurate spatiotemporal calibration. Recently, targetless calibration techniques based on implicit neural representations have proven to provide precise and robust results. Nevertheless, such methods are inherently slow to train given the high computational overhead caused by the large number of sampled points required for volume rendering. With the recent introduction of 3D Gaussian Splatting as a faster alternative to implicit representation methods, we propose leveraging this new rendering approach to achieve faster multi-sensor calibration. We introduce 3DGS-Calib, a new calibration method that relies on the speed and rendering accuracy of 3D Gaussian splatting to achieve multimodal spatiotemporal calibration that is accurate, robust, and with a substantial speed-up compared to methods relying on implicit neural representations. We demonstrate the superiority of our proposal with experimental results on sequences from KITTI-360, a widely used driving dataset.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that5_03">
             10:30-10:45, Paper ThAT5.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2574'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Automatic Spatial Calibration of Near-Field MIMO Radar with Respect to Optical Depth Sensors
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#391148" title="Click to go to the Author Index">
             Wirth, Vanessa
            </a>
           </td>
           <td class="r">
            Friedrich-Alexander-Universitt Erlangen-Nrnberg
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396827" title="Click to go to the Author Index">
             Brunig, Johanna
            </a>
           </td>
           <td class="r">
            Friedrich-Alexander-Universitt Erlangen-Nrnberg, Institute Of
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396828" title="Click to go to the Author Index">
             Khouri, Danti
            </a>
           </td>
           <td class="r">
            Friedrich-Alexander-Universitt Erlangen-Nrnberg, Institute Of
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396830" title="Click to go to the Author Index">
             Gutsche, Florian
            </a>
           </td>
           <td class="r">
            Friedrich-Alexander-Universitt Erlangen-Nrnberg, Visual Comput
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#101604" title="Click to go to the Author Index">
             Vossiek, Martin
            </a>
           </td>
           <td class="r">
            University of Erlangen-Nrnberg
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#199578" title="Click to go to the Author Index">
             Weyrich, Tim
            </a>
           </td>
           <td class="r">
            Friedrich-Alexander-Universitt Erlangen-Nrnberg
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#293605" title="Click to go to the Author Index">
             Stamminger, Marc
            </a>
           </td>
           <td class="r">
            Universitt Erlangen-Nrnberg
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2574" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#calibration_and_identification" title="Click to go to the Keyword Index">
               Calibration and Identification
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#sensor_fusion" title="Click to go to the Keyword Index">
               Sensor Fusion
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#range_sensing" title="Click to go to the Keyword Index">
               Range Sensing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Despite an emerging interest in MIMO radar, the utilization of its complementary strengths in combination with optical depth sensors has so far been limited to far-field applications, due to the challenges that arise from mutual sensor calibration in the near field. In fact, most related approaches in the autonomous industry propose target-based calibration methods using corner reflectors that have proven to be unsuitable for the near field.
             <p>
              In contrast, we propose a novel, joint calibration approach for optical RGB-D sensors and MIMO radars that is designed to operate in the radars near-field range, within centimeters from the sensors. Our pipeline consists of a bespoke calibration target, allowing for automatic target detection and localization, followed by the spatial calibration of the two sensor coordinate systems through target registration.
              <p>
               We validate our approach using two different depth sensing technologies from the optical domain. The experiments show the efficiency and accuracy of our calibration for various target displacements, as well as its robustness of our localization in terms of signal ambiguities.
              </p>
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that5_04">
             10:45-11:00, Paper ThAT5.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3015'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Identification and Validation of the Dynamic Model of a Tendon-Driven Anthropomorphic Finger
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#338803" title="Click to go to the Author Index">
             Li, Junnan
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#296908" title="Click to go to the Author Index">
             Chen, Lingyun
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#220371" title="Click to go to the Author Index">
             Ringwald, Johannes
            </a>
           </td>
           <td class="r">
            Technische Universitt Mnchen
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#210169" title="Click to go to the Author Index">
             Pozo Fortuni, Edmundo
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#223070" title="Click to go to the Author Index">
             Ganguly, Amartya
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#108317" title="Click to go to the Author Index">
             Haddadin, Sami
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3015" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#calibration_and_identification" title="Click to go to the Keyword Index">
               Calibration and Identification
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#multifingered_hands" title="Click to go to the Keyword Index">
               Multifingered Hands
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#tendon_wire_mechanism" title="Click to go to the Keyword Index">
               Tendon/Wire Mechanism
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This study addresses the absence of an identification framework to quantify a comprehensive dynamic model of human and anthropomorphic tendon-driven fingers, which is necessary to investigate the physiological properties of human fingers and improve the control of robotic hands. First, a generalized dynamic model was formulated, which takes into account the inherent properties of such a mechanical system. This includes rigid-body dynamics, coupling matrix, joint viscoelasticity, and tendon friction. Then, we propose a methodology comprising a series of experiments, for step-wise identification and validation of this dynamic model. Moreover, an experimental setup was designed and constructed that features actuation modules and peripheral sensors to facilitate the identification process. To verify the proposed methodology, a 3D-printed robotic finger based on the index finger design of the Dexmart hand was developed, and the proposed experiments were executed to identify and validate its dynamic model. This study could be extended to explore the identification of cadaver hands, aiming for a consistent dataset from a single cadaver specimen to improve the development of musculoskeletal hand models.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that6">
             <b>
              ThAT6
             </b>
            </a>
           </td>
           <td class="r">
            Room 6
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#that6" title="Click to go to the Program at a Glance">
             <b>
              Aerial Systems: Mechanics and Control
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#142369" title="Click to go to the Author Index">
             Loianno, Giuseppe
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#179559" title="Click to go to the Author Index">
             Seneviratne, Lakmal
            </a>
           </td>
           <td class="r">
            L. D. Seneviratne Is with Kings College London, UK, and Robotics Institute of Khalifa University, UAE
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that6_01">
             10:00-10:15, Paper ThAT6.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('276'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Full-Pose Trajectory Tracking of Overactuated Multi-Rotor Aerial Vehicles with Limited Actuation Abilities
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#204761" title="Click to go to the Author Index">
             Hamandi, Mahmoud
            </a>
           </td>
           <td class="r">
            New York University Abu Dhabi
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#347587" title="Click to go to the Author Index">
             Al-Ali, Ismail
            </a>
           </td>
           <td class="r">
            Khalifa University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#179559" title="Click to go to the Author Index">
             Seneviratne, Lakmal
            </a>
           </td>
           <td class="r">
            L. D. Seneviratne Is with Kings College London, UK, and Robotics
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#104988" title="Click to go to the Author Index">
             Franchi, Antonio
            </a>
           </td>
           <td class="r">
            University of Twente / Sapienza University of Rome
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#328150" title="Click to go to the Author Index">
             Zweiri, Yahya
            </a>
           </td>
           <td class="r">
            Khalifa University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab276" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__mechanics_and_control" title="Click to go to the Keyword Index">
               Aerial Systems: Mechanics and Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a novel optimization-based full-pose trajectory tracking method to control overactuated multi-rotor aerial vehicles with limited actuation abilities. The proposed method allocates feasible control inputs to track a reference trajectory, while ensuring the tracking of the reference position, and while tracking the closest feasible attitude. The optimization simultaneously searches for a feasible trajectory and corresponding feasible control inputs from the infinite possible solutions, while ensuring smooth control inputs. The proposed real-time algorithm is tested in extensive simulation on multiple platforms with fixed and actuated propellers. The simulation experiments show the ability of the proposed approach to exploit the complex set of feasible forces and moments of overactuated platforms while allocating smooth feasible control inputs.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that6_02">
             10:15-10:30, Paper ThAT6.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              RGBlimp: Robotic Gliding Blimp - Design, Modeling, Development, and Aerodynamics Analysis
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#326479" title="Click to go to the Author Index">
             Cheng, Hao
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#362162" title="Click to go to the Author Index">
             Sha, Zeyu
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#319520" title="Click to go to the Author Index">
             Zhu, Yongjian
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#148116" title="Click to go to the Author Index">
             Zhang, Feitian
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__mechanics_and_control" title="Click to go to the Keyword Index">
               Aerial Systems: Mechanics and Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#dynamics" title="Click to go to the Keyword Index">
               Dynamics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mechanism_design" title="Click to go to the Keyword Index">
               Mechanism Design
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             A miniature robotic blimp, as one type of lighter-than-air aerial vehicle, has attracted increasing attention in the science and engineering field for its long flight duration and safe aerial locomotion. While a variety of miniature robotic blimps have been developed over the past decade, most of them utilize the buoyant lift and neglect the aerodynamic lift in their design, thus leading to a mediocre aerodynamic performance, particularly in terms of aerodynamic efficiency and aerodynamic stability. This letter proposes a new design of miniature robotic blimp that combines desirable features of both a robotic blimp and a fixed-wing glider, named the Robotic Gliding Blimp, or RGBlimp. This robot, equipped with an envelope filled with helium and a pair of wings, uses an internal moving mass and a pair of propellers for its locomotion control. This letter presents the design, dynamic modeling, prototyping, and system identification of the RGBlimp. To the best of the authors' knowledge, this is the first effort to systematically design and develop such a miniature robotic blimp with moving mass control and hybrid lifts. Experimental results are presented to validate the design and the dynamic model of the RGBlimp. Analysis of the RGBlimp aerodynamics is conducted which confirms the performance improvement of the proposed RGBlimp in aerodynamic efficiency and flight stability.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that6_03">
             10:30-10:45, Paper ThAT6.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('29'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Efficient Optimization-Based Cable Force Allocation for Geometric Control of a Multirotor Team Transporting a Payload
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#365857" title="Click to go to the Author Index">
             Wahba, Khaled
            </a>
           </td>
           <td class="r">
            Technical University of Berlin
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#183179" title="Click to go to the Author Index">
             Hoenig, Wolfgang
            </a>
           </td>
           <td class="r">
            TU Berlin
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab29" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__mechanics_and_control" title="Click to go to the Keyword Index">
               Aerial Systems: Mechanics and Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#control_architectures_and_programming" title="Click to go to the Keyword Index">
               Control Architectures and Programming
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We consider transporting a heavy payload that is attached to multiple multirotors. The current state-of-the-art controllers either do not avoid inter-robot collision at all, leading to crashes when tasked with
             <p>
              carrying payloads that are small in size compared to the cable lengths,
              <p>
               or use computational demanding nonlinear optimization. We propose an efficient optimization-based cable force allocation for a geometric payload transport controller to effectively avoid such collisions, while retaining the stability properties of the geometric controller. Our approach introduces a cascade of carefully designed quadratic programs that can be solved efficiently on highly constrained embedded flight controllers. We show that our approach exceeds the state-of-the-art controllers in terms of scalability by at least an order of magnitude for up to 10 robots. We demonstrate our method on challenging scenarios with up to three small multirotors with various payloads and cable lengths, where our controller runs in real-time directly on a microcontroller on the robots.
              </p>
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that6_04">
             10:45-11:00, Paper ThAT6.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3631'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              From Propeller Damage Estimation and Adaptation to Fault Tolerant Control: Enhancing Quadrotor Resilience
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#297802" title="Click to go to the Author Index">
             Mao, Jeffrey
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#354477" title="Click to go to the Author Index">
             Yeom, Jennifer
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#374480" title="Click to go to the Author Index">
             Nair, Suraj
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#142369" title="Click to go to the Author Index">
             Loianno, Giuseppe
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3631" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__mechanics_and_control" title="Click to go to the Keyword Index">
               Aerial Systems: Mechanics and Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Micro Aerial Vehicles (MAVs) such as quadrotors are becoming ubiquitous in applications such as search and rescue~cite{MISHRA20201} and aerial photography~cite{ye_photography}. However, MAV-related accidents hinder the growth of the industry and erode public confidence in drone safety. As a result, it is necessary to develop inference and control approaches that can guarantee safe and reliable flight in case of system damage. Propellers on MAVs are susceptible to damage, especially in the event of collisions or after prolonged use, primarily due to their size, location, and lightweight construction. In addition, detecting the occurrence and location of propeller failures is challenging. Indirectly sensing individual motor thrusts is further complicated due to the nonlinear dynamics of the quadrotor and the difficulty of measuring higher order angular acceleration or moment terms. Our work specifically introduces an adaptive control scheme for quadrotors to autonomously infer and adjust flight behavior in response to propeller damages or failures.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that7">
             <b>
              ThAT7
             </b>
            </a>
           </td>
           <td class="r">
            Room 7
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#that7" title="Click to go to the Program at a Glance">
             <b>
              Surgical Robotics II
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#324238" title="Click to go to the Author Index">
             Korzeniowski, Przemyslaw
            </a>
           </td>
           <td class="r">
            Sano Centre for Computational Medicine
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#199203" title="Click to go to the Author Index">
             Song, Cheol
            </a>
           </td>
           <td class="r">
            DGIST
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that7_01">
             10:00-10:15, Paper ThAT7.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1793'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Saturation in the Null-Space (SNS) for Tele-Operated Surgery: Prioritized Motion Control for RCM and Joint Limit Constraints
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#195879" title="Click to go to the Author Index">
             Kana, Sreekanth
            </a>
           </td>
           <td class="r">
            KARL STORZ VentureONE Pte. Ltd
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#125025" title="Click to go to the Author Index">
             Prez Arias, Antonia
            </a>
           </td>
           <td class="r">
            N/A
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397239" title="Click to go to the Author Index">
             Kahlau, Robert
            </a>
           </td>
           <td class="r">
            Undisclosed
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#186495" title="Click to go to the Author Index">
             Kanajar, Pavan
            </a>
           </td>
           <td class="r">
            KARL STORZ VentureONE Germany GmbH
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#179614" title="Click to go to the Author Index">
             Sharma, Shashank
            </a>
           </td>
           <td class="r">
            KARLSTORZ VentureONE Germany GmbH
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1793" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#surgical_robotics__laparoscopy" title="Click to go to the Keyword Index">
               Surgical Robotics: Laparoscopy
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#surgical_robotics__planning" title="Click to go to the Keyword Index">
               Surgical Robotics: Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#telerobotics_and_teleoperation" title="Click to go to the Keyword Index">
               Telerobotics and Teleoperation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper showcases the application of the Saturation in the Null Space (SNS) algorithm to establish task prioritization and coordination within a tele-operated minimally invasive robotic surgical setting. In our work, SNS prioritizes achieving Remote Center of Motion (RCM) constraint, ensuring safe instrument manipulation, while respecting joint constraints for uninterrupted robot operation. This prioritization allows for accommodating the tracking of the surgeon's motion, within the capabilities defined by RCM and joint constraints. We investigate both the velocity and acceleration control variants of the SNS algorithm, incorporating bespoke adjustments to tailor the original algorithm to the intricate requirements of surgical applications. Through simulations and experiments, this work aims to demonstrate the effectiveness of SNS in enhancing the safety and controllability of tele-operated surgery, paving the way for its integration in various surgical procedures.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that7_02">
             10:15-10:30, Paper ThAT7.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1949'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              FF-SRL: High Performance GPU-Based Surgical Simulation for Robot Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#122182" title="Click to go to the Author Index">
             Dall'Alba, Diego
            </a>
           </td>
           <td class="r">
            University of Verona
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#355067" title="Click to go to the Author Index">
             Naskrt, Micha
            </a>
           </td>
           <td class="r">
            SANO
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#377604" title="Click to go to the Author Index">
             Kamiska, Sabina
            </a>
           </td>
           <td class="r">
            Sano - Centre for Computational Personalized Medicine
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#324238" title="Click to go to the Author Index">
             Korzeniowski, Przemyslaw
            </a>
           </td>
           <td class="r">
            Sano Centre for Computational Medicine
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1949" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#surgical_robotics__laparoscopy" title="Click to go to the Keyword Index">
               Surgical Robotics: Laparoscopy
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#simulation_and_animation" title="Click to go to the Keyword Index">
               Simulation and Animation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robotic surgery is a rapidly developing field that can greatly benefit from the automation of surgical tasks. However, training techniques such as Reinforcement Learning (RL) require a high number of task repetitions, which are generally unsafe and impractical to perform on real surgical systems. This stresses the need for simulated surgical environments, which are not only realistic, but also computationally efficient and scalable. We introduce FF-SRL (Fast and Flexible Surgical Reinforcement Learning), a high-performance learning environment for robotic surgery. In FF-SRL both physics simulation and RL policy training reside entirely on a single GPU. This avoids typical bottlenecks associated with data transfer between the CPU and GPU, leading to accelerated learning rates. Our results show that FF-SRL reduces the training time of a complex tissue manipulation task by an order of magnitude, down to a couple of minutes, compared to a common CPU/GPU simulator. Such speed-up may facilitate the experimentation with RL techniques and contribute to the development of new generation of surgical systems. To this end, we make our code publicly available to the community.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that7_03">
             10:30-10:45, Paper ThAT7.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('192'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              An Online RCM Adjusting System for Robot-Assisted Retinal Surgeries
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#244757" title="Click to go to the Author Index">
             Xia, Jun
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#286728" title="Click to go to the Author Index">
             Wang, Ting
            </a>
           </td>
           <td class="r">
            The First Affiliated Hospital of Fujian Medical University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#285136" title="Click to go to the Author Index">
             Ni, Huanqi
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#318582" title="Click to go to the Author Index">
             Li, Yanlin
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#286553" title="Click to go to the Author Index">
             Chen, Ruoxi
            </a>
           </td>
           <td class="r">
            Southwest University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#160079" title="Click to go to the Author Index">
             Nasseri, M. Ali
            </a>
           </td>
           <td class="r">
            Technische Universitaet Muenchen
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#244766" title="Click to go to the Author Index">
             Lin, Haotian
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University, Zhongshan Ophthalmic Center
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#193739" title="Click to go to the Author Index">
             Huang, Kai
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab192" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#surgical_robotics__steerable_catheters_needles" title="Click to go to the Keyword Index">
               Surgical Robotics: Steerable Catheters/Needles
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#surgical_robotics__planning" title="Click to go to the Keyword Index">
               Surgical Robotics: Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#model_learning_for_control" title="Click to go to the Keyword Index">
               Model Learning for Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In robot-assisted retinal surgery, a Remote Center of Motion (RCM) allows the surgical instrument to rotate around a distal fixed point without any lateral translations. The RCM point should be perfectly aligned inside the trocar.Otherwise, unexpected tool translations at the expected remote center will enlarge the force applied to the trocar and consequently result in post-operative complications. Due to the narrow size of the trocar and the lack of real-time detection equipment, the RCM point is hard to be perfectly located inside the trocar. Even if the RCM is perfectly aligned, the movement of the tissue around the eyeball could make it inappropriate again. In this paper, inspired by the control strategy of surgeons, an online RCM adjusting strategy is proposed. Instead of only using one fixed RCM point, to restrict the force between the surgical tool and the trocar, the proposed strategy adjusts the position of the RCM point during the motion. The results show our approach significantly reduces the force between the robot end-effector and surgical port by 64.2%. In addition, the results also demonstrate that our approach complies the RCM trajectories without deforming or spoiling the working space, which is significantly important for obeying surgeon's instructions in practice.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that7_04">
             10:45-11:00, Paper ThAT7.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('451'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              An Optical Interferometer-Based Force Sensor System for Enhancing Precision in Epidural Injection Procedure
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392625" title="Click to go to the Author Index">
             Cho, Gichan
            </a>
           </td>
           <td class="r">
            DGIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#214102" title="Click to go to the Author Index">
             Im, Jintaek
            </a>
           </td>
           <td class="r">
            DGIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392601" title="Click to go to the Author Index">
             Kwon, Hyun-Jung
            </a>
           </td>
           <td class="r">
            Asan Medical Center
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#199203" title="Click to go to the Author Index">
             Song, Cheol
            </a>
           </td>
           <td class="r">
            DGIST
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab451" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#surgical_robotics__steerable_catheters_needles" title="Click to go to the Keyword Index">
               Surgical Robotics: Steerable Catheters/Needles
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#in_hand_manipulation" title="Click to go to the Keyword Index">
               In-Hand Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In minimally invasive pain management procedures, precise needle positioning is paramount for effective treatment and patient safety. Traditional techniques like the loss-of-resistance (LOR) method may be insufficient, especially in patients with narrowed epidural spaces. The use of imaging tools such as C-arms carries risks due to radiation exposure for medical professionals. A new system for detecting the epidural space based on optical interferometry is proposed to tackle this issue. Prior research has focused on force measurement systems to identify tissue puncture or rupture. Although mechanical sensors have been utilized, they add bulk and complexity to systems. Optical sensors like Fiber Bragg grating (FBG) and Fabry-Prot interferometer (FPI) offer stable, high-resolution measurements suitable for complex biological tissues. This study aims to develop a sensor and needle system for epidural injections, incorporating quantitative metrics for validation. An optical interferometer-based force measurement sensor was integrated into a commercial epidural needle, and calibration was performed to establish a correlation between system output and actual force. The system employs a graphical user interface (GUI) to identify puncture points based on abrupt force decreases. A user study involving interventionalists assessed the systems performance by measuring invasive depth and success rates. The user study demonstrated that the proposed sensorized system could detect the puncture with an average success rate of 72.63 %. This study represents a significant advancement toward safer and more precise epidural procedures, addressing critical clinical considerations for practical applications.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that8">
             <b>
              ThAT8
             </b>
            </a>
           </td>
           <td class="r">
            Room 8
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#that8" title="Click to go to the Program at a Glance">
             <b>
              Localization IV
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#100204" title="Click to go to the Author Index">
             Lee, Joo-Ho
            </a>
           </td>
           <td class="r">
            Ritsumeikan University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that8_01">
             10:00-10:15, Paper ThAT8.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('350'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Representing 3D Sparse Map Points and Lines for Camera Relocalization
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#330184" title="Click to go to the Author Index">
             Bui, Bach-Thuan
            </a>
           </td>
           <td class="r">
            Ritsumeikan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#387509" title="Click to go to the Author Index">
             Bui, Huy Hoang
            </a>
           </td>
           <td class="r">
            Ritsumeikan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#184289" title="Click to go to the Author Index">
             Tran, Dinh Tuan
            </a>
           </td>
           <td class="r">
            College of Information Science and Engineering, Ritsumeikan Univ
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#100204" title="Click to go to the Author Index">
             Lee, Joo-Ho
            </a>
           </td>
           <td class="r">
            Ritsumeikan University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab350" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Recent advancements in visual localization and mapping have demonstrated considerable success in integrating point and line features. However, expanding the localization framework to include additional mapping components frequently results in increased demand for memory and computational resources dedicated to matching tasks. In this study, we show how a lightweight neural network can learn to represent both 3D point and line features, and exhibit leading pose accuracy by harnessing the power of multiple learned mappings. Specifically, we utilize a single transformer block to encode line features, effectively transforming them into distinctive point-like descriptors. Subsequently, we treat these point and line descriptor sets as distinct yet interconnected feature sets. Through the integration of self- and cross-attention within several graph layers, our method effectively refines each feature before regressing 3D maps using two simple MLPs. In comprehensive experiments, our indoor localization findings surpass those of Hloc and Limap across both point-based and line-assisted configurations. Moreover, in outdoor scenarios, our method secures a significant lead, marking the most considerable enhancement over state-of-the-art learning-based methodologies. The source code and demo videos of this work are publicly available at: https://thpjp.github.io/pl2map/.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that8_02">
             10:15-10:30, Paper ThAT8.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('596'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Pos2VPR: Fast Position Consistency Validation with Positive Sample Mining for Hierarchical Place Recognition
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#377155" title="Click to go to the Author Index">
             Zou, Dehao
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#217143" title="Click to go to the Author Index">
             Qian, Xiaolong
            </a>
           </td>
           <td class="r">
            Northeastern University, China
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#219010" title="Click to go to the Author Index">
             Zhang, Yunzhou
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#338569" title="Click to go to the Author Index">
             Zhao, Xinge
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#345959" title="Click to go to the Author Index">
             Wang, Zhuo
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab596" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Visual place recognition (VPR) is a challenging issue for robotics and autonomous systems, focusing on utilizing visual information for robot localization. Currently, hierarchical architecture is being employed by growing works, which embraces RANSAC-based geometric verification for re-ranking. However, RANSAC is time-consuming and only employs geometric information while neglecting other potential information that could be useful for re-ranking. Here we propose a fast position consistency via local patch (PCLP) algorithm to take the position of task-relevant patch-descriptor into account. Without training, it only costs little time but performs better than other re-ranking methods that rely on geometric consistency verification. In this paper, we present a unified place recognition framework that incorporates an aggregation module to extract global features for retrieval and a PCLP validation module to filter local patch for re-ranking. Meanwhile, we propose a RANSAC-based tightly coupled learning (R-TCL) strategy to discover the best positive sample for training robust models. Unlike common sample mining methods, we introduce RANSAC into the sample mining process, achieving trade-off between efficiency and accuracy. Due to improved positive sample mining strategy and novel position validation module, our model is named as
             <i>
              Pos
             </i>
             <sup>
              2
             </sup>
             VPR. Remarkably,
             <i>
              Pos
             </i>
             <sup>
              2
             </sup>
             VPR outperforms state-of-the-art methods on four major datasets with extremely short running time.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that8_03">
             10:30-10:45, Paper ThAT8.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('625'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              WSCLoc: Weakly-Supervised Sparse-View Camera Relocalization
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#311652" title="Click to go to the Author Index">
             Wang, Jialu
            </a>
           </td>
           <td class="r">
            Oxford
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#376394" title="Click to go to the Author Index">
             Zhou, Kaichen
            </a>
           </td>
           <td class="r">
            University of Oxford
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#129241" title="Click to go to the Author Index">
             Markham, Andrew
            </a>
           </td>
           <td class="r">
            Oxford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#104367" title="Click to go to the Author Index">
             Trigoni, Niki
            </a>
           </td>
           <td class="r">
            University of Oxford
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab625" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Despite the advancements in deep learning for camera relocalization tasks, obtaining ground truth pose labels required for the training process remains a costly endeavor. While current weakly supervised methods excel in lightweight label generation, their performance notably declines in scenarios with sparse views. In response to this challenge, we introduce WSCLoc, a system capable of being customized to various deep learning-based relocalization models to enhance their performance under weakly-supervised and sparse view conditions. This is realized with two stages. In the initial stage, WSCLoc employs a multilayer perceptron-based structure called WFT-Nerf to co-optimize image reconstruction quality and initial pose information. To ensure a stable learning process, we incorporate temporal information as input. Furthermore, instead of optimizing SE(3), we opt for mathfrak{sim}(3) optimization to explicitly enforce a scale constraint. In the second stage, we co-optimize the pre-trained WFT-Nerf and WFT-Pose. This optimization is enhanced by Time-Encoding based Random View Synthesis and supervised by inter-frame geometric constraints that consider pose, depth, and RGB information.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that8_04">
             10:45-11:00, Paper ThAT8.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('759'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              CRPlace: Camera-Radar Fusion with BEV Representation for Place Recognition
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392348" title="Click to go to the Author Index">
             Fu, Shaowei
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#231281" title="Click to go to the Author Index">
             Duan, Yifan
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#300074" title="Click to go to the Author Index">
             Li, Yao
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#338247" title="Click to go to the Author Index">
             Meng, Chengzhen
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#326244" title="Click to go to the Author Index">
             Wang, Yingjie
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#148006" title="Click to go to the Author Index">
             Ji, Jianmin
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#291452" title="Click to go to the Author Index">
             Zhang, Yanyong
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab759" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#sensor_fusion" title="Click to go to the Keyword Index">
               Sensor Fusion
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The integration of complementary characteristics from camera and radar data has emerged as an effective approach in 3D object detection. However, such fusion-based methods remain unexplored for place recognition, an equally important task for autonomous systems. Given that place recognition relies on the similarity between a query scene and the corresponding candidate scene, the stationary background of a scene is expected to play a crucial role in the task. As such, current well-designed camera-radar fusion methods for 3D object detection can hardly take effect in place recognition because they mainly focus on dynamic foreground objects. In this paper, a background-attentive camera-radar fusion-based method, named CRPlace, is proposed to generate background-attentive global descriptors from multi-view images and radar point clouds for accurate place recognition. To extract stationary background features effectively, we design an adaptive module that generates the background-attentive mask by utilizing the camera BEV feature and radar dynamic points. With the guidance of a background mask, we devise a bidirectional cross-attention-based spatial fusion strategy to facilitate comprehensive spatial interaction between the background information of the camera BEV feature and the radar BEV feature. As the first camera-radar fusion-based place recognition network, CRPlace has been evaluated thoroughly on the nuScenes dataset. The results show that our algorithm outperforms a variety of baseline methods across a comprehensive set of metrics (recall@1 reaches 91.2%).
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that9">
             <b>
              ThAT9
             </b>
            </a>
           </td>
           <td class="r">
            Room 9
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#that9" title="Click to go to the Program at a Glance">
             <b>
              Motion and Path Planning IV
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#395060" title="Click to go to the Author Index">
             Trumpp, Raphael
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that9_01">
             10:00-10:15, Paper ThAT9.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('156'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Online Multi-Agent Pickup and Delivery with Task Deadlines
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#389898" title="Click to go to the Author Index">
             Makino, Hiroya
            </a>
           </td>
           <td class="r">
            Toyota Central R&amp;D Labs., Inc
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#167934" title="Click to go to the Author Index">
             Ito, Seigo
            </a>
           </td>
           <td class="r">
            Toyota Central R&amp;D Labs., Inc
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab156" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#planning__scheduling_and_coordination" title="Click to go to the Keyword Index">
               Planning, Scheduling and Coordination
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#intelligent_transportation_systems" title="Click to go to the Keyword Index">
               Intelligent Transportation Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Managing delivery deadlines in automated warehouses and factories is crucial for maintaining customer satisfaction and ensuring seamless production. This study introduces the problem of online multi-agent pickup and delivery with task deadlines (MAPD-D), an advanced variant of the online MAPD problem incorporating delivery deadlines. In the MAPD problem, agents must manage a continuous stream of delivery tasks online. Tasks are added at any time. Agents must complete their tasks while avoiding collisions with each other. MAPD-D introduces a dynamic, deadline-driven approach that incorporates task deadlines, challenging the conventional MAPD frameworks. To tackle MAPD-D, we propose a novel algorithm named deadline-aware token passing (D-TP). The D-TP algorithm calculates pickup deadlines and assigns tasks while balancing execution cost and deadline proximity. Additionally, we introduce the D-TP with task swaps (D-TPTS) method to further reduce task tardiness, enhancing flexibility and efficiency through task-swapping strategies. Numerical experiments were conducted in simulated warehouse environments to showcase the effectiveness of the proposed methods. Both D-TP and D-TPTS demonstrated significant reductions in task tardiness compared to existing methods. Our methods contribute to efficient operations in automated warehouses and factories with delivery deadlines.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that9_02">
             10:15-10:30, Paper ThAT9.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('159'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              MARPF: Multi-Agent and Multi-Rack Path Finding
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#389898" title="Click to go to the Author Index">
             Makino, Hiroya
            </a>
           </td>
           <td class="r">
            Toyota Central R&amp;D Labs., Inc
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#389908" title="Click to go to the Author Index">
             Ohama, Yoshihiro
            </a>
           </td>
           <td class="r">
            Toyota Central R&amp;D Labs., Inc
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#167934" title="Click to go to the Author Index">
             Ito, Seigo
            </a>
           </td>
           <td class="r">
            Toyota Central R&amp;D Labs., Inc
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab159" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#factory_automation" title="Click to go to the Keyword Index">
               Factory Automation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#planning__scheduling_and_coordination" title="Click to go to the Keyword Index">
               Planning, Scheduling and Coordination
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In environments where many automated guided vehicles (AGVs) operate, planning efficient, collision-free paths is essential. Related research has mainly focused on environments with pre-defined passages, resulting in space inefficiency. We attempt to relax this assumption. In this study, we define multi-agent and multi-rack path finding (MARPF) as the problem of planning paths for AGVs to convey target racks to their designated locations in environments without passages. In such environments, an AGV without a rack can pass under racks, whereas one with a rack cannot pass under racks to avoid collisions. MARPF entails conveying the target racks without collisions, while the obstacle racks are relocated to prevent any interference with the target racks. We formulated MARPF as an integer linear programming problem in a network flow. To distinguish situations in which an AGV is or is not loading a rack, the proposed method introduces two virtual layers into the network. We optimized the AGVs' movements to move obstacle racks and convey the target racks. The formulation and applicability of the algorithm were validated through numerical experiments. The results indicated that the proposed algorithm addressed issues in environments with dense racks.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that9_03">
             10:30-10:45, Paper ThAT9.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1005'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Smooth Invariant Interpolation on Lie Groups with Prescribed Terminal Conditions for Robot Motion Planning and Modeling of Soft Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#103731" title="Click to go to the Author Index">
             Mueller, Andreas
            </a>
           </td>
           <td class="r">
            Johannes Kepler University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#353999" title="Click to go to the Author Index">
             Marauli, Tobias
            </a>
           </td>
           <td class="r">
            Johannes Kepler University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#144188" title="Click to go to the Author Index">
             Gattringer, Hubert
            </a>
           </td>
           <td class="r">
            Johannes Kepler University Linz
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1005" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#modeling__control__and_learning_for_soft_robots" title="Click to go to the Keyword Index">
               Modeling, Control, and Learning for Soft Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#kinematics" title="Click to go to the Keyword Index">
               Kinematics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Interpolation of rigid body motions, or a general frame motion in Euclidean space, is a recurring topic in robotics. It boils down to generating trajectories in a Lie group, either SE(3) or SO(3)xR3, with given initial and/or terminal values. To this end, spline interpolation schemes were developed where the canonical coordinates are represented by cubic splines. They allow for prescribing initial velocity and acceleration only. Many robotic applications require trajectories with prescribed terminal conditions, however. In this paper, a novel interpolation scheme is presented that admits prescribing the terminal pose, velocity and acceleration, or the initial condition. As example, the scheme is applied to a rendezvous task of a UAV, the trajectory reconstruction from motion capture data, and to describing the deformation of a Cosserat beam as relevant for soft robotics. The presented interpolation scheme can be directly applied to the motion parameterization in terms of (dual) quaternions.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that9_04">
             10:45-11:00, Paper ThAT9.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1006'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              RaceMOP: Mapless Online Path Planning for Multi-Agent Autonomous Racing Using Residual Policy Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395060" title="Click to go to the Author Index">
             Trumpp, Raphael
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#356467" title="Click to go to the Author Index">
             Javanmardi, Ehsan
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395063" title="Click to go to the Author Index">
             Nakazato, Jin
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#309839" title="Click to go to the Author Index">
             Tsukada, Manabu
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#237765" title="Click to go to the Author Index">
             Caccamo, Marco
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1006" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#nonholonomic_motion_planning" title="Click to go to the Keyword Index">
               Nonholonomic Motion Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#machine_learning_for_robot_control" title="Click to go to the Keyword Index">
               Machine Learning for Robot Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The interactive decision-making in multi-agent autonomous racing offers insights valuable beyond the domain of self-driving cars. Mapless online path planning is particularly of practical appeal but poses a challenge for safely overtaking opponents due to the limited planning horizon. To address this, we introduce RaceMOP, a novel method for mapless online path planning designed for multi-agent racing of F1TENTH cars. Unlike classical planners that rely on predefined racing lines, RaceMOP operates without a map, utilizing only local observations to execute high-speed overtaking maneuvers. Our approach combines an artificial potential field method as a base policy with residual policy learning to enable long-horizon planning. We advance the field by introducing a novel approach for policy fusion with the residual policy directly in probability space. Extensive experiments on twelve simulated racetracks validate that RaceMOP is capable of long-horizon decision-making with robust collision avoidance during overtaking maneuvers. RaceMOP demonstrates superior handling over existing mapless planners and generalizes to unknown racetracks, affirming its potential for broader applications in robotics. Our code is available at http://github.com/raphajaner/racemop.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that10">
             <b>
              ThAT10
             </b>
            </a>
           </td>
           <td class="r">
            Room 10
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#that10" title="Click to go to the Program at a Glance">
             <b>
              Deep Learning for Visual Perception I
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that10_01">
             10:00-10:15, Paper ThAT10.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('366'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              WidthFormer: Toward Efficient Transformer-Based BEV View Transformation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#390731" title="Click to go to the Author Index">
             Yang, Chenhongyi
            </a>
           </td>
           <td class="r">
            University of Edinburgh
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#390781" title="Click to go to the Author Index">
             Lin, Tianwei
            </a>
           </td>
           <td class="r">
            Horizon Robotics
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#204470" title="Click to go to the Author Index">
             Huang, Lichao
            </a>
           </td>
           <td class="r">
            Horizon Robotics Inc
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#390732" title="Click to go to the Author Index">
             Crowley, Elliot J.
            </a>
           </td>
           <td class="r">
            University of Edinburgh
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab366" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present WidthFormer, a novel transformer-based module to compute Bird's-Eye-View (BEV) representations from multi-view cameras for real-time autonomous-driving applications. WidthFormer is computationally efficient, robust and does not require any special engineering effort to deploy. We first introduce a novel 3D positional encoding mechanism capable of accurately encapsulating 3D geometric information, which enables our model to compute high-quality BEV representations with only a single transformer decoder layer. This mechanism is also beneficial for existing sparse 3D object detectors. Inspired by the recently proposed works, we further improve our model's efficiency by vertically compressing the image features when serving as attention keys and values, and then we develop two modules to compensate for potential information loss due to feature compression. Experimental evaluation on the widely-used nuScenes 3D object detection benchmark demonstrates that our method outperforms previous approaches across different 3D detection architectures. More importantly, our model is highly efficient. For example, when using 256times 704 input images, it achieves 1.5 ms and 2.8 ms latency on NVIDIA 3090 GPU and Horizon Journey-5 computation solutions. Furthermore, WidthFormer also exhibits strong robustness to different degrees of camera perturbations. Our study offers valuable insights into the deployment of BEV transformation methods in real-world, complex road environments. Code is available at https://github.com/ChenhongyiYang/WidthFormer.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that10_02">
             10:15-10:30, Paper ThAT10.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('865'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              ARDuP: Active Region Video Diffusion for Universal Policies
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#391483" title="Click to go to the Author Index">
             Huang, Shuaiyi
            </a>
           </td>
           <td class="r">
            University of Maryland, College Park
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#289666" title="Click to go to the Author Index">
             Levy, Mara
            </a>
           </td>
           <td class="r">
            University of Maryland, College Park
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#342351" title="Click to go to the Author Index">
             Jiang, Zhenyu
            </a>
           </td>
           <td class="r">
            The Unversity of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#236420" title="Click to go to the Author Index">
             Anandkumar, Anima
            </a>
           </td>
           <td class="r">
            Caltech
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#203352" title="Click to go to the Author Index">
             Zhu, Yuke
            </a>
           </td>
           <td class="r">
            The University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#280356" title="Click to go to the Author Index">
             Fan, Linxi
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#233075" title="Click to go to the Author Index">
             Huang, De-An
            </a>
           </td>
           <td class="r">
            NVIDIA
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#176399" title="Click to go to the Author Index">
             Shrivastava, Abhinav
            </a>
           </td>
           <td class="r">
            University of Maryland, College Park
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab865" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Sequential decision-making can be formulated as a text-conditioned video generation problem, where a video planner, guided by a text-defined goal, generates future frames visualizing planned actions, from which control actions are subsequently derived. In this work, we introduce Active Region Video Diffusion for Universal Policies (ARDuP), a novel framework for video-based policy learning that emphasizes the generation of active regions, i.e. potential interaction areas, enhancing the conditional policys focus on interactive areas critical for task execution. This innovative framework integrates active region conditioning with latent diffusion models for video planning and employs latent representations for direct action decoding during inverse dynamic modeling. By utilizing motion cues in videos for automatic active region discovery, our method eliminates the need for manual annotations of active regions. We validate ARDuPs efficacy via extensive experiments on simulator CLIPort and the real-world dataset BridgeData v2, achieving notable improvements in success rates and generating convincingly realistic video plans.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that10_03">
             10:30-10:45, Paper ThAT10.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('979'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SNF-Feat: Semantic-Guided Negative-Sample-Free Representation Learning for Local Feature Extraction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392302" title="Click to go to the Author Index">
             Zhou, Xun
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#313879" title="Click to go to the Author Index">
             Yan, Qingqing
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397159" title="Click to go to the Author Index">
             Zhu, Minghao
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397182" title="Click to go to the Author Index">
             Hu, Mengxian
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#134932" title="Click to go to the Author Index">
             Liu, Chengju
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#118155" title="Click to go to the Author Index">
             Chen, Qijun
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab979" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#representation_learning" title="Click to go to the Keyword Index">
               Representation Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Local feature extraction constitutes a foundational module crucial for numerous downstream tasks of computer vision. Its primary challenge lies in the generation of discriminative feature representations. Prior methodologies have employed contrastive learning within their pipelines, yet have encountered limitations stemming from inherent conflicts within their training data, including the ambiguity of negative samples and the distortion of positive samples. In this study, we propose a semantic-guided negative-sample-free method for local feature learning, denoted as SNF-Feat. Our framework entails dense patch-level representation learning without reliance on negative samples, aiming to ensure that descriptors derived from transformed views of the same local area exhibit predictive capability towards each other. To assess the impact of positive sample distortion, we harness high-level semantic information to derive point-wise loss weights. Furthermore, we establish a self-supervised feature learning paradigm that extends our utilization of datasets. Experimental results demonstrate the superior performance of our method across a range of typical datasets and tasks in comparison to state-of-the-art approaches.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that10_04">
             10:45-11:00, Paper ThAT10.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1644'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              MonoPlane: Exploiting Monocular Geometric Cues for Generalizable 3D Plane Reconstruction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#266571" title="Click to go to the Author Index">
             Zhao, Wang
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#370570" title="Click to go to the Author Index">
             Liu, Jiachen
            </a>
           </td>
           <td class="r">
            Pennsylvania State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392081" title="Click to go to the Author Index">
             Zhang, Sheng
            </a>
           </td>
           <td class="r">
            Bytedance Inc
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396965" title="Click to go to the Author Index">
             Li, Yishu
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392106" title="Click to go to the Author Index">
             Chen, Sili
            </a>
           </td>
           <td class="r">
            ByteDance
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#377761" title="Click to go to the Author Index">
             Huang, Sharon X.
            </a>
           </td>
           <td class="r">
            The Pennsylvania State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#129927" title="Click to go to the Author Index">
             Liu, Yong-Jin
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#255728" title="Click to go to the Author Index">
             Guo, Hengkai
            </a>
           </td>
           <td class="r">
            ByteDance AI Lab
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1644" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#big_data_in_robotics_and_automation" title="Click to go to the Keyword Index">
               Big Data in Robotics and Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a generalizable 3D plane detection and reconstruction framework named MonoPlane. Unlike previous robust estimator-based works (which require multiple images or RGB-D input) and learning-based works (which suffer from domain shift), MonoPlane combines the best of two worlds and establishes a plane reconstruction pipeline based on monocular geometric cues, resulting in accurate, robust and scalable 3D plane detection and reconstruction in the wild. Specifically, we first leverage large-scale pre-trained neural networks to obtain the depth and surface normals from a single image. These monocular geometric cues are then incorporated into a proximity-guided RANSAC framework to sequentially fit each plane instance. We exploit effective 3D point proximity and model such proximity via a graph within RANSAC to guide the plane fitting from noisy monocular depths, followed by image-level multi-plane joint optimization to improve the consistency among all plane instances. We further design a simple but effective pipeline to extend this single-view solution to sparse-view 3D plane reconstruction. Extensive experiments on a list of datasets demonstrate our superior zero-shot generalizability over baselines, achieving state-of-the-art plane reconstruction performance in a transferring setting. Our code is available at https://github.com/thuzhaowang/MonoPlane.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that11">
             <b>
              ThAT11
             </b>
            </a>
           </td>
           <td class="r">
            Room 11
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#that11" title="Click to go to the Program at a Glance">
             <b>
              Multi-Robot Systems IV
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#113384" title="Click to go to the Author Index">
             Saska, Martin
            </a>
           </td>
           <td class="r">
            Czech Technical University in Prague
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that11_01">
             10:00-10:15, Paper ThAT11.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('650'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Real-Time Bandwidth-Efficient Occupancy Grid Map Synchronization for Multi-Robot Systems
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#393441" title="Click to go to the Author Index">
             Shi, Liuyu
            </a>
           </td>
           <td class="r">
            The University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#312242" title="Click to go to the Author Index">
             Yin, Longji
            </a>
           </td>
           <td class="r">
            The University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#296515" title="Click to go to the Author Index">
             Kong, Fanze
            </a>
           </td>
           <td class="r">
            The University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#292033" title="Click to go to the Author Index">
             Ren, Yunfan
            </a>
           </td>
           <td class="r">
            The University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#320284" title="Click to go to the Author Index">
             Zhu, Fangcheng
            </a>
           </td>
           <td class="r">
            The University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#345322" title="Click to go to the Author Index">
             Tang, Benxu
            </a>
           </td>
           <td class="r">
            The University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#204381" title="Click to go to the Author Index">
             Zhang, Fu
            </a>
           </td>
           <td class="r">
            University of Hong Kong
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab650" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robot swarms are increasingly being applied in various domains. However, due to the inherent limitation imposed by low real-time communication bandwidth, the synchronization of environmental information among multiple robots remains a persistent and challenging problem in practical applications. In response to this challenge, we introduce a comprehensive framework for synchronizing occupancy grid maps (OGMs) in practical multi-robot systems that operate under communication bandwidth constraints. In our research, we elaborately design the data structure of transmitted map packages and employ the Hilbert space-filling curve for voxel sorting. By adopting this approach, data redundancy is effectively increased, resulting in lower information entropy and significantly reducing the volume of communication data. Finally, our framework outperforms the benchmark method by reducing the average and maximum bandwidth usage by more than 10 times in high-resolution scenarios. Moreover, our method has been successfully applied in the multi-UAV autonomous navigation application, demonstrating its real-time and bandwidth-efficient nature, as well as its practical value.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that11_02">
             10:15-10:30, Paper ThAT11.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('685'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Adaptive Model Predictive Control for Differential-Algebraic Systems towards a Higher Path Accuracy for Physically Coupled Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#393550" title="Click to go to the Author Index">
             Ye, Xin
            </a>
           </td>
           <td class="r">
            FZI Research Center for Information Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#393551" title="Click to go to the Author Index">
             Handwerker, Karl
            </a>
           </td>
           <td class="r">
            FZI Research Center for Information Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#245275" title="Click to go to the Author Index">
             Hohmann, Sren
            </a>
           </td>
           <td class="r">
            Institute of Control Systems, Karlsruhe Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab685" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#dual_arm_manipulation" title="Click to go to the Keyword Index">
               Dual Arm Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The physical coupling between robots has the potential to improve the capabilities of multi-robot systems in challenging manufacturing processes. However, the path tracking accuracy of physically coupled robots is not studied adequately, especially considering the uncertain kinematic parameters, the mechanical elasticity, and the built-in controllers of off-the-shelf robots. This paper addresses these issues with a novel differential-algebraic system model which is verified against measurement data from real execution. The uncertain kinematic parameters are estimated online to adapt the model. Consequently, an adaptive model predictive controller is designed as a coordinator between the robots. The controller achieves a path tracking error reduction of 88.6% compared to the state-of-the-art benchmark in the simulation.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that11_03">
             10:30-10:45, Paper ThAT11.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('910'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Asynchronous Spatial-Temporal Allocation for Trajectory Planning of Heterogeneous Multi-Agent Systems
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#319593" title="Click to go to the Author Index">
             Chen, Yuda
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#372826" title="Click to go to the Author Index">
             Dong, Haoze
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#248847" title="Click to go to the Author Index">
             Li, Zhongkui
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab910" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#collision_avoidance" title="Click to go to the Keyword Index">
               Collision Avoidance
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             To plan the trajectories of a large-scale heterogeneous swarm, sequentially or synchronously distributed methods usually become intractable due to the lack of global clock synchronization. To this end, we provide a novel asynchronous spatial-temporal allocation method. Specifically, between a pair of agents, the allocation is proposed to determine their corresponding derivable time-stamped space and can be updated in an asynchronous way, by inserting a waiting duration between two consecutive replanning steps. It is theoretically shown that the inter-agent collision is avoided and the allocation ensures timely updates. Comprehensive simulations and comparisons with state-of-the-art baselines validate the effectiveness of the proposed method and illustrate its improvement in completion time and moving distance. Finally, hardware experiments are carried out, where 8 heterogeneous unmanned ground vehicles with onboard computation navigate in cluttered scenarios with high agility.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that11_04">
             10:45-11:00, Paper ThAT11.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1486'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              BuzzRacer: A Palm-Sized Autonomous Vehicle Platform for Testing Multi-Agent Adversarial Decision-Making
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#313193" title="Click to go to the Author Index">
             Zhang, Zhiyuan
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#101179" title="Click to go to the Author Index">
             Tsiotras, Panagiotis
            </a>
           </td>
           <td class="r">
            Georgia Tech
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1486" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#education_robotics" title="Click to go to the Keyword Index">
               Education Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#software_architecture_for_robotic_and_automation" title="Click to go to the Keyword Index">
               Software Architecture for Robotic and Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present BuzzRacer, a palm-sized autonomous vehicle platform suitable for multi-agent autonomous racing. BuzzRacer consists of two parts. First, a software framework with multiple racetrack environments, dynamic simulation, visualization, and control pipelines. Second, a miniature autonomous vehicle platform capable of 1g acceleration and 3.5m/s top speed. BuzzRacer is an open-source project currently used at Georgia Tech in a project-based robotics course and research projects for experimental validation and benchmarking of novel planning and control algorithms
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that12">
             <b>
              ThAT12
             </b>
            </a>
           </td>
           <td class="r">
            Room 12
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#that12" title="Click to go to the Program at a Glance">
             <b>
              Imitation Learning II
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#101828" title="Click to go to the Author Index">
             Ogata, Tetsuya
            </a>
           </td>
           <td class="r">
            Waseda University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that12_01">
             10:00-10:15, Paper ThAT12.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('221'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Multi-Task Real-Robot Data with Gaze Attention for Dual-Arm Fine Manipulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#272186" title="Click to go to the Author Index">
             Kim, Heecheol
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#111270" title="Click to go to the Author Index">
             Ohmura, Yoshiyuki
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#105628" title="Click to go to the Author Index">
             Kuniyoshi, Yasuo
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab221" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Deep imitation learning is a promising approach in robotic manipulation, enabling robots to acquire versatile and adaptable skills. In such research, by learning various tasks, robots achieved generality across multiple objects. However, such multi-task robot datasets have mainly focused on single-arm tasks that are relatively imprecise and not addressed the fine-grained object manipulation that robots are expected to perform in the real world. In this study, we introduce a dataset for diverse object manipulation that includes dual-arm tasks and/or tasks that require fine manipulation. We generated a dataset containing 224k episodes (150 hours, 1,104 language instructions) that includes dual-arm fine tasks, such as bowl-moving, pencil-case opening, and banana-peeling. This dataset is publicly available. Additionally, this dataset includes visual attention signals, dual-action labels that separate actions into robust reaching trajectories or precise interactions with objects, and language instructions, all aimed at achieving robust and precise object manipulation. We applied the dataset to our Dual-Action and Attention, which is a model that we designed for fine-grained dual-arm manipulation tasks that is robust to covariate shift. We tested the model in over 7k trials for real robot manipulation tasks, which demonstrated its capability to perform fine manipulation.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that12_02">
             10:15-10:30, Paper ThAT12.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2460'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Neural ODE-Based Imitation Learning (NODE-IL): Data-Efficient Imitation Learning for Long-Horizon Multi-Skill Robot Manipulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#378164" title="Click to go to the Author Index">
             Zhao, Shiyao
            </a>
           </td>
           <td class="r">
            University of Edinburgh
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#309718" title="Click to go to the Author Index">
             Xu, Yucheng
            </a>
           </td>
           <td class="r">
            University of Edinburgh
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#244242" title="Click to go to the Author Index">
             Kasaei, Mohammadreza
            </a>
           </td>
           <td class="r">
            University of Edinburgh
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#179020" title="Click to go to the Author Index">
             Khadem, Mohsen
            </a>
           </td>
           <td class="r">
            University of Edinburgh
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#125824" title="Click to go to the Author Index">
             Li, Zhibin (Alex)
            </a>
           </td>
           <td class="r">
            University College London
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2460" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#model_learning_for_control" title="Click to go to the Keyword Index">
               Model Learning for Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In robotics, acquiring new skills through Imitation Learning (IL) is crucial for handling diverse complex tasks. However, model-free IL faces data inefficiency and prolonged training time, whereas model-based methods struggle to obtain accurate models. To address these challenges, we developed Neural ODE-based Imitation Learning (NODE-IL), a novel model-based imitation learning framework that employs Neural Ordinary Differential Equations (Neural ODEs) for learning task dynamics and control policies. NODE-IL comprises (1) Dynamic-NODE for learning the continuous differentiable tasks transition dynamics model, and (2) Control-NODE for learning a long-horizon control policy in an MPC fashion, which are trained holistically. Extensively evaluated on challenging manipulation tasks, NODE-IL demonstrates significant advantages in data efficiency, requiring less than 70 samples to achieve robust performance. It outperforms Behavioral Cloning from Observation (BCO) and Gaussian Process Imitation Learning (GP-IL) methods, achieving a 70% higher average success rate, and reducing translation errors for high-precision tasks, which demonstrates its robustness and accuracy as an effective and efficient imitation learning approach for learning complex manipulation tasks.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that12_03">
             10:30-10:45, Paper ThAT12.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3407'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Data Efficient Behavior Cloning for Fine Manipulation Via Continuity-Based Corrective Labels
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396860" title="Click to go to the Author Index">
             Deshpande, Abhay
            </a>
           </td>
           <td class="r">
            University of Washington
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#270123" title="Click to go to the Author Index">
             Ke, Liyiming
            </a>
           </td>
           <td class="r">
            University of Washington
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396864" title="Click to go to the Author Index">
             Pfeifer, Quinn
            </a>
           </td>
           <td class="r">
            University of Washington
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#180419" title="Click to go to the Author Index">
             Gupta, Abhishek
            </a>
           </td>
           <td class="r">
            University of Washington
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#105832" title="Click to go to the Author Index">
             Srinivasa, Siddhartha
            </a>
           </td>
           <td class="r">
            University of Washington
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3407" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#dexterous_manipulation" title="Click to go to the Keyword Index">
               Dexterous Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We consider imitation learning with access only to expert demonstrations, whose real-world application is often limited by covariate shift due to compounding errors during execution. We investigate the effectiveness of the Continuity-based Corrective Labels for Imitation Learning (CCIL) framework in mitigating this issue for real-world fine manipulation tasks. CCIL generates corrective labels by learning a locally continuous dynamics model from demonstrations to guide the agent back toward expert states. Through extensive experiments on insertion and fine grasping tasks, we provide the first empirical validation that CCIL can significantly improve imitation learning performance despite discontinuities present in contact-rich manipulation. We find that: (1) real-world manipulation exhibits sufficient local smoothness to apply CCIL, (2) generated corrective labels are most beneficial in low-data regimes, and (3) label filtering based on estimated dynamics model error enables performance gains. To effectively apply CCIL to robotic domains, we offer a practical instantiation of the framework and insights into design choices and hyperparameter selection. Our work demonstrates CCIL's practicality for alleviating compounding errors in imitation learning on physical robots.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that12_04">
             10:45-11:00, Paper ThAT12.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3446'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              From LLMs to Actions: Latent Codes As Bridges in Hierarchical Robot Control
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#204951" title="Click to go to the Author Index">
             Shentu, Yide
            </a>
           </td>
           <td class="r">
            University of California -- Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#233715" title="Click to go to the Author Index">
             Wu, Shiyao
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#235949" title="Click to go to the Author Index">
             Rajeswaran, Aravind
            </a>
           </td>
           <td class="r">
            Meta AI
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#107568" title="Click to go to the Author Index">
             Abbeel, Pieter
            </a>
           </td>
           <td class="r">
            UC Berkeley
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3446" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Hierarchical control for robotics has long been plagued by the need to have a well defined interface layer to communicate between high-level task planners and low-level policies. With the advent of LLMs, language has been emerging as a prospective interface layer. However, this has several limitations. Not all tasks can be decomposed into steps that are easily expressible in natural language (e.g. performing a dance routine). Further, it makes end-to-end finetuning on embodied data challenging due to domain shift and catastrophic forgetting. We introduce our method -- Latent Codes as Bridges (LCB) -- as an alternate architecture to overcome these limitations. LCB uses a learnable latent code to act as a bridge between LLMs and low-level policies. This enables LLMs to flexibly communicate goals in the task plan without being entirely constrained by language limitations. Additionally, it enables end-to-end finetuning without destroying the embedding space of word tokens learned during pre-training. Through experiments on Language Table and Calvin, two common language based benchmarks for embodied agents, we find that method~outperforms baselines (including those w/ GPT-4V) that leverage pure language as the interface layer on tasks that require reasoning and multi-step behaviors.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that13">
             <b>
              ThAT13
             </b>
            </a>
           </td>
           <td class="r">
            Room 13
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#that13" title="Click to go to the Program at a Glance">
             <b>
              Sensor Fusion II
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that13_01">
             10:00-10:15, Paper ThAT13.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('504'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              DeRO: Dead Reckoning Based on Radar Odometry with Accelerometers Aided for Robot Localization
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#382600" title="Click to go to the Author Index">
             Do, Hoang Viet
            </a>
           </td>
           <td class="r">
            Sejong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392890" title="Click to go to the Author Index">
             Kim, Yong Hun
            </a>
           </td>
           <td class="r">
            Sejong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392886" title="Click to go to the Author Index">
             Lee, Joo Han
            </a>
           </td>
           <td class="r">
            Sejong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392895" title="Click to go to the Author Index">
             Lee, Min Ho
            </a>
           </td>
           <td class="r">
            Sejong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#142989" title="Click to go to the Author Index">
             Song, Jin Woo
            </a>
           </td>
           <td class="r">
            Sejong University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab504" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#sensor_fusion" title="Click to go to the Keyword Index">
               Sensor Fusion
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper, we propose a radar odometry structure that directly utilizes radar velocity measurements for dead reckoning while maintaining its ability to update estimations within the Kalman filter framework. Specifically, we employ the Doppler velocity obtained by a 4D Frequency Modulated Continuous Wave (FMCW) radar in conjunction with gyroscope data to calculate poses. This approach helps mitigate high drift resulting from accelerometer biases and double integration. Instead, tilt angles measured by gravitational force are utilized alongside relative distance measurements from radar scan matching for the filter's measurement update. Additionally, to further enhance the system's accuracy, we estimate and compensate for the radar velocity scale factor. The performance of the proposed method is verified through five real-world open-source datasets. The results demonstrate that our approach reduces position error by 62% and rotation error by 66% on average compared to the state-of-the-art radar-inertial fusion method in terms of absolute trajectory error.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that13_02">
             10:15-10:30, Paper ThAT13.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('940'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              GMMCalib: Extrinsic Calibration of LiDAR Sensors Using GMM-Based Joint Registration
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#388344" title="Click to go to the Author Index">
             Tahiraj, Ilir
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#393694" title="Click to go to the Author Index">
             Fent, Felix
            </a>
           </td>
           <td class="r">
            TU Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#394503" title="Click to go to the Author Index">
             Hafemann, Philipp
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#283711" title="Click to go to the Author Index">
             Ye, Egon
            </a>
           </td>
           <td class="r">
            BMW AG
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#194660" title="Click to go to the Author Index">
             Lienkamp, Markus
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab940" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#sensor_fusion" title="Click to go to the Keyword Index">
               Sensor Fusion
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#sensor_networks" title="Click to go to the Keyword Index">
               Sensor Networks
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#intelligent_transportation_systems" title="Click to go to the Keyword Index">
               Intelligent Transportation Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             State-of-the-art LiDAR calibration frameworks mainly use non-probabilistic registration methods such as Iterative Closest Point (ICP) and its variants. These methods suffer from biased results due to their pair-wise registration procedure as well as their sensitivity to initialization and parameterization. This often leads to misalignments in the calibration process. Probabilistic registration methods compensate for these drawbacks by specifically modeling the probabilistic nature of the observations. This paper presents GMMCalib, an automatic target-based extrinsic calibration approach for multi-LiDAR systems. Using an implementation of a Gaussian Mixture Model (GMM)-based registration method that allows joint registration of multiple point clouds, this data-driven approach is compared to ICP algorithms. We perform simulation experiments using the digital twin of the EDGAR research vehicle and validate the results in a real-world environment. We also address the local minima problem when using registration methods for extrinsic sensor calibration and use a distance-based metric to evaluate the calibration results. Our results show that an increase in robustness against sensor miscalibrations can be achieved by using GMM-based registration algorithms.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that13_03">
             10:30-10:45, Paper ThAT13.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1670'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              FlexLoc: Conditional Neural Networks for Zero-Shot Sensor Perspective Invariance in Object Localization with Distributed Multimodal Sensors
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#340789" title="Click to go to the Author Index">
             Wu, Jason
            </a>
           </td>
           <td class="r">
            University of California, Los Angeles
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392396" title="Click to go to the Author Index">
             Wang, Ziqi
            </a>
           </td>
           <td class="r">
            University of California, Los Angeles
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397031" title="Click to go to the Author Index">
             Ouyang, Xiaomin
            </a>
           </td>
           <td class="r">
            University of California, Los Angeles
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392423" title="Click to go to the Author Index">
             Jeong, Ho Lyun
            </a>
           </td>
           <td class="r">
            University of California, Los Angeles
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392525" title="Click to go to the Author Index">
             Samplawski, Colin
            </a>
           </td>
           <td class="r">
            University of Massachusetts Amherst
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#334714" title="Click to go to the Author Index">
             Kaplan, Lance
            </a>
           </td>
           <td class="r">
            DEVCOM Army Research Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392454" title="Click to go to the Author Index">
             Marlin, Benjamin
            </a>
           </td>
           <td class="r">
            UMass Amherst
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#172053" title="Click to go to the Author Index">
             Srivastava, Mani
            </a>
           </td>
           <td class="r">
            UCLA
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1670" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#sensor_fusion" title="Click to go to the Keyword Index">
               Sensor Fusion
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#sensor_networks" title="Click to go to the Keyword Index">
               Sensor Networks
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Localization is a critical technology for various applications ranging from navigation and surveillance to assisted living. Localization systems typically fuse information from sensors viewing the scene from different perspectives to estimate the target location while also employing multiple modalities for enhanced robustness and accuracy. Recently, such systems have employed end-to-end deep neural models trained on large datasets due to their superior performance and ability to handle data from diverse sensor modalities. However, such neural models are often trained on data collected from a particular set of sensor poses (i.e., locations and orientations). During real-world deployments, slight deviations from these sensor poses can result in extreme inaccuracies. To address this challenge, we introduce FlexLoc, which employs conditional neural networks to inject node perspective information to adapt the localization pipeline. Specifically, a small subset of model weights are derived from node poses at run time, enabling accurate generalization to unseen perspectives with minimal additional overhead. Our evaluations on a multimodal, multiview indoor tracking dataset showcase that FlexLoc improves the localization accuracy by almost 50% in the zero-shot case (no calibration data available) compared to the baselines. The source code of FlexLoc is available in https://github.com/nesl/FlexLoc.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that13_04">
             10:45-11:00, Paper ThAT13.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1778'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              VIVO: A Visual-Inertial-Velocity Odometry with Online Calibration in Challenging Condition
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#315209" title="Click to go to the Author Index">
             Han, Fuzhang
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#324442" title="Click to go to the Author Index">
             Jia, Shenhan
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392156" title="Click to go to the Author Index">
             Yu, Jiyu
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#379112" title="Click to go to the Author Index">
             Wei, Yufei
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#339972" title="Click to go to the Author Index">
             Huang, Wenjun
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#156231" title="Click to go to the Author Index">
             Wang, Yue
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#113216" title="Click to go to the Author Index">
             Xiong, Rong
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1778" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#sensor_fusion" title="Click to go to the Keyword Index">
               Sensor Fusion
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#visual_inertial_slam" title="Click to go to the Keyword Index">
               Visual-Inertial SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             State estimation is a central component of autonomous navigation. To date, many methods presented have a disruptive potential for application, such as visual-inertial odometry (VIO), wheel and leg odometry (for short, body odometry). However, most of them are prone to fail in some challenging conditions like high-dynamic street scenes and sustain aggressive movements. To this end, in this paper, we present a novel visual-inertial-velocity odometry (VIVO) framework which incorporates velocity measurement provided by the proprioceptive sensing into the MSCKF-based VIO in a tightly coupled fashion. Furthermore, considering that the imprecise extrinsic parameters can severely undermine the state estimation performance, we hence perform VIVO along with online calibration of the body odometry's extrinsic parameters by adding them to the estimated state vector. The generic VIVO can be deployed for a broad spectrum of robot models ranging from wheeled robots to legged robots. Both simulation and real-world experiments are performed to extensively validate the robustness and accuracy of the proposed method in challenging scenarios using wheeled and legged robot models, respectively.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that14">
             <b>
              ThAT14
             </b>
            </a>
           </td>
           <td class="r">
            Room 14
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#that14" title="Click to go to the Program at a Glance">
             <b>
              Transfer Learning
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#229949" title="Click to go to the Author Index">
             Saito, Namiko
            </a>
           </td>
           <td class="r">
            The University of Edinburgh
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that14_01">
             10:00-10:15, Paper ThAT14.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1839'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Latent Object Characteristics Recognition with Visual to Haptic-Audio Cross-Modal Transfer Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#229949" title="Click to go to the Author Index">
             Saito, Namiko
            </a>
           </td>
           <td class="r">
            The University of Edinburgh
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#205452" title="Click to go to the Author Index">
             Moura, Joao
            </a>
           </td>
           <td class="r">
            The University of Edinburgh
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396801" title="Click to go to the Author Index">
             Uchida, Hiroki
            </a>
           </td>
           <td class="r">
            Waseda University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#103221" title="Click to go to the Author Index">
             Vijayakumar, Sethu
            </a>
           </td>
           <td class="r">
            University of Edinburgh
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1839" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#representation_learning" title="Click to go to the Keyword Index">
               Representation Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#sensorimotor_learning" title="Click to go to the Keyword Index">
               Sensorimotor Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Recognising the characteristics of objects while a robot handles them is crucial for adjusting motions that ensure stable and efficient interactions with containers. Ahead of realising stable and efficient robot motions for handling/transferring the containers, this work aims to recognise the latent unobservable object characteristics. While vision is commonly used for object recognition by robots, it is ineffective for detecting hidden objects. However, recognising objects indirectly using other sensors is a challenging task. To address this challenge, we propose a cross-modal transfer learning approach from vision to haptic-audio. We initially train the model with vision, directly observing the target object. Subsequently, we transfer the latent space learned from vision to a second module, trained only with haptic-audio and motor data. This transfer learning framework facilitates the representation of object characteristics using indirect sensor data, thereby improving recognition accuracy. For evaluating the recognition accuracy of our proposed learning framework we selected shape, position, and orientation as the object characteristics. Finally, we demonstrate online recognition of both trained and untrained objects using the humanoid robot Nextage Open.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that14_02">
             10:15-10:30, Paper ThAT14.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('136'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Cross-Architecture Auxiliary Feature Space Translation for Efficient Few-Shot Personalized Object Detection
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#318135" title="Click to go to the Author Index">
             Barbato, Francesco
            </a>
           </td>
           <td class="r">
            University of Padova
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#318083" title="Click to go to the Author Index">
             Michieli, Umberto
            </a>
           </td>
           <td class="r">
            Samsung Research
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#391356" title="Click to go to the Author Index">
             Moon, Jijoong
            </a>
           </td>
           <td class="r">
            Samsung Research Korea
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#133184" title="Click to go to the Author Index">
             Zanuttigh, Pietro
            </a>
           </td>
           <td class="r">
            University of Padua
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#170089" title="Click to go to the Author Index">
             Ozay, Mete
            </a>
           </td>
           <td class="r">
            Samsung Research
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab136" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#transfer_learning" title="Click to go to the Keyword Index">
               Transfer Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#incremental_learning" title="Click to go to the Keyword Index">
               Incremental Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Recent years have seen object detection robotic systems deployed in several personal devices (e.g., home robots and appliances). This has highlighted a challenge in their design, i.e., they cannot efficiently update their knowledge to distinguish between general classes and user-specific instances (e.g., a dog vs. user's dog). We refer to this challenging task as Instance-level Personalized Object Detection (IPOD). The personalization task requires many samples for model tuning and optimization in a centralized server, raising privacy concerns. An alternative is provided by approaches based on recent large-scale Foundation Models, but their compute costs preclude on-device applications. In our work we tackle both problems at the same time, designing a Few-Shot IPOD strategy called AuXFT. We introduce a conditional coarse-to-fine few-shot learner to refine the coarse predictions made by an efficient object detector, showing that using an off-the-shelf model leads to poor personalization due to neural collapse. Therefore, we introduce a Translator block that generates an auxiliary feature space where features generated by a self-supervised model (e.g., DINOv2) are distilled without impacting the performance of the detector. We validate AuXFT on three publicly available datasets and one in-house benchmark designed for the IPOD task, achieving remarkable gains in all considered scenarios with excellent time-complexity trade-off: AuXFT reaches a performance of 80% its upper bound at just 32% of the inference time, 13% of VRAM and 19% of the model size.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that14_03">
             10:30-10:45, Paper ThAT14.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1219'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Sim2Real Transfer for Audio-Visual Navigation with Frequency-Adaptive Acoustic Field Prediction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#237583" title="Click to go to the Author Index">
             Chen, Changan
            </a>
           </td>
           <td class="r">
            UT Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395520" title="Click to go to the Author Index">
             Ramos Chen, Jordi
            </a>
           </td>
           <td class="r">
            The University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395973" title="Click to go to the Author Index">
             Tomar, Anshul
            </a>
           </td>
           <td class="r">
            University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#285477" title="Click to go to the Author Index">
             Grauman, Kristen
            </a>
           </td>
           <td class="r">
            UT Austin and Facebook AI Research
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1219" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#transfer_learning" title="Click to go to the Keyword Index">
               Transfer Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#robot_audition" title="Click to go to the Keyword Index">
               Robot Audition
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#audio_visual_slam" title="Click to go to the Keyword Index">
               Audio-Visual SLAM
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Sim2real transfer has received increasing attention lately due to its success in transferring robotic policies learned in simulation to the real world. While significant progress has been made in transferring vision-based navigation policies, the current sim2real strategy for audio-visual navigation remains limited to basic data augmentation. Sound differs from light in that it spans across much wider frequencies and thus requires a different solution for sim2real. To understand how the acoustic sim2real gap varies with frequencies, we first define a novel acoustic field prediction (AFP) task that predicts the local sound pressure field. We then train frequency-specific AFP models in simulation and measure the prediction errors on collected real data. We propose a frequency-adaptive strategy that intelligently selects the best frequency band for prediction based on both the measured prior and the energy distribution of the received audio, which improves the generalization on real data. Coupled with waypoint navigation, we show the navigation policy not only improves navigation performance in simulation but also transfers successfully to real robots. This work demonstrates the potential of building autonomous agents that can see, hear, and act entirely from simulation, and transferring them to the real world.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that14_04">
             10:45-11:00, Paper ThAT14.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2998'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Skill Transfer and Discovery for Sim-To-Real Learning: A Representation-Based Viewpoint
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#297222" title="Click to go to the Author Index">
             Ma, Haitong
            </a>
           </td>
           <td class="r">
            Harvard University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398824" title="Click to go to the Author Index">
             Ren, Zhaolin
            </a>
           </td>
           <td class="r">
            Harvard University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#346312" title="Click to go to the Author Index">
             Dai, Bo
            </a>
           </td>
           <td class="r">
            Google Brain
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#269237" title="Click to go to the Author Index">
             Li, Na
            </a>
           </td>
           <td class="r">
            Harvard University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2998" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#transfer_learning" title="Click to go to the Keyword Index">
               Transfer Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#machine_learning_for_robot_control" title="Click to go to the Keyword Index">
               Machine Learning for Robot Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We study sim-to-real skill transfer and discovery in the context of robotics control using representation learning. We draw inspiration from spectral decomposition of Markov decision processes. The spectral decomposition brings about representation that can linearly represent the state-action value function induced by any policies, thus can be regarded as skills. The skill representations are transferable across arbitrary tasks with the same transition dynamics. Moreover, to handle the sim-to-real gap in the dynamics, we propose a skill discovery algorithm that learns new skills caused by the sim-to-real gap from real-world data. We promote the discovery of new skills by enforcing orthogonal constraints between the skills to learn and the skills from simulators, and then synthesize the policy using the enlarged skill sets. We demonstrate our methodology by transferring quadrotor controllers from simulators to Crazyflie 2.1 quadrotors. We show that we can learn the skill representations from a single simulator task and transfer these to multiple different real-world tasks including hovering, taking off, landing and trajectory tracking. Our skill discovery approach helps narrow the sim-to-real gap and improve the real-world controller performance by up to 30.2%.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt1">
             <b>
              ThBT1
             </b>
            </a>
           </td>
           <td class="r">
            Room 1
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thbt1" title="Click to go to the Program at a Glance">
             <b>
              SLAM II
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#185927" title="Click to go to the Author Index">
             Yuan, Shenghai
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt1_01">
             11:00-11:15, Paper ThBT1.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3645'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Tracking by Detection: Robust Indoor RGB-D Odometry Leveraging Key Local Manhattan World
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#378929" title="Click to go to the Author Index">
             Zhou, Zhiyu
            </a>
           </td>
           <td class="r">
            Wuhan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#193655" title="Click to go to the Author Index">
             Gao, Zhi
            </a>
           </td>
           <td class="r">
            Temasek Laboratories @ NUS
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#385443" title="Click to go to the Author Index">
             Xu, Jingzhong
            </a>
           </td>
           <td class="r">
            School of Remote Sensing and Information Engineering, Wuhan Univ
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3645" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In indoor scenes, pose estimation drift in SLAM systems is challenging to mitigate due to factors like low texture and texture repetition. Several studies utilize Manhattan structural information to achieve low-drift rotation estimation consequently reducing the cumulative error. However, the application scenarios of these methods are limited by the Manhattan assumption. To address this problem, we propose a robust RGBD odometry for tracking in more general structured scenes, which can represent a wider range of scenes by leveraging Atlanta World assumption, consisting of a vertical direction and multiple horizontal directions. To this end, we design tracking by detection algorithms for extracting Key Local Manhattan World in each frame, which is defined as the most stable structural information in current frame. Specifically, we detect local Manhattan Worlds in current frame, if a more stable structural feature emerges, it becomes the new tracking target. The angle between these Manhattan Worlds are then computed, constructing the Atlanta World. For accurate pose estimation, we use Key Local Manhattan World to first estimate low-drift rotation followed by estimating translation using point-line structural features. Extensive experiments on public benchmark and self-recorded datasets show that our method outperforms existing state-of-the-art methods with a significant margin of 22% while extending their applicability to the Atlanta World.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt1_02">
             11:15-11:30, Paper ThBT1.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3726'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              RSS: Robust Stereo SLAM with Novel Extraction and Full Exploitation of Plane Features
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#385263" title="Click to go to the Author Index">
             Wang, Haolin
            </a>
           </td>
           <td class="r">
            Institute of Automation, Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#231587" title="Click to go to the Author Index">
             Wei, Hao
            </a>
           </td>
           <td class="r">
            University of Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#304558" title="Click to go to the Author Index">
             Xu, Zewen
            </a>
           </td>
           <td class="r">
            Institute of Automation, Chinese Academy of Science
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#385264" title="Click to go to the Author Index">
             Lv, Zeren
            </a>
           </td>
           <td class="r">
            Beijing University of Chemical Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#266292" title="Click to go to the Author Index">
             Zhang, Pengju
            </a>
           </td>
           <td class="r">
            University of Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#385087" title="Click to go to the Author Index">
             An, Ning
            </a>
           </td>
           <td class="r">
            China Coal Research Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#236315" title="Click to go to the Author Index">
             Tang, Fulin
            </a>
           </td>
           <td class="r">
            Institute of Automation, Chinese Academy of Sciences, University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#120553" title="Click to go to the Author Index">
             Wu, Yihong
            </a>
           </td>
           <td class="r">
            National Laboratory of Pattern Recognition, InstituteofAutomatio
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3726" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Planar structures, prevalent in man-made environments, can be observed by a camera for significant periods of time due to their large spatial presence. These structures provide strong planar regularities for Simultaneous Localization and Mapping (SLAM) systems, facilitating long-term navigation. Therefore, we propose a novel point-plane-based stereo SLAM system, fully regularized by plane features within a unified non-linear optimization framework. The core of our method is an
             <p>
              accurate and efficient stereo plane extraction algorithm with strict 2D
              <p>
               and 3D outlier rejection mechanisms, effectively extracting main planes
               <p>
                from robust stereo correspondences and enabling real-time point-plane association. Furthermore, we introduce a novel optimization formulation, incorporating geometric feature (point and plane) and across-feature (point-on-plane) constraints that promote each other through the mutual constraints between associated point and plane features, which fully exploits plane constraints to improve the SLAM system performance. The proposed plane extraction algorithm is evaluated on the EuRoC MAV dataset, achieving significant improvements in number, accuracy, reliability, and efficiency over the state-of-the-art (SOTA) stereo point-plane-based system. The results of
                <p>
                 an ablation study on two public datasets show that the proposed SLAM system outperforms the SOTA stereo point-plane-based system in both accuracy and robustness, and further demonstrate the mutual enhancement
                 <p>
                  between the two types of constraints.
                 </p>
                </p>
               </p>
              </p>
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt1_03">
             11:30-11:45, Paper ThBT1.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             SemanticTopoLoop: Semantic Loop Closure with 3D Topological Graph Based on Quadric-Level Object Map
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#309611" title="Click to go to the Author Index">
             Cao, Zhenzhong
            </a>
           </td>
           <td class="r">
            Nankai University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#280381" title="Click to go to the Author Index">
             Zhang, Qianyi
            </a>
           </td>
           <td class="r">
            Nankai University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#389799" title="Click to go to the Author Index">
             Guang, Jinzheng
            </a>
           </td>
           <td class="r">
            Nankai University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#318111" title="Click to go to the Author Index">
             Wu, Shichao
            </a>
           </td>
           <td class="r">
            Nankai University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#279727" title="Click to go to the Author Index">
             Hu, Zhengxi
            </a>
           </td>
           <td class="r">
            Nankai University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#106992" title="Click to go to the Author Index">
             Liu, Jingtai
            </a>
           </td>
           <td class="r">
            Nankai University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt1_04">
             11:45-12:00, Paper ThBT1.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3732'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SR-LIVO: LiDAR-Inertial-Visual Odometry and Mapping with Sweep Reconstruction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#307947" title="Click to go to the Author Index">
             Yuan, Zikang
            </a>
           </td>
           <td class="r">
            Huazhong University, Wuhan, 430073, China
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#385637" title="Click to go to the Author Index">
             Deng, Jie
            </a>
           </td>
           <td class="r">
            Huazhong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#385638" title="Click to go to the Author Index">
             Ming, Ruiye
            </a>
           </td>
           <td class="r">
            Huazhong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#344165" title="Click to go to the Author Index">
             Lang, Fengtian
            </a>
           </td>
           <td class="r">
            Huazhong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#201893" title="Click to go to the Author Index">
             Yang, Xin
            </a>
           </td>
           <td class="r">
            Huazhong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3732" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#sensor_fusion" title="Click to go to the Keyword Index">
               Sensor Fusion
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Existing LiDAR-inertial-visual odometry and mapping (LIV-OAM) systems mainly utilize the LiDAR-inertial odometry (LIO) module for structure reconstruction and the LiDARassisted visual-inertial odometry (VIO) module for color rendering. However, the performance of existing LiDAR-assisted VIO module doesnt match the accuracy delivered by LIO systems in the scenarios containing rich textures and geometric structures (i.e., without failure mode for both camera and LiDAR). This letter introduces SR-LIVO, an advanced and novel LIV-OAM system employing sweep reconstruction to align reconstructed sweeps with image timestamps. This allows the LIO module to accurately determine states at all imaging moments, enhancing pose accuracy and processing efficiency. Experimental results on two public datasets demonstrate that: 1) our SR-LIVO outperforms the existing stateof-the-art LIV-OAM systems in both pose accuracy, rendering performance and runtime efficiency; 2) In scenarios with rich textures and geometric structures, the LIO framework can provide more accurate pose than existing LiDAR-assisted VIO framework, and thus helps rendering. We have released our source code to contribute to the community development in this field.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt2">
             <b>
              ThBT2
             </b>
            </a>
           </td>
           <td class="r">
            Room 2
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thbt2" title="Click to go to the Program at a Glance">
             <b>
              Marine Robotics III
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt2_01">
             11:00-11:15, Paper ThBT2.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1769'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Dynamic SpectraFormer for Ultra-High Resolution Underwater Image Enhancement
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#355651" title="Click to go to the Author Index">
             Hu, Zhiqiang
            </a>
           </td>
           <td class="r">
            Tokyo University of Science
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#355653" title="Click to go to the Author Index">
             Yu, Tao
            </a>
           </td>
           <td class="r">
            Tokyo Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#154213" title="Click to go to the Author Index">
             Huang, Shouren
            </a>
           </td>
           <td class="r">
            Tokyo University of Science
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#101392" title="Click to go to the Author Index">
             Ishikawa, Masatoshi
            </a>
           </td>
           <td class="r">
            University of Tokyo
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1769" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#marine_robotics" title="Click to go to the Keyword Index">
               Marine Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#ai_based_methods" title="Click to go to the Keyword Index">
               AI-Based Methods
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_transportation" title="Click to go to the Keyword Index">
               Computer Vision for Transportation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Underwater images suffer from color distortion, haze, and poor visibility due to light refraction and absorption in water. These challenges significantly impact the utilization of Autonomous Underwater Vehicles (AUVs) or marine robots. Typically, color and brightness distortions manifest at lower frequencies, while edge and texture distortions are prevalent at higher frequencies. Traditional methods struggle to con- currently rectify these mixed distortions as they primarily concentrate on the spatial domain. To address these issues, we introduce the Dynamic SpectraFormer, which enhances underwater image through a frequency domain transformer. The Dynamic SpectraFormer introduces a ultra-high resolution sparse spectrum attention module, which could capture the long-term dependency without losing the universal approx- imating power. Additionally, we have developed a dynamic spectrum weight generation layer that serves as an adaptive spectrum band selector, accentuating critical frequency bands and suppressing less relevant ones. Consequently, this method significantly improves underwater image quality by address- ing both high- and low-frequency distortions. Our extensive ablation studies and comparative evaluations consolidate the Dynamic SpectraFormers efficacy across multiple underwater image enhancement benchmarks.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt2_02">
             11:15-11:30, Paper ThBT2.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1965'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              QO-Net: Query Optimization Underwater Object Detection Network
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#331385" title="Click to go to the Author Index">
             Tian, Jiandong
            </a>
           </td>
           <td class="r">
            Chinese Academy of Science
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#380262" title="Click to go to the Author Index">
             Sun, Hongyang
            </a>
           </td>
           <td class="r">
            Nanjing University of Posts and Telecommunications
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#163781" title="Click to go to the Author Index">
             Fan, Baojie
            </a>
           </td>
           <td class="r">
            Nanjing University of Posts and Telecommunications
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397601" title="Click to go to the Author Index">
             Xu, Hongxin
            </a>
           </td>
           <td class="r">
            Delft University of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1965" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#marine_robotics" title="Click to go to the Keyword Index">
               Marine Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
              QO-Net UODDQO-Net  Conv-Trans Layer  CNN  Transformer  QO-Net QO-Net UODD  20000  UODDBrackish  TrashCan QO-Net 
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt2_03">
             11:30-11:45, Paper ThBT2.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2948'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Adaptive Multi-Altitude Search and Sampling of Sparsely Distributed Natural Phenomena
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#280124" title="Click to go to the Author Index">
             Todd, Jessica
            </a>
           </td>
           <td class="r">
            MIT
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#178169" title="Click to go to the Author Index">
             McCammon, Seth
            </a>
           </td>
           <td class="r">
            Woods Hole Oceanographic Institution
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#118013" title="Click to go to the Author Index">
             Girdhar, Yogesh
            </a>
           </td>
           <td class="r">
            Woods Hole Oceanographic Institution
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#101682" title="Click to go to the Author Index">
             Roy, Nicholas
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#110914" title="Click to go to the Author Index">
             Yoerger, Dana
            </a>
           </td>
           <td class="r">
            Woods Hole Oceanographic Institution
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2948" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#marine_robotics" title="Click to go to the Keyword Index">
               Marine Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#environment_monitoring_and_management" title="Click to go to the Keyword Index">
               Environment Monitoring and Management
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#reactive_and_sensor_based_planning" title="Click to go to the Keyword Index">
               Reactive and Sensor-Based Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper, we propose a novel method for autonomously seeking out sparsely distributed targets in an unknown underwater environment. Our Sparse Adaptive Search and Sample (SASS) algorithm mixes low-altitude observations of discrete targets with high-altitude observations of the surrounding substrates. By using prior information about the distribution of targets across substrate types in combination with belief modelling over these substrates in the environment, high-altitude observations provide information that allows SASS to quickly guide the robot to areas with high target densities. A maximally informative path is autonomously constructed online using Monte Carlo Tree Search with a novel acquisition function to guide the search to maximise observations of unique targets. We demonstrate our approach in a set of simulated trials using a novel generative species model. SASS consistently outperforms the canonical boustrophedon planner by up to 36% in seeking out unique targets in the first 75 - 90% of time it takes for a boustrophedon survey. Additionally, we verify the performance of SASS on two real world coral reef datasets.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt2_04">
             11:45-12:00, Paper ThBT2.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3206'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Underwater Hyperspectral Imaging for Measuring Seafloor Reflectance
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398963" title="Click to go to the Author Index">
             Zhang, Hongjie
            </a>
           </td>
           <td class="r">
            The University of Sydney
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#238129" title="Click to go to the Author Index">
             Billings, Gideon
            </a>
           </td>
           <td class="r">
            University of Sydney, Australian Center for Field Robotics
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#266508" title="Click to go to the Author Index">
             Shields, Jackson
            </a>
           </td>
           <td class="r">
            University of Sydney
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#106827" title="Click to go to the Author Index">
             Williams, Stefan Bernard
            </a>
           </td>
           <td class="r">
            University of Sydney
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3206" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#marine_robotics" title="Click to go to the Keyword Index">
               Marine Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#environment_monitoring_and_management" title="Click to go to the Keyword Index">
               Environment Monitoring and Management
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             A known challenge for computer vision methods applied to the underwater domain is that nonlinear attenuation of light in underwater environments distorts the color signal in captured imagery, resulting in inconsistent color and contrast at varying distances to an imaged target. While surface reflectance can provide a useful cue for classifying imagery of the seafloor by object or substrate types, color inconsistency makes robust classification challenging. We introduce a method that leverages hyperspectral imagery with an underwater light formation model and structure from motion to estimate the intrinsic optical properties of the underwater environment and correct seafloor reflectance estimates from radiance measurements. We show that our method enables consistent surface reflectance estimates under both artificial and ambient lighting conditions and is readily integrated on small underwater vehicle platforms, such as a BlueROV.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt3">
             <b>
              ThBT3
             </b>
            </a>
           </td>
           <td class="r">
            Room 3
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thbt3" title="Click to go to the Program at a Glance">
             <b>
              Grippers and Other End-Effectors
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#103658" title="Click to go to the Author Index">
             Watanabe, Tetsuyou
            </a>
           </td>
           <td class="r">
            Kanazawa University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt3_01">
             11:00-11:15, Paper ThBT3.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('50'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Contact Representation in Robotic Mechanical Systems Employing Reduced Models
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#222875" title="Click to go to the Author Index">
             Raoofian, Ali
            </a>
           </td>
           <td class="r">
            McGill University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#363630" title="Click to go to the Author Index">
             Dai, Xu
            </a>
           </td>
           <td class="r">
            McGill University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#115939" title="Click to go to the Author Index">
             Kovecses, Jozsef
            </a>
           </td>
           <td class="r">
            McGill University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab50" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#contact_modeling" title="Click to go to the Keyword Index">
               Contact Modeling
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#dynamics" title="Click to go to the Keyword Index">
               Dynamics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#grippers_and_other_end_effectors" title="Click to go to the Keyword Index">
               Grippers and Other End-Effectors
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Contact interactions play a major role in the dynamic analysis of robotic arms, where they can be represented as unilateral constraints. However, incorporating these contacts into the system dynamic model is a challenging task, given the numerous ways to account for them. This paper presents and compares two different approaches to contact modelling, highlighting how adopting a different perspective can avoid constraint redundancies and indeterminate problems. To this end, a co-simulation setup is employed as the primary framework to address the differences in the contact modelling approaches and the corresponding formulations. In a co-simulation setup, a system is divided into subsystems that exchange information at pre-determined communication points through the interface. Between the communication time points, the subsystems are integrated independently while they require updated interface variables from other subsystems. Hence, it is necessary to approximate these variables. In a model-based approximation, a reduced model of the subsystem emulates its dynamic behaviour at the interface. This paper addresses challenges in developing a representative reduced order model for a mechanical subsystem with contacts and proposes solutions to incorporate changes in contact states in the reduced model. It will be shown how basic assumptions in the contact dynamic incorporation can influence the simulation outcome. To demonstrate the proposed solution, a robotic arm model and its operations are used as a case study.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt3_02">
             11:15-11:30, Paper ThBT3.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('126'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              G.O.G: A Versatile Gripper-On-Gripper Design for Bimanual Cloth Manipulation with a Single Robotic Arm
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#332520" title="Click to go to the Author Index">
             Lee, Dongmyoung
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#332538" title="Click to go to the Author Index">
             Chen, Wei
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#359996" title="Click to go to the Author Index">
             Chen, Xiaoshuai
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#138488" title="Click to go to the Author Index">
             Rojas, Nicolas
            </a>
           </td>
           <td class="r">
            The AI Institute
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab126" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#grippers_and_other_end_effectors" title="Click to go to the Keyword Index">
               Grippers and Other End-Effectors
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#grasping" title="Click to go to the Keyword Index">
               Grasping
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#service_robotics" title="Click to go to the Keyword Index">
               Service Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The manipulation of garments poses research challenges due to their deformable nature and the extensive variability in shapes and sizes. Despite numerous attempts by researchers to address these via approaches involving robot perception and control, there has been a relatively limited interest in resolving it through the co-development of robot hardware. Consequently, the majority of studies employ off-the-shelf grippers in conjunction with dual robot arms to enable bimanual manipulation and high dexterity. However, this dual-arm system increases the overall cost of the robotic system as well as its control complexity in order to tackle robot collisions and other robot coordination issues. As an alternative approach, we propose to enable bimanual cloth manipulation using a single robot arm via novel end effector design---sharing dexterity skills between manipulator and gripper rather than relying entirely on robot arm coordination. To this end, we introduce a new gripper, called G.O.G., based on a gripper-on-gripper structure where the first gripper independently regulates the span, up to 500mm, between its fingers which are in turn also grippers. These finger grippers consist of a variable friction module that enables two grasping modes: firm and sliding grasps. Household item and cloth object benchmarks are employed to evaluate the performance of the proposed design, encompassing both experiments on the gripper design itself and on cloth manipulation. Experimental results demonstrate the potential of the introduced ideas to undertake a range of bimanual cloth manipulation tasks with a single robot arm.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt3_03">
             11:30-11:45, Paper ThBT3.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('861'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Single-Motor Robotic Gripper with Multi-Surface Fingers for Variable Grasping Configurations
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#190314" title="Click to go to the Author Index">
             Nishimura, Toshihiro
            </a>
           </td>
           <td class="r">
            Kanazawa University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#109044" title="Click to go to the Author Index">
             Suzuki, Yosuke
            </a>
           </td>
           <td class="r">
            Kanazawa University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#110120" title="Click to go to the Author Index">
             Tsuji, Tokuo
            </a>
           </td>
           <td class="r">
            Kanazawa University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#103658" title="Click to go to the Author Index">
             Watanabe, Tetsuyou
            </a>
           </td>
           <td class="r">
            Kanazawa University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab861" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#grippers_and_other_end_effectors" title="Click to go to the Keyword Index">
               Grippers and Other End-Effectors
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mechanism_design" title="Click to go to the Keyword Index">
               Mechanism Design
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#grasping" title="Click to go to the Keyword Index">
               Grasping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This study proposes a novel robotic gripper with variable grasping configurations for grasping various objects. The fingers of the developed gripper incorporate multiple different surfaces. The gripper possesses the function of altering the finger surfaces facing a target object by rotating the fingers in its longitudinal direction. In the proposed design equipped with two fingers, the two fingers incorporate three and four surfaces, respectively, resulting in the nine available grasping configurations by the combination of these finger surfaces. The developed gripper is equipped with the functions of opening/closing its fingers for grasping and rotating its fingers to alter the grasping configuration- all achieved with a single motor. To enable the two motions using a single motor, this study introduces a self-motion switching mechanism utilizing magnets. This mechanism automatically transitions between gripper motions based on the direction of the motor rotation when the gripper is fully opened. In this state, rotating the motor towards closing initiates the finger closing action, while further opening the fingers from the fully opened state activates the finger rotation. This paper presents the gripper design, the mechanics of the self-motion switching mechanism, the control method, and the grasping configuration selection strategy. The performance of the gripper is experimentally demonstrated.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt4">
             <b>
              ThBT4
             </b>
            </a>
           </td>
           <td class="r">
            Room 4
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thbt4" title="Click to go to the Program at a Glance">
             <b>
              Flexible Robots
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#180462" title="Click to go to the Author Index">
             Kuntz, Alan
            </a>
           </td>
           <td class="r">
            University of Utah
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt4_01">
             11:00-11:15, Paper ThBT4.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2325'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Dynamics-Based Trajectory Planning for Vibration Suppression of a Flexible Long-Reach Robotic Manipulator System
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398100" title="Click to go to the Author Index">
             Chen, Anthony Siming
            </a>
           </td>
           <td class="r">
            The University of Manchester
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#223593" title="Click to go to the Author Index">
             Lopez Pulgarin, Erwin Jose
            </a>
           </td>
           <td class="r">
            University of Bristol
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#134556" title="Click to go to the Author Index">
             Herrmann, Guido
            </a>
           </td>
           <td class="r">
            University of Manchester
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#156371" title="Click to go to the Author Index">
             Lanzon, Alexander
            </a>
           </td>
           <td class="r">
            The University of Manchester
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#198814" title="Click to go to the Author Index">
             Carrasco, Joaquin
            </a>
           </td>
           <td class="r">
            The University of Manchester
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#190398" title="Click to go to the Author Index">
             Lennox, Barry
            </a>
           </td>
           <td class="r">
            The University of Manchester
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398128" title="Click to go to the Author Index">
             Carrera-Knowles, Benji
            </a>
           </td>
           <td class="r">
            Jacobs
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398129" title="Click to go to the Author Index">
             Brotherhood, John
            </a>
           </td>
           <td class="r">
            Jacobs
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#209459" title="Click to go to the Author Index">
             Sakaue, Tomoki
            </a>
           </td>
           <td class="r">
            Tokyo Electric Power Company Holdings, Inc
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#361932" title="Click to go to the Author Index">
             Kaiqiang, Zhang
            </a>
           </td>
           <td class="r">
            UK Atomic Energy Authority
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2325" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#flexible_robotics" title="Click to go to the Keyword Index">
               Flexible Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We address the unique challenge of vibration suppression for a flexible long-reach robotic manipulator system, namely, the through-wall deployment (TWD) system that is used in nuclear environments. This paper proposes a novel dynamics-based trajectory optimization approach, which minimizes both the acceleration and the jerk at the manipulator's joints, as well as the vibrations of the flexible long-reach boom where the manipulator's base is mounted. Firstly, we create an integrated model for the system dynamics based on the knowledge of the robotic manipulator and the acceleration data from the vibration tests. We then develop an original procedure for generating the high-order polynomial trajectory that guarantees the zero-boundary condition for a flexible number of optimization parameters and waypoints. Following the simulation of a multi-objective optimization scheme, the optimized trajectory is experimentally validated on the practical TWD system with around 28% vibration reduction on average compared to the benchmark. Importantly, this reduction is achieved without compromising on the average speed of motion. The methodology is transferable to a wider range of flexible robotic manipulator systems with similar characteristics.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt4_02">
             11:15-11:30, Paper ThBT4.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2384'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Strong Compliant Grasps Using a Cable-Driven Soft Gripper
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#341605" title="Click to go to the Author Index">
             Xie, Gregory
            </a>
           </td>
           <td class="r">
            MIT
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#221352" title="Click to go to the Author Index">
             Chin, Lillian
            </a>
           </td>
           <td class="r">
            UT Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#229363" title="Click to go to the Author Index">
             Kim, Byungchul
            </a>
           </td>
           <td class="r">
            MIT
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#173337" title="Click to go to the Author Index">
             Holladay, Rachel
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#101388" title="Click to go to the Author Index">
             Rus, Daniela
            </a>
           </td>
           <td class="r">
            MIT
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2384" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#grippers_and_other_end_effectors" title="Click to go to the Keyword Index">
               Grippers and Other End-Effectors
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#soft_robot_materials_and_design" title="Click to go to the Keyword Index">
               Soft Robot Materials and Design
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The natural flexibility of soft robotic grippers allows for versatile and compliant grasping. However, this same flexibility can restrict the grippers strength. Striking a balance between compliance and strength is essential for effective soft grippers. In this work, we present Flexible Robust Observant Gripper (FROG), a soft gripper that is both compliant and strong. We describe the mechanical design of the gripper, characterize the soft flexures used in the design, and analyze the grasp forces generated by the gripper. Utilizing the structure of the gripper, we develop feedforward grasp controllers and a classifier to distinguish between grasp types. Grasping experiments show that FROG can effectively grasp a variety of objects, including very soft or delicate items. Holding force tests show that our gripper can conform to the grasped object and exert large grasp forces.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt4_03">
             11:30-11:45, Paper ThBT4.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1978'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Identification of Flexible Joint Robot Inertia Matrix Using Frequency Response Analysis
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#347993" title="Click to go to the Author Index">
             Choi, Kiyoung
            </a>
           </td>
           <td class="r">
            Deagu Gyeongbuk Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#378571" title="Click to go to the Author Index">
             Song, JunHo
            </a>
           </td>
           <td class="r">
            Daegu Gyeongbuk Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#216543" title="Click to go to the Author Index">
             Yun, WonBum
            </a>
           </td>
           <td class="r">
            Daegu Gyeongbuk Institute of Science and Technology (DGIST)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#291840" title="Click to go to the Author Index">
             Lee, Deokjin
            </a>
           </td>
           <td class="r">
            Daegu Gyeongbuk Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#129982" title="Click to go to the Author Index">
             Oh, Sehoon
            </a>
           </td>
           <td class="r">
            DGIST
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1978" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#compliant_joints_and_mechanisms" title="Click to go to the Keyword Index">
               Compliant Joints and Mechanisms
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#dynamics" title="Click to go to the Keyword Index">
               Dynamics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a novel, nonlinearity robust identification method for deriving the inertia matrix of multi-DOF Flexible Joint Robots (FJR), utilizing resonance and anti-resonance frequencies in the Frequency Response Functions (FRF). Our proposed method overcomes the limitations of conventional approaches, which are susceptible to mechanical nonlinearities, leading to inaccurate models. By leveraging frequency domain techniques, our approach effectively mitigates the influence of nonlinear characteristics, providing a more accurate and reliable means of robot control. Furturmore, the paper highlights the benefits of frequency domain system identification, including nonlinear robustness and the ability to decompose the flexible joint into motor and load components. Finally, a novel sequential excitation algorithm was proposed to obtain the inertia matrix of a multi-DOF robot manipulator without relying on complex theories or optimization. The effectiveness of the proposed algorithm was verified through simulation and experiment.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt4_04">
             11:45-12:00, Paper ThBT4.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2878'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Exploring Modal Switch in Metamaterial-Based Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398252" title="Click to go to the Author Index">
             Jordan, Britton
            </a>
           </td>
           <td class="r">
            University of Utah
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#343713" title="Click to go to the Author Index">
             Esser, Daniel
            </a>
           </td>
           <td class="r">
            Vanderbilt University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398261" title="Click to go to the Author Index">
             Kim, Jeonghyeon
            </a>
           </td>
           <td class="r">
            Sogang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#236654" title="Click to go to the Author Index">
             Cho, Brian Y
            </a>
           </td>
           <td class="r">
            University of Utah
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#101891" title="Click to go to the Author Index">
             Webster III, Robert James
            </a>
           </td>
           <td class="r">
            Vanderbilt University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#180462" title="Click to go to the Author Index">
             Kuntz, Alan
            </a>
           </td>
           <td class="r">
            University of Utah
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2878" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#compliant_joints_and_mechanisms" title="Click to go to the Keyword Index">
               Compliant Joints and Mechanisms
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#soft_robot_materials_and_design" title="Click to go to the Keyword Index">
               Soft Robot Materials and Design
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Mechanical metamaterials are microscale patterned structures that are designed to have specific mechanical properties at a macro-scale that are atypical of natural materials. Robotic manipulators composed of these materials can exhibit deformation and motion capabilities that can be customized and easily fabricated. However, as of now, the motion capability of such manipulators are encoded in their physical composition and cannot be changed. This paper presents multi-modal metamaterial-based robot prototypes which can switch between the behaviors found in two different metamaterials. Two such robots are explored, a bending/shearing robot and a bending/twisting robot. The robot design is described in detail, including how the robots toggle between behavior modes via mechanical actuation of a sliding rod insert. Multi-modal robots are compared to their single-mode equivalents to characterize their capabilities. The single-mode behaviors are largely preserved in the multi-modal innovations. The multi-modal prototypes also demonstrate variable rigidity. We discuss the feasibility of using robots of this design as part of a robotic surgical system.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt5">
             <b>
              ThBT5
             </b>
            </a>
           </td>
           <td class="r">
            Room 5
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thbt5" title="Click to go to the Program at a Glance">
             <b>
              Robot Estimation
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#129946" title="Click to go to the Author Index">
             Monje, Concepcin A.
            </a>
           </td>
           <td class="r">
            University Carlos III of Madrid
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#179240" title="Click to go to the Author Index">
             Della Santina, Cosimo
            </a>
           </td>
           <td class="r">
            TU Delft
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt5_01">
             11:00-11:15, Paper ThBT5.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('822'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Efficient Estimation of Frequency Response Functions of Industrial Robots Using the Local Rational Method
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#255382" title="Click to go to the Author Index">
             Zimmermann, Stefanie Antonia
            </a>
           </td>
           <td class="r">
            Linkping University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#105104" title="Click to go to the Author Index">
             Moberg, Stig
            </a>
           </td>
           <td class="r">
            ABB AB
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab822" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#calibration_and_identification" title="Click to go to the Keyword Index">
               Calibration and Identification
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#industrial_robots" title="Click to go to the Keyword Index">
               Industrial Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#dynamics" title="Click to go to the Keyword Index">
               Dynamics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Nonparametric estimates of frequency response functions (FRFs) are often suitable for describing the dynamics of a mechanical system. If treating these estimates as measurements, they can be used for parametric identification of, e.g., a gray-box model. This paper shows that a more accurate parametric model can be identified based on local parametric FRF estimates, giving a shorter total experiment time, compared to classical methods. Classical methods for nonparametric FRF estimation of MIMO (Multiple Input Multiple Output) systems require at least as many experiments as the system has inputs. Local parametric FRF estimation methods have been developed for avoiding multiple experiments. In this paper, these local methods are adapted and applied for estimating the FRFs of a 6-axes robotic manipulator, which is a nonlinear MIMO system operating in closed loop. The aim is to reduce the experiment time and amount of data needed for identification. The resulting FRFs are analyzed in an experimental study and compared to estimates obtained by classical MIMO techniques.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt5_02">
             11:15-11:30, Paper ThBT5.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3452'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Robot Kinematics Model Estimation Using Inertial Sensors for On-Site Building Robotics
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#277431" title="Click to go to the Author Index">
             Sato, Hiroya
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#224035" title="Click to go to the Author Index">
             Makabe, Tasuku
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#202369" title="Click to go to the Author Index">
             Yanokura, Iori
            </a>
           </td>
           <td class="r">
            University of Tokyo
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#219727" title="Click to go to the Author Index">
             Yamaguchi, Naoya
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#106350" title="Click to go to the Author Index">
             Okada, Kei
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#106348" title="Click to go to the Author Index">
             Inaba, Masayuki
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3452" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#calibration_and_identification" title="Click to go to the Keyword Index">
               Calibration and Identification
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#kinematics" title="Click to go to the Keyword Index">
               Kinematics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#model_learning_for_control" title="Click to go to the Keyword Index">
               Model Learning for Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In order to make robots more useful in a variety of environments, they need to be highly portable so that they can be transported to wherever they are needed, and highly storable so that they can be stored when not in use. We propose ``on-site robotics'', which uses parts procured at the location where the robot will be active, and propose a new solution to the problem of portability and storability. In this paper, as a proof of concept for on-site robotics, we describe a method for estimating the kinematic model of a robot by using inertial measurement units (IMU) sensor module on rigid links, estimating the relative orientation between modules from angular velocity, and estimating the relative position from the measurement of centrifugal force.
             <p>
              At the end of this paper, as an evaluation for this method, we present an experiment in which a robot made up of wooden sticks reaches a target position. In this experiment, even if the combination of the links is changed, the robot is able to reach the target position again immediately after estimation, showing that it can operate even after being reassembled. Our implementation is available on url{https://github.com/hiroya1224/urdf_estimation_with_imus}.
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt5_03">
             11:30-11:45, Paper ThBT5.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3175'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Look Gauss, No Pose: Novel View Synthesis Using Gaussian Splatting without Accurate Pose Initialization
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398904" title="Click to go to the Author Index">
             Schmidt, Christian
            </a>
           </td>
           <td class="r">
            RWTH Aachen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398913" title="Click to go to the Author Index">
             Piekenbrinck, Jens
            </a>
           </td>
           <td class="r">
            RWTH Aachen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#115270" title="Click to go to the Author Index">
             Leibe, Bastian
            </a>
           </td>
           <td class="r">
            RWTH Aachen University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3175" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#computational_geometry" title="Click to go to the Keyword Index">
               Computational Geometry
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             3D Gaussian Splatting has recently emerged as a powerful tool for fast and accurate novel-view synthesis from a set of posed input images. However, like most novel-view synthesis approaches, it relies on accurate camera pose information, limiting its applicability in real-world scenarios where acquiring accurate camera poses can be challenging or even impossible. We propose an extension to the 3D Gaussian Splatting framework by optimizing the extrinsic camera parameters with respect to photometric residuals. We derive the analytical gradients and integrate their computation with the existing high-performance CUDA implementation. This enables downstream tasks such as 6-DoF camera pose estimation as well as joint reconstruction and camera refinement. In particular, we achieve rapid convergence and high accuracy for pose estimation on real-world scenes. Our method enables fast reconstruction of 3D scenes without requiring accurate pose information by jointly optimizing geometry and camera poses, while achieving state-of-the-art results in novel-view synthesis. Our approach is considerably faster to optimize than most competing methods, and several times faster in rendering. We show results on real-world scenes and complex trajectories through simulated environments, achieving state-of-the-art results on LLFF while reducing runtime by two to four times compared to the most efficient competing method.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt5_04">
             11:45-12:00, Paper ThBT5.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2025'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              State Estimation of an Adaptive 3-Finger Gripper Using Recurrent Neural Networks
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#226196" title="Click to go to the Author Index">
             Jonetzko, Yannick
            </a>
           </td>
           <td class="r">
            TAMS / University of Hamburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#349595" title="Click to go to the Author Index">
             Na, Theresa Alexandra Aurelia
            </a>
           </td>
           <td class="r">
            University of Hamburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#256857" title="Click to go to the Author Index">
             Fiedler, Niklas
            </a>
           </td>
           <td class="r">
            University of Hamburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#106951" title="Click to go to the Author Index">
             Zhang, Jianwei
            </a>
           </td>
           <td class="r">
            University of Hamburg
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2025" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#underactuated_robots" title="Click to go to the Keyword Index">
               Underactuated Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#grasping" title="Click to go to the Keyword Index">
               Grasping
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#model_learning_for_control" title="Click to go to the Keyword Index">
               Model Learning for Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Adaptive grippers enable easy and robust grasping of diverse objects by adapting to their shapes and enclosing them. However, determining the exact state of the hand remains challenging. This is not always straightforward but is often necessary to assess grip success, quality, or the pose of the object. In this work, we present two deep learning approaches using recurrent neural networks to successfully estimate the joint states of the Robotiq 3-Finger Adaptive Gripper. The models are compared with an existing analytical approach, which does not distinguish between the fingers of the hand and calculates the three angles of their respective joints using joint limits, contact information, and motor position in a transition model. We test the differences in accuracy with our networks by not distinguishing between the fingers as the analytical approach for the first model and by looking at the entire hand in the second model. Our experiments demonstrate that the model considering the entire hand outperforms the other two approaches, is more robust against object movements and achieves an average joint position accuracy of 2.29 degrees.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt6">
             <b>
              ThBT6
             </b>
            </a>
           </td>
           <td class="r">
            Room 6
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thbt6" title="Click to go to the Program at a Glance">
             <b>
              Aerial Systems: Perception and Autonomy I
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#179403" title="Click to go to the Author Index">
             Oh, Jean
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#220123" title="Click to go to the Author Index">
             Agarwal, Saurav
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt6_01">
             11:00-11:15, Paper ThBT6.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('146'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Visual Servoing NMPC Applied to UAVs for Photovoltaic Array Inspection
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#232554" title="Click to go to the Author Index">
             Velasco Snchez, Edison Patricio
            </a>
           </td>
           <td class="r">
            Universidad De Alicante
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#367981" title="Click to go to the Author Index">
             Recalde, Luis F.
            </a>
           </td>
           <td class="r">
            Universidad Indoamrica
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#333499" title="Click to go to the Author Index">
             Guevara, Bryan S.
            </a>
           </td>
           <td class="r">
            Universidad Nacional De San Juan
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#380811" title="Click to go to the Author Index">
             Varela-Alds, Jos
            </a>
           </td>
           <td class="r">
            Universidad Tecnolgica Indoamrica
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#117343" title="Click to go to the Author Index">
             Candelas, Francisco A.
            </a>
           </td>
           <td class="r">
            University of Alicante
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#104432" title="Click to go to the Author Index">
             Puente, Santiago
            </a>
           </td>
           <td class="r">
            University of Alicante
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#380812" title="Click to go to the Author Index">
             Gandolfo, Daniel C.
            </a>
           </td>
           <td class="r">
            Universidad Nacional De San Juan INAUT
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab146" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#visual_servoing" title="Click to go to the Keyword Index">
               Visual Servoing
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The photovoltaic (PV) industry is seeing a significant shift toward large-scale solar plants, where traditional inspection methods have proven to be time-consuming and costly. Currently, the predominant approach to PV inspection using unmanned aerial vehicles (UAVs) is based on the capture and detailed analysis of aerial images (photogrammetry). However, the photogrammetry approach presents limitations, such as an increased amount of useless data and potential issues related to image resolution that negatively impact the detection process during high-altitude flights. In this work, we develop a visual servoing control system with dynamic compensation using nonlinear model predictive control (NMPC) applied to a UAV. This system is capable of accurately tracking the middle of the underlying PV array at various frontal velocities and height constraints, ensuring the acquisition of detailed images during low-altitude flights. The visual servoing controller is based on extracting features using RGB-D images and employing a Kalman filter to estimate the edges of the PV arrays. Furthermore, this work demonstrates the proposal in both simulated and real-world environments using the commercial aerial vehicle (DJI Matrice 100), with the purpose of showcasing the results of the architecture.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt6_02">
             11:15-11:30, Paper ThBT6.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('171'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SoRTS: Learned Tree Search for Long Horizon Social Robot Navigation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#325455" title="Click to go to the Author Index">
             Navarro, Ingrid
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#277438" title="Click to go to the Author Index">
             Patrikar, Jay
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#354872" title="Click to go to the Author Index">
             Dantas, Joao
            </a>
           </td>
           <td class="r">
            Institute for Advanced Studies
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#354873" title="Click to go to the Author Index">
             Baijal, Rohan
            </a>
           </td>
           <td class="r">
            University of Washington
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#354878" title="Click to go to the Author Index">
             Higgins, Ian
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#104304" title="Click to go to the Author Index">
             Scherer, Sebastian
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#179403" title="Click to go to the Author Index">
             Oh, Jean
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab171" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__perception_and_autonomy" title="Click to go to the Keyword Index">
               Aerial Systems: Perception and Autonomy
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#human_aware_motion_planning" title="Click to go to the Keyword Index">
               Human-Aware Motion Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#safety_in_hri" title="Click to go to the Keyword Index">
               Safety in HRI
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The fast-growing demand for fully autonomous robots in shared spaces calls for developing trustworthy agents that can safely and seamlessly navigate crowded environments. Recent models for motion prediction show promise in characterizing social interactions in such environments. However, using them for downstream navigation can lead to unsafe behavior due to their myopic decision-making. Prompted by this, we propose Social Robot Tree Search (SoRTS), an algorithm for safe robot navigation in social domains. SoRTS aims to augment existing socially aware motion prediction models for long-horizon navigation using Monte Carlo Tree Search.
             <p>
              We use social navigation in general aviation as a case study to evaluate our approach and further the research in full-scale aerial autonomy. In doing so, we introduce XPlaneROS, a high-fidelity aerial simulator that enables human-robot interaction. We use XPlaneROS to conduct a first-of-its-kind user study where 26 FAA-certified pilots interact with a human pilot, our algorithm, and its ablation. Our results, supported by statistical evidence, show that SoRTS exhibits comparable performance to competent human pilots, significantly outperforming its ablation. Finally, we complement these results with a broad set of self-play experiments to showcase our algorithm's performance in scenarios with increasing complexity.
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt6_03">
             11:30-11:45, Paper ThBT6.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3646'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Star-Searcher: A Complete and Efficient Aerial System for Autonomous Target Search in Complex Unknown Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#371295" title="Click to go to the Author Index">
             Luo, Yiming
            </a>
           </td>
           <td class="r">
            The University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#379776" title="Click to go to the Author Index">
             Zhuang, Zixuan
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#288062" title="Click to go to the Author Index">
             Pan, Neng
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#340168" title="Click to go to the Author Index">
             Feng, Chen
            </a>
           </td>
           <td class="r">
            Hong Kong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#142354" title="Click to go to the Author Index">
             Shen, Shaojie
            </a>
           </td>
           <td class="r">
            Hong Kong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#200893" title="Click to go to the Author Index">
             Gao, Fei
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#169114" title="Click to go to the Author Index">
             Cheng, Hui
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#225160" title="Click to go to the Author Index">
             Zhou, Boyu
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3646" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__perception_and_autonomy" title="Click to go to the Keyword Index">
               Aerial Systems: Perception and Autonomy
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#search_and_rescue_robots" title="Click to go to the Keyword Index">
               Search and Rescue Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper tackles the challenge of autonomous target search using unmanned aerial vehicles (UAVs) in complex unknown environments. To fill the gap in systematic approaches for this task, we introduce Star-Searcher, an aerial system featuring specialized sensor suites, mapping, and planning modules to optimize searching. Path planning challenges due to increased inspection requirements are addressed through a hierarchical planner with a visibility-based viewpoint clustering method. This simplifies planning by breaking it into global and local sub-problems, ensuring efficient global and local path coverage in real-time. Furthermore, our global path planning employs a history-aware mechanism to reduce motion inconsistency from frequent map changes, significantly enhancing search efficiency. We conduct comparisons with state-of-the-art methods in both simulation and the real world, demonstrating shorter flight paths, reduced time, and higher target search completeness. Our approach will be open-sourced for community benefit.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt6_04">
             11:45-12:00, Paper ThBT6.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3710'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              3D Active Metric-Semantic SLAM
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#283123" title="Click to go to the Author Index">
             Tao, Yuezhan
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#224065" title="Click to go to the Author Index">
             Liu, Xu
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#239282" title="Click to go to the Author Index">
             Spasojevic, Igor
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#220123" title="Click to go to the Author Index">
             Agarwal, Saurav
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#104342" title="Click to go to the Author Index">
             Kumar, Vijay
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3710" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__perception_and_autonomy" title="Click to go to the Keyword Index">
               Aerial Systems: Perception and Autonomy
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#perception_action_coupling" title="Click to go to the Keyword Index">
               Perception-Action Coupling
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this letter, we address the problem of exploration and metric-semantic mapping of multi-floor GPS-denied indoor environments using Size Weight and Power (SWaP) constrained aerial robots. Most previous work in exploration assumes that robot localization is solved. However, neglecting the state uncertainty of the agent can ultimately lead to cascading errors both in the resulting map and in the state of the agent itself. Furthermore, actions that reduce localization errors may be at direct odds with the exploration task. We develop a framework that balances the efficiency of exploration with actions that reduce the state uncertainty of the agent. In particular, our algorithmic approach for active metric-semantic SLAM is built upon sparse information abstracted from raw problem data, to make it suitable for SWaP-constrained robots. Furthermore, we integrate this framework within a fully autonomous aerial robotic system that achieves autonomous exploration in cluttered, 3D environments. From extensive real-world experiments, we showed that by including Semantic Loop Closure (SLC), we can reduce the robot pose estimation errors by over 90% in translation and approximately 75% in yaw, and the uncertainties in pose estimates and semantic maps by over 70% and 65%, respectively. Although discussed in the context of indoor multi-floor exploration, our system can be used for various other applications, such as infrastructure inspection and precision agriculture where reliable GPS data may not be available.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt7">
             <b>
              ThBT7
             </b>
            </a>
           </td>
           <td class="r">
            Room 7
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thbt7" title="Click to go to the Program at a Glance">
             <b>
              Human-Robot Interaction II
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#137752" title="Click to go to the Author Index">
             Leonetti, Matteo
            </a>
           </td>
           <td class="r">
            King's College London
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#167990" title="Click to go to the Author Index">
             Bera, Aniket
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt7_01">
             11:00-11:15, Paper ThBT7.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1908'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Probabilistic Inference of Human Capabilities from Passive Observations
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#328910" title="Click to go to the Author Index">
             Tisnikar, Peter
            </a>
           </td>
           <td class="r">
            King's College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#213444" title="Click to go to the Author Index">
             Canal, Gerard
            </a>
           </td>
           <td class="r">
            King's College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#137752" title="Click to go to the Author Index">
             Leonetti, Matteo
            </a>
           </td>
           <td class="r">
            King's College London
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1908" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#human_centered_robotics" title="Click to go to the Keyword Index">
               Human-Centered Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#probabilistic_inference" title="Click to go to the Keyword Index">
               Probabilistic Inference
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Modern robots need to adapt to diverse human partners with whom they collaborate. To this end, learning a representation of human capabilities enables the robot to personalize their behaviour to their collaborators across multiple tasks. We propose CApability Modeling from Observations (CAMO), a model-based estimation algorithm, in which human capabilities that parameterize a given model are inferred from observations of the human behaviour on known collaborative tasks. We apply the method to joint limit learning in order to predict future trajectories of a 7-DOF manipulator arm. Furthermore, we show that CAMO can be used as a sub-task assignment routine in a simulated human--robot collaboration scenario, allowing the robot to adapt its task allocation to perform tasks that the person is not able to do.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt7_02">
             11:15-11:30, Paper ThBT7.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('316'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Using Augmented Reality in Human-Robot Assembly: A Comparative Study of Eye-Gaze and Hand-Ray Pointing Methods
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#323396" title="Click to go to the Author Index">
             Tadeja, Slawomir Konrad
            </a>
           </td>
           <td class="r">
            University of Cambridge
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#384675" title="Click to go to the Author Index">
             Zhou, Tianye
            </a>
           </td>
           <td class="r">
            University of Cambridge
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#384676" title="Click to go to the Author Index">
             Capponi, Matteo
            </a>
           </td>
           <td class="r">
            Politecnico Di Torino
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#144407" title="Click to go to the Author Index">
             Walas, Krzysztof, Tadeusz
            </a>
           </td>
           <td class="r">
            Poznan University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#384677" title="Click to go to the Author Index">
             Bohn, Thomas
            </a>
           </td>
           <td class="r">
            University of Cambridge
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#320074" title="Click to go to the Author Index">
             Forni, Fulvio
            </a>
           </td>
           <td class="r">
            University of Cambridge
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab316" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#human_robot_teaming" title="Click to go to the Keyword Index">
               Human-Robot Teaming
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#virtual_reality_and_interfaces" title="Click to go to the Keyword Index">
               Virtual Reality and Interfaces
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Collaborative robots (cobots) are a promising technology for frontline workers in industry. They can support tasks that cannot be fully automated but are repetitive, fatiguing, boring, or dangerous for humans. Although cobots are explicitly designed to work with humans, they remain primarily non-intuitive and difficult to collaborate with. Thus, there is a need for new interaction approaches to facilitate efficient human-robot collaboration. Recently, we could see emerging examples of using augmented reality (AR) to assist a worker in collaborative task execution with a cobot. However, for such an approach to provide truly efficient support for the seamless bimanual task execution, we need to first investigate interaction methods offered by an AR interface. To that end, we performed a study with sixteen participants to compare eye-gaze and hand-ray pointing methods for part selection in collaborative, manual assembly tasks. The results of our study show that both techniques provide similar perceived usability, with the eye-gaze selection leading to significantly shorter completion times.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt7_03">
             11:30-11:45, Paper ThBT7.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2662'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              TrustNavGPT: Trust-Driven Audio-Guided Robot Navigation under Uncertainty with Large Language Models
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398425" title="Click to go to the Author Index">
             Sun, Xingpeng
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398467" title="Click to go to the Author Index">
             Zhang, Yiran
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398433" title="Click to go to the Author Index">
             Tang, Xindi
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#274898" title="Click to go to the Author Index">
             Bedi, Amrit Singh
            </a>
           </td>
           <td class="r">
            University of Maryland, College Park
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#167990" title="Click to go to the Author Index">
             Bera, Aniket
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2662" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#acceptability_and_trust" title="Click to go to the Keyword Index">
               Acceptability and Trust
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#social_hri" title="Click to go to the Keyword Index">
               Social HRI
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Large language models (LLMs) exhibit a wide range of promising capabilities - from step-by-step planning to commonsense reasoning - that provide utility for robot navigation. However, as humans communicate with robots in the real world, ambiguity and uncertainty may be embedded inside spoken instructions. While LLMs are proficient at processing text in human conversations, they often encounter difficulties with the nuances of verbal instructions and, thus, remain prone to hallucinate trust in human command. In this work, we present modelname, an LLM-based audio-guided navigation agent that uses affective cues in spoken communicationelements such as tone and inflection that convey meaning beyond wordsallowing it to assess the trustworthiness of human commands and make effective, safe decisions. Experiments across a variety of simulation and real-world setups show a 70.46% success rate in catching command uncertainty and an 80% success rate in finding the target, 48.30%, and 55% outperform existing LLM-based navigation methods, respectively. Additionally, modelname shows remarkable resilience against adversarial attacks, highlighted by a 22%+ less decrease ratio than the existing LLM navigation method in success rate. Our approach provides a lightweight yet effective approach that extends existing LLMs to model audio vocal features embedded in the voice command and model uncertainty for safe robotic navigation. For more information, visit the https://xingpengsun0.github.io/trustnav/
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt7_04">
             11:45-12:00, Paper ThBT7.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2676'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Comparison of Audible, Visual, and Multi-Modal Communication for Multi-Robot Supervision and Situational Awareness
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#384807" title="Click to go to the Author Index">
             Attfield, Richard
            </a>
           </td>
           <td class="r">
            Monash University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#100067" title="Click to go to the Author Index">
             Croft, Elizabeth
            </a>
           </td>
           <td class="r">
            University of Victoria
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#108781" title="Click to go to the Author Index">
             Kulic, Dana
            </a>
           </td>
           <td class="r">
            Monash University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2676" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#human_robot_teaming" title="Click to go to the Keyword Index">
               Human-Robot Teaming
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#human_factors_and_human_in_the_loop" title="Click to go to the Keyword Index">
               Human Factors and Human-in-the-Loop
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#design_and_human_factors" title="Click to go to the Keyword Index">
               Design and Human Factors
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Multi-robot supervision becomes increasingly cognitively demanding as the ratio of robots to human supervisors rises, potentially leading to situational awareness (SA) losses and robot system failures. Nonverbal cues have been employed to direct supervisor attention and prevent awareness loss in diverse human-computer interaction (HCI) settings. This paper compares the effects of uni-modal and multi-modal audiovisual nonverbal cues on supervisor SA in a multi-robot supervision task. In a simulation-based navigation scenario, 50 participants monitored a multi-robot mission and responded to supervision requests from the robots. We evaluated supervisor SA using response speed and the situational awareness global assessment technique. Results demonstrate that supervisor awareness hinges on the communication method employed by the robots, with greater significance observed at higher awareness levels and when the robot-to-human ratio is higher. Findings also indicate the utility of sonification mapping in human-multi-robot interactions and the benefits of multi-modal cues for sustaining awareness during multi-robot supervision.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt8">
             <b>
              ThBT8
             </b>
            </a>
           </td>
           <td class="r">
            Room 8
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thbt8" title="Click to go to the Program at a Glance">
             <b>
              Localization V
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#104207" title="Click to go to the Author Index">
             Lee, Dongjun
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt8_01">
             11:00-11:15, Paper ThBT8.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1358'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              UWB-Based Localization System Considering Antenna Anisotropy and NLOS/Multipath Conditions
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#331806" title="Click to go to the Author Index">
             Kim, Taekyun
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396339" title="Click to go to the Author Index">
             Yoon, Byoungkwon
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#104207" title="Click to go to the Author Index">
             Lee, Dongjun
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1358" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#sensor_fusion" title="Click to go to the Keyword Index">
               Sensor Fusion
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#calibration_and_identification" title="Click to go to the Keyword Index">
               Calibration and Identification
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Ultra-wideband (UWB) communication technology has gained attention in robotics due to its ability to provide range measurements possibly with centimeter-level accuracy. Nevertheless, practical UWB range measurements are susceptible to disturbances from multiple sources, including the anisotropic characteristics of antennas, non-line-of-sight (NLOS) conditions, and multipath propagation. In this paper, we introduce a UWB range measurement model that addresses these sources of error. To accommodate the effects of antenna anisotropy, we adopt real spherical harmonics to represent directional bias in the UWB range measurement model. To handle delayed measurements induced by NLOS conditions and multipath propagation, an asymmetric heavy-tailed distribution is utilized to model the measurement noise. We calibrate this measurement model based on the maximum likelihood estimation method and propose a UWB-based localization system based on that. Our localization system provides: 1) anchor self-calibration, which identifies anchor placement by fusing visual-inertial-ranging measurements based on continuous-time state representation; and 2) filtering-based state estimation, which applies our measurement model into Kalman filtering framework via iterative update algorithm. Experimental validation is conducted to demonstrate the effectiveness of the measurement model for our localization system. We open source our implementation of the proposed UWB-based localization system at https://github.com/INRoL/inrol_uwb_localization.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt8_02">
             11:15-11:30, Paper ThBT8.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1591'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SDFT: Structural Discrete Fourier Transform for Place Recognition and Traversability Analysis
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#359148" title="Click to go to the Author Index">
             Umemura, Ayumi
            </a>
           </td>
           <td class="r">
            Tohoku University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#254363" title="Click to go to the Author Index">
             Sakurada, Ken
            </a>
           </td>
           <td class="r">
            National Institute of Advanced Industrial Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#103844" title="Click to go to the Author Index">
             Onishi, Masaki
            </a>
           </td>
           <td class="r">
            National Inst. of AIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#103649" title="Click to go to the Author Index">
             Yoshida, Kazuya
            </a>
           </td>
           <td class="r">
            Tohoku University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1591" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#space_robotics_and_automation" title="Click to go to the Keyword Index">
               Space Robotics and Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The ability to associate the current location with previously visited places is an essential aspect of autonomous ground robots. Unstructured environments such as planetary surfaces pose a significant challenge for robots because their terrain is less distinctive. Meanwhile, traversability must be analyzed simultaneously for safe navigation. In the past, place recognition research has rarely considered traversability analysis despite its significance. This is because the structural information of terrains becomes quickly implicit during the encoding process. This paper provides a method that explicitly addresses both problems: place recognition and traversability analysis. It proposes a discrete Fourier transform (DFT) to represent the frequency components embedded in ground curvature, which underlies both concepts. Our place recognition function demonstrates excellent performance in extensive experiments using challenging planetary &amp; urban datasets while estimating traversability that other approaches find difficult to handle.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt8_03">
             11:30-11:45, Paper ThBT8.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1723'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              CATOA: Cooperative Calibration of Timestamp Measurements for Distributed Multi-Robot Localization
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#386571" title="Click to go to the Author Index">
             Wen, Feiyang
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#387221" title="Click to go to the Author Index">
             Zhao, Hanying
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#243258" title="Click to go to the Author Index">
             Jincheng, Yu
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#339627" title="Click to go to the Author Index">
             Cui, Shulin
            </a>
           </td>
           <td class="r">
            Meituan
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#198388" title="Click to go to the Author Index">
             Shen, Yuan
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1723" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#sensor_networks" title="Click to go to the Keyword Index">
               Sensor Networks
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Ultra-wideband (UWB) is a popular technology for robotic localization in global positioning system (GPS)-challenged and vision-obstructed scenarios. In UWB localization systems, distance information is extracted from ToA and ToD timestamp measurements. However, these measurements are easily influenced by hardware limitations and complex propagation environments, making an effective calibration method crucial for achieving high-accuracy ranging. This paper proposes a cooperative timestamp calibration method, which effectively mitigates ranging errors with scalability, adaptability, and flexibility. Our approach reduces the calibration complexity from O(N^2) to O(N) for networks within N nodes and allows for distributed implementation to lower communication costs. The enabler is developing a new timestamp measurement model that can rectify all timestamps across different devices in a unified manner, coupled with the introduction of cooperative model training techniques that accommodate both feasible and infeasible scenarios for precisely labeling node positions. Real-world experimental results show that our method reduces the ranging error from 38.02 cm to 8.17 cm within a fully labeled 4-node network and from 16.77 cm to 9.61 cm in an 8-node network without labeling.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt8_04">
             11:45-12:00, Paper ThBT8.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1840'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Fast Global Point Cloud Registration Using Semantic NDT
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#204786" title="Click to go to the Author Index">
             Schirmer, Robert
            </a>
           </td>
           <td class="r">
            Robert Bosch GmbH
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#114463" title="Click to go to the Author Index">
             Vaskevicius, Narunas
            </a>
           </td>
           <td class="r">
            Robert Bosch GmbH
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#107180" title="Click to go to the Author Index">
             Biber, Peter
            </a>
           </td>
           <td class="r">
            Robert Bosch GmbH
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#101642" title="Click to go to the Author Index">
             Stachniss, Cyrill
            </a>
           </td>
           <td class="r">
            University of Bonn
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1840" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robust and accurate point cloud registration is an essential part of many robotic tasks such as SLAM or object pose retrieval. In this paper, we address the problem of global 3D point cloud registration, i.e., the task of estimating the rigid 3D isometry transform between a source and a target point cloud without any initial guess. Typically, the problem is solved by extracting and matching features to find a data association and then computing a transform that minimizes the squared distance between points. Our approach combines the normal distributions transform and oriented point pair framework and introduces the NDT distance histogram to quickly generate and test candidate transforms. Our method further exploits semantic information if available for greater speed. We implement our algorithm in C++ and compare it to other state-of-the-art approaches on a diverse set of environments. Our evaluation shows that our method outperforms the other approaches, especially concerning run-time and compute efficiency.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt9">
             <b>
              ThBT9
             </b>
            </a>
           </td>
           <td class="r">
            Room 9
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thbt9" title="Click to go to the Program at a Glance">
             <b>
              Motion and Path Planning V
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#211685" title="Click to go to the Author Index">
             Ren, Zhongqiang
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#237063" title="Click to go to the Author Index">
             Chamzas, Constantinos
            </a>
           </td>
           <td class="r">
            Worcester Polytechnic Institute
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt9_01">
             11:00-11:15, Paper ThBT9.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1028'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Agile and Safe Trajectory Planning for Quadruped Navigation with Motion Anisotropy Awareness
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#337326" title="Click to go to the Author Index">
             Zhang, Wentao
            </a>
           </td>
           <td class="r">
            Huazhong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#311096" title="Click to go to the Author Index">
             Xu, Shaohang
            </a>
           </td>
           <td class="r">
            Huazhong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#320276" title="Click to go to the Author Index">
             Cai, Peiyuan
            </a>
           </td>
           <td class="r">
            Huazhong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#152107" title="Click to go to the Author Index">
             Zhu, Lijun
            </a>
           </td>
           <td class="r">
            Huazhong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1028" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#collision_avoidance" title="Click to go to the Keyword Index">
               Collision Avoidance
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Quadruped robots demonstrate robust and agile movements in various terrains; however, their navigation autonomy is still insufficient. One of the challenges is that the motion capabilities of the quadruped robot are anisotropic along different directions, which significantly affects the safety of quadruped robot navigation. This paper proposes a navigation framework that takes into account the motion anisotropy of quadruped robots including kinodynamic trajectory generation, nonlinear trajectory optimization, and nonlinear model predictive control. In simulation and real robot tests, we demonstrate that our motion-anisotropy-aware navigation framework could: (1) generate more efficient trajectories and realize more agile quadruped navigation; (2) significantly improve the navigation safety in challenging scenarios. The implementation is realized as an open-source package at https://github.com/ZWT006/agile_navigation.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt9_02">
             11:15-11:30, Paper ThBT9.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1094'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Mixed-Integer Conic Program for the Moving-Target Traveling Salesman Problem Based on a Graph of Convex Sets
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395102" title="Click to go to the Author Index">
             George Philip, Allen
            </a>
           </td>
           <td class="r">
            Texas A&amp;M University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#211685" title="Click to go to the Author Index">
             Ren, Zhongqiang
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#130289" title="Click to go to the Author Index">
             Rathinam, Sivakumar
            </a>
           </td>
           <td class="r">
            TAMU
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#104011" title="Click to go to the Author Index">
             Choset, Howie
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1094" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#planning__scheduling_and_coordination" title="Click to go to the Keyword Index">
               Planning, Scheduling and Coordination
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper introduces a new formulation that finds the optimum for the Moving-Target Traveling Salesman Problem (MT-TSP), which seeks to find a shortest path for an agent, that starts at a depot, visits a set of moving targets exactly once within their assigned time-windows, and returns to the depot. The formulation relies on the key idea that when the targets move along lines, their trajectories become convex sets within the space-time coordinate system. The problem then reduces to finding the shortest path within a graph of convex sets, subject to some speed constraints. We compare our formulation with the current state-of-the-art Mixed Integer Conic Program (MICP) formulation for the MT-TSP. The experimental results show that our formulation outperforms the MICP for instances with up to 20 targets, with up to two orders of magnitude reduction in runtime, and up to a 60% tighter optimality gap. We also show that the solution cost from the convex relaxation of our formulation provides significantly tighter lower-bounds for the MT-TSP than the ones from the MICP.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt9_03">
             11:30-11:45, Paper ThBT9.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1624'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Expansion-GRR: Efficient Generation of Smooth Global Redundancy Resolution Roadmaps
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#303656" title="Click to go to the Author Index">
             Zhong, Zhuoyun
            </a>
           </td>
           <td class="r">
            Worcester Polytechnic Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#124281" title="Click to go to the Author Index">
             Li, Zhi
            </a>
           </td>
           <td class="r">
            Worcester Polytechnic Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#237063" title="Click to go to the Author Index">
             Chamzas, Constantinos
            </a>
           </td>
           <td class="r">
            Worcester Polytechnic Institute
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1624" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#redundant_robots" title="Click to go to the Keyword Index">
               Redundant Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#kinematics" title="Click to go to the Keyword Index">
               Kinematics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Global redundancy resolution (GRR) roadmaps is a novel concept in robotics that facilitates the mapping from task space paths to configuration space paths in a legible, predictable, and repeatable way. Such roadmaps could find widespread utility in applications such as safe teleoperation, consistent path planning, and motion primitives generation. However, previous methods to compute GRR roadmaps often necessitate a lengthy computation time and produce non-smooth paths, limiting their practical efficacy. To address this challenge, we introduce a novel method Expansion-GRR that leverages efficient configuration space projections and enables rapid generation of smooth roadmaps that satisfy the task constraints. Additionally, we propose a simple multi-seed strategy that further enhances the final quality. We conducted experiments in simulation with a 5-link planar manipulator and a Kinova arm. We were able to generate the Expansion-GRR roadmaps up to 2 orders of magnitude faster while achieving higher smoothness. We also demonstrate the utility of the GRR roadmaps in teleoperation tasks where our method outperformed prior methods and reactive IK solvers in terms of success rate and solution quality.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt9_04">
             11:45-12:00, Paper ThBT9.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1692'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Asymptotically Optimal Lazy Lifelong Sampling-Based Algorithm for Efficient Motion Planning in Dynamic Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#351477" title="Click to go to the Author Index">
             Huang, Lu
            </a>
           </td>
           <td class="r">
            City University of Hongkong
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#278580" title="Click to go to the Author Index">
             Jing, Xingjian
            </a>
           </td>
           <td class="r">
            City University of Hong Kong
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1692" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The paper introduces an asymptotically optimal lifelong sampling-based path planning algorithm that combines the merits of lifelong planning algorithms and lazy search algorithms for rapid replanning in dynamic environments where edge evaluation is expensive. By evaluating only sub-path candidates for the optimal solution, the algorithm saves considerable evaluation time and thereby reduces the overall planning cost. It employs a novel informed rewiring cascade to efficiently repair the search tree when the underlying search graph changes. Simulation results demonstrate that the algorithm outperforms various state-of-the-art sampling-based planners in addressing both static and dynamic motion planning problems.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt10">
             <b>
              ThBT10
             </b>
            </a>
           </td>
           <td class="r">
            Room 10
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thbt10" title="Click to go to the Program at a Glance">
             <b>
              Deep Learning for Visual Perception II
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt10_01">
             11:00-11:15, Paper ThBT10.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('379'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              GenerOcc: Self-Supervised Framework of Real-Time 3D Occupancy Prediction for Monocular Generic Cameras
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392284" title="Click to go to the Author Index">
             Pan, Xianghui
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#314030" title="Click to go to the Author Index">
             Du, Jiayuan
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#134932" title="Click to go to the Author Index">
             Liu, Chengju
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#118155" title="Click to go to the Author Index">
             Chen, Qijun
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#323995" title="Click to go to the Author Index">
             Su, Shuai
            </a>
           </td>
           <td class="r">
            Tongji University, China
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#394928" title="Click to go to the Author Index">
             Zong, Wenhao
            </a>
           </td>
           <td class="r">
            DominantTech
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#233208" title="Click to go to the Author Index">
             Wang, Xiao
            </a>
           </td>
           <td class="r">
            DominantTech
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab379" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In the context of 3D scene perception tasks, the significance of 3D occupancy prediction has been progressively growing, aiming to forecast the occupancy state of voxels in a discrete 3D space. However, existing methods typically exhibit several limitations, such as restricted adaptability to non-pinhole cameras due to fixed camera parameters, heavy reliance on 3D annotations because of the inability to project 3D output back to the camera plane, and inferior real-time inference performance resulting from the conversion process from 2D to 3D features. To address these constrains, we introduce GenerOcc, a self-supervised framework of real-time 3D occupancy prediction for monocular generic cameras. We have collected the fisheye Dominant dataset to confirm the compatibility of our ray-based camera model with non-pinhole cameras. By transforming the occupancy prediction task into a depth estimation task in a self-supervised manner, we eliminate dependency on 3D annotations. Furthermore, we propose a parametric voxel probability distribution module that leverages 2D features to quickly predict 3D occupancy without 3D representations of the scene. Additionally, our GenerOcc has been extensively evaluated on public pinhole Occ3D-nuScenes dataset and our proprietary fisheye Dominant dataset, both yielding impressive performance.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt10_02">
             11:15-11:30, Paper ThBT10.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('457'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              LiOn-XA: Unsupervised Domain Adaptation Via LiDAR-Only Cross-Modal Adversarial Training
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#335909" title="Click to go to the Author Index">
             Kreutz, Thomas
            </a>
           </td>
           <td class="r">
            Technical University Darmstadt
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#390736" title="Click to go to the Author Index">
             Lemke, Jens
            </a>
           </td>
           <td class="r">
            Technical University of Darmstadt
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#338133" title="Click to go to the Author Index">
             Mhlhuser, Max
            </a>
           </td>
           <td class="r">
            Technical University of Darmstadt
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#338005" title="Click to go to the Author Index">
             Sanchez Guinea, Alejandro
            </a>
           </td>
           <td class="r">
            TU Darmstadt
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab457" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#range_sensing" title="Click to go to the Keyword Index">
               Range Sensing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper, we propose LiOn-XA, an unsupervised domain adaptation (UDA) approach that combines LiDAR-Only Cross-Modal (X) learning with Adversarial training for 3D LiDAR point cloud semantic segmentation to bridge the domain gap arising from environmental and sensor setup changes. Unlike existing works that exploit multiple data modalities like point clouds and RGB image data, we address UDA in scenarios where RGB images might not be available and show that two distinct LiDAR data representations can learn from each other for UDA. More specifically, we leverage 3D voxelized point clouds to preserve important geometric structure in combination with 2D projection-based range images that provide information such as object orientations or surfaces. To further align the feature space between both domains, we apply adversarial training using both features and predictions of both 2D and 3D neural networks. Our experiments on 3 real-to-real adaptation scenarios demonstrate the effectiveness of our approach, achieving new state-of-the-art performance when compared to previous uni- and multi-model UDA methods. Our source code is publicly available at https://github.com/JensLe97/lion-xa.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt10_03">
             11:30-11:45, Paper ThBT10.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('848'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              NeuSurfEmb: A Complete Pipeline for Dense Correspondence-Based 6D Object Pose Estimation without CAD Models
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#311697" title="Click to go to the Author Index">
             Milano, Francesco
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#150492" title="Click to go to the Author Index">
             Chung, Jen Jen
            </a>
           </td>
           <td class="r">
            The University of Queensland
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#226533" title="Click to go to the Author Index">
             Blum, Hermann
            </a>
           </td>
           <td class="r">
            Uni Bonn | Lamarr Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#100088" title="Click to go to the Author Index">
             Siegwart, Roland
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#142888" title="Click to go to the Author Index">
             Ott, Lionel
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab848" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             State-of-the-art approaches for 6D object pose estimation assume the availability of CAD models and require the user to manually set up physically-based rendering (PBR) pipelines for synthetic training data generation. Both factors limit the application of these methods in real-world scenarios. In this work, we present a pipeline that does not require CAD models and allows training a state-of-the-art pose estimator requiring only a small set of real images as input. Our method is based on a NeuS2 object representation, that we learn through a semi-automated procedure based on Structure-from-Motion (SfM) and object-agnostic segmentation. We exploit the novel-view synthesis ability of NeuS2 and simple cut-and-paste augmentation to automatically generate photorealistic object renderings, which we use to train the correspondence-based SurfEmb pose estimator. We evaluate our method on the LINEMOD-Occlusion dataset, extensively studying the impact of its individual components and showing competitive performance with respect to approaches based on CAD models and PBR data. We additionally demonstrate the ease of use and effectiveness of our pipeline on self-collected real-world objects, showing that our method outperforms state-of-the-art CAD-model-free approaches, with better accuracy and robustness to mild occlusions. To allow the robotics community to benefit from this system, we will publicly release it at https://www.github.com/ethz-asl/neusurfemb.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt10_04">
             11:45-12:00, Paper ThBT10.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1576'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Recurrent Non-Rigid Point Cloud Registration
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#352247" title="Click to go to the Author Index">
             Cao, Yue
            </a>
           </td>
           <td class="r">
            ANU
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#314758" title="Click to go to the Author Index">
             Cheng, Ziang
            </a>
           </td>
           <td class="r">
            The Australian National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#143237" title="Click to go to the Author Index">
             Li, Hongdong
            </a>
           </td>
           <td class="r">
            Australian National University and NICTA
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1576" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#ai_based_methods" title="Click to go to the Keyword Index">
               AI-Based Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Non-rigid point cloud registration remains a significant challenge in 3D computer vision due to the complexity of structural deforms, lack of overlaps, and sensitivity to initialization. This paper introduces a framework inspired by the recent success in recurrent architecture, adapted to accommodate the unique characteristics of point clouds. More specifically, we design a recurrent update network block for progressively refining local registration results under a local rigidity assumption, starting from an initial global SE(3) alignment. Through comparison, our method consistently outperforms competing methods in standard metrics, achieving a 33% reduction in EPE on the 4DLoMatch benchmark compared to the second-best method. To the best of our knowledge, the proposed method is the first to successfully demonstrate that the recurrent update strategy can effectively address the non-rigid registration task with large displacement, significant deform, and low overlap.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt11">
             <b>
              ThBT11
             </b>
            </a>
           </td>
           <td class="r">
            Room 11
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thbt11" title="Click to go to the Program at a Glance">
             <b>
              Multi-Robot Systems V
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#167990" title="Click to go to the Author Index">
             Bera, Aniket
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#180884" title="Click to go to the Author Index">
             Sartoretti, Guillaume Adrien
            </a>
           </td>
           <td class="r">
            National University of Singapore (NUS)
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt11_01">
             11:00-11:15, Paper ThBT11.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1504'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              D3G: Learning Multi-Robot Coordination from Demonstrations
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#286295" title="Click to go to the Author Index">
             Zhou, Yizhi
            </a>
           </td>
           <td class="r">
            George Mason University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#227280" title="Click to go to the Author Index">
             Jin, Wanxin
            </a>
           </td>
           <td class="r">
            Arizona State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#280358" title="Click to go to the Author Index">
             Wang, Xuan
            </a>
           </td>
           <td class="r">
            George Mason University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1504" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#distributed_robot_systems" title="Click to go to the Keyword Index">
               Distributed Robot Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper develops a new Distributed approach for solving the inverse problem of a Differentiable Dynamic Game (D3G), which enables robots to learn multi-robot coordination from given demonstrations. We formulate multi-robot coordination as the Nash equilibrium of a parameterized dynamic game, where the behavior of each robot is dictated by an objective function that also depends on the behavior of its neighboring robots. The coordination thus can be adapted by tuning the parameters of the objective and the local dynamics of each robot. The proposed algorithm enables each robot to automatically tune such parameters in a distributed and coordinated fashion --- only using the data of its neighbors without global information. Its key novelty is the development of a distributed solver for a diff-KKT condition that can enhance scalability and reduce the computational load for gradient computation. We test the proposed algorithm in simulation with heterogeneous robots given different task configurations. The results demonstrate its effectiveness and generalizability for learning multi-robot coordination from demonstrations.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt11_02">
             11:15-11:30, Paper ThBT11.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1660'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SiCP: Simultaneous Individual and Cooperative Perception for 3D Object Detection in Connected and Automated Vehicles
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#394585" title="Click to go to the Author Index">
             Qu, Deyuan
            </a>
           </td>
           <td class="r">
            University of North Texas
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#378484" title="Click to go to the Author Index">
             Chen, Qi
            </a>
           </td>
           <td class="r">
            Toyota Motor North America, InfoTech Labs
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396492" title="Click to go to the Author Index">
             Bai, Tianyu
            </a>
           </td>
           <td class="r">
            University of North Texas
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#239059" title="Click to go to the Author Index">
             Lu, Hongsheng
            </a>
           </td>
           <td class="r">
            Toyota Motor North America
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#204190" title="Click to go to the Author Index">
             Fan, Heng
            </a>
           </td>
           <td class="r">
            University of North Texas
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#138554" title="Click to go to the Author Index">
             Zhang, Hao
            </a>
           </td>
           <td class="r">
            University of Massachusetts Amherst
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395451" title="Click to go to the Author Index">
             Fu, Song
            </a>
           </td>
           <td class="r">
            University of North Texas
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392430" title="Click to go to the Author Index">
             Yang, Qing
            </a>
           </td>
           <td class="r">
            University of North Texas
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1660" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Cooperative perception for connected and automated vehicles is traditionally achieved through the fusion of feature maps from two or more vehicles. However, the absence of feature maps shared from other vehicles can lead to a significant decline in 3D object detection performance for cooperative perception models compared to standalone 3D detection models. This drawback impedes the adoption of cooperative perception as vehicle resources are often insufficient to concurrently employ two perception models. To tackle this issue, we present Simultaneous Individual and Cooperative Perception (SiCP), a generic framework that supports a wide range of the state-of-the-art standalone perception backbones and enhances them with a novel Dual-Perception Network (DP-Net) designed to facilitate both individual and cooperative perception. In addition to its lightweight nature with only 0.13M parameters, DP-Net is robust and retains crucial gradient information during feature map fusion. As demonstrated in a comprehensive evaluation on the V2V4Real and OPV2V datasets, thanks to DP-Net, SiCP surpasses state-of-the-art cooperative perception solutions while preserving the performance of standalone perception solutions. The source code can be found at https://github.com/DarrenQu/SiCP.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt11_03">
             11:30-11:45, Paper ThBT11.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2252'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Optimizing Crowd-Aware Multi-Agent Path Finding through Local Broadcasting with Graph Neural Networks
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#355117" title="Click to go to the Author Index">
             Pham, Phu
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#167990" title="Click to go to the Author Index">
             Bera, Aniket
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2252" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#path_planning_for_multiple_mobile_robots_or_agents" title="Click to go to the Keyword Index">
               Path Planning for Multiple Mobile Robots or Agents
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#distributed_robot_systems" title="Click to go to the Keyword Index">
               Distributed Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Multi-Agent Path Finding (MAPF) in crowded environments presents a challenging problem in motion planning, aiming to find collision-free paths for all agents in the system. MAPF finds a wide range of applications in various domains, including aerial swarms, autonomous warehouse robotics, and self-driving vehicles. Current approaches to MAPF generally fall into two main categories: centralized and decentralized planning. Centralized planning suffers from the curse of dimensionality when the number of agents or states increases and thus does not scale well in large and complex environments. On the other hand, decentralized planning enables agents to engage in real-time path planning within a partially observable environment, demonstrating implicit coordination. However, they suffer from slow convergence and performance degradation in dense environments. In this paper, we introduce CRAMP, a novel crowd-aware decentralized reinforcement learning approach to address this problem by enabling efficient local communication among agents via Graph Neural Networks (GNNs), facilitating situational awareness and decision-making capabilities in congested environments. We test CRAMP on simulated environments and demonstrate that our method outperforms the state-of-the-art decentralized methods for MAPF on various metrics. CRAMP improves the solution quality up to 59% measured in makespan and collision count, and up to 35% improvement in success rate in comparison to previous methods.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt11_04">
             11:45-12:00, Paper ThBT11.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2294'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Inverse Submodular Maximization with Application to Human-In-The-Loop Multi-Robot Multi-Objective Coverage Control
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#256036" title="Click to go to the Author Index">
             Shi, Guangyao
            </a>
           </td>
           <td class="r">
            University of Southern California
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#101856" title="Click to go to the Author Index">
             Sukhatme, Gaurav
            </a>
           </td>
           <td class="r">
            University of Southern California
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2294" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#cooperating_robots" title="Click to go to the Keyword Index">
               Cooperating Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#human_factors_and_human_in_the_loop" title="Click to go to the Keyword Index">
               Human Factors and Human-in-the-Loop
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We consider a new type of inverse combinatorial optimization, Inverse Submodular Maximization (ISM), for human-in-the-loop multi-robot coordination. Forward combinatorial optimization, defined as the process of solving a combinatorial problem given the reward (cost)-related parameters, is widely used in multi-robot coordination. In the standard pipeline, the reward (cost)-related parameters are designed offline by domain experts first and then these parameters are utilized for coordinating robots online. What if we need to change these parameters by non-expert human supervisors who watch over the robots during tasks to adapt to some new requirements? We are interested in the case where human supervisors can suggest what actions to take and the robots need to change the internal parameters based on such suggestions. We study such problems from the perspective of inverse combinatorial optimization, i.e., the process of finding parameters given solutions to the problem. Specifically, we propose a new formulation for ISM, in which we aim to find a new set of parameters that minimally deviate from the current parameters and can make the greedy algorithm output actions the same as those suggested by humans. We show that such problems can be formulated as a Mixed Integer Quadratic Program (MIQP). However, MIQP involves exponentially binary variables, making it intractable for the existing solver when the problem size is large. We propose a new algorithm under the Branch &amp; Bound paradigm to solve such problems. In numerical simulations, we demonstrate how to use ISM in multi-robot multi-objective coverage control, and we show that the proposed algorithm achieves significant advantages in running time and peak memory usage compared to directly using the existing solver.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt12">
             <b>
              ThBT12
             </b>
            </a>
           </td>
           <td class="r">
            Room 12
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thbt12" title="Click to go to the Program at a Glance">
             <b>
              Learning from Humans
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#268360" title="Click to go to the Author Index">
             Betz, Johannes
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt12_01">
             11:00-11:15, Paper ThBT12.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2612'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Long-Horizon Visual Action Based Food Acquisition
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#297131" title="Click to go to the Author Index">
             Bhaskar, Amisha
            </a>
           </td>
           <td class="r">
            University of Maryland, College Park
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#344189" title="Click to go to the Author Index">
             Liu, Rui
            </a>
           </td>
           <td class="r">
            University of Maryland
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#192654" title="Click to go to the Author Index">
             Sharma, Vishnu D.
            </a>
           </td>
           <td class="r">
            University of Maryland
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#256036" title="Click to go to the Author Index">
             Shi, Guangyao
            </a>
           </td>
           <td class="r">
            University of Southern California
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#128263" title="Click to go to the Author Index">
             Tokekar, Pratap
            </a>
           </td>
           <td class="r">
            University of Maryland
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2612" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#human_centered_robotics" title="Click to go to the Keyword Index">
               Human-Centered Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robotic Assisted Feeding (RAF) addresses the fundamental need for individuals with mobility impairments to regain autonomy in feeding themselves. The goal of RAF is to use a robot arm to acquire and transfer food to individuals from the table. Existing RAF methods primarily focus on solid foods, leaving a gap in manipulation strategies for semi- solid and deformable foods. We present Long-horizon Visual Action-based (LAVA) food acquisition of liquid, semisolid, and deformable foods. Long-horizon refers to the goal of clearing the bowl by sequentially acquiring the food from the bowl. LAVA is hierarchical: (1) At the highest level, we determine primitives using ScoopNet. (2) At the mid-level, LAVA finds parameters for the low-level primitives. (3) At the lowest level, LAVA carries out action execution using behaviour cloning. We validate LAVA on real-world acquisition trials involving granular, liquid, semisolid, and deformable foods along with fruit chunks and soup. Across 46 bowls, LAVA acquires much more efficiently than baselines with a success rate of 89  4%, and generalizes across realistic plate variations such as varying positions, varieties, and amount of food in the bowl. Datasets and supplementary materials can be found on our website.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt12_02">
             11:15-11:30, Paper ThBT12.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3114'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Learning Bimanual Manipulation Policies for Bathing Bed-Bound People
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#281891" title="Click to go to the Author Index">
             Gu, Yijun
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#114394" title="Click to go to the Author Index">
             Demiris, Yiannis
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3114" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#physical_human_robot_interaction" title="Click to go to the Keyword Index">
               Physical Human-Robot Interaction
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#bimanual_manipulation" title="Click to go to the Keyword Index">
               Bimanual Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#physically_assistive_devices" title="Click to go to the Keyword Index">
               Physically Assistive Devices
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Assistive robots hold promise in enhancing the quality of life for older adults and people with mobility impairments in daily bed bathing routines. When providing bathing assistance to bed-bound people, human caregivers often support the joints when lifting the arms and legs to properly wash and dry occluded areas. This research introduces a novel approach to robotic bed bathing manipulation, where a bimanual robot learns to lift a target limb while controlling a cleaning tool to bath the surface within safe force bounds. To ensure safe, cooperative bath manipulation, our work combines Multi-Agent Reinforcement Learning (MARL) framework with a variable impedance action space enabling adaptive interaction with the environment and carefully-designed reward functions regulating contact force on the human body. Simulation results demonstrate improved bathing area coverage compared to unimanual models and exhibit great adaptability to contact-rich interaction within a safe force boundary. We validate our approach across various human body sizes, showcasing its generalizability. We also transfer our models to a physical Baxter robot bathing a medical-grade manikin. We further incorporate a force tracking controller with the trained models to enhance adaptation to noisy real-world bathing scenarios. To the best of our knowledge, this is the first robot-assisted bed bathing application that performs autonomous bathing around the human body using bimanual robot arms.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt12_03">
             11:30-11:45, Paper ThBT12.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1690'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Learning Human-To-Humanoid Real-Time Whole-Body Teleoperation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#379756" title="Click to go to the Author Index">
             He, Tairan
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#278518" title="Click to go to the Author Index">
             Luo, Zhengyi
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#379763" title="Click to go to the Author Index">
             Xiao, Wenli
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#308697" title="Click to go to the Author Index">
             Zhang, Chong
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#170561" title="Click to go to the Author Index">
             Kitani, Kris
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#171352" title="Click to go to the Author Index">
             Liu, Changliu
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#236627" title="Click to go to the Author Index">
             Shi, Guanya
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1690" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#telerobotics_and_teleoperation" title="Click to go to the Keyword Index">
               Telerobotics and Teleoperation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#humanoid_robot_systems" title="Click to go to the Keyword Index">
               Humanoid Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present Human to Humanoid (H2O), a reinforcement learning (RL) based framework that enables real-time whole-body teleoperation of a full-sized humanoid robot with only an RGB camera. To create a large-scale retargeted motion dataset of human movements for humanoid robots, we propose a scalable "sim-to-data" process to filter and pick feasible motions using a privileged motion imitator. Afterwards, we train a robust real-time humanoid motion imitator in simulation using these refined motions and transfer it to the real humanoid robot in a zero-shot manner. We successfully achieve teleoperation of dynamic whole-body motions in real-world scenarios, including walking, back jumping, kicking, turning, waving, pushing, boxing, etc. To the best of our knowledge, this is the first demonstration to achieve learning-based real-time whole-body humanoid teleoperation.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt12_04">
             11:45-12:00, Paper ThBT12.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1527'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Translating Agent-Environment Interactions from Humans to Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#183414" title="Click to go to the Author Index">
             Shankar, Tanmay
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#375557" title="Click to go to the Author Index">
             Chawla, Chaitanya
            </a>
           </td>
           <td class="r">
            TU Munich, Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396757" title="Click to go to the Author Index">
             Hassan, Almutwakel Khalid
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#179403" title="Click to go to the Author Index">
             Oh, Jean
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1527" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#representation_learning" title="Click to go to the Keyword Index">
               Representation Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Humans are remarkably adept at imitating other people performing tasks, afforded by their ability to abstract away irrelevant details and focus on the task strategy of the demonstrator. In this paper, we take steps towards enabling robots with this ability, and present a framework, TransAct, to do so. TransAct first builds on prior skill learning work to learn abstract representations of common agent-environment interactions in manipulation tasks, ex. a robot pouring from a cup. Given a human demonstration of an unseen unknown task, TransAct then translates the underlying sequence of interactions, i.e., the human task strategy, to a robot learner. Through experiments on real-world human and robot datasets, we demonstrate TransAct's ability to accurately represent diverse agent-environment interactions. Moreover, TransAct empowers robots to consume human task demonstrations and compose corresponding interactions with similar environmental effects to perform the tasks themselves in a zero shot manner, without access to paired demonstrations or dense annotations. We present visualizations of our results at https://sites.google.com/view/interaction-abstractions.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt13">
             <b>
              ThBT13
             </b>
            </a>
           </td>
           <td class="r">
            Room 13
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thbt13" title="Click to go to the Program at a Glance">
             <b>
              Sensor Fusion III
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#233262" title="Click to go to the Author Index">
             Hosseinzadeh, Mehdi
            </a>
           </td>
           <td class="r">
            The Australian Institute for Machine Learning (AIML) -- the University of Adelaide
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt13_01">
             11:00-11:15, Paper ThBT13.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1794'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Event-Free Moving Object Segmentation from Moving Ego Vehicle
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#340024" title="Click to go to the Author Index">
             Zhou, Zhuyun
            </a>
           </td>
           <td class="r">
            University of Burgundy (Universit De Bourgogne), France
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#285487" title="Click to go to the Author Index">
             Wu, Zongwei
            </a>
           </td>
           <td class="r">
            University of Wurzburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#171278" title="Click to go to the Author Index">
             Paudel, Danda Pani
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#116702" title="Click to go to the Author Index">
             Boutteau, Rmi
            </a>
           </td>
           <td class="r">
            Universit De Rouen Normandie
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#339756" title="Click to go to the Author Index">
             Yang, Fan
            </a>
           </td>
           <td class="r">
            Univ. Bourgogne Franche-Comt
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#107640" title="Click to go to the Author Index">
             Van Gool, Luc
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#145365" title="Click to go to the Author Index">
             Timofte, Radu
            </a>
           </td>
           <td class="r">
            University of Wurzburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#158998" title="Click to go to the Author Index">
             Ginhac, Dominique
            </a>
           </td>
           <td class="r">
            Univ Burgundy
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1794" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#sensor_fusion" title="Click to go to the Keyword Index">
               Sensor Fusion
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#intelligent_transportation_systems" title="Click to go to the Keyword Index">
               Intelligent Transportation Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Moving object segmentation (MOS) in dynamic scenes is an important, challenging, but under-explored research topic for autonomous driving, especially for sequences obtained from moving ego vehicles. Most segmentation methods leverage motion cues obtained from optical flow maps. However, since these methods are often based on optical flows that are pre-computed from successive RGB frames, this neglects the temporal consideration of events occurring within the inter-frame, consequently constraining its ability to discern objects exhibiting relative staticity but genuinely in motion. To address these limitations, we propose to exploit event cameras for better video understanding, which provide rich motion cues without relying on optical flow. To foster research in this area, we first introduce a novel large-scale dataset called DSEC-MOS for moving object segmentation from moving ego vehicles, which is the first of its kind. For benchmarking, we select various mainstream methods and rigorously evaluate them on our dataset. Subsequently, we devise EmoFormer, a novel network able to exploit the event data. For this purpose, we fuse the event temporal prior with spatial semantic maps to distinguish genuinely moving objects from the static background, adding another level of dense supervision around our object of interest. Our proposed network relies only on event data for training but does not require event input during inference, making it directly comparable to frame-only methods in terms of efficiency and more widely usable in many application cases. The exhaustive comparison highlights a significant performance improvement of our method over all other methods. The source code and dataset will be made at https://github.com/ZZY-Zhou/DSEC-MOS.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt13_02">
             11:15-11:30, Paper ThBT13.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2442'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Deep Visual Odometry with Events and Frames
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#368688" title="Click to go to the Author Index">
             Pellerito, Roberto
            </a>
           </td>
           <td class="r">
            University of Zurich / ETH
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#270552" title="Click to go to the Author Index">
             Cannici, Marco
            </a>
           </td>
           <td class="r">
            University of Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#244064" title="Click to go to the Author Index">
             Gehrig, Daniel
            </a>
           </td>
           <td class="r">
            University of Zurich / ETH
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#378347" title="Click to go to the Author Index">
             Belhadj, Joris
            </a>
           </td>
           <td class="r">
            European Space Agency, Noordwijk, the Netherlands
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#378355" title="Click to go to the Author Index">
             Dubois-Matra, Olivier
            </a>
           </td>
           <td class="r">
            European Space Agency, Noordwijk, the Netherlands
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#378351" title="Click to go to the Author Index">
             Casasco, Massimo
            </a>
           </td>
           <td class="r">
            European Space Agency, Noordwijk, the Netherlands
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#105662" title="Click to go to the Author Index">
             Scaramuzza, Davide
            </a>
           </td>
           <td class="r">
            University of Zurich
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2442" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#sensor_fusion" title="Click to go to the Keyword Index">
               Sensor Fusion
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Visual Odometry (VO) is crucial for autonomous robotic navigation, especially in GPS-denied environments like planetary terrains. To improve robustness, recent model-based VO systems have begun combining standard and event-based cameras. While event cameras excel in low-light and high-speed motion, standard cameras provide dense and easier-to-track features. However, the field of image- and event-based VO still predominantly relies on model-based methods and is yet to fully integrate recent image-only advancements leveraging end-to-end learning-based architectures. Seamlessly integrating the two modalities remains challenging due to their different nature, one asynchronous, the other not, limiting the potential for a more effective image- and event-based VO. We introduce RAMP-VO, the first end-to-end learned image- and event-based VO system. It leverages novel Recurrent, Asynchronous, and Massively Parallel (RAMP) encoders capable of fusing asynchronous events with image data, providing 8x faster inference and 33% more accurate predictions than existing solutions. Despite being trained only in simulation, RAMP-VO outperforms previous methods on the newly introduced Apollo and Malapert datasets, and on existing benchmarks, where it improves image- and event-based methods by 58.8% and 30.6%, paving the way for robust and asynchronous VO in space.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt13_03">
             11:30-11:45, Paper ThBT13.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3346'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Efficient-PIP: Large-Scale Pixel-Level Aligned Image Pair Generation for Cross-Time Infrared-RGB Translation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#187327" title="Click to go to the Author Index">
             Li, Jian
            </a>
           </td>
           <td class="r">
            National University of Defense Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#391804" title="Click to go to the Author Index">
             Fei, Kexin
            </a>
           </td>
           <td class="r">
            National University of Defense Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#326300" title="Click to go to the Author Index">
             Sun, Yi
            </a>
           </td>
           <td class="r">
            National University of Defense Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397979" title="Click to go to the Author Index">
             Wang, Jie
            </a>
           </td>
           <td class="r">
            National University of Defense Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#295902" title="Click to go to the Author Index">
             Liu, Bokai
            </a>
           </td>
           <td class="r">
            National University of Defense Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#279838" title="Click to go to the Author Index">
             Zhou, Zongtan
            </a>
           </td>
           <td class="r">
            National University of Defense Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#293439" title="Click to go to the Author Index">
             Zheng, Yongbin
            </a>
           </td>
           <td class="r">
            National University of Defense Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#142827" title="Click to go to the Author Index">
             Sun, Zhenping
            </a>
           </td>
           <td class="r">
            National University of Defense Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3346" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#sensor_fusion" title="Click to go to the Keyword Index">
               Sensor Fusion
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#autonomous_vehicle_navigation" title="Click to go to the Keyword Index">
               Autonomous Vehicle Navigation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Generative models are gaining momentum in both academic and industrial applications driven by the availability of large-scale datasets, especially in tasks involving Image-to-Image Translation. Meanwhile, poor human perception of nighttime environment has led to a demand for translation from night-vision infrared to day-vision RGB images. However, collecting such cross-modal training data at the same time is impossible due to the thermal imaging properties of infrared cameras, the challenge lies in constructing image pairs during the day and at night respectively, where the requirement for data alignment poses significant difficulties. In this paper, we propose a Pixel-level aligned Image Pair generation framework PIP to explore efficient colorization of high-resolution infrared images. Specifically, we first construct a 3D high-precision point cloud map for the purpose of establishing the correlation between day and night scenes. Corresponding point clouds of modal images are collected simultaneously during data acquisition to obtain image sensor poses by Global Matching with the map, which allows us to calculate the transformation relationship from infrared to RGB image coordinate systems based on the sensor parameters and depth information of the map. Leveraging the relationship, the pixel values of RGB image is projected onto the infrared image followed by optimization as the colored image. Accordingly, we present a dataset NUDT-PIP, the first of its kind containing large-scale pixel-level aligned cross-time infraredRGB image pairs of complicated real road scenes. Experimental results demonstrate the reliability and strong applicability of our dataset in Image-to-Image Translation. Our code will be released at https://github.com/wjjjjyourFA/NUDT-PIP.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt13_04">
             11:45-12:00, Paper ThBT13.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('789'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Reality Fusion: Robust Real-Time Immersive Mobile Robot Teleoperation with Volumetric Visual Data Fusion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#367043" title="Click to go to the Author Index">
             Li, Ke
            </a>
           </td>
           <td class="r">
            Deutsches Elektronen-Synchrotron DESY
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#394039" title="Click to go to the Author Index">
             Bacher, Reinhard
            </a>
           </td>
           <td class="r">
            Deutsches Elektronen-Synchrotron DESY
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#340312" title="Click to go to the Author Index">
             Schmidt, Susanne
            </a>
           </td>
           <td class="r">
            Universitt Hamburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#394009" title="Click to go to the Author Index">
             Leemans, Wim
            </a>
           </td>
           <td class="r">
            Deutsches Elektronen-Synchrotron DESY
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#196316" title="Click to go to the Author Index">
             Steinicke, Frank
            </a>
           </td>
           <td class="r">
            HCI / University of Hamburg
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab789" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#virtual_reality_and_interfaces" title="Click to go to the Keyword Index">
               Virtual Reality and Interfaces
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#telerobotics_and_teleoperation" title="Click to go to the Keyword Index">
               Telerobotics and Teleoperation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We introduce Reality Fusion, a novel robot teleoperation system that localizes, streams, projects, and merges a typical onboard depth sensor with a photorealistic, high resolution, high framerate, and wide FoV rendering of the complex remote environment represented as 3D Gaussian splats (3DGS). Our framework enables robust egocentric and exocentric robot teleoperation in immersive VR, with the 3DGS effectively extending spatial information of a depth sensor with limited FoV and balancing the trade-off between data streaming costs and data visual quality. We evaluated our framework through a user study with 24 participants, which revealed that Reality Fusion leads to significantly better user performance, situation awareness, and user preferences. To support further research and development, we provide an open-source implementation with an easy-to-replicate custom-made telepresence robot, a high-performance virtual reality 3DGS renderer, and an immersive robot control package. (Source code: url{https://github.com/uhhhci/RealityFusion})
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt14">
             <b>
              ThBT14
             </b>
            </a>
           </td>
           <td class="r">
            Room 14
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thbt14" title="Click to go to the Program at a Glance">
             <b>
              Swarm Robotics
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#190179" title="Click to go to the Author Index">
             Reina, Andreagiovanni
            </a>
           </td>
           <td class="r">
            Universit Libre De Bruxelles
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#191758" title="Click to go to the Author Index">
             Hiraki, Takefumi
            </a>
           </td>
           <td class="r">
            Cluster Metaverse Lab
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt14_01">
             11:00-11:15, Paper ThBT14.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('105'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Decentralized Trajectory Planning for Formation Flight in Unknown and Dense Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#351267" title="Click to go to the Author Index">
             Zeng, Jianxin
            </a>
           </td>
           <td class="r">
            Hunan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#157693" title="Click to go to the Author Index">
             Wang, Yaonan
            </a>
           </td>
           <td class="r">
            Hunan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#157684" title="Click to go to the Author Index">
             Miao, Zhiqiang
            </a>
           </td>
           <td class="r">
            Hunan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#158922" title="Click to go to the Author Index">
             He, Wei
            </a>
           </td>
           <td class="r">
            University of Science and Technology Beijing
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#103003" title="Click to go to the Author Index">
             Wang, Hesheng
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab105" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#swarm_robotics" title="Click to go to the Keyword Index">
               Swarm Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             For aerial swarms, formation flight has been applied in various scenes. However, most existing works do not consider balancing the conflicting requirements among keeping formation, keeping the smoothness of trajectories, and obstacle avoidance within the limited time. To address this issue, we propose a decentralized trajectory planning framework for formation flight in unknown and dense environments. To ensure that feasible trajectories can be found within the limited time, the formation optimization problem is decoupled into formation affine transformation and iterative trajectory generation. Firstly, the optimization problem based on affine transformation is designed to obtain the optimal affine transformation sequence, which provides the formation reference of trajectory optimization. Secondly, the iterative optimization framework of trajectory planning is designed, which balances the conflicting requirements of formation, smooth flight, and obstacle avoidance. Besides, to escape the local minima caused by non-convex dense environments, the method of topological path planning is designed to provide distinctive initial solutions for trajectory optimization. Finally, the proposed methods are proven to be effective through the simulations and real-world experiments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt14_02">
             11:15-11:30, Paper ThBT14.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('139'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Language-Guided Pattern Formation for Swarm Robotics with Multi-Agent Reinforcement Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#322520" title="Click to go to the Author Index">
             Liu, Hsu-Shen
            </a>
           </td>
           <td class="r">
            National Tsing Hua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#343456" title="Click to go to the Author Index">
             Kuroki, So
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#347321" title="Click to go to the Author Index">
             Kozuno, Tadashi
            </a>
           </td>
           <td class="r">
            Omron Sinic X
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#389300" title="Click to go to the Author Index">
             Sun, Wei-Fang
            </a>
           </td>
           <td class="r">
            NVIDIA
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#246632" title="Click to go to the Author Index">
             Lee, Chun-Yi
            </a>
           </td>
           <td class="r">
            National Tsing Hua University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab139" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#swarm_robotics" title="Click to go to the Keyword Index">
               Swarm Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper explores leveraging the vast knowledge encoded in Large Language Models (LLMs) to tackle pattern formation challenges for swarm robotics systems. A new framework, named LGPF (Language-Guided Pattern Formation), is proposed to address these challenges. The framework breaks down the pattern formation into two key components: pattern synthesis and swarm robotics control. For the former, this study utilizes the exceptional few-shot generalizability of LLMs to translate high-level natural language descriptions into the desired spatial pattern coordinates. This approach allows for overcoming previous limitations in representing and designing complex patterns. The framework further employs a centralized training with decentralized execution (CTDE) based multi-agent reinforcement learning (MARL) approach to control the swarm robots in forming the specified pattern while avoiding collisions. The decentralized policies learned with the CTDE-based MARL algorithm consider coordination between robots without direct communication under a partially observable setup. To validate the effectiveness of our framework, we perform extensive experiments in both simulation and real-world environments. These experiments validate LGPF's effectiveness in accurately and safely forming diverse user-specified patterns.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt14_03">
             11:30-11:45, Paper ThBT14.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1018'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Robot Swarm Control Based on Smoothed Particle Hydrodynamics for Obstacle-Unaware Navigation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#381492" title="Click to go to the Author Index">
             Eguchi, Michikuni
            </a>
           </td>
           <td class="r">
            University of Tsukuba
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#274687" title="Click to go to the Author Index">
             Nishimura, Mai
            </a>
           </td>
           <td class="r">
            Omron Sinic X
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392119" title="Click to go to the Author Index">
             Yoshida, Shigeo
            </a>
           </td>
           <td class="r">
            OMRON SINIC X Corporation
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#191758" title="Click to go to the Author Index">
             Hiraki, Takefumi
            </a>
           </td>
           <td class="r">
            Cluster Metaverse Lab
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1018" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#swarm_robotics" title="Click to go to the Keyword Index">
               Swarm Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#path_planning_for_multiple_mobile_robots_or_agents" title="Click to go to the Keyword Index">
               Path Planning for Multiple Mobile Robots or Agents
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#collision_avoidance" title="Click to go to the Keyword Index">
               Collision Avoidance
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robot swarms hold immense potential for performing complex tasks far beyond the capabilities of individual robots. However, the challenge in unleashing this potential is the robots' limited sensory capabilities, which hinder their ability to detect and adapt to unknown obstacles in real-time. To overcome this limitation, we introduce a novel robot swarm control method with an indirect obstacle detector using a smoothed particle hydrodynamics (SPH) model. The indirect obstacle detector can predict the collision with an obstacle and its collision point solely from the robot's velocity information. This approach enables the swarm to effectively and accurately navigate environments without the need for explicit obstacle detection, significantly enhancing their operational robustness and efficiency. Our method's superiority is quantitatively validated through a comparative analysis, showcasing its significant navigation and pattern formation improvements under obstacle-unaware conditions.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt14_04">
             11:45-12:00, Paper ThBT14.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1842'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Miscommunication between Robots Can Improve Group Accuracy in Best-Of-N Decision-Making
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#284825" title="Click to go to the Author Index">
             Zakir, Raina
            </a>
           </td>
           <td class="r">
            Universit Libre De Bruxelles
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#110886" title="Click to go to the Author Index">
             Dorigo, Marco
            </a>
           </td>
           <td class="r">
            Universit Libre De Bruxelles
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#190179" title="Click to go to the Author Index">
             Reina, Andreagiovanni
            </a>
           </td>
           <td class="r">
            Universit Libre De Bruxelles
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1842" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#swarm_robotics" title="Click to go to the Keyword Index">
               Swarm Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#distributed_robot_systems" title="Click to go to the Keyword Index">
               Distributed Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Making fast and accurate consensus decisions through local communication and decentralised control in a swarm of simple robots can be a very challenging endeavour. In swarms of robots with limited capabilities, consensus decisions can be made using simple voting rules. In our study, the robots use rules based on the cross-inhibition model, which describes a voting mechanism observed in the house-hunting honeybee, that has been shown to efficiently allow consensus achievement in distributed robotic systems. The cross-inhibition mechanism has been shown to lead to a highly stable consensus, preventing the correction of possible group decision errors which can happen, for example, due to high noise in robots estimations. In this paper, we investigate the impact of miscommunication on the speed-accuracy trade-off in consensus decision-making in the context of a binary discrimination problemi.e., choosing collectively the best of two options. We evaluate the accuracy of decision-making theoretically, using continuous and finite-size models, and experimentally in a collective perception scenario, using swarms of 100 simulated robots and 50 real Kilobots. Our study suggests that a certain level of miscommunication (or communication noise) among agents can increase the decisions accuracy and, thus, can serve an important functional role in making collective decisions in robot swarms.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thi4n">
             <b>
              ThI4N
             </b>
            </a>
           </td>
           <td class="r">
            Poster Area
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thi4n" title="Click to go to the Program at a Glance">
             <b>
              Interactive Session 4
             </b>
            </a>
           </td>
           <td class="r">
            Interactive Poster session
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thf6o">
             <b>
              ThF6O
             </b>
            </a>
           </td>
           <td class="r">
            Auditorium
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thf6o" title="Click to go to the Program at a Glance">
             <b>
              Forum 6 - Empowering Diverse Voices in Robotics
             </b>
            </a>
           </td>
           <td class="r">
            Forum
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thf6o_01">
             09:00-12:00, Paper ThF6O.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             Empowering Diverse Voices in Robotics
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#240076" title="Click to go to the Author Index">
             Ashour, Reem
            </a>
           </td>
           <td class="r">
            Khalifa University of Science and Technology
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thf7o">
             <b>
              ThF7O
             </b>
            </a>
           </td>
           <td class="r">
            Room 17/18
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thf7o" title="Click to go to the Program at a Glance">
             <b>
              Forum 7 - Human-Avatars Symbiosis: Can You Imagine a Future Society Where
              <br/>
              You Can Remotely Control Multiple Avatars?
             </b>
            </a>
           </td>
           <td class="r">
            Forum
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#101694" title="Click to go to the Author Index">
             Hagita, Norihiro
            </a>
           </td>
           <td class="r">
            ATR
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#203329" title="Click to go to the Author Index">
             Horikawa, Yukiko
            </a>
           </td>
           <td class="r">
            Advanced Telecommunications Research Institute International
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thf7o_01">
             09:00-12:00, Paper ThF7O.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             Human-Avatars Symbiosis  Can You Imagine a Future Society Where You Can Remotely Control Multiple Avatars? 
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#101694" title="Click to go to the Author Index">
             Hagita, Norihiro
            </a>
           </td>
           <td class="r">
            ATR
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#101839" title="Click to go to the Author Index">
             Dario, Paolo
            </a>
           </td>
           <td class="r">
            Scuola Superiore Sant'Anna
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#103488" title="Click to go to the Author Index">
             Sanfeliu, Alberto
            </a>
           </td>
           <td class="r">
            Universitat Politcnica De Cataluyna
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#101596" title="Click to go to the Author Index">
             Ishiguro, Hiroshi
            </a>
           </td>
           <td class="r">
            Osaka University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#203329" title="Click to go to the Author Index">
             Horikawa, Yukiko
            </a>
           </td>
           <td class="r">
            Advanced Telecommunications Research Institute International
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thp3l">
             <b>
              ThP3L
             </b>
            </a>
           </td>
           <td class="r">
            Auditorium
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thp3l" title="Click to go to the Program at a Glance">
             <b>
              Plenary 3: Embodiment of AI and Biomechanis/Neuroscience, by Yoshihiko
              <br/>
              Nakamura
             </b>
            </a>
           </td>
           <td class="r">
            Plenary session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#101725" title="Click to go to the Author Index">
             Fiorini, Paolo
            </a>
           </td>
           <td class="r">
            University of Verona
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thp3l_01">
             12:00-13:00, Paper ThP3L.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             Embodiment of AI and Biomechanis/Neuroscience
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#100082" title="Click to go to the Author Index">
             Nakamura, Yoshihiko
            </a>
           </td>
           <td class="r">
            Mohamed bin Zayed University of Artificial Intelligence
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thlu_br">
             <b>
              ThLU_BR
             </b>
            </a>
           </td>
           <td class="r">
            Auditorium
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thlu_br" title="Click to go to the Program at a Glance">
             <b>
              Award Luncheon (bentos)
             </b>
            </a>
           </td>
           <td class="r">
            Session
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thk3n">
             <b>
              ThK3N
             </b>
            </a>
           </td>
           <td class="r">
            Auditorium
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thk3n" title="Click to go to the Program at a Glance">
             <b>
              Keynote Session 3 - AI &amp; Robotics
             </b>
            </a>
           </td>
           <td class="r">
            Keynote session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#109813" title="Click to go to the Author Index">
             Nagai, Yukie
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thk3n_02">
             14:00-15:30, Paper ThK3N.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             Frugal AI
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#105982" title="Click to go to the Author Index">
             Caputo, Barbara
            </a>
           </td>
           <td class="r">
            Sapienza University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thk3n_03">
             14:00-15:30, Paper ThK3N.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             On Complex Reasoning, Agency, and Virtual Life Via Next Generation Foundation Models Beyond Lingual and Visual Intelligence -- How a Young University Helps Lead a Young Country in the Premier League of Global AI
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#109265" title="Click to go to the Author Index">
             Xing, Eric
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thk3n_03">
             14:00-15:30, Paper ThK3N.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             -
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#351534" title="Click to go to the Author Index">
             Debbah, Merouane
            </a>
           </td>
           <td class="r">
            Technology Innovation Institute
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thk3n_04">
             14:00-15:30, Paper ThK3N.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             -
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#106951" title="Click to go to the Author Index">
             Zhang, Jianwei
            </a>
           </td>
           <td class="r">
            University of Hamburg
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thk3n_05">
             14:00-15:30, Paper ThK3N.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             Soft Robot Control
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#129946" title="Click to go to the Author Index">
             Monje, Concepcin A.
            </a>
           </td>
           <td class="r">
            University Carlos III of Madrid
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thk3n_06">
             14:00-15:30, Paper ThK3N.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             Towards Generalist Robots: Embodied Large Model System Empowered by Synthetic Data
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#155911" title="Click to go to the Author Index">
             Wang, He
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thpi5t1">
             <b>
              ThPI5T1
             </b>
            </a>
           </td>
           <td class="r">
            Room 1
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thpi5t1" title="Click to go to the Program at a Glance">
             <b>
              Legged Robot Systems II
             </b>
            </a>
           </td>
           <td class="r">
            Teaser Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#114431" title="Click to go to the Author Index">
             Huang, Guoquan
            </a>
           </td>
           <td class="r">
            University of Delaware
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t1_01">
             15:30-16:30, Paper ThPI5T1.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('561'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Research on Autonomous Navigation of Dual-Mode Wheel-Legged Robot
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#393117" title="Click to go to the Author Index">
             Wang, Wen
            </a>
           </td>
           <td class="r">
            Hohai University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#361941" title="Click to go to the Author Index">
             Xu, Xiaobin
            </a>
           </td>
           <td class="r">
            Hohai University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392920" title="Click to go to the Author Index">
             Chen, Ziheng
            </a>
           </td>
           <td class="r">
            Hohai University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#393055" title="Click to go to the Author Index">
             Yang, Jian
            </a>
           </td>
           <td class="r">
            Yangzhou University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392925" title="Click to go to the Author Index">
             Ran, Yingying
            </a>
           </td>
           <td class="r">
            Hohai University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#365100" title="Click to go to the Author Index">
             Tan, Zhiying
            </a>
           </td>
           <td class="r">
            Hohai University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#294603" title="Click to go to the Author Index">
             Luo, Minzhou
            </a>
           </td>
           <td class="r">
            Hohai University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab561" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#climbing_robots" title="Click to go to the Keyword Index">
               Climbing Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#energy_and_environment_aware_automation" title="Click to go to the Keyword Index">
               Energy and Environment-Aware Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In order to improve the terrain adaptability and energy efficiency of wheel-legged robot in complex environment, a dual-mode navigation system based on robot energy consumption model is proposed. Firstly, the obstacle trafficability is evaluated according to the maximum obstacle crossing capability of the robot, and the two-dimensional grid map is preprocessed. Secondly, the established energy consumption model is integrated into the evaluation function of A* algorithm, and the eight-adjacency expansion mode is improved to search the surrounding nodes according to the obstacle characteristics of the robot. In the obstacle-crossing area, the obstacle-crossing and obstacle-bypassing modes are intelligently switched based on the principle of minimum energy consumption. Finally, a dual-mode robot navigation system is built, and the experimental results show that the proposed navigation system reduces the average energy consumption, path length, and steering angle by 16.8%, 24.7%, and 31.18%, respectively.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t1_02">
             15:30-16:30, Paper ThPI5T1.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1662'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Real-Time Coupled Centroidal Motion and Footstep Planning for Biped Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#246976" title="Click to go to the Author Index">
             Bartlett, Tara
            </a>
           </td>
           <td class="r">
            The University of Sydney
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#109851" title="Click to go to the Author Index">
             Manchester, Ian
            </a>
           </td>
           <td class="r">
            University of Sydney
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1662" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#humanoid_and_bipedal_locomotion" title="Click to go to the Keyword Index">
               Humanoid and Bipedal Locomotion
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#dynamics" title="Click to go to the Keyword Index">
               Dynamics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents an algorithm that finds a centroidal motion and footstep plan for a acf{SLIP}-like bipedal robot model substantially faster than real-time. This is achieved with a novel representation of the dynamic footstep planning problem, where each point in the environment is considered a potential foothold that can apply a force to the center of mass to keep on on a desired trajectory. For a biped, up to two such footholds per time step must be selected, and we approximate this cardinality constraint with an iteratively reweighted l_1-norm minimization. Along with a linearizing approximation of an angular momentum constraint, this results in a quadratic program can be solved for a contact schedule and center of mass trajectory with automatic gait discovery. A 2~s planning horizon with 13 time steps and 20 surfaces available at each time is solved in 142~ms, roughly ten times faster than comparable existing methods in the literature. We demonstrate the versatility of this program in a variety of simulated environments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t1_03">
             15:30-16:30, Paper ThPI5T1.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3312'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Understanding How a 3-Dimensional ZMP Exactly Decouples the Horizontal and Vertical Dynamics of the CoM-ZMP Model
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#273116" title="Click to go to the Author Index">
             Onishi, Yuki
            </a>
           </td>
           <td class="r">
            Tokyo Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#102023" title="Click to go to the Author Index">
             Kajita, Shuuji
            </a>
           </td>
           <td class="r">
            Chubu University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3312" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#humanoid_and_bipedal_locomotion" title="Click to go to the Keyword Index">
               Humanoid and Bipedal Locomotion
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#motion_control" title="Click to go to the Keyword Index">
               Motion Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The CoM-ZMP model represents the dominant behaviour of bipedal locomotion with surface contact. However, once the centre of mass (CoM) position goes out of a predefined spatial plane, the horizontal dynamics of the model can couple with its vertical dynamics to be nonlinear. This study theoretically investigates the properties of the 3-dimensional zero moment point (ZMP), lying apart from the actual ground to resolve the coupling. The presented discussion includes the compatibility of the 3D ZMP with ZMPs used in preceding research, such as the linear inverted pendulum mode, the existence of a virtual repellent point considering the arbitrary vertical CoM motion, the parameter invariance of the CoM-ZMP model, and feasible regions of the ZMP.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t1_04">
             15:30-16:30, Paper ThPI5T1.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3353'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Online Determination of Legged Kinematics
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#379481" title="Click to go to the Author Index">
             Burgul, Chinmay
            </a>
           </td>
           <td class="r">
            University of Delaware
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#246864" title="Click to go to the Author Index">
             Lee, Woosik
            </a>
           </td>
           <td class="r">
            University of Delaware
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#202853" title="Click to go to the Author Index">
             Geneva, Patrick
            </a>
           </td>
           <td class="r">
            University of Delaware
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#114431" title="Click to go to the Author Index">
             Huang, Guoquan
            </a>
           </td>
           <td class="r">
            University of Delaware
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3353" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#sensor_fusion" title="Click to go to the Keyword Index">
               Sensor Fusion
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#kinematics" title="Click to go to the Keyword Index">
               Kinematics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Legged robots are emerging, and legged locomotion is in critical need, which requires precise leg-body kinematics to execute control commands or plan motion trajectories. This paper proposes online state estimation to determine legged kinematics of robots with an arbitrary number of legs, which includes the kinematic parameters of the leg-body transformation, time offset and the leg link lengths. In particular, we advocate an in-place dance gait for kinematic determination where the toes remain static on the ground and serve as static landmarks as in SLAM. As a visual-inertial sensor is typically available onboard robot and located at the floating base, we leverage efficient MSCKF-based visual-inertial navigation to estimate legged kinematics. To this end, we analytically derive the legged kinematic measurements and tightly fuse them along with visualinertial measurements for MSCKF update of both the legs kinematics and bodys motion. The proposed method has been extensively validated in both simulations and experiments with different quadrupeds, showing its robustness and accuracy
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t1_05">
             15:30-16:30, Paper ThPI5T1.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3419'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              HILMA-Res: A General Hierarchical Framework Via Residual RL for Combining Quadrupedal Locomotion and Manipulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#341937" title="Click to go to the Author Index">
             Huang, Xiaoyu
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#325511" title="Click to go to the Author Index">
             Liao, Qiayuan
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#341932" title="Click to go to the Author Index">
             Ni, Yiming
            </a>
           </td>
           <td class="r">
            University of California Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#237641" title="Click to go to the Author Index">
             Li, Zhongyu
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#268839" title="Click to go to the Author Index">
             Smith, Laura
            </a>
           </td>
           <td class="r">
            UC Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#156706" title="Click to go to the Author Index">
             Levine, Sergey
            </a>
           </td>
           <td class="r">
            UC Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#219706" title="Click to go to the Author Index">
             Peng, Xue Bin
            </a>
           </td>
           <td class="r">
            Simon Fraser University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#138299" title="Click to go to the Author Index">
             Sreenath, Koushil
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3419" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This work presents HILMA-Res, a hierarchical framework leveraging reinforcement learning to tackle manipulation tasks while performing continuous locomotion using quadrupedal robots. Unlike most previous efforts that focus on solving a specific task, HILMA-Res is designed to be general for various loco-manipulation tasks that require quadrupedal robots to maintain sustained mobility. The novel design of this framework tackles the challenges of integrating continuous locomotion control and manipulation using legs. It develops an operational space locomotion controller that can track arbitrary robot end-effector (toe) trajectories while walking at different velocities. This controller is designed to be general to different downstream tasks, and therefore, can be utilized in high-level manipulation planning policy to address specific tasks. To demonstrate the versatility of this framework, we utilize HILMA-Res to tackle several challenging loco-manipulation tasks using a quadrupedal robot in the real world. These tasks span from leveraging state-based policy to vision-based policy, from training purely from the simulation data to learning from real-world data. In these tasks, HILMA-Res shows better performance than other methods.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t1_06">
             15:30-16:30, Paper ThPI5T1.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3513'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              StaccaToe: A Single-Leg Robot That Mimics the Human Leg and Toe
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#306310" title="Click to go to the Author Index">
             Perera, Kankanige Nisal Minula
            </a>
           </td>
           <td class="r">
            University of Massachusetts Amherst
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#341431" title="Click to go to the Author Index">
             Yu, Shangqun
            </a>
           </td>
           <td class="r">
            University of Massachusetts Amherst
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#289078" title="Click to go to the Author Index">
             Marew, Daniel
            </a>
           </td>
           <td class="r">
            University of Massachusetts Amherst
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#338206" title="Click to go to the Author Index">
             Tang, Mack
            </a>
           </td>
           <td class="r">
            University of Maryland College Park
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#341149" title="Click to go to the Author Index">
             Suzuki, Ken
            </a>
           </td>
           <td class="r">
            University of Massachusetts Amherst
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#399151" title="Click to go to the Author Index">
             McCormack, Aidan
            </a>
           </td>
           <td class="r">
            University of Massachusetts Amherst
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#286851" title="Click to go to the Author Index">
             Zhu, Shifan
            </a>
           </td>
           <td class="r">
            University of Massachusetts Amherst
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#111431" title="Click to go to the Author Index">
             Kim, Yong-Jae
            </a>
           </td>
           <td class="r">
            Korea University of Technology and Education
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#168080" title="Click to go to the Author Index">
             Kim, Donghyun
            </a>
           </td>
           <td class="r">
            University of Massachusetts Amherst
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3513" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mechanism_design" title="Click to go to the Keyword Index">
               Mechanism Design
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#whole_body_motion_planning_and_control" title="Click to go to the Keyword Index">
               Whole-Body Motion Planning and Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We introduce StaccaToe, a human-scale, electric motor-powered single-leg robot designed to rival the agility of human locomotion through two distinctive attributes: an actuated toe and a co-actuation configuration inspired by the human leg. Leveraging the foundational design of HyperLegs lower leg mechanism, we develop a stand-alone robot by incorporating new link designs, custom-designed power electronics, and a refined control system. Unlike previous jumping robots that rely on either special mechanisms (e.g., springs and clutches) or hydraulic/pneumatic actuators, StaccaToe employs electric motors without energy storage mechanisms. This choice underscores our ultimate goal of developing a practical, high-performance humanoid robot capable of human-like, stable walking as well as explosive dynamic movements. In this paper, we aim to empirically evaluate the balance capability and the exertion of explosive ground reaction forces of our toe and co-actuation mechanisms. Throughout extensive hardware and controller development, StaccaToe showcases its control fidelity by demonstrating a balanced tip-toe stance and dynamic jump. This study is significant for three key reasons: 1) StaccaToe represents the first human-scale, electric motor-driven single-leg robot to execute dynamic maneuvers without relying on specialized mechanisms; 2) our research provides empirical evidence of the benefits of replicating critical human leg attributes in robotic design; and 3) we explain the design process for creating agile legged robots, the details that have been scantily covered in academic literature.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t1_07">
             15:30-16:30, Paper ThPI5T1.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3528'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Structural Optimization of Lightweight Bipedal Robot Via SERL
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#365615" title="Click to go to the Author Index">
             Cheng, Yi
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#399254" title="Click to go to the Author Index">
             Han, Chenxi
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#399269" title="Click to go to the Author Index">
             Min, Yuheng
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#189167" title="Click to go to the Author Index">
             Liu, Houde
            </a>
           </td>
           <td class="r">
            Shenzhen Graduate School, Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#232600" title="Click to go to the Author Index">
             Ye, Linqi
            </a>
           </td>
           <td class="r">
            Shanghai University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#399253" title="Click to go to the Author Index">
             Liu, Hang
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3528" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Designing a bipedal robot is a complex and challenging task, especially when dealing with a multitude of structural parameters. Traditional design methods often rely on human intuition and experience. However, such approaches are time-consuming, labor-intensive, lack theoretical guidance, and struggle to obtain optimal design results within vast design spaces, thus failing to fully exploit the inherent performance potential of robots. In this context, this paper introduces the SERL (Structure Evolution via Reinforcement Learning) algo- rithm, which combines reinforcement learning for locomotion task with evolution algorithms. The aim is to identify the opti- mal parameter combinations within a given multidimensional design space. Through the SERL algorithm, we successfully designed a bipedal robot named Wow Orin, where the optimal leg length is obtained through optimization based on body structure and motor torque. We have experimentally validated the effectiveness of the SERL algorithm, which is capable of optimizing the best structure within specified design space and task conditions. Additionally, to assess the performance gap between our designed robot and the current state-of-the-art robots, we compared Wow Orin with mainstream bipedal robots Cassie and Unitree H1 in the same simulated environment. A series of experimental results demonstrate the outstanding performance of Wow Orin, further validating the feasibility of applying the SERL algorithm to practical design.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t1_08">
             15:30-16:30, Paper ThPI5T1.8
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3529'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Quadruped Robot Traversing 3D Complex Environments with Limited Perception
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#365615" title="Click to go to the Author Index">
             Cheng, Yi
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#399253" title="Click to go to the Author Index">
             Liu, Hang
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#382895" title="Click to go to the Author Index">
             Pan, Guoping
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#189167" title="Click to go to the Author Index">
             Liu, Houde
            </a>
           </td>
           <td class="r">
            Shenzhen Graduate School, Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#232600" title="Click to go to the Author Index">
             Ye, Linqi
            </a>
           </td>
           <td class="r">
            Shanghai University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3529" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Traversing 3-D complex environments has always been a significant challenge for legged locomotion. Existing methods typically rely on external sensors such as vision and lidar to preemptively react to obstacles by acquiring environmental information. However, in scenarios like nighttime or dense forests, external sensors often fail to function properly, necessitating robots to rely on proprioceptive sensors to perceive diverse obstacles in the environment and respond promptly. This task is undeniably challenging. Our research finds that methods based on collision detection can enhance a robots per- ception of environmental obstacles. In this work, we propose an end-to-end learning-based quadruped robot motion controller that relies solely on proprioceptive sensing. This controller can accurately detect, localize, and agilely respond to collisions in unknown and complex 3D environments, thereby improving the robots traversability in complex environments. We demonstrate in both simulation and real-world experiments that our method enables quadruped robots to successfully traverse challenging obstacles in various complex environments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t1_09">
             15:30-16:30, Paper ThPI5T1.9
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('281'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Development of Bidirectional Series Elastic Actuator with Torsion Coil Spring and Implementation to the Legged Robot
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#388432" title="Click to go to the Author Index">
             Koda, Yuta
            </a>
           </td>
           <td class="r">
            Sony Interactive Entertainment
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#390159" title="Click to go to the Author Index">
             Osawa, Hiroshi
            </a>
           </td>
           <td class="r">
            Sony Interactive Entertainment
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#307293" title="Click to go to the Author Index">
             Nagatsuka, Norio
            </a>
           </td>
           <td class="r">
            Sony Interactive Entertainment
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#390154" title="Click to go to the Author Index">
             Kariya, Shinichi
            </a>
           </td>
           <td class="r">
            Sony Interactive Entertainment
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#255238" title="Click to go to the Author Index">
             Inagawa, Taeko
            </a>
           </td>
           <td class="r">
            Sony Interactive Entertainment
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#286772" title="Click to go to the Author Index">
             Ishizuka, Kensaku
            </a>
           </td>
           <td class="r">
            Sony Interactive Entertainment
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab281" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#actuation_and_joint_mechanisms" title="Click to go to the Keyword Index">
               Actuation and Joint Mechanisms
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mechanism_design" title="Click to go to the Keyword Index">
               Mechanism Design
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Many studies have been conducted on Series Elastic Actuators (SEA) for robot joints because they are effective in terms of flexibility, safety, and energy efficiency. The ability of SEA to robustly handle unexpected disturbances has raised expectations for practical applications in environments where robots interact with humans. On the other hand, the development and commercialization of small robots for indoor entertainment applications is also actively underway, and it is thought that by using SEA in these robots, dynamic movements such as jumping and running can be realized. In this work, we developed a small and lightweight SEA using coil springs as elastic elements. By devising a method for fixing the coil spring, it is possible to absorb shock and perform highly accurate force measurement in both rotational directions with a simple structure. In addition, to verify the effectiveness of the developed SEA, we created a small single-legged robot with SEA implemented in the three joints of the hip, knee, and ankle, and we conducted a drop test. By adjusting the initial posture and control gain of each joint, we confirmed that flexible landing and continuous hopping are possible with simple PD position control. The measurement results showed that SEA is effective in terms of shock absorption and energy reuse. This work was performed for research purposes only.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t1_10">
             15:30-16:30, Paper ThPI5T1.10
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2284'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Evaluation and Design Recommendations for a Folding Morphing-Wheg Robot for Nuclear Characterisation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#340542" title="Click to go to the Author Index">
             Murphy, Dominic
            </a>
           </td>
           <td class="r">
            University of the West of England
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#112409" title="Click to go to the Author Index">
             Giuliani, Manuel
            </a>
           </td>
           <td class="r">
            Kempten University of Applied Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#130081" title="Click to go to the Author Index">
             Bremner, Paul
            </a>
           </td>
           <td class="r">
            University of the West of England
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2284" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#field_robots" title="Click to go to the Keyword Index">
               Field Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#tendon_wire_mechanism" title="Click to go to the Keyword Index">
               Tendon/Wire Mechanism
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper explores the design and development of a folding robot required to survey and characterize nuclear facilities only accessible via 150 mm diameter entry ducts. The enclosed legacy facilities at old nuclear sites like Sellafield in the UK have this sort of limited access. When a site reaches the end of its operational life, it must be decommissioned and the resulting waste material must be safely disposed of. The condition, radioactive characteristics, and accessibility of the enclosed environments are unknown; for decommissioning to occur, these environments must be mapped and characterized. For a robot to carry out this task, one of the key requirements is the ability of the robot to traverse rough terrain and obstacles that could be found inside the facility. To accommodate this, while fitting through the entry duct, the chosen design utilizes morphing whegs (i.e., wheel-legs) for locomotion. These are shape-changing wheels that can open out into a set of legs that rotate around an axle, allowing greater traction, diameter, and object traversal ability than wheels alone. The design and morphology of a folding morphing-wheg robot for nuclear characterization, as well as the manufacture and testing of a prototype, is discussed in this paper. A preliminary evaluation of the robot has shown it is capable of climbing up a maximum step height of 150 mm while having a wheel dimension of 100 mm and being able to fit through a 150 mm duct.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t1_11">
             15:30-16:30, Paper ThPI5T1.11
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3117'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Safety-Critical Autonomous Inspection of Distillation Columns Using Quadrupedal Robots Equipped with Roller Arms
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#142294" title="Click to go to the Author Index">
             Lee, Jaemin
            </a>
           </td>
           <td class="r">
            North Carolina State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#201505" title="Click to go to the Author Index">
             Kim, Jeeseop
            </a>
           </td>
           <td class="r">
            Caltech
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#134049" title="Click to go to the Author Index">
             Ames, Aaron
            </a>
           </td>
           <td class="r">
            Caltech
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3117" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#industrial_robots" title="Click to go to the Keyword Index">
               Industrial Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#motion_control" title="Click to go to the Keyword Index">
               Motion Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper proposes a comprehensive framework designed for the autonomous inspection of complex environments, with a specific focus on multi-tiered settings such as distillation column trays. Leveraging quadruped robots equipped with roller arms, and through the use of onboard perception, we integrate essential motion components including: locomotion, safe and dynamic transitions between trays, and intermediate motions that bridge a variety of motion primitives. Given the slippery and confined nature of column trays, it is critical to ensure safety of the robot during inspection, therefore we employ a safety filter and footstep re-planning based upon control barrier function representations of the environment. Our framework integrates all system components into a state machine encoding the developed safety-critical planning and control elements to guarantee safety-critical autonomy, enabling autonomous and safe navigation and inspection of distillation columns. Experimental validation in an environment, consisting of industrial-grade chemical distillation trays, highlights the effectiveness of our multi-layered architecture.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t1_12">
             15:30-16:30, Paper ThPI5T1.12
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3215'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Learning Visual Quadrupedal Loco-Manipulation from Demonstrations
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#373748" title="Click to go to the Author Index">
             He, Zhengmao
            </a>
           </td>
           <td class="r">
            The Hong Kong University of Science and Technology (Guangzhou)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398801" title="Click to go to the Author Index">
             Lei, Kun
            </a>
           </td>
           <td class="r">
            Shanghai Qizhi Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#344963" title="Click to go to the Author Index">
             Ze, Yanjie
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#138299" title="Click to go to the Author Index">
             Sreenath, Koushil
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#237641" title="Click to go to the Author Index">
             Li, Zhongyu
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#220412" title="Click to go to the Author Index">
             Xu, Huazhe
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3215" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#mobile_manipulation" title="Click to go to the Keyword Index">
               Mobile Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Quadruped robots are progressively being integrated into human environments. Despite the growing locomotion capabilities of quadrupedal robots, their interaction with objects in realistic scenes is still limited. While additional robotic arms on quadrupedal robots enable manipulating objects, they are sometimes redundant given that a quadruped robot is essentially a mobile unit equipped with four limbs, each possessing 3 degrees of freedom (DoFs). Hence, we aim to empower a quadruped robot to execute real-world manipulation tasks using only its legs. We decompose the loco-manipulation process into a low-level reinforcement learning (RL)-based controller and a high-level Behavior Cloning (BC)-based planner. By parameterizing the manipulation trajectory, we synchronize the efforts of the upper and lower layers, thereby leveraging the advantages of both RL and BC. Our approach is validated through simulations and real-world experiments, demonstrating the robot's ability to perform tasks that demand mobility and high precision, such as lifting a basket from the ground while moving, closing a dishwasher, pressing a button, and pushing a door.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t1_13">
             15:30-16:30, Paper ThPI5T1.13
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('692'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              PA-LOCO: Learning Perturbation-Adaptive Locomotion for Quadruped Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#378968" title="Click to go to the Author Index">
             Xiao, Zhiyuan
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#353912" title="Click to go to the Author Index">
             Zhang, Xinyu
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392570" title="Click to go to the Author Index">
             Zhou, Xiang
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#280085" title="Click to go to the Author Index">
             Zhang, Qingrui
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab692" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#bioinspired_robot_learning" title="Click to go to the Keyword Index">
               Bioinspired Robot Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#transfer_learning" title="Click to go to the Keyword Index">
               Transfer Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Locomotion control is still a challenging task for quadruped robots traversing diverse terrains amidst unforeseen disturbances. Recently, privileged learning has been employed to learn reliable and robust quadrupedal locomotion over various terrains based on a teacher-student architecture. However, its one-encoder structure is not adequate in addressing external force perturbations. The student policy would experience inevitable performance degradation due to the feature embedding discrepancy between the feature encoder of the teacher policy and the one of the student policy. Hence, this paper presents a privileged learning framework with multiple feature encoders and a residual policy network for robust and reliable quadruped locomotion subject to various external perturbations. The multi-encoder structure can decouple latent features from different privileged information, ultimately leading to enhanced performance of the learned policy in terms of robustness, stability, and reliability. The efficiency of the proposed feature encoding module is analyzed in depth using extensive simulation data. The introduction of the residual policy network helps mitigate the performance degradation experienced by the student policy that attempts to clone the behaviors of a teacher policy. The proposed framework is evaluated on a Unitree GO1 robot, showcasing its performance enhancement over the state-of-the-art privileged learning algorithm through extensive experiments conducted on diverse terrains. Ablation studies are conducted to illustrate the efficiency of the residual policy network.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t1_14">
             15:30-16:30, Paper ThPI5T1.14
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3203'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Robust Agility Via Learned Zero Dynamics Policies
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#244314" title="Click to go to the Author Index">
             Csomay-Shanklin, Noel
            </a>
           </td>
           <td class="r">
            California Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#286520" title="Click to go to the Author Index">
             Compton, William
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#300052" title="Click to go to the Author Index">
             Jimenez Rodriguez, Ivan Dario
            </a>
           </td>
           <td class="r">
            California Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#202340" title="Click to go to the Author Index">
             Ambrose, Eric
            </a>
           </td>
           <td class="r">
            California Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#236412" title="Click to go to the Author Index">
             Yue, Yisong
            </a>
           </td>
           <td class="r">
            California Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#116276" title="Click to go to the Author Index">
             Ames, Aaron
            </a>
           </td>
           <td class="r">
            California Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3203" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#underactuated_robots" title="Click to go to the Keyword Index">
               Underactuated Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#machine_learning_for_robot_control" title="Click to go to the Keyword Index">
               Machine Learning for Robot Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We study the design of robust and agile controllers for hybrid underactuated systems. Our approach breaks down the task of creating a stabilizing controller into: 1) learning a mapping that is invariant under optimal control, and 2) driving the actuated coordinates to the output of that mapping. This approach, termed Zero Dynamics Policies, exploits the structure of underactuation by restricting the inputs of the target mapping to the subset of degrees of freedom that cannot be directly actuated, thereby achieving significant dimension reduction. Furthermore, we retain the stability and constraint satisfaction of optimal control while reducing the online computational overhead. We prove that controllers of this type stabilize hybrid underactuated systems and experimentally validate our approach on the 3D hopping platform, ARCHER. Over the course of 3000 hops the proposed framework demonstrates robust agility, maintaining stable hopping while rejecting disturbances on rough terrain.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t1_15">
             15:30-16:30, Paper ThPI5T1.15
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3440'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Preliminary Result of Cury: A Backdrivable Leg Design Using Linear Actuators
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#399128" title="Click to go to the Author Index">
             Guan, Zhongtao
            </a>
           </td>
           <td class="r">
            ShanghaiTech University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#399055" title="Click to go to the Author Index">
             Chen, Yiming
            </a>
           </td>
           <td class="r">
            Shanghaitech University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#399062" title="Click to go to the Author Index">
             Zhu, Junlei
            </a>
           </td>
           <td class="r">
            ShanghaiTech University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#399149" title="Click to go to the Author Index">
             Hu, Yu
            </a>
           </td>
           <td class="r">
            Shanghaitech University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#252486" title="Click to go to the Author Index">
             Bai, Weibang
            </a>
           </td>
           <td class="r">
            Shanghaitech University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#399101" title="Click to go to the Author Index">
             Chen, Jiahao
            </a>
           </td>
           <td class="r">
            ShanghaiTech University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3440" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#hardware_software_integration_in_robotics" title="Click to go to the Keyword Index">
               Hardware-Software Integration in Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#engineering_for_robotic_systems" title="Click to go to the Keyword Index">
               Engineering for Robotic Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#humanoid_robot_systems" title="Click to go to the Keyword Index">
               Humanoid Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper reports the design, simulation, and experiment of a robotic leg prototype named Cury, which has the potential to achieve minimal clearance and excellent backdrivability. Inspired by human walking data, the actuator design incorporates four-bar linkages and ball screws, and is further optimized to meet the torque requirement. The Webots simulation is used to obtain the closed-loop chain description of the robotic leg from fits URDF specification, and this simulation is used to assess the actuator output requirements at a given predefined trajectory. Leveraging customized ac motors and drives, Cury demonstrates satisfactory trajectory tracking performance using a simple controller. The motor drive design files and Webots simulation files are open-sourced.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thpi5t2">
             <b>
              ThPI5T2
             </b>
            </a>
           </td>
           <td class="r">
            Room 2
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thpi5t2" title="Click to go to the Program at a Glance">
             <b>
              Soft and Flexible Robotics I
             </b>
            </a>
           </td>
           <td class="r">
            Teaser Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#128250" title="Click to go to the Author Index">
             Wen, Li
            </a>
           </td>
           <td class="r">
            Beihang University
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#101975" title="Click to go to the Author Index">
             Althoefer, Kaspar
            </a>
           </td>
           <td class="r">
            Queen Mary University of London
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t2_01">
             15:30-16:30, Paper ThPI5T2.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2120'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              To Help or Not to Help: LLM-Based Attentive Support for Human-Robot Group Interactions
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#202185" title="Click to go to the Author Index">
             Tanneberg, Daniel
            </a>
           </td>
           <td class="r">
            Honda Research Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#228590" title="Click to go to the Author Index">
             Ocker, Felix
            </a>
           </td>
           <td class="r">
            Honda Research Institute Europe
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#130472" title="Click to go to the Author Index">
             Hasler, Stephan
            </a>
           </td>
           <td class="r">
            Honda Research Institute Europe
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#375520" title="Click to go to the Author Index">
             Deigmoeller, Joerg
            </a>
           </td>
           <td class="r">
            Honda Research Institute Europe
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#106319" title="Click to go to the Author Index">
             Belardinelli, Anna
            </a>
           </td>
           <td class="r">
            Honda Research Institute Europe
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#303168" title="Click to go to the Author Index">
             Wang, Chao
            </a>
           </td>
           <td class="r">
            Honda Research Institute Europe GmbH
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#129258" title="Click to go to the Author Index">
             Wersing, Heiko
            </a>
           </td>
           <td class="r">
            Honda Research Institute Europe
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#141889" title="Click to go to the Author Index">
             Sendhoff, Bernhard
            </a>
           </td>
           <td class="r">
            Honda Research Institute Europe GmbH
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#105270" title="Click to go to the Author Index">
             Gienger, Michael
            </a>
           </td>
           <td class="r">
            Honda Research Institute Europe
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2120" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#robot_companions" title="Click to go to the Keyword Index">
               Robot Companions
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#social_hri" title="Click to go to the Keyword Index">
               Social HRI
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#physical_human_robot_interaction" title="Click to go to the Keyword Index">
               Physical Human-Robot Interaction
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             How can a robot provide unobtrusive physical support within a group of humans? We present Attentive Support, a novel interaction concept for robots to support a group of humans. It combines scene perception, dialogue acquisition, situation understanding, and behavior generation with the common-sense reasoning capabilities of Large Language Models (LLMs). In addition to following user instructions, Attentive Support is capable of deciding when and how to support the humans, and when to remain silent to not disturb the group. With a diverse set of scenarios, we show and evaluate the robot's attentive behavior, which supports and helps the humans when required, while not disturbing if no help is needed.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t2_02">
             15:30-16:30, Paper ThPI5T2.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1376'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Modelling and Analysis of Joint-To-End Variable Stiffness for Cable-Driven Hyper-Redundant Manipulator
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396417" title="Click to go to the Author Index">
             Zhang, Hongyang
            </a>
           </td>
           <td class="r">
            Huazhong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#247713" title="Click to go to the Author Index">
             Wang, Shuting
            </a>
           </td>
           <td class="r">
            Huazhong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396430" title="Click to go to the Author Index">
             Li, Hu
            </a>
           </td>
           <td class="r">
            Huazhong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#222762" title="Click to go to the Author Index">
             Xie, Yuanlong
            </a>
           </td>
           <td class="r">
            Huazhong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1376" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#dynamics" title="Click to go to the Keyword Index">
               Dynamics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#kinematics" title="Click to go to the Keyword Index">
               Kinematics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             To ensure operational accuracy and flexibility in confined environments, the cable-driven hyper-redundant manipulator needs to take into account both compliance and stiffness. Although the cable-driven method enables the manipulator to have adjustable stiffness, the theoretical analyses and studies on the effects of various factors on the stiffness are insufficient, leading to the possibility that the existing variable stiffness strategies may disrupt the equilibrium state of the manipulator. Accordingly, this paper presents the modeling and analysis of joint-to-end variable stiffness for cable-driven hyper-redundant manipulator. First, the multi-layered static models are constructed to decouple and characterise the complex robotic arm system by combining the manipulator kinematics with the virtual work principle. Then, the joint-to-end analytical stiffness models are developed to explore the influencing factors of stiffness, and relevant stiffness indicators are designed to evaluate the stiffness level. Finally, with platform validation and numerical method, the connections between cable tension, joint angle, joint stiffness and end stiffness are analysed, thereby summarising the variable stiffness characteristics of the cable-driven super-redundant manipulator.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t2_03">
             15:30-16:30, Paper ThPI5T2.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1380'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Adaptive Smith Predictor Fractional Control of a Tele-Operated Flexible Link Robot
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396375" title="Click to go to the Author Index">
             Gharab, Saddam
            </a>
           </td>
           <td class="r">
            UCLM
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396353" title="Click to go to the Author Index">
             Ben Ftima, Salma
            </a>
           </td>
           <td class="r">
            Phd Student
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#102348" title="Click to go to the Author Index">
             Feliu, Vicente
            </a>
           </td>
           <td class="r">
            Escuela Tcnica Superior De IngenierosIndustriales/Universidad D
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1380" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#dynamics" title="Click to go to the Keyword Index">
               Dynamics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#flexible_robotics" title="Click to go to the Keyword Index">
               Flexible Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#telerobotics_and_teleoperation" title="Click to go to the Keyword Index">
               Telerobotics and Teleoperation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This work addresses the adaptive Smith predictor control of a tele-operated robot composed of a flexible link. Measurements of the angle of the motor that moves the arm by using an encoder and the moment at the base of the arm by using two strain gauges are fed back. These strain gauges normally present noticeable offset and high-frequency noise. In order to implement such control, the simultaneous real-time characterizations of the main vibration mode - which changes with the carried payload - and the delay of the tele-operated robot - which is time-varying - are carried out using a new algorithm-based on an algebraic identification technique, which is robust to the previous strain gauge disturbances. This algorithm is faster than others previously developed, being especially suited to implement an adaptive version of the Smith predictor. The control of the tip position of the robot is closed using a fractional order controller, which has the advantage of removing the steady-state error introduced by the strain gauges offset on the tip position. This adaptive control system is subsequently evaluated on a prototype. Simulated and experimental results are presented demonstrating the speed, accuracy, and robustness of the performed control system.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t2_04">
             15:30-16:30, Paper ThPI5T2.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2730'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Dynamic Model and Experimental Validation of a Haptic Robot Based on a Flexible Antenna Mounted on an Omnidirectional Platform
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397488" title="Click to go to the Author Index">
             Merida-Calvo, Luis
            </a>
           </td>
           <td class="r">
            Escuela Tcnica Superior De Ingeniera Industrial (Ciudad Real)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#346445" title="Click to go to the Author Index">
             Haro-Olmo, Maria Isabel
            </a>
           </td>
           <td class="r">
            University of Castilla La Mancha
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#102348" title="Click to go to the Author Index">
             Feliu, Vicente
            </a>
           </td>
           <td class="r">
            Escuela Tcnica Superior De IngenierosIndustriales/Universidad D
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2730" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#dynamics" title="Click to go to the Keyword Index">
               Dynamics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#haptics_and_haptic_interfaces" title="Click to go to the Keyword Index">
               Haptics and Haptic Interfaces
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#force_and_tactile_sensing" title="Click to go to the Keyword Index">
               Force and Tactile Sensing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The development of new measurement systems for mobile robots has been a growing interest in recent years, particularly tactile systems based on bioinspired sensor antennae, such as whiskers and antennae found in animals and insects. This work focuses on studying a mobile robot equipped with such systems. Specifically, a dynamic model is developed for an omnidirectional robot with a sensing antenna, considering the planar motion of the system and taking into account the gravity effect. The extended Hamilton principle is applied to derive the equations of motion for the mobile platform, while the boundary-value problem is formulated for the antenna. Subsequently, modal analysis is employed to obtain a unique solution for the sensor antenna model, which is then validated using experimental data.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t2_05">
             15:30-16:30, Paper ThPI5T2.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('499'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Robotic Object Insertion with a Soft Wrist through Sim-To-Real Privileged Training
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#242257" title="Click to go to the Author Index">
             Fuchioka, Yuni
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#165436" title="Click to go to the Author Index">
             Beltran-Hernandez, Cristian Camilo
            </a>
           </td>
           <td class="r">
            OMRON SINIC X Corporation
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#284374" title="Click to go to the Author Index">
             Hai, Nguyen
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#178939" title="Click to go to the Author Index">
             Hamaya, Masashi
            </a>
           </td>
           <td class="r">
            OMRON SINIC X Corporation
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab499" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#soft_robot_applications" title="Click to go to the Keyword Index">
               Soft Robot Applications
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#modeling__control__and_learning_for_soft_robots" title="Click to go to the Keyword Index">
               Modeling, Control, and Learning for Soft Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#compliant_assembly" title="Click to go to the Keyword Index">
               Compliant Assembly
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This study addresses contact-rich object insertion tasks under unstructured environments using a robot with a soft wrist, enabling safe contact interactions. For the unstructured environments, we assume that there are uncertainties in object grasp and hole pose and that the soft wrist pose cannot be directly measured. Recent methods employ learning approaches and force/torque sensors for contact localization; however, they require data collection in the real world. This study proposes a sim-to-real approach using a privileged training strategy. This method has two steps. 1) The teacher policy is trained to complete the task with sensor inputs and ground truth privileged information such as the peg pose, and then 2) the student encoder is trained with data produced from teacher policy rollouts to estimate the privileged information from sensor history. We performed sim-to-real experiments under grasp and hole pose uncertainties. This resulted in 100%, 95%, and 80% success rates for circular peg insertion with 0, +5, and -5 degree peg misalignments, respectively, and start positions randomly shifted pm 10 mm from a default position. Also, we tested the proposed method adapted with a square peg that was never seen during training. Additional simulation evaluations reveal that using the privileged strategy improves success rates compared to training with only simulated sensor data. Our results demonstrate the advantage of using sim-to-real privileged training for soft robots, which has the potential to alleviate human engineering efforts for robotic assembly.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t2_06">
             15:30-16:30, Paper ThPI5T2.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('748'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Design and Control of a Soft Supernumerary Robotic Limb Based on Fiber-Reinforced Actuator
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#352751" title="Click to go to the Author Index">
             Zhang, Tianyi
            </a>
           </td>
           <td class="r">
            Nanjing University of Aeronautics and Astronaut
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#221850" title="Click to go to the Author Index">
             Xu, Jiajun
            </a>
           </td>
           <td class="r">
            Nanjing University of Aeronautics and Astronautics
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#352746" title="Click to go to the Author Index">
             Lu, Yonghua
            </a>
           </td>
           <td class="r">
            Nanjing University of Aeronautics and Astronautics
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#393468" title="Click to go to the Author Index">
             Zhao, Mengcheng
            </a>
           </td>
           <td class="r">
            Nanjing University of Aeronautics and Astronautics
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#350105" title="Click to go to the Author Index">
             Huang, Kaizhen
            </a>
           </td>
           <td class="r">
            Nanjing University of Aeronautics and Astronautics
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#363095" title="Click to go to the Author Index">
             Chen, Bai
            </a>
           </td>
           <td class="r">
            Nanjing University of Aeronautics and Astronautics
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#188296" title="Click to go to the Author Index">
             Hou, Xuyan
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#169150" title="Click to go to the Author Index">
             Li, You-Fu
            </a>
           </td>
           <td class="r">
            City University of Hong Kong
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab748" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#soft_robot_applications" title="Click to go to the Keyword Index">
               Soft Robot Applications
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#wearable_robotics" title="Click to go to the Keyword Index">
               Wearable Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#motion_control" title="Click to go to the Keyword Index">
               Motion Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Supernumerary robotic limbs (SRLs) provide additional wearable limbs to enhance the users physical abilities. Most SRLs employ rigid structures, resulting in uncomfortable wearing experience and insufficient flexible manipulation. As a new type of SRL, soft SRLs offer operational flexibility, lightweight structure, and wearing safety, compensating for the shortcomings of rigid SRLs. However, due to the complex actuation mechanisms, soft SRLs pose challenges in multiple deformations and accurate controlling. In this paper, a soft SRL actuated by fiber-reinforced actuators (FRAs) is proposed. A kinematic model is established to capture the posture of the SRL. A control system is proposed to adjust the SRL posture precisely by configuration of the FRAs. Finally, the accuracy of the proposed control strategy is verified through experiments, and the SRL prototype exhibits flexibility and adaptability to various scenarios, effectively assisting users in accomplishing complex tasks.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t2_07">
             15:30-16:30, Paper ThPI5T2.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('669'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Programming Passive Fingertip Deformation for Improved Grasping and Manipulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#178758" title="Click to go to the Author Index">
             Puhlmann, Steffen
            </a>
           </td>
           <td class="r">
            TU Berlin
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#374228" title="Click to go to the Author Index">
             Weber, Lion-Constantin
            </a>
           </td>
           <td class="r">
            TU Berlin
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#140759" title="Click to go to the Author Index">
             Hoeppner, Hannes
            </a>
           </td>
           <td class="r">
            Berliner Hochschule Fr Technik, BHT
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab669" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#soft_robot_materials_and_design" title="Click to go to the Keyword Index">
               Soft Robot Materials and Design
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#compliant_joints_and_mechanisms" title="Click to go to the Keyword Index">
               Compliant Joints and Mechanisms
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#grippers_and_other_end_effectors" title="Click to go to the Keyword Index">
               Grippers and Other End-Effectors
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Soft robots exhibit complex behaviors despite simple control, due to their inherently compliant hardware which passively deforms upon contact -- a concept commonly referred to as morphological computation. To fully determine the behavior of soft robots, not only their control software but also their passive behavior needs to be programmed. We show that deliberate programming of passive deformation in soft fingertips can significantly influence the grasping and manipulation performance of various robotic grippers. For this, the fingertips display strategically modulated compliance levels across their palmar surface, realized through adjustments to the local thickness of a lattice structure within their soft material, resulting in desired passive deformation. The grippers are operated by human participants, solving diverse tasks involving a variety of objects. We analyze 2025 human trials and show that the distinct passive behaviors programmed into the fingertips significantly affect grasping and manipulation performance. Furthermore, we discovered that specific compliance profiles consistently demonstrate superior performance, indicating that not merely inherent softness by itself, but a purposeful combination of varying compliance levels plays a pivotal role in successful soft interaction.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t2_08">
             15:30-16:30, Paper ThPI5T2.8
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1098'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Circular Soft Pneumatic Actuator with Bi-Directional Bending Behavior
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#350095" title="Click to go to the Author Index">
             Circe, Jeannette
            </a>
           </td>
           <td class="r">
            The Cooper Union
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#350346" title="Click to go to the Author Index">
             Giglia, Michael
            </a>
           </td>
           <td class="r">
            The Cooper Union
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#383327" title="Click to go to the Author Index">
             Rivera, Isaiah
            </a>
           </td>
           <td class="r">
            The Cooper Union
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#382973" title="Click to go to the Author Index">
             Vardanyan, Ani
            </a>
           </td>
           <td class="r">
            The Cooper Union
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#350344" title="Click to go to the Author Index">
             Bunt, Brandon, Kiau
            </a>
           </td>
           <td class="r">
            The Cooper Union
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#192139" title="Click to go to the Author Index">
             Rosen, Michelle
            </a>
           </td>
           <td class="r">
            The Cooper Union
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1098" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#soft_sensors_and_actuators" title="Click to go to the Keyword Index">
               Soft Sensors and Actuators
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#soft_robot_materials_and_design" title="Click to go to the Keyword Index">
               Soft Robot Materials and Design
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#grasping" title="Click to go to the Keyword Index">
               Grasping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Abstract Most existing soft robotic actuators require multiple chambers to achieve multi-directional bending. In this paper, we present the design, fabrication, and analysis of a novel circular actuator. The actuator is made from a single soft material and is capable of bi-directional bending using only positive pressure in a single chamber. To accurately predict the required pressure for all bending angles, we developed an analytical model for the full range of motion - both uncurling and curling. We tested and analyzed actuators fabricated out of four types of silicone and found that the softest actuators have a bending range from the initial bending angle, 210, to 0, and to 225 in the other direction in 112 kPa. When constraining the actuators, we found they can create up to 5.1 N of blocked force in 210 kPa. Additionally, we demonstrate that a single circular actuator can grasp objects of various weights up to 800 g from both the inside and outside.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t2_09">
             15:30-16:30, Paper ThPI5T2.9
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1407'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Laser-Induced Graphene-Based Flexible Multimodal Sensor for Material and Texture Perception
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#345856" title="Click to go to the Author Index">
             Duo, Youning
            </a>
           </td>
           <td class="r">
            Beihang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395000" title="Click to go to the Author Index">
             Duan, Jinxi
            </a>
           </td>
           <td class="r">
            Beihang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396519" title="Click to go to the Author Index">
             Chen, Xingyu
            </a>
           </td>
           <td class="r">
            Beihang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#256564" title="Click to go to the Author Index">
             Liu, Wenbo
            </a>
           </td>
           <td class="r">
            Beihang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#394990" title="Click to go to the Author Index">
             Wang, Shengxue
            </a>
           </td>
           <td class="r">
            Beihang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#128250" title="Click to go to the Author Index">
             Wen, Li
            </a>
           </td>
           <td class="r">
            Beihang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1407" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#soft_sensors_and_actuators" title="Click to go to the Keyword Index">
               Soft Sensors and Actuators
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#soft_robot_materials_and_design" title="Click to go to the Keyword Index">
               Soft Robot Materials and Design
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#force_and_tactile_sensing" title="Click to go to the Keyword Index">
               Force and Tactile Sensing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Humans can perceive and interact with their surroundings through multiple senses. For intelligent robots, multimodal sensors are crucial for them to perceive and understand the environment. In this work, we propose a multi-layered flexible multimodal sensor based on laser-induced graphene, capable of detecting both touchless signals (such as the distance from external objects and the material of them) and tactile signals (three-dimensional force). The sensor has advantages in durability and stability. Under normal force, the sensitivity is 7.449% N^(1) in the range of 0 N to 1.5 N and 0.273% N^(1) in the range of 1.5 N to 30 N, with fast response (17 ms) and recovery (37 ms). Furthermore, using Convolutional Neural Networks (CNN) model, we develop an intelligent soft robot system capable of distinguishing objects of different materials and fabric textures with accuracies of 99% and 88.75%, respectively. The proposed flexible multimodal sensor holds a significant on the perception and interaction of intelligent robots with the environment.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t2_10">
             15:30-16:30, Paper ThPI5T2.10
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2273'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Reconfigurable Soft Gripper Based on Eversion and Electroadhesion for Cluttered Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#359601" title="Click to go to the Author Index">
             Ragab, Dana
            </a>
           </td>
           <td class="r">
            University of Sussex
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398059" title="Click to go to the Author Index">
             Rendon-Morales, Elizabeth
            </a>
           </td>
           <td class="r">
            University of Sussex
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#101975" title="Click to go to the Author Index">
             Althoefer, Kaspar
            </a>
           </td>
           <td class="r">
            Queen Mary University of London
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#176249" title="Click to go to the Author Index">
             Godaba, Hareesh
            </a>
           </td>
           <td class="r">
            University of Sussex
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2273" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#soft_robot_applications" title="Click to go to the Keyword Index">
               Soft Robot Applications
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#soft_robot_materials_and_design" title="Click to go to the Keyword Index">
               Soft Robot Materials and Design
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#grippers_and_other_end_effectors" title="Click to go to the Keyword Index">
               Grippers and Other End-Effectors
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robotic grasping in cluttered and real-world human environments is a challenging task. It requires unique kinematic capabilities to deal with spatial constraints as well as compliance and softness to offer collision safety and safe manipulation of sensitive objects. To address this challenge, we propose a novel robotic gripper with two steerable fingers whose lengths can be adjusted by way of a soft eversion mechanism. This enables the gripper to work in confined spaces while interacting safely with the environment. We also developed a new Electroadhesion (EA) pad design with a multilayer structure and a single insulating layer that can be safely integrated with the evertable fingers avoiding short-circuiting or dielectric breakdown to enhance the gripper payload. The resulting gripper can retrieve an object in a confined space and partially occluded by a barrier. It exhibits remarkable versatility in terms of object sizes, grasping objects with varying widths at least ranging from 70 mm to 600 mm. These results provide a promising avenue for new robotic applications in real-world environments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t2_11">
             15:30-16:30, Paper ThPI5T2.11
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('214'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Integrated Electronic Circuitry for Soft Robots Using Multi-Material FDM Printing
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#239793" title="Click to go to the Author Index">
             Aygul, Cem
            </a>
           </td>
           <td class="r">
            Worcester Polytechnic Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#383172" title="Click to go to the Author Index">
             Pandey, Ritwik
            </a>
           </td>
           <td class="r">
            Worcester Polytechnic Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#383177" title="Click to go to the Author Index">
             Kothimbakam, Krishram
            </a>
           </td>
           <td class="r">
            Worcester Polytechnic Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#383180" title="Click to go to the Author Index">
             Yilmaz Akkaya, Ceren
            </a>
           </td>
           <td class="r">
            Worcester Polytechnic Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#383176" title="Click to go to the Author Index">
             Rao, Pratap
            </a>
           </td>
           <td class="r">
            Worcester Polytechnic Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#205813" title="Click to go to the Author Index">
             Nemitz, Markus
            </a>
           </td>
           <td class="r">
            Tufts University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab214" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#soft_robot_materials_and_design" title="Click to go to the Keyword Index">
               Soft Robot Materials and Design
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#additive_manufacturing" title="Click to go to the Keyword Index">
               Additive Manufacturing
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#soft_robot_applications" title="Click to go to the Keyword Index">
               Soft Robot Applications
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The integration of electronics into compliant materials is typically complex, cumbersome, and jeopardizes system-level compliance. Using multi-material fused deposition modeling, we introduce a framework in which components of a soft robot and conductive traces are deposited in a single print. Our novel procedure for attaching discrete electronic components to printed conductive traces using toluene solvent ensures reliable electrical connections by significantly reducing contact resistance by over an order of magnitude compared to existing methods. This fabrication pipeline is an additional key component that contributes to the broader objective of establishing a fully automated fabrication process for soft robots with integrated electronics. We demonstrate a complete assembly of a terrestrial soft robot and showcase its resilience against physical impacts.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t2_12">
             15:30-16:30, Paper ThPI5T2.12
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('160'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Optimal Sensing in Soft Pneumatic Actuators Via Stretchable Optical Waveguides
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#221554" title="Click to go to the Author Index">
             ALJaber, Faisal
            </a>
           </td>
           <td class="r">
            Qatar University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#278574" title="Click to go to the Author Index">
             Hassan, Ahmed
            </a>
           </td>
           <td class="r">
            Queen Mary University of London
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#271926" title="Click to go to the Author Index">
             Vitanov, Ivan
            </a>
           </td>
           <td class="r">
            Queen Mary, University of London
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#389655" title="Click to go to the Author Index">
             Almeadadi, Noora
            </a>
           </td>
           <td class="r">
            Qatar University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#389929" title="Click to go to the Author Index">
             Alhajri, Hind
            </a>
           </td>
           <td class="r">
            Qatar University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#389767" title="Click to go to the Author Index">
             AlEnazi, Sara
            </a>
           </td>
           <td class="r">
            Qatar University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#389930" title="Click to go to the Author Index">
             Al-Marri, Rashid
            </a>
           </td>
           <td class="r">
            Qatar University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#389657" title="Click to go to the Author Index">
             Choe, Pilsung
            </a>
           </td>
           <td class="r">
            Qatar University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#101975" title="Click to go to the Author Index">
             Althoefer, Kaspar
            </a>
           </td>
           <td class="r">
            Queen Mary University of London
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab160" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#soft_sensors_and_actuators" title="Click to go to the Keyword Index">
               Soft Sensors and Actuators
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#soft_robot_materials_and_design" title="Click to go to the Keyword Index">
               Soft Robot Materials and Design
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#soft_robot_applications" title="Click to go to the Keyword Index">
               Soft Robot Applications
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Stretchable optical waveguides have been explored as a route to enhancing the sensing capabilities of soft actuators. Certain properties and qualities they possess recommend them for this task  their biological plausibility, compliance, low power consumption, and heightened responsiveness to external stimuli. Though well regarded for their efficiency, their practical application warrants a more detailed examination as regards their sensitivity, robustness, and resilience when integrated with various manipulators. There is a dearth of comprehensive, wide-ranging studies that investigate the relationship between soft sensors and actuators, both in terms of integration and sensor performance  the present study endeavours to fill this void. Here we present a series of findings as to the interdependent relationship at the nexus of soft actuator sensorisation, sensitivity and responsiveness. Building on our previous work and prior waveguide designs, we examine the influence of sensor location and placement along the deformation axis on responsiveness, repeatability, and longevity. Location is key as, during bending, one side experiences tension, while the other compression. Placement is identified as straight or loose. Three PneuNet-based actuators were used in three design configurations: one without any additional modifications, one with a few rigid exoskeleton reinforcements, and one covered fully with rigid exoskeleton reinforcements. Each design enables the straightforward integration of sensors, so that the relationship between soft actuator design and sensor performance is easy to assess when applying various pressure intakes (from 0 to 7.1 psi) to actuate the bending motion.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t2_13">
             15:30-16:30, Paper ThPI5T2.13
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2904'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Agonist-Antagonist Pouch Motors: Bidirectional Soft Actuators Enhanced by Thermally Responsive Peltier Elements
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#329573" title="Click to go to the Author Index">
             Exley, Trevor
            </a>
           </td>
           <td class="r">
            University of North Texas
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#393541" title="Click to go to the Author Index">
             Wijesundara Mudiyanselage, Rashmi Diviyanjali
            </a>
           </td>
           <td class="r">
            University of North Texas
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#393932" title="Click to go to the Author Index">
             Tan, Nathan
            </a>
           </td>
           <td class="r">
            Advanced Robotic Manipulators Lab, the University of North Texas
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#391961" title="Click to go to the Author Index">
             Sunkara, Akshay
            </a>
           </td>
           <td class="r">
            ARM Lab
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#393259" title="Click to go to the Author Index">
             He, Xinyu
            </a>
           </td>
           <td class="r">
            The Texas Academy of Mathematics and Science at University of No
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392336" title="Click to go to the Author Index">
             Wang, Shuopu
            </a>
           </td>
           <td class="r">
            University of North Texas
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#393662" title="Click to go to the Author Index">
             Chan, Bonnie
            </a>
           </td>
           <td class="r">
            ARM Lab, University of North Texas
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392930" title="Click to go to the Author Index">
             Jain, Aditya Jain
            </a>
           </td>
           <td class="r">
            University of North Texas
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392677" title="Click to go to the Author Index">
             Espinosa, Luis
            </a>
           </td>
           <td class="r">
            University of North Texas
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#132713" title="Click to go to the Author Index">
             Jafari, Amir
            </a>
           </td>
           <td class="r">
            University of North Texas
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2904" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#soft_sensors_and_actuators" title="Click to go to the Keyword Index">
               Soft Sensors and Actuators
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#soft_robot_applications" title="Click to go to the Keyword Index">
               Soft Robot Applications
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#grippers_and_other_end_effectors" title="Click to go to the Keyword Index">
               Grippers and Other End-Effectors
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this study, we introduce a novel Mylar-based pouch motor design that leverages the reversible actuation capabilities of Peltier junctions to enable agonist-antagonist muscle mimicry in soft robotics. Addressing the limitations of traditional silicone-based materials, such as leakage and phase-change fluid degradation, our pouch motors filled with Novec 7000 provide a durable and leak-proof solution for geometric modeling. The integration of flexible Peltier junctions offers a significant advantage over conventional Joule heating methods by allowing active and reversible heating and cooling cycles. This innovation not only enhances the reliability and longevity of soft robotic applications but also broadens the scope of design possibilities, including the development of agonist-antagonist artificial muscles, grippers with can manipulate through flexion and extension, and an anchor-slip style simple crawler design. Our findings indicate that this approach could lead to more efficient, versatile, and durable robotic systems, marking a significant advancement in the field of soft robotics.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t2_14">
             15:30-16:30, Paper ThPI5T2.14
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3489'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Design of a Pneumatically Driven 3D-Printed Under-Actuated Soft Robot with Programmable Stiffness
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#399117" title="Click to go to the Author Index">
             Mustafa, Zaid
            </a>
           </td>
           <td class="r">
            Sabanci University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#147823" title="Click to go to the Author Index">
             Turkseven, Melih
            </a>
           </td>
           <td class="r">
            Sabanci University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3489" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#soft_sensors_and_actuators" title="Click to go to the Keyword Index">
               Soft Sensors and Actuators
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#soft_robot_applications" title="Click to go to the Keyword Index">
               Soft Robot Applications
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#compliant_joints_and_mechanisms" title="Click to go to the Keyword Index">
               Compliant Joints and Mechanisms
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Soft robotic technologies are laying the foundations for a wide range of applications involving manipulation, gripping, and locomotion. In particular, soft pneumatic ac- tuators have been thoroughly examined due to their ease of implementation and their ability to yield sophisticated motions. Still, it is not uncommon for such actuators to be coupled with another form of actuation to provide more complexity and depth of configuration options. In this paper, we propose a novel soft pneumatic actuator coupled with stiffness programmable polylactic acid (PLA) elements. The deformations of these 3D-printed biodegradable polymer elements are dictated by controlled electrically induced pressure and heating inputs. The proposed design is based on heating wires embedded in the PLA elements. Changes in the elastic properties of PLA near the glass transition temperature (Tg ) coupled with pneumatic actuation are exploited to activate the actuators variable stiffness features. The proposed actuator is able to achieve an increase of 19.5% in its original length and 15 angular deformation. We demonstrate the actuators ability to perform different reconfigurable orientations based on different temperature and pressure inputs.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t2_15">
             15:30-16:30, Paper ThPI5T2.15
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2772'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Enabling Maintainablity of Robot Programs in Assembly by Extracting Compositions of Force and Position-Based Robot Skills from Learning-From-Demonstration Models
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#225161" title="Click to go to the Author Index">
             Bargmann, Daniel
            </a>
           </td>
           <td class="r">
            Fraunhofer IPA
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#159490" title="Click to go to the Author Index">
             Kraus, Werner
            </a>
           </td>
           <td class="r">
            Fraunhofer IPA
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#101331" title="Click to go to the Author Index">
             Huber, Marco F.
            </a>
           </td>
           <td class="r">
            University of Stuttgart
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2772" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#compliant_assembly" title="Click to go to the Keyword Index">
               Compliant Assembly
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#acceptability_and_trust" title="Click to go to the Keyword Index">
               Acceptability and Trust
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#software_tools_for_robot_programming" title="Click to go to the Keyword Index">
               Software Tools for Robot Programming
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             To this day, only a small number of industrial robots is used in assembly. One key reason for this is that specific contact situations require the introduction of force-control schemes. The parameters for those schemes are hard to select in practice, because they require in-depth expertise about the robot and the process. Learning-from-Demonstration (LfD) provides a powerful approach to intuitively parameterize robot programs by demonstrating the task at hand. However, when dimensions increase by including force or orientation, many LfD algorithms are hard to verify, understand and maintain, requiring expert knowledge to make adaptions, effectively making it a black-box. This property renders them ineffective for usage in industrial applications. We build upon a system of composable skills, that can be easily adapted by experts without the need to demonstrate the task again. This approach to skill- based robot programming promises to address the issues of readability and maintainability by sequencing robot movements in skills and breaking them down into understandable subgoals. In this paper, we combine skill-based programming with LfD, preserving both maintainability and intuitive parameterization. We present (a) an approach to parameterize and create sequences of hierarchies of force- and/or position-controlled robot skills from a LfD model, (b) which can be adapted by a user by hand with few, basic and understandable parameters, and (c) show its applicability on the real-world example of terminal clamp assembly. We achieve a reduction in teach-in time of 53.8% for variants, increased robustness against variance, and efficient tight stacking of clamps with a gap of  1 mm.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t2_16">
             15:30-16:30, Paper ThPI5T2.16
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1228'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Effects of Fiber Number and Density on Fiber Jamming: Towards Follow-The-Leader Deployment of a Continuum Robot
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#374674" title="Click to go to the Author Index">
             Qian, Chen
            </a>
           </td>
           <td class="r">
            University of New South Wales
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#291685" title="Click to go to the Author Index">
             Liu, Tangyou
            </a>
           </td>
           <td class="r">
            The University of New South Wales
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#149133" title="Click to go to the Author Index">
             Wu, Liao
            </a>
           </td>
           <td class="r">
            University of New South Wales
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1228" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#soft_robot_applications" title="Click to go to the Keyword Index">
               Soft Robot Applications
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#soft_robot_materials_and_design" title="Click to go to the Keyword Index">
               Soft Robot Materials and Design
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Fiber jamming modules (FJMs) offer flexibility and quick stiffness variation, making them suitable for follow-the-leader (FTL) motions in continuum robots, which is ideal for minimally invasive surgery (MIS). However, their potential has not been fully exploited, particularly in designing and manufacturing small-sized FJMs with high stiffness variation. Although existing research has focused on factors like fiber materials and geometry to maximize stiffness variation, the results often do not apply to FJMs for MIS due to size constraints. Meanwhile, other factors such as fiber number and packing density, less significant to large FJMs but critical to small-sized FJMs, have received insufficient investigation regarding their impact on the stiffness variation for FTL deployment. In this paper, we design and fabricate FJMs with a diameter of 4mm. Through theoretical and experimental analysis, we find that fiber number and packing density significantly affect both absolute stiffness and stiffness variation. Our experiments confirm the feasibility of using FJMs in a medical FTL robot design. The optimal configuration is a 4mm FJM with 0.4mm fibers at a 56% packing density, achieving up to 3400% stiffness variation. A video demonstration of a prototype robot using the suggested parameters for achieving FTL motions can be found at https://youtu.be/7pI5U0z7kcE.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thpi5t3">
             <b>
              ThPI5T3
             </b>
            </a>
           </td>
           <td class="r">
            Room 3
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thpi5t3" title="Click to go to the Program at a Glance">
             <b>
              Human-Robot Interaction (HRI) II
             </b>
            </a>
           </td>
           <td class="r">
            Teaser Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#110186" title="Click to go to the Author Index">
             Secchi, Cristian
            </a>
           </td>
           <td class="r">
            Univ. of Modena &amp; Reggio Emilia
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#103050" title="Click to go to the Author Index">
             Marino, Alessandro
            </a>
           </td>
           <td class="r">
            University of Cassino and Southern Lazio
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t3_01">
             15:30-16:30, Paper ThPI5T3.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1187'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Design and Evaluation of a Prototype Tactile Scanner for Active Sensing of Proximal Objects
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#336508" title="Click to go to the Author Index">
             Dechaux, Amaury
            </a>
           </td>
           <td class="r">
            Laboratoire d'Informatique, Robotique Et Microelectronique De Mo
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#183085" title="Click to go to the Author Index">
             Kitazaki, Michiteru
            </a>
           </td>
           <td class="r">
            Toyohashi University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#123153" title="Click to go to the Author Index">
             Lagarde, Julien
            </a>
           </td>
           <td class="r">
            University Montpellier 1
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#132506" title="Click to go to the Author Index">
             Ganesh, Gowrishankar
            </a>
           </td>
           <td class="r">
            Centre National De La Recherche Scientifique (CNRS)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1187" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#product_design__development_and_prototyping" title="Click to go to the Keyword Index">
               Product Design, Development and Prototyping
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#design_and_human_factors" title="Click to go to the Keyword Index">
               Design and Human Factors
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#haptics_and_haptic_interfaces" title="Click to go to the Keyword Index">
               Haptics and Haptic Interfaces
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Tactile interfaces, that can convey information to humans via tactile feedback, are still relatively rare. In this study we present a prototype tactile scanner, that fixes onto a users arm, and using arrays of capacitive sensors and vibratory motors, provides users with a sense of the proximity of objects near their arm. Through two experiments, we show that the device enables users to detect not just the position of objects, but also estimate their shapes and orientations. Finally, in a third experiment, we compare the user accuracy with the tactile scanner and their accuracy with real touch.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t3_02">
             15:30-16:30, Paper ThPI5T3.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('791'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Automatic Dietary Monitoring Using Inertial Sensor in Smartwatch
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#389949" title="Click to go to the Author Index">
             Pavlov, Konstantin
            </a>
           </td>
           <td class="r">
            Samsung Research
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#393855" title="Click to go to the Author Index">
             Tsepulin, Vladimir
            </a>
           </td>
           <td class="r">
            Samsung Research
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#393858" title="Click to go to the Author Index">
             Lutsyak, Nikolay
            </a>
           </td>
           <td class="r">
            Samsung Research
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#393859" title="Click to go to the Author Index">
             Khasianov, Rasul
            </a>
           </td>
           <td class="r">
            Samsung Research
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#393860" title="Click to go to the Author Index">
             Simchuk, Egor
            </a>
           </td>
           <td class="r">
            Samsung Research
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#393861" title="Click to go to the Author Index">
             Perchik, Alexey
            </a>
           </td>
           <td class="r">
            Samsung Research
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#393862" title="Click to go to the Author Index">
             Elena, Volkova
            </a>
           </td>
           <td class="r">
            Samsung Research
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab791" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#health_care_management" title="Click to go to the Keyword Index">
               Health Care Management
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#human_centered_automation" title="Click to go to the Keyword Index">
               Human-Centered Automation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#datasets_for_human_motion" title="Click to go to the Keyword Index">
               Datasets for Human Motion
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper investigates the problem of eating activity detection using motion data from an off-the-shelf smartwatch. The development and integration of the algorithm for detecting eating activity will make it easier for users to monitor their eating habits. For development of a such algorithm about 27500 hours of data were collected from 91 participants. Moreover, a reliable and interpreted approach with adjustable tolerance for model quality estimation in realworld conditions is proposed in this work. The algorithm based on end-to-end neural network (NN) for eating events detection with special postprocessing was developed by our research group. It recognizes eating events with 1 minute delay from the beginning of food intake. For a such tolerance it achieves F1- score of 0.90 in average (at free-living scenario test) for users wearing smartwatches either on dominant or on non-dominant hand. To the best of authors knowledge, the algorithm provides the best performance of any existing solution or described in the literature.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t3_03">
             15:30-16:30, Paper ThPI5T3.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1454'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Interactive Reward Tuning: Interactive Visualization for Preference Elicitation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#390695" title="Click to go to the Author Index">
             Shi, Danqing
            </a>
           </td>
           <td class="r">
            Aalto University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#350293" title="Click to go to the Author Index">
             Zhu, Shibei
            </a>
           </td>
           <td class="r">
            Aalto University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392372" title="Click to go to the Author Index">
             Weinkauf, Tino
            </a>
           </td>
           <td class="r">
            KTH Royal Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396625" title="Click to go to the Author Index">
             Oulasvirta, Antti
            </a>
           </td>
           <td class="r">
            Aalto University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1454" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#human_factors_and_human_in_the_loop" title="Click to go to the Keyword Index">
               Human Factors and Human-in-the-Loop
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#design_and_human_factors" title="Click to go to the Keyword Index">
               Design and Human Factors
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In reinforcement learning, tuning reward weights in the reward function is necessary to align behavior with user preferences. However, current approaches, which use pairwise comparisons for preference elicitation, are inefficient, because they miss much of the human ability to explore and judge groups of candidate solutions. The paper presents a novel visualization-based approach that better exploits the user's ability to quickly recognize interesting directions for reward tuning. It breaks down the tuning problem by using the visual information-seeking principle: overview first, zoom and filter, then details-on-demand. Following this principle, we built a visualization system comprising two interactively linked views: 1) an embedding view showing a contextual overview of all sampled behaviors and 2) a sample view displaying selected behaviors and visualizations of the detailed time-series data. A user can efficiently explore large sets of samples by iterating between these two views. The paper demonstrates that the proposed approach is capable of tuning rewards for challenging behaviors. The simulation-based evaluation shows that the system can reach optimal solutions with fewer queries relative to baselines.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t3_04">
             15:30-16:30, Paper ThPI5T3.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2199'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Foot Arch Stiffness-Based Dynamic Plantar Support Control of Human Walking Gait with Active Pneumatic Insoles
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#310781" title="Click to go to the Author Index">
             Liu, Chenhao
            </a>
           </td>
           <td class="r">
            School of Mechanical Engineering, Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#102943" title="Click to go to the Author Index">
             Yi, Jingang
            </a>
           </td>
           <td class="r">
            Rutgers University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#385695" title="Click to go to the Author Index">
             He, Long
            </a>
           </td>
           <td class="r">
            Zhiyuan Research Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#316664" title="Click to go to the Author Index">
             Zhang, Yijun
            </a>
           </td>
           <td class="r">
            The First Affiliated Hospital Zhejiang University School of Medi
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#393386" title="Click to go to the Author Index">
             Zhang, Xiufeng
            </a>
           </td>
           <td class="r">
            National Research Center for Rehabilitation Technical Aids
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#103419" title="Click to go to the Author Index">
             Liu, Tao
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2199" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#human_factors_and_human_in_the_loop" title="Click to go to the Keyword Index">
               Human Factors and Human-in-the-Loop
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#prosthetics_and_exoskeletons" title="Click to go to the Keyword Index">
               Prosthetics and Exoskeletons
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#hydraulic_pneumatic_actuators" title="Click to go to the Keyword Index">
               Hydraulic/Pneumatic Actuators
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The human foot arch plays a significant role in bearing weight, keeping balance, and walking efficiently. In this study, we present a pneumatic arch support insole (PASI) and a foot arch stiffness-based dynamic plantar support control to reduce the metabolic cost of walking. We first obtain the foot arch quasi-stiffness estimation over the gait phase by conducting multiple subject experiments. The design and modeling of the PASI and foot-insole interactions are then discussed. A model predictive control scheme is presented to dynamically regulate the foot arch stiffness over the gait phase by using the active PASI. We validate the system experimentally and conduct multi-subject walking tests. The results show that dynamic plantar support control of the foot arch stiffness reduces the metabolic cost by 7.99% compared to regular walking. In contrast, passive support without dynamic regulation increases the metabolic cost by 6.60%. The new pneumatic insoles and dynamic support method demonstrate promising potential for everyday and medical applications.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t3_05">
             15:30-16:30, Paper ThPI5T3.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3480'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Fast Explicit-Input Assistance for Teleoperation in Clutter
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#225387" title="Click to go to the Author Index">
             Walker, Nick
            </a>
           </td>
           <td class="r">
            University of Washington
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#205333" title="Click to go to the Author Index">
             Yang, Xuning
            </a>
           </td>
           <td class="r">
            NVIDIA
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#155114" title="Click to go to the Author Index">
             Garg, Animesh
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#106008" title="Click to go to the Author Index">
             Cakmak, Maya
            </a>
           </td>
           <td class="r">
            University of Washington
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#104592" title="Click to go to the Author Index">
             Fox, Dieter
            </a>
           </td>
           <td class="r">
            University of Washington
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#131170" title="Click to go to the Author Index">
             Prez-D'Arpino, Claudia
            </a>
           </td>
           <td class="r">
            NVIDIA
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3480" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#human_factors_and_human_in_the_loop" title="Click to go to the Keyword Index">
               Human Factors and Human-in-the-Loop
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#human_centered_robotics" title="Click to go to the Keyword Index">
               Human-Centered Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#telerobotics_and_teleoperation" title="Click to go to the Keyword Index">
               Telerobotics and Teleoperation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The performance of prediction-based assistance for robot teleoperation degrades in unseen or goal-rich environments due to incorrect or quickly-changing intent inferences. Poor predictions can confuse operators or cause them to change their control input to implicitly signal their goal. We present a new assistance interface for robotic manipulation where an operator can explicitly communicate a manipulation goal by pointing the end-effector. The pointing target specifies a region for local pose generation and optimization, providing interactive control over grasp and placement pose candidates. We evaluate this explicit pointing interface against an implicit inference-based assistance scheme and an unassisted control condition in a within-subjects user study (N=20), where participants teleoperate a simulated robot to complete a multi-step singulation and stacking task in cluttered environments. We find that operators prefer the explicit interface, experience fewer pick failures and report lower cognitive workload. Our code is available at: https://github.com/NVlabs/fast-explicit-teleop
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t3_06">
             15:30-16:30, Paper ThPI5T3.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('184'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              PGA: Personalizing Grasping Agents with Single Human-Robot Interaction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#351447" title="Click to go to the Author Index">
             Kim, Junghyun
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#351450" title="Click to go to the Author Index">
             Kang, Gi-Cheon
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#277535" title="Click to go to the Author Index">
             Kim, Jaein
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#377175" title="Click to go to the Author Index">
             Yang, Seoyun
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#377188" title="Click to go to the Author Index">
             Jung, Minjoon
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#133606" title="Click to go to the Author Index">
             Zhang, Byoung-Tak
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab184" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#multi_modal_perception_for_hri" title="Click to go to the Keyword Index">
               Multi-Modal Perception for HRI
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#learning_categories_and_concepts" title="Click to go to the Keyword Index">
               Learning Categories and Concepts
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Language-Conditioned Robotic Grasping (LCRG) aims to develop robots that comprehend and grasp objects based on natural language instructions. While the ability to understand personal objects like my wallet facilitates more natural interaction with human users, current LCRG systems only allow generic language instructions, e.g., the black-colored wallet next to the laptop. To this end, we introduce a task scenario GraspMine alongside a novel dataset aimed at pinpointing and grasping personal objects given personal indicators via learning from a single human-robot interaction, rather than a large labeled dataset. Our proposed method, Personalized Grasping Agent (PGA), addresses GraspMine by leveraging the unlabeled image data of the user's environment, called Reminiscence. Specifically, PGA acquires personal object information by a user presenting a personal object with its associated indicator, followed by PGA inspecting the object by rotating it. Based on the acquired information, PGA pseudo-labels objects in the Reminiscence by our proposed label propagation algorithm. Harnessing the information acquired from the interactions and the pseudo-labeled objects in the Reminiscence, PGA adapts the object grounding model to grasp personal objects. This results in significant efficiency while previous LCRG systems rely on resource-intensive human annotations---necessitating hundreds of labeled data to learn my wallet. Moreover, PGA outperforms baseline methods across all metrics and even shows comparable performance compared to the fully-supervised method, which learns from 9k annotated data samples. We further validate PGA's real-world applicability by employing a physical robot to execute GrsapMine. Code and data are publicly available at https://github.com/JHKim-snu/PGA.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t3_07">
             15:30-16:30, Paper ThPI5T3.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1488'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Boosting 3D Visual Grounding by Object-Centric Referring Network
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#393689" title="Click to go to the Author Index">
             Ren, Ruilong
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398021" title="Click to go to the Author Index">
             Cao, Jian
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395633" title="Click to go to the Author Index">
             Xu, Weichen
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397835" title="Click to go to the Author Index">
             Fu, Tianhao
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395579" title="Click to go to the Author Index">
             Dong, Yilei
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395694" title="Click to go to the Author Index">
             Xu, Xinxin
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397834" title="Click to go to the Author Index">
             Hu, Zicong
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#330040" title="Click to go to the Author Index">
             Zhang, Xing
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1488" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#multi_modal_perception_for_hri" title="Click to go to the Keyword Index">
               Multi-Modal Perception for HRI
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             3D visual grounding is tasked with locating a specific object within a 3D scene, as described by a given textual reference. This task is challenging because it requires (1) the accurate recognition of various objects in a 3D scene and (2) the understanding of spatial relations in the description. However, current studies encounter difficulties in situations where multiple similar objects are present or when the descriptions involve intricate and abstract relations. In this paper, a novel, simple, and efficient Object-Centric Referring network, namely 3D-OCR, is presented to take high-quality semantic representation and deep relation modeling into account. Specifically, an offline Fine-grained Semantic Enhancement (FSE) module is designed to reinforce the object-centric semantic awareness with fine-grained high-quality object semantic representations. To achieve superior object-centric relation awareness, we propose a Deep Relation Modeling (DRM) module with the explicit and implicit relation self-attention module, enriching object features with relational context. Moreover, we utilize a vision-language contrastive loss to further improve the matching process between point cloud and language. Comprehensive experiments conducted on the challenging ScanRefer and Nr3D datasets corroborate the exceptional performance of our method, with an increase of +1.47% on ScanRefer and +1.2% on Nr3D.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t3_08">
             15:30-16:30, Paper ThPI5T3.8
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('333'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Adaptive Passivation of Admittance Controllers by Bypassing Power to Null Space on Redundant Manipulators
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#326114" title="Click to go to the Author Index">
             Yun, Yeoil
            </a>
           </td>
           <td class="r">
            Sungkyunkwan Univ
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#243000" title="Click to go to the Author Index">
             Oh, DongJun
            </a>
           </td>
           <td class="r">
            SungKyunKwan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#318406" title="Click to go to the Author Index">
             Song, Eun Jeong
            </a>
           </td>
           <td class="r">
            SungKyunKwan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#102438" title="Click to go to the Author Index">
             Choi, Hyouk Ryeol
            </a>
           </td>
           <td class="r">
            Sungkyunkwan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#104237" title="Click to go to the Author Index">
             Moon, Hyungpil
            </a>
           </td>
           <td class="r">
            Sungkyunkwan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#111437" title="Click to go to the Author Index">
             Koo, Ja Choon
            </a>
           </td>
           <td class="r">
            Sungkyunkwan University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab333" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#physical_human_robot_interaction" title="Click to go to the Keyword Index">
               Physical Human-Robot Interaction
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#redundant_robots" title="Click to go to the Keyword Index">
               Redundant Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The significance of physical human-robot interaction (pHRI) with collaborative robots in the industry is growing steadily. Within this domain, an admittance controller is crucial for enabling robots to follow or assist human intentions. However, a persistent challenge with admittance controllers is ensuring their passivity. Various strategies have been developed to address this issue by adjusting the control signals derived from the admittance model. While these strategies achieve passivity, they often inadvertently impact collaborative performance, preventing the system from accurately aligning with the intended dynamics. Accordingly, this paper introduces an adaptive hierarchical control approach for redundant robots to handle this problem. This approach diverts non-passive power into the null space without diminishing the robot's responsiveness to human input. Implementing this null-space controller involves the dynamic adjustment of compliance control error, ensuring joint limit avoidance while facilitating integration with energy tanks for enhanced reliability. Moreover, the method enables the calculation of adaptive error gain in a closed form, simplifying its real-time application. Experimental validation with a 7-DOF manipulator showed a reduction of non-passive energy from 1.61 J to 0.12 J without compromising task performance.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t3_09">
             15:30-16:30, Paper ThPI5T3.9
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('450'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Learning-Based Adaptive Admittance Controller for Efficient and Safe pHRI in Contact-Rich Manufacturing Tasks
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#321442" title="Click to go to the Author Index">
             Pourakbarian Niaz, Pouya
            </a>
           </td>
           <td class="r">
            Koc University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392608" title="Click to go to the Author Index">
             Erzin, Engin
            </a>
           </td>
           <td class="r">
            Koc University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#115356" title="Click to go to the Author Index">
             Basdogan, Cagatay
            </a>
           </td>
           <td class="r">
            Koc University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab450" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#physical_human_robot_interaction" title="Click to go to the Keyword Index">
               Physical Human-Robot Interaction
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#intention_recognition" title="Click to go to the Keyword Index">
               Intention Recognition
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper proposes an adaptive admittance controller for improving efficiency and safety in physical human-robot interaction (pHRI) tasks in small-batch manufacturing that involve contact with stiff environments, such as drilling, polishing, cutting, etc. We aim to minimize human effort and task completion time while maximizing precision and stability during the contact of the machine tool attached to the robots end-effector with the workpiece. To this end, a two-layered learning-based human intention recognition mechanism is proposed, utilizing only the kinematic and kinetic data from the robot and two force sensors. A subtask detector recognizes the human intent by estimating which phase of the task is being performed, e.g., Idle, Tool-Attachment, Driving, and Contact. Simultaneously, a motion estimator continuously quantifies intent more precisely during the Driving to predict when Contact will begin. The controller is adapted online according to the subtask while allowing early adaptation before the Contact to maximize precision and safety and prevent potential instabilities. Three sets of pHRI experiments were performed with multiple subjects under various conditions. Spring compression experiments were performed in virtual environments to train the data-driven models and validate the proposed adaptive system, and drilling experiments were performed in the physical world to test the proposed methods efficacy in real-life scenarios. Experimental results show subtask classification accuracy of 84% and motion estimation R2 score of 0.96. Furthermore, 57% lower human effort was achieved during Driving as well as 53% lower oscillation amplitude at Contact as a result of the proposed system.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t3_10">
             15:30-16:30, Paper ThPI5T3.10
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('973'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Force and Velocity Prediction in Human-Robot Collaborative Transportation Tasks through Video Retentive Networks
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#286633" title="Click to go to the Author Index">
             Dominguez-Vidal, Jose Enrique
            </a>
           </td>
           <td class="r">
            Institut De Robtica I Informtica Industrial, CSIC-UPC
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#103488" title="Click to go to the Author Index">
             Sanfeliu, Alberto
            </a>
           </td>
           <td class="r">
            Universitat Politcnica De Cataluyna
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab973" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#physical_human_robot_interaction" title="Click to go to the Keyword Index">
               Physical Human-Robot Interaction
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#intention_recognition" title="Click to go to the Keyword Index">
               Intention Recognition
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this article, we propose a generalization of a Deep Learning State-of-the-Art architecture such as Retentive Networks so that it can accept video sequences as input. With this generalization, we design a force/velocity predictor applied to the medium-distance Human-Robot collaborative object transportation task. We achieve better results than with our previous predictor by reaching success rates in testset of up to 93.7% in predicting the force to be exerted by the human and up to 96.5% in the velocity of the human-robot pair during the next 1 s, and up to 91.0% and 95.0% respectively in real experiments. This new architecture also manages to improve inference times by up to 32.8% with different graphics cards. Finally, an ablation test allows us to detect that one of the input variables used so far, such as the position of the task goal, could be discarded allowing this goal to be chosen dynamically by the human instead of being pre-set.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t3_11">
             15:30-16:30, Paper ThPI5T3.11
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1350'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Development of a Super-Thin and Fast Omnidirectional Treadmill through a Novel Helical Transmission Mechanism
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#161103" title="Click to go to the Author Index">
             Pyo, Sanghun
            </a>
           </td>
           <td class="r">
            Gwangju Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#374806" title="Click to go to the Author Index">
             Choi, Jinsun
            </a>
           </td>
           <td class="r">
            Gwangju Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#106386" title="Click to go to the Author Index">
             Yoon, Jungwon
            </a>
           </td>
           <td class="r">
            Gwangju Institutue of Science and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1350" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#physical_human_robot_interaction" title="Click to go to the Keyword Index">
               Physical Human-Robot Interaction
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mechanism_design" title="Click to go to the Keyword Index">
               Mechanism Design
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#virtual_reality_and_interfaces" title="Click to go to the Keyword Index">
               Virtual Reality and Interfaces
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             To effectively enhance the spatial sensation for an immersive user experience (UX) in virtual reality (VR), the locomotion interface (LI) is one of the most critical factors. To offer a room-scale LI suitable for use in typical office or home environments, it is imperative that the LI device enables a natural walking experience while occupying minimal space. To realize LI for all directions, an omnidirectional treadmill (ODT) can successfully achieve 2-dimensional holonomic motion (X and Y axes) and provide the most natural walking experience, such as walking on real ground. However, a problem arises due to the excessive system thickness caused by the torus structure of a large treadmill (X-axis) carrying several small treadmills (Y-axis), along with the installation of a complex power transmission mechanism inside the ODT. To reduce the thickness of its double-layer and complex structure, we propose an ODT with a novel transmission mechanism. The proposed ODT utilizes helical timing pulleys (HTPs) to generate Y-axis motion and helical gears (HGs) to synchronized-actuate the HTPs. As a result, the proposed ODT achieves a super-thin configuration and fast actuation performance. A pilot test of the proposed ODT was conducted to assess its maximum performance. The results indicate achievable speeds of 3.175 m/s and 4 m/s, along with an acceleration of 5 m/s for both the X and Y axes, respectively.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t3_12">
             15:30-16:30, Paper ThPI5T3.12
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1354'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Compliant Blind Handover Control for Human-Robot Collaboration
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#299387" title="Click to go to the Author Index">
             Ferrari, Davide
            </a>
           </td>
           <td class="r">
            University of Modena and Reggio Emilia
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#277746" title="Click to go to the Author Index">
             Pupa, Andrea
            </a>
           </td>
           <td class="r">
            University of Modena and Reggio Emilia
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#110186" title="Click to go to the Author Index">
             Secchi, Cristian
            </a>
           </td>
           <td class="r">
            Univ. of Modena &amp; Reggio Emilia
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1354" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#physical_human_robot_interaction" title="Click to go to the Keyword Index">
               Physical Human-Robot Interaction
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#safety_in_hri" title="Click to go to the Keyword Index">
               Safety in HRI
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a Human-Robot Blind Handover architecture within the context of Human-Robot Collaboration (HRC). The focus lies on a blind handover scenario where the operator is intentionally faced away, focused in a task, and requires an object from the robot. In this context, it is imperative for the robot to autonomously manage the entire handover process. Key considerations include ensuring safety while handing the object to the operators hand, and detect the proper timing to release the object. The article explores strategies to navigate these challenges, emphasizing the need for a robot to operate safely and independently in facilitating blind handovers, thereby contributing to the advancement of HRC protocols and fostering a natural and efficient collaboration between humans and robots.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t3_13">
             15:30-16:30, Paper ThPI5T3.13
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1818'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Perception-Driven Shared Control Architecture for Agricultural Robots Performing Harvesting Tasks
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#356584" title="Click to go to the Author Index">
             Palmieri, Jozsef
            </a>
           </td>
           <td class="r">
            University of Cassino and Southern Lazio
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#179809" title="Click to go to the Author Index">
             Di Lillo, Paolo
            </a>
           </td>
           <td class="r">
            University of Cassino and Southern Lazio
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#103488" title="Click to go to the Author Index">
             Sanfeliu, Alberto
            </a>
           </td>
           <td class="r">
            Universitat Politcnica De Cataluyna
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#103050" title="Click to go to the Author Index">
             Marino, Alessandro
            </a>
           </td>
           <td class="r">
            University of Cassino and Southern Lazio
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1818" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#physical_human_robot_interaction" title="Click to go to the Keyword Index">
               Physical Human-Robot Interaction
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#robotics_and_automation_in_agriculture_and_forestry" title="Click to go to the Keyword Index">
               Robotics and Automation in Agriculture and Forestry
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper introduces a shared control framework designed specifically for agricultural mobile manipulators engaged in harvesting operations. The shared control strategy allows for achieving such operations by dynamically exchanging the control between the robotic system and a human operator depending on the uncertainty in the environment perception. For this purpose, the robot's behavior is dynamically adapted to switch between two control modes with a different level of autonomy of the robot. The level of autonomy is encoded in two different admittance behaviors which are included in a first-order Hierarchical Quadratic Programming (HQP) control framework, that allows the robot to simultaneously address other control objectives at the same time. Experimental results with a dual-arm mobile robot, developed as part of the EU-funded CANOPIES project, demonstrate the effectiveness of the proposed method in real conditions.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t3_14">
             15:30-16:30, Paper ThPI5T3.14
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1934'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Transparency Evaluation for the Kinematic Design of the Harnesses through Human-Exoskeleton Interaction Modeling
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397481" title="Click to go to the Author Index">
             Bezzini, Riccardo
            </a>
           </td>
           <td class="r">
            Scuola Superiore Sant'Anna
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#101830" title="Click to go to the Author Index">
             Avizzano, Carlo Alberto
            </a>
           </td>
           <td class="r">
            Scuola Superiore Sant'Anna
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#272454" title="Click to go to the Author Index">
             Porcini, Francesco
            </a>
           </td>
           <td class="r">
            PERCRO Laboratory, TeCIP Institute, SantAnna School of Advanced
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#124616" title="Click to go to the Author Index">
             Filippeschi, Alessandro
            </a>
           </td>
           <td class="r">
            Scuola Superiore Sant'Anna
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1934" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#physical_human_robot_interaction" title="Click to go to the Keyword Index">
               Physical Human-Robot Interaction
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#dynamics" title="Click to go to the Keyword Index">
               Dynamics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#prosthetics_and_exoskeletons" title="Click to go to the Keyword Index">
               Prosthetics and Exoskeletons
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Lower Limb Exoskeletons (LLEs) are wearable robotic systems that provide mechanical power to the user. Human-exoskeleton (HE) connections must guarantee the subsistence of the users natural behavior during the interaction, avoiding the exertion of undesired forces, i.e., the robot must be transparent. Since transparency is an essential feature of exoskeletons design, numerous works focus on its maximization, e.g., employing passive joints at the HE interfaces. Given the inherent complications of repeatedly prototyping and experimentally testing a device, modeling the exoskeleton and its physical interaction with the user emerges as an extremely valuable approach for assessing the design effects. This paper proposes a novel method to compare different exoskeleton configurations with a flexible simulation tool. This approach contemplates simulating the dynamics of the device, including its interaction with the wearer, to evaluate multiple connection mechanism designs along with the kinematics and actuation of the LLE. This evaluation is based on the minimization of the interaction wrenches through an optimization process that includes the impedance parameters at the interfaces as optimization variables and the similarity of the LLEs joint variables trajectories with the motion of the wearers articulations. Exploratory tests are conducted using the Wearable Walker LLE in different configurations and measuring the interaction forces. Experimental data are then compared to the optimization outcomes, proving that the proposed method provides contact wrench estimations consistent with the collected measurements and previous outcomes from the literature.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t3_15">
             15:30-16:30, Paper ThPI5T3.15
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3288'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Design and Validation of Soft Flexible Aerial Robot for Safe Human-Robot Interaction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#368786" title="Click to go to the Author Index">
             Jia, Fuhua
            </a>
           </td>
           <td class="r">
            University of Nottingham, Ningbo, China
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#371589" title="Click to go to the Author Index">
             Zheng, Zihao
            </a>
           </td>
           <td class="r">
            University of Nottingham Ningbo China
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#399078" title="Click to go to the Author Index">
             Li, Cheng'ao
            </a>
           </td>
           <td class="r">
            University of Nottingham Ningbo China
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#371339" title="Click to go to the Author Index">
             Xiao, Junlin
            </a>
           </td>
           <td class="r">
            University of Nottingham Ningbo China
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#370925" title="Click to go to the Author Index">
             Li, Rui
            </a>
           </td>
           <td class="r">
            Umea University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#320285" title="Click to go to the Author Index">
             Yang, Xiaoying
            </a>
           </td>
           <td class="r">
            University of Nottingham
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#207200" title="Click to go to the Author Index">
             Rushworth, Adam
            </a>
           </td>
           <td class="r">
            The University of Nottingham, Ningbo China
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#364032" title="Click to go to the Author Index">
             Ijaz, Salman
            </a>
           </td>
           <td class="r">
            University of Nottingham Ningbo China
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3288" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#safety_in_hri" title="Click to go to the Keyword Index">
               Safety in HRI
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#robot_safety" title="Click to go to the Keyword Index">
               Robot Safety
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__mechanics_and_control" title="Click to go to the Keyword Index">
               Aerial Systems: Mechanics and Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This work addresses the critical challenge of integrating drones into human-aerial robot interaction by presenting a novel Soft Flexible Aerial Robot (SFAR) design. SFAR features an innovative low-pressure inflatable airbag structure that replaces traditional rigid frames, enhancing safety by mitigating collision risks with humans and payloads. To control this unconventional aerial platform, we present a virtual link dynamics model and a semi-model-based control strategy that exploit the drone's unique design. Our contributions include the pioneering design of an aerial robot specifically for HARI, a novel control framework that balances flight performance with passive safety, and the validation of SFAR through real-world experiments, demonstrating its ability to perform at par with traditional rigid-body drones while offering enhanced safety features for seamless and safe integration into human environments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t3_16">
             15:30-16:30, Paper ThPI5T3.16
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('472'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SmartKit User-Friendly Robot with Multiple Operating Systems
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#353320" title="Click to go to the Author Index">
             Chen, Guanyu
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#391771" title="Click to go to the Author Index">
             Zhou, Yiqun
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#353554" title="Click to go to the Author Index">
             Yang, Guoqing
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#353546" title="Click to go to the Author Index">
             Lv, Pan
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#353551" title="Click to go to the Author Index">
             Li, Hong
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab472" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#software_architecture_for_robotic_and_automation" title="Click to go to the Keyword Index">
               Software Architecture for Robotic and Automation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#embedded_systems_for_robotic_and_automation" title="Click to go to the Keyword Index">
               Embedded Systems for Robotic and Automation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#human_centered_robotics" title="Click to go to the Keyword Index">
               Human-Centered Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Mobile robots have become extensively involved in human activities, taking on arduous tasks and providing significant assistance. Robot capabilities have been continuously enhanced, from simple chassis control to path planning and SLAM. Mixed criticality systems enable mobile robots to handle tasks of varying criticality by integrating multiple operating systems, allowing them to accomplish a wide range of tasks. However, besides improving robot computing performance, we should remember that robots are designed to serve humans. Reliability, usability, and affordability are all critical factors for robot design.
             <p>
              We introduce SmartKit, a mixed criticality system (MCS) for mobile robots. Leveraging the efficiency in hardware utilization brought by virtualization, SmartKit can execute tasks of different criticality efficiently and securely. This paper will present the software and hardware architecture of SmartKit and provide performance and functionality validation of the robot system.
             </p>
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thpi5t4">
             <b>
              ThPI5T4
             </b>
            </a>
           </td>
           <td class="r">
            Room 4
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thpi5t4" title="Click to go to the Program at a Glance">
             <b>
              Robot Vision III
             </b>
            </a>
           </td>
           <td class="r">
            Teaser Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#105608" title="Click to go to the Author Index">
             Knoll, Alois
            </a>
           </td>
           <td class="r">
            Tech. Univ. Muenchen TUM
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t4_01">
             15:30-16:30, Paper ThPI5T4.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('177'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              3D Object Visibility Prediction in Autonomous Driving
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#390187" title="Click to go to the Author Index">
             Luo, Chuanyu
            </a>
           </td>
           <td class="r">
            Ilmenau University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#390191" title="Click to go to the Author Index">
             Cheng, Nuo
            </a>
           </td>
           <td class="r">
            Nuo.cheng@tu-Ilmenau.de
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#390192" title="Click to go to the Author Index">
             Zhong, Ren
            </a>
           </td>
           <td class="r">
            Great Wall Motor Co., Ltd
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#390194" title="Click to go to the Author Index">
             Jiang, Haipeng
            </a>
           </td>
           <td class="r">
            Great Wall Motor Co., Ltd
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#390195" title="Click to go to the Author Index">
             Chen, Wenyu
            </a>
           </td>
           <td class="r">
            Ilmenau University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#390208" title="Click to go to the Author Index">
             Wang, Aoli
            </a>
           </td>
           <td class="r">
            Ilmenau University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#212147" title="Click to go to the Author Index">
             Li, Pu
            </a>
           </td>
           <td class="r">
            Department of Simulation and Optimal Processes, Institute of Aut
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab177" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_transportation" title="Click to go to the Keyword Index">
               Computer Vision for Transportation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#robot_safety" title="Click to go to the Keyword Index">
               Robot Safety
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             With the rapid advancement of hardware and software technologies, research in autonomous driving has seen significant growth. The prevailing framework for multi-sensor autonomous driving encompasses sensor installation, perception, path planning, decision-making, and motion control. At the perception phase, a common approach involves utilizing neural networks to infer 3D bounding box (Bbox) attributes from raw sensor data, including classification, size, and orientation. In this paper, we present a novel attribute and its corresponding algorithm: 3D object visibility. By incorporating multi-task learning, the introduction of this attribute, visibility, negligibly affects the models effectiveness and efficiency. Our proposal of this attribute and its computational strategy aims to expand the capabilities for downstream tasks, thereby enhancing the safety and reliability of real-time autonomous driving in real-world scenarios.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t4_02">
             15:30-16:30, Paper ThPI5T4.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('234'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Mini-PointNetPlus: A Local Feature Descriptor in Deep Learning Model for Real-Time 3D Environment Perception
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#390187" title="Click to go to the Author Index">
             Luo, Chuanyu
            </a>
           </td>
           <td class="r">
            Ilmenau University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#390191" title="Click to go to the Author Index">
             Cheng, Nuo
            </a>
           </td>
           <td class="r">
            Nuo.cheng@tu-Ilmenau.de
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#391121" title="Click to go to the Author Index">
             Ma, Sikun
            </a>
           </td>
           <td class="r">
            LiangDao GmbH
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#391124" title="Click to go to the Author Index">
             Xiang, Jun
            </a>
           </td>
           <td class="r">
            LiangDao GmbH
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#391125" title="Click to go to the Author Index">
             Li, Xiaohan
            </a>
           </td>
           <td class="r">
            LiangDao GmbH
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#391126" title="Click to go to the Author Index">
             Lei, Shengguang
            </a>
           </td>
           <td class="r">
            LiangDao GmbH
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#212147" title="Click to go to the Author Index">
             Li, Pu
            </a>
           </td>
           <td class="r">
            Department of Simulation and Optimal Processes, Institute of Aut
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab234" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_transportation" title="Click to go to the Keyword Index">
               Computer Vision for Transportation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Common deep learning models for 3D real-time environment perception often use pillarization/voxelization methods to convert point cloud data into pillars/voxels and then process it with a 2D/3D convolutional neural network (CNN). The pioneer work PointNet has been widely applied as a local feature descriptor, a fundamental component in deep learning models for 3D perception, to extract features of a point cloud. This is achieved by using a symmetric max-pooling operator which provides unique pillar/voxel features. However, by ignoring most of the points, the max-pooling operator causes an information loss, which reduces the model performance. To address this issue, we propose a novel local feature descriptor, mini-PointNetPlus, as an alternative for plug-and-play to PointNet. Our basic idea is to separately project the data points to the individual features considered, each leading to a permutation invariant. Thus, the proposed descriptor transforms an unordered point cloud to a stable order. The vanilla PointNet is proved to be a special case of our mini-PointNetPlus. Due to fully utilizing the features by the proposed descriptor, we demonstrate in experiment a considerable performance improvement for 3D perception.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t4_03">
             15:30-16:30, Paper ThPI5T4.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('649'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Automatic Image Annotation for Mapped Features Detection
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#340659" title="Click to go to the Author Index">
             Noizet, Maxime
            </a>
           </td>
           <td class="r">
            Universit De Technologie De Compigne
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#149924" title="Click to go to the Author Index">
             Xu, Philippe
            </a>
           </td>
           <td class="r">
            ENSTA Paris, Institut Polytechnique De Paris
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#105182" title="Click to go to the Author Index">
             Bonnifait, Philippe
            </a>
           </td>
           <td class="r">
            Univ. of Technology of Compiegne
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab649" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_transportation" title="Click to go to the Keyword Index">
               Computer Vision for Transportation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#intelligent_transportation_systems" title="Click to go to the Keyword Index">
               Intelligent Transportation Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Detecting road features is a key enabler for autonomous driving and localization. For instance, a reliable detection of poles which are widespread in road environments can improve localization. Modern deep learning-based perception systems need a significant amount of annotated data. Automatic annotation avoids time-consuming and costly manual annotation. Because automatic methods are prone to errors, managing annotation uncertainty is crucial to ensure a proper learning process. Fusing multiple annotation sources on the same dataset can be an efficient way to reduce the errors. This not only improves the quality of annotations, but also improves the learning of perception models. In this paper, we consider the fusion of three automatic annotation methods in images: feature projection from a high accuracy vector map combined with a lidar, image segmentation and lidar segmentation. Our experimental results demonstrate the significant benefits of multi-modal automatic annotation for pole detection through a comparative evaluation on manually annotated images. Finally, the resulting multi-modal fusion is used to fine-tune an object detection model for pole base detection using unlabeled data, showing overall improvements achieved by enhancing network specialization. The dataset is publicly available.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t4_04">
             15:30-16:30, Paper ThPI5T4.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('687'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              AutoJoin: Efficient Adversarial Training against Gradient-Free Perturbations for Robust Maneuvering Via Denoising Autoencoder and Joint Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#378133" title="Click to go to the Author Index">
             Villarreal, Michael
            </a>
           </td>
           <td class="r">
            University of Tennessee, Knoxville
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#348744" title="Click to go to the Author Index">
             Poudel, Bibek
            </a>
           </td>
           <td class="r">
            University of Tennessee Knoxville
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#393560" title="Click to go to the Author Index">
             Wickman, Ryan
            </a>
           </td>
           <td class="r">
            University of Memphis
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#204388" title="Click to go to the Author Index">
             Shen, Yu
            </a>
           </td>
           <td class="r">
            University of Maryland
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#233248" title="Click to go to the Author Index">
             Li, Weizi
            </a>
           </td>
           <td class="r">
            University of Tennessee, Knoxville
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab687" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_transportation" title="Click to go to the Keyword Index">
               Computer Vision for Transportation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             With the growing use of machine learning algorithms and ubiquitous sensors, many perception-to-control systems are being developed and deployed. To ensure their trustworthiness, improving their robustness through adversarial training is one potential approach. We propose a gradient-free adversarial training technique, named AutoJoin, to effectively and efficiently produce robust models for image-based maneuvering. Compared to other state-of-the-art methods with testing on over 5M images, AutoJoin achieves significant performance increases up to the 40% range against perturbations while improving on clean performance up to 300%. AutoJoin is also highly efficient, saving up to 86% time per training epoch and 90% training data over other state-of-the-art techniques. The core idea of AutoJoin is to use a decoder attachment to the original regression model creating a denoising autoencoder within the architecture. This architecture allows the tasks maneuvering and denoising sensor input to be jointly learnt and reinforce each others performance.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t4_05">
             15:30-16:30, Paper ThPI5T4.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1254'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Supervised Articulation Angles Estimation for Multi-Articulated Vehicles Based on Panoramic Camera System
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396049" title="Click to go to the Author Index">
             Liu, Weimin
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396070" title="Click to go to the Author Index">
             Wang, Wenjun
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396051" title="Click to go to the Author Index">
             Sun, Zhaocong
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1254" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_transportation" title="Click to go to the Keyword Index">
               Computer Vision for Transportation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_manufacturing" title="Click to go to the Keyword Index">
               Computer Vision for Manufacturing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Articulation angle plays a significant role in determining the motion of a complex dynamic system such as a multi-articulated vehicle. By engineering practice, articulation angles are measured using mechanical angle sensors that are delicate to physical damage. To overcome this problem, this study proposed a supervised articulation angle estimation method based on the panoramic camera system of multi-articulated vehicles. By constructing neural network that takes images of surrounding environment captured by spatially adjacent cameras as input, and takes temporal dependency as well as data imbalanced distribution into consideration, we show that the proposed vision-only method could make accurate estimations either on collected dataset or field experiment. Results of our experiments verified the validity and feasibility of the proposed method in playing as an alternative to mechanical angle sensors without bringing additional hardware setting expenses.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t4_06">
             15:30-16:30, Paper ThPI5T4.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1255'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SGOR: Outlier Removal by Leveraging Semantic and Geometric Information for Robust Point Cloud Registration
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#371285" title="Click to go to the Author Index">
             Zhao, Guiyu
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#372858" title="Click to go to the Author Index">
             Guo, Zhentao
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#107906" title="Click to go to the Author Index">
             Ma, Hongbin
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1255" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_transportation" title="Click to go to the Keyword Index">
               Computer Vision for Transportation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper, we introduce a new outlier removal method that fully leverages geometric and semantic information, to achieve robust registration. Current semantic-based registration methods only use semantics for point-to-point or instance semantic correspondence generation, which has two problems. First, these methods are highly dependent on the correctness of semantics. They perform poorly in scenarios with incorrect semantics and sparse semantics. Second, the use of semantics is limited only to the correspondence generation, resulting in bad performance in the weak geometry scene. To solve these problems, on the one hand, we propose secondary ground segmentation and loose semantic consistency based on regional voting. It improves the robustness to semantic correctness by reducing the dependence on single-point semantics. On the other hand, we propose semantic-geometric consistency for outlier removal, which makes full use of semantic information and significantly improves the quality of correspondences. In addition, a two-stage hypothesis verification is proposed, which solves the problem of incorrect transformation selection in the weak geometry scene. In the outdoor dataset, our method demonstrates superior performance, boosting a 22.5 percentage points improvement in registration recall and achieving better robustness under various conditions. Our code is available.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t4_07">
             15:30-16:30, Paper ThPI5T4.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1396'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Self-Supervised Monocular Depth Estimation in Challenging Environments Based on Illumination Compensation PoseNet
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396101" title="Click to go to the Author Index">
             Hou, Shengyu
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#241189" title="Click to go to the Author Index">
             Song, Wenjie
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#340016" title="Click to go to the Author Index">
             Wang, Rongchuan
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#241193" title="Click to go to the Author Index">
             Wang, Meiling
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#128868" title="Click to go to the Author Index">
             Yang, Yi
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#128869" title="Click to go to the Author Index">
             Fu, Mengyin
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1396" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_transportation" title="Click to go to the Keyword Index">
               Computer Vision for Transportation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Self-supervised depth estimation has attracted much attention due to its ability to improve the 3D perception capabilities of unmanned systems. However, existing unsupervised frameworks rely on the assumption of photometric consistency, which may not hold in challenging environments such as night-time, rainy nights, or snowy winters due to complex lighting and reflections, resulting in inconsistent photometry across different frames for the same pixel. To address this problem, we propose a self-supervised monocular depth estimation unified framework that can handle these complex scenarios, which has the following characteristics: (1) an Illumination Compensation PoseNet (ICP) is designed, which is based on the classic Phong illumination theory and compensates for lighting changes in adjacent frames by estimating per-pixel transformations; (2) a Dual-Axis Transformer (DAT) block is proposed as the backbone network of the depth encoder, which infers the depth of local repeat-texture areas through spatial-channel dual-dimensional global context information of images. Experimental results demonstrate that our approach achieves state-of-the-art depth estimation results in complex environments on the challenging Oxford RobotCar dataset.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t4_08">
             15:30-16:30, Paper ThPI5T4.8
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2053'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Vehicle Trajectory Prediction with Soft Behavior Constraints
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#375340" title="Click to go to the Author Index">
             Ye, Ke
            </a>
           </td>
           <td class="r">
            Xi'an Jiaotong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#323296" title="Click to go to the Author Index">
             Zhou, Sanping
            </a>
           </td>
           <td class="r">
            Xian JIaotong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#323283" title="Click to go to the Author Index">
             Kang, Miao
            </a>
           </td>
           <td class="r">
            Xian Jiaotong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#320498" title="Click to go to the Author Index">
             Fu, Jingwen
            </a>
           </td>
           <td class="r">
            Xi, an Jiaotong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#116919" title="Click to go to the Author Index">
             Zheng, Nanning
            </a>
           </td>
           <td class="r">
            Xi'an Jiaotong University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2053" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_transportation" title="Click to go to the Keyword Index">
               Computer Vision for Transportation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#intelligent_transportation_systems" title="Click to go to the Keyword Index">
               Intelligent Transportation Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Trajectory prediction plays a crucial role in autonomous driving, but it is challenging due to the multimodal nature of future trajectories. Behavior information is frequently employed to capture more diverse modalities of future trajectories. Traditional behavior information is typically hard-encoded, which is often inaccurate and inadequate for reflecting future multimodality. Therefore, we introduce the concept of soft vehicle behavior, which is represented as a probability distribution over a predefined comprehensive set of behaviors. This approach allows for a more rational depiction of vehicle behavior and captures potential future driving modalities. Based on it, we propose a new soft behavior-constrained vehicle trajectory prediction framework. The framework consists of a backbone and a lightweight and plug-and-play behavior prediction module, which is used to imbue soft behavior constraints to assist in representation learning. We integrated the behavior prediction module into five representative trajectory predictors and achieved improvements of at least 4.2% in minFDE(K=5) on the nuScenes dataset and 0.5% in minFDE(K=6) on the Argoverse 1 motion forecasting dataset. These universal increments prove the effectiveness and generalizability of soft behavior constraints in vehicle trajectory prediction.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t4_09">
             15:30-16:30, Paper ThPI5T4.9
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2126'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Efficient Motion Prediction: A Lightweight &amp; Accurate Trajectory Prediction Model with Fast Training and Inference Speed
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#366845" title="Click to go to the Author Index">
             Prutsch, Alexander
            </a>
           </td>
           <td class="r">
            Graz University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#107451" title="Click to go to the Author Index">
             Bischof, Horst
            </a>
           </td>
           <td class="r">
            Graz University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#239458" title="Click to go to the Author Index">
             Possegger, Horst
            </a>
           </td>
           <td class="r">
            Graz University of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2126" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_transportation" title="Click to go to the Keyword Index">
               Computer Vision for Transportation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#autonomous_vehicle_navigation" title="Click to go to the Keyword Index">
               Autonomous Vehicle Navigation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             For efficient and safe autonomous driving, it is essential that autonomous vehicles can predict the motion of other traffic agents. While highly accurate, current motion prediction models often impose significant challenges in terms of training resource requirements and deployment on embedded hardware. We propose a new efficient motion prediction model, which achieves highly competitive benchmark results while training only a few hours on a single GPU. Due to our lightweight architectural choices and the focus on reducing the required training resources, our model can easily be applied to custom datasets. Furthermore, its low inference latency makes it particularly suitable for deployment in autonomous applications with limited computing resources.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t4_10">
             15:30-16:30, Paper ThPI5T4.10
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2132'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Enhancing LiDAR Scene Upsampling with Instance-Aware Feature-Embedding and Attention Mechanism
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#373019" title="Click to go to the Author Index">
             Wang, Wei-Ren
            </a>
           </td>
           <td class="r">
            National Yang Ming Chiao Tung University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396146" title="Click to go to the Author Index">
             Do, You-Sheng
            </a>
           </td>
           <td class="r">
            National Yang Ming Chiao Tung University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#249645" title="Click to go to the Author Index">
             Lin, Wen-Chieh
            </a>
           </td>
           <td class="r">
            National Yang Ming Chiao Tung University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#103458" title="Click to go to the Author Index">
             Wang, Chieh-Chih
            </a>
           </td>
           <td class="r">
            National Yang Ming Chiao Tung University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2132" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_transportation" title="Click to go to the Keyword Index">
               Computer Vision for Transportation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#autonomous_vehicle_navigation" title="Click to go to the Keyword Index">
               Autonomous Vehicle Navigation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Scanning LiDAR is one of the widely used sensors in autonomous vehicles; however, the inherent sparsity of LiDAR point clouds often affects its performance. To address this issue, upsampling methods could be employed to enhance low-resolution LiDAR data. Although there have been methods on upsampling of single-object point clouds recently in computer vision, they tend to generate a considerable amount of artifacts when dealing with real-world LiDAR scenes consisting of multiple objects. In this paper, we propose a solution to tackle this problem by introducing an instance embedding auxiliary task and a context attention module. With our auxiliary learning architecture, the network can learn features that benefit both the primary upsampling task and the auxiliary instance embedding task. This training design enables the point generation process to be carried out separately and significantly reduces artifacts of the upsampling results on the SemanticKITTI dataset, particularly in areas surrounding instances. By leveraging these techniques to improve the model's understanding of the relationship between objects and the background in LiDAR scenes, we achieve an overall 4% to 10% improvement in whole-scene upsampling.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t4_11">
             15:30-16:30, Paper ThPI5T4.11
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2179'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              All-Day Depth Completion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#375911" title="Click to go to the Author Index">
             Ezhov, Vadim
            </a>
           </td>
           <td class="r">
            Yale University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#374947" title="Click to go to the Author Index">
             Park, Hyoungseob
            </a>
           </td>
           <td class="r">
            Yale University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#373870" title="Click to go to the Author Index">
             Zhang, Zhaoyang
            </a>
           </td>
           <td class="r">
            Yale University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#375869" title="Click to go to the Author Index">
             Upadhyay, Rishi
            </a>
           </td>
           <td class="r">
            University of California, Los Angeles
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#286661" title="Click to go to the Author Index">
             Zhang, Howard
            </a>
           </td>
           <td class="r">
            UCLA
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#375026" title="Click to go to the Author Index">
             Chandrappa, Chethan Chinder
            </a>
           </td>
           <td class="r">
            UCLA
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#374927" title="Click to go to the Author Index">
             Kadambi, Achuta
            </a>
           </td>
           <td class="r">
            UCLA
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#374667" title="Click to go to the Author Index">
             Ba, Yunhao
            </a>
           </td>
           <td class="r">
            Sony, UCLA
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#373898" title="Click to go to the Author Index">
             Dorsey, Julie
            </a>
           </td>
           <td class="r">
            Yale University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#233966" title="Click to go to the Author Index">
             Wong, Alex
            </a>
           </td>
           <td class="r">
            Yale University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2179" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_transportation" title="Click to go to the Keyword Index">
               Computer Vision for Transportation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We propose a method for depth estimation under different illumination conditions, i.e., day and night time. As photometry is uninformative in regions under low-illumination, we tackle the problem through a multi-sensor fusion approach, where we take as input an additional synchronized sparse point cloud (i.e., from a LiDAR) projected onto the image plane as a sparse depth map, along with a camera image. The crux of our method lies in the use of the abundantly available synthetic data to first approximate the 3D scene structure by learning a mapping from sparse to (coarse) dense depth maps along with their predictive uncertainty  we term this, SpaDe. In poorly illuminated regions where photometric intensities do not afford the inference of local shape, the coarse approximation of scene depth serves as a prior; the uncertainty map is then used with the image to guide refinement through an uncertainty-driven residual learning (URL) scheme. The resulting depth completion network leverages complementary strengths from both modalities  depth is sparse but insensitive to illumination and in metric scale, and image is dense but sensitive with scale ambiguity. SpaDe can be used in a plug-and-play fashion, which allows for 24% improvement when augmented onto existing methods to preprocess sparse depth. We demonstrate URL on the nuScenes dataset where we improve over all baselines by an average 12.39% in all-day scenarios, 12.02% when tested specifically for daytime, and 14.95% for nighttime scenes.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t4_12">
             15:30-16:30, Paper ThPI5T4.12
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3091'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Self-Supervised Motion Segmentation with Confidence-Aware Loss Functions for Handling Occluded Pixels and Uncertain Optical Flow Predictions
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396057" title="Click to go to the Author Index">
             Chen, Chung-Yu
            </a>
           </td>
           <td class="r">
            National Yang Ming Chiao Tung University, Hsinchu, Taiwan
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397469" title="Click to go to the Author Index">
             Lai, Bo-Yun
            </a>
           </td>
           <td class="r">
            National Yang Ming Chiao Tung University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#350465" title="Click to go to the Author Index">
             Huang, Ying-Shiuan
            </a>
           </td>
           <td class="r">
            National Yang Ming Chiao Tung University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#249645" title="Click to go to the Author Index">
             Lin, Wen-Chieh
            </a>
           </td>
           <td class="r">
            National Yang Ming Chiao Tung University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#103458" title="Click to go to the Author Index">
             Wang, Chieh-Chih
            </a>
           </td>
           <td class="r">
            National Yang Ming Chiao Tung University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3091" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_transportation" title="Click to go to the Keyword Index">
               Computer Vision for Transportation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#autonomous_vehicle_navigation" title="Click to go to the Keyword Index">
               Autonomous Vehicle Navigation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In driving scenarios, motion segmentation is a crucial and fundamental component that is needed for many tasks. Recently, a self-supervised multitasking framework was proposed for driving scenarios. It simultaneously trains motion segmentation, optical flow, depth, and ego-motion models without annotated data. The self-supervised architecture derives training signals from training data via loss functions. If these loss functions lack robustness, they may result in model inaccuracies. To reduce the bad influences of occlusion and optical flow estimation errors on motion segmentation, we propose two loss functions: (1) Soft-Per-Pixel-Minimum (Soft-PPM) loss that excludes occluded pixels while balancing the contribution of each frame on the loss function temporally; (2) Flow difference loss that excludes pixels with unclear motion states to diminish the effect of optical flow estimation errors. Our loss function design is based on the key insight that information such as depth and optical flow can be used to train motion segmentation models and act as a reliable measure for pixels during training. Our approach can improve segmentation accuracy for both moving and static objects and has achieved IoU scores on moving and static classes comparable to the state-of-the-art methods on the KITTI dataset.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t4_13">
             15:30-16:30, Paper ThPI5T4.13
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3124'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              EC-IoU: Orienting Safety for Object Detectors Via Ego-Centric Intersection-Over-Union
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#297343" title="Click to go to the Author Index">
             Liao, Brian Hsuan-Cheng
            </a>
           </td>
           <td class="r">
            DENSO AUTOMOTIVE Deutschland GmbH
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#130911" title="Click to go to the Author Index">
             Cheng, Chih-Hong
            </a>
           </td>
           <td class="r">
            Chalmers University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#103128" title="Click to go to the Author Index">
             Esen, Hasan
            </a>
           </td>
           <td class="r">
            DENSO AUTOMOTIVE Deutschland GmbH
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#105608" title="Click to go to the Author Index">
             Knoll, Alois
            </a>
           </td>
           <td class="r">
            Tech. Univ. Muenchen TUM
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3124" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_transportation" title="Click to go to the Keyword Index">
               Computer Vision for Transportation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#robot_safety" title="Click to go to the Keyword Index">
               Robot Safety
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents Ego-Centric Intersection-over-Union (EC-IoU), addressing the limitation of the standard IoU measure in characterizing safety-related performance for object detectors in navigating contexts. Concretely, we propose a weighting mechanism to refine IoU, allowing it to assign a higher score to a prediction that covers closer points of a ground-truth object from the ego agent's perspective. The proposed EC-IoU measure can be used in typical evaluation processes to select object detectors with better safety-related performance for downstream tasks. It can also be integrated into common loss functions for model fine-tuning. While geared towards safety, our experiment with the KITTI dataset demonstrates the performance of a model trained on EC-IoU can be better than that of a variant trained on IoU in terms of mean Average Precision as well.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t4_14">
             15:30-16:30, Paper ThPI5T4.14
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3192'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              LiDAR-Camera Online Calibration by Representing Local Feature and Global Spatial Context
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#270130" title="Click to go to the Author Index">
             Moon, SeongJoo
            </a>
           </td>
           <td class="r">
            KAIST, SAPEON
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#287304" title="Click to go to the Author Index">
             Lee, Sebin
            </a>
           </td>
           <td class="r">
            KAIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398949" title="Click to go to the Author Index">
             He, Dong
            </a>
           </td>
           <td class="r">
            Sapeon Inc
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#120231" title="Click to go to the Author Index">
             Yoon, Sung-eui
            </a>
           </td>
           <td class="r">
            KAIST
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3192" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_transportation" title="Click to go to the Keyword Index">
               Computer Vision for Transportation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#sensor_fusion" title="Click to go to the Keyword Index">
               Sensor Fusion
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             LiDAR-camera calibration plays a crucial role in autonomous driving. However, despite pre-deployment calibration, operation-induced factors such as physical vibrations and temperature variations degrade calibration accuracy, leading to the environmental perception performance deterioration. Recently, methods have been studied to calibrate online without a target board by understanding the relative attributes of LiDAR and camera. This paper proposes a novel framework for LiDAR-camera online calibration. It employs a Transformer network to learn crucial interactions between the information from cameras and LiDAR sensors. Additionally, the design enables more effective calibration between the two sensors by utilizing correspondence point information. This allows the utilization of global spatial context and achieves high performance by integrating information across modalities. Experimental results indicate that our method demonstrates superior performance compared to state-of-the-art benchmarks.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t4_15">
             15:30-16:30, Paper ThPI5T4.15
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3505'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              LDIP: Real-Time On-Road Object Detection with Depth Estimation from a Single Image
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398760" title="Click to go to the Author Index">
             Xu, Chengpeng
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#167903" title="Click to go to the Author Index">
             Sun, Xiao
            </a>
           </td>
           <td class="r">
            Hefei University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398832" title="Click to go to the Author Index">
             Xu, Yangyang
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#290600" title="Click to go to the Author Index">
             Wang, Ruolin
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3505" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_transportation" title="Click to go to the Keyword Index">
               Computer Vision for Transportation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Detecting on-road objects with absolute depth information is one of the most crucial tasks in the field of autonomous driving to ensure safety. However, traditional 2D object detection aims to classify and locate objects in image space, lacking the capability to acquire depth information. While 3D object detection and pixel-level depth detection tasks can provide accurate depth information for objects, they are challenging to deploy in real-world scenarios due to their significant inference overhead. In this paper, we propose a novel deep learning-based model named the Location and Depth Information Perceptron (LDIP), designed to provide positional, categorical, and absolute depth information for given objects in the images.
             <p>
              We first conducted model training and validation on the vehicle-side autonomous driving datasetKITTI. The experimental results show that we achieved a 68.6% mAP in object recognition tasks and an RMSE of 0.101 and AbsRel of 2.327 in depth estimation tasks, all of which represent state-of-the-art performance in comparable tasks. Subsequently, we fine-tuned the trained model on DAIR, where the validated mAP, AbsRel, and RMSE reached 65.4%, 0.092, and 2.461 respectively. This demonstrates the robustness and generalization of our model across different types of road datasets.
              <p>
               Moreover, in comparison to other models, our model is more compact while maintaining accuracy, achieving an inference speed of 70 frames per second on an NVIDIA 4060 GPU, thus making it deployable in practical scenarios. Relevant code is available at https://github.com/xcp-ustc/LDIP.
              </p>
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t4_16">
             15:30-16:30, Paper ThPI5T4.16
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2095'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              DSVT: Dynamic 3D Surround View for Tractor-Trailer Vehicles Based on Real-Time Pose Estimation with Drop Model
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#350140" title="Click to go to the Author Index">
             Dong, Zhipeng
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#128869" title="Click to go to the Author Index">
             Fu, Mengyin
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#324113" title="Click to go to the Author Index">
             Liang, Hao
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#350120" title="Click to go to the Author Index">
             Zhu, Chunhui
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#128868" title="Click to go to the Author Index">
             Yang, Yi
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2095" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#omnidirectional_vision" title="Click to go to the Keyword Index">
               Omnidirectional Vision
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#intelligent_transportation_systems" title="Click to go to the Keyword Index">
               Intelligent Transportation Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_transportation" title="Click to go to the Keyword Index">
               Computer Vision for Transportation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In recent years, 3D surround view systems have attracted a lot of attention in the field of advanced driver assistance systems (ADAS). However, the foundational assumption of unchanging camera poses in traditional 3D surround view systems, which is designed for single-unit vehicles, results in a failure to manage the non-rigid connections characteristic of tractor-trailer vehicles. Moreover, tractor-trailer vehicles have the feature of long bodies and large wheelbases, leading to severe distortions and abrupt changes in the rendering results of previous 3D texture mapping models. In this paper, we propose DSVT, a dynamic 3D surround view system for tractor-trailer vehicles, designed to address the aforementioned issues. Specifically, we develop a dynamic surround image stitching algorithm based on relative pose estimation, which estimates the relative poses between cameras and stitches all images together to generate a 2D panoramic image. Subsequently, a novel 3D drop model is proposed, mapping the 2D panoramic image onto the 3D model for panoramic viewing. Our system can run in real time on Nvidia AGX Orin. Experimental results in real tractor-trailer scenes show that our system can achieve more accurate and natural visual effects.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thpi5t5">
             <b>
              ThPI5T5
             </b>
            </a>
           </td>
           <td class="r">
            Room 5
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thpi5t5" title="Click to go to the Program at a Glance">
             <b>
              Deep Learning V
             </b>
            </a>
           </td>
           <td class="r">
            Teaser Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#204350" title="Click to go to the Author Index">
             Dong, Huixu
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t5_01">
             15:30-16:30, Paper ThPI5T5.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('440'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Cross-Modal Self-Supervised Learning with Effective Contrastive Units for LiDAR Point Clouds
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#274571" title="Click to go to the Author Index">
             Cai, Mu
            </a>
           </td>
           <td class="r">
            University of Wisconsin-Madison
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#273883" title="Click to go to the Author Index">
             Luo, Chenxu
            </a>
           </td>
           <td class="r">
            Johns Hopkins University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#170579" title="Click to go to the Author Index">
             Lee, Yong Jae
            </a>
           </td>
           <td class="r">
            UW-Madison
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392094" title="Click to go to the Author Index">
             Yang, Xiaodong
            </a>
           </td>
           <td class="r">
            QCraft
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab440" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#representation_learning" title="Click to go to the Keyword Index">
               Representation Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             3D perception in LiDAR point clouds is crucial for a self-driving vehicle to properly act in 3D environment. However, manually labeling point clouds is hard and costly. There has been a growing interest in self-supervised pre-training of 3D perception models. Following the success of contrastive learning in images, current methods mostly conduct contrastive pre-training on point clouds only. Yet an autonomous driving vehicle is typically supplied with multiple sensors including cameras and LiDAR. In this context, we systematically study single modality, cross-modality, and multi-modality for contrastive learning of point clouds, and show that cross-modality wins over other alternatives. In addition, considering the huge difference between the training sources in 2D images and 3D point clouds, it remains unclear how to design more effective contrastive units for LiDAR. We therefore propose the instance-aware and similarity-balanced contrastive units that are tailored for self-driving point clouds. Extensive experiments reveal that our approach achieves remarkable performance gains over various point cloud models across the downstream perception tasks of LiDAR based 3D object detection and 3D semantic segmentation on the four popular benchmarks including Waymo Open Dataset, nuScenes, SemanticKITTI and ONCE.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t5_02">
             15:30-16:30, Paper ThPI5T5.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3390'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              DAP: Diffusion-Based Affordance Prediction for Multi-Modality Storage
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#219127" title="Click to go to the Author Index">
             Chang, Haonan
            </a>
           </td>
           <td class="r">
            Rutgers University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#377752" title="Click to go to the Author Index">
             Boyalakuntla, Kowndinya
            </a>
           </td>
           <td class="r">
            Rutgers University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#285764" title="Click to go to the Author Index">
             Liu, Yuhan
            </a>
           </td>
           <td class="r">
            Rutgers University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#354495" title="Click to go to the Author Index">
             Zhang, Xinyu
            </a>
           </td>
           <td class="r">
            Rutgers University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#239585" title="Click to go to the Author Index">
             Schramm, Liam
            </a>
           </td>
           <td class="r">
            Rutgers University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#114191" title="Click to go to the Author Index">
             Boularias, Abdeslam
            </a>
           </td>
           <td class="r">
            Rutgers University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3390" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#representation_learning" title="Click to go to the Keyword Index">
               Representation Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Solving storage problemswhere objects must be accurately placed into containers with precise orientations and positionspresents a distinct challenge that extends beyond traditional rearrangement tasks. These challenges are primarily due to the need for fine-grained 6D manipulation and the inherent multi-modality of solution spaces, where multiple viable goal configurations exist for the same storage container. We present a novel Diffusion-based Affordance Prediction (DAP) pipeline for the multi-modal object storage problem. DAP leverages a two-step approach, initially identifying a placeable region on the container and then precisely computing the relative pose between the object and that region. Existing methods either struggle with multi-modality issues or computation-intensive training. Our experiments demonstrate DAP's superior performance and training efficiency over the current state-of-the-art RPDiff, achieving remarkable results on the RPDiff benchmark. Instead of requiring days of training time as previous methods, DAP only requires hours of training time, making it easy to be deployed on new tasks.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t5_03">
             15:30-16:30, Paper ThPI5T5.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('620'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Object Pose Estimation by Camera Arm Control Based on Viewpoint Estimation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#393346" title="Click to go to the Author Index">
             Mizuno, Tomoki
            </a>
           </td>
           <td class="r">
            University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#393369" title="Click to go to the Author Index">
             Yabashi, Kazuya
            </a>
           </td>
           <td class="r">
            University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#126893" title="Click to go to the Author Index">
             Tasaki, Tsuyoshi
            </a>
           </td>
           <td class="r">
            Meijo University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab620" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We have developed a new method to estimate a Next Viewpoint (NV) which is effective for pose estimation of unknown simple-shaped products for product display robots in retail stores. Pose estimation methods using Neural Networks (NN) based on an RGBD camera are highly accurate, but their accuracy significantly decreases when the camera acquires few texture and shape features at a current view point. However, it is difficult for previous mathematical model-based methods to estimate effective NV because the simple shaped objects have few shape features. Therefore, we focus on the relationship between pose estimation and NV estimation. When the pose estimation is more accurate, the NV estimation is more accurate. Therefore, we develop a new pose estimation NN that estimates NV simultaneously. Experimental results showed that our NV estimation realized pose estimation success rate 77.3%, which was 7.4pt higher than mathematical model-based NV calculation did. Moreover, we verified that the robot using our method displayed 84.2% of products.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t5_04">
             15:30-16:30, Paper ThPI5T5.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1679'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              CoPa: General Robotic Manipulation through Spatial Constraints of Parts with Foundation Models
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396959" title="Click to go to the Author Index">
             Huang, Haoxu
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#389865" title="Click to go to the Author Index">
             Lin, Fanqi
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396960" title="Click to go to the Author Index">
             Hu, Yingdong
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#232454" title="Click to go to the Author Index">
             Wang, Shengjie
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#191564" title="Click to go to the Author Index">
             Gao, Yang
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1679" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#ai_based_methods" title="Click to go to the Keyword Index">
               AI-Based Methods
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#manipulation_planning" title="Click to go to the Keyword Index">
               Manipulation Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Foundation models pre-trained on web-scale data are shown to encapsulate extensive world knowledge beneficial for robotic manipulation in the form of task planning. However, the actual physical implementation of these plans often relies on task-specific learning methods, which require significant data collection and struggle with generalizability. In this work, we introduce Robotic Manipulation through Spatial Constraints of Parts (CoPa), a novel framework that leverages the common sense knowledge embedded within foundation models to generate a sequence of 6-DoF end-effector poses for open-world robotic manipulation. Specifically, we decompose the manipulation process into two phases: task-oriented grasping and task-aware motion planning. In the task-oriented grasping phase, we employ foundation vision-language models (VLMs) to select the objects grasping part through a novel coarse-to-fine grounding mechanism. During the task-aware motion planning phase, VLMs are utilized again to identify the spatial geometry constraints of task-relevant object parts, which are then used to derive post-grasp poses. We also demonstrate how CoPa can be seamlessly integrated with existing robotic planning algorithms to accomplish complex, long-horizon tasks. Our comprehensive real-world experiments show that CoPa possesses a fine-grained physical understanding of scenes, capable of handling open-set instructions and objects with minimal prompt engineering and without additional training. Project page: https://copa-2024.github.io/
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t5_05">
             15:30-16:30, Paper ThPI5T5.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2876'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              MultipleCupSuctionNet: Deep Neural Network for Detecting Grasp Pose of a Vacuum Gripper with Multiple Suction Cups Based on YOLO Feature Map Affine Transformation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#242246" title="Click to go to the Author Index">
             Jiang, Ping
            </a>
           </td>
           <td class="r">
            Toshiba Corporation
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#224177" title="Click to go to the Author Index">
             Komoda, Kazuma
            </a>
           </td>
           <td class="r">
            Toshiba Corporation
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#213230" title="Click to go to the Author Index">
             Han, Haifeng
            </a>
           </td>
           <td class="r">
            Toshiba Corporation
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#123265" title="Click to go to the Author Index">
             Ooga, Jun'ichiro
            </a>
           </td>
           <td class="r">
            Toshiba Corporation
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2876" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#grasping" title="Click to go to the Keyword Index">
               Grasping
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#ai_based_methods" title="Click to go to the Keyword Index">
               AI-Based Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Multiple-suction-cup grasp is preferable for picking large and heavy objects in warehouses. Deep learning methods have been widely used to predict grasp point position for single-suction-cup grasp, but few studies have examined grasp pose detection for a gripper with multiple suction cups. This study proposes MultipleCupSuctionNet, which is a deep neural network for detecting multiple cup grasp pose. To address the challenge of direct regression of poses, this neural network first infers the surface mask to compute the surface normal to obtain the direction of the gripper z-axis. The feature maps of the surfaces are then affine-transformed to surface image coordinates, based on which gripper position and rotation angle around the z-axis are predicted. Such a neural network design makes grasp pose learning easier because there is no need to consider the orientation of the surfaces so that 2D poses respective to the surface are learned. Feature map affine transformation saves computation cost because there is no need to first transform images and then extract the features to obtain surface features. Further, for each predicted grasp pose, the overlap area between cup and surface is calculated to determine which cup should be used when grasping. MultipleCupSuctionNet exhibited competitive performance (80.1% prediction accuracy) particularly in dense scenes compared with state-of-the-art planners (Dex-Net and a model-free multiple-suction-cup grasp planner). Physical picking experiments were conducted using a robot employing the proposed neural network. The experimental results showed that our robot achieved an average success rate of 94.5% for picking common objects in warehouses.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t5_06">
             15:30-16:30, Paper ThPI5T5.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3142'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Discretizing SO(2)-Equivariant Features for Robotic Kitting
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#225597" title="Click to go to the Author Index">
             Zhou, Jiadong
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#194250" title="Click to go to the Author Index">
             Zeng, Yadan
            </a>
           </td>
           <td class="r">
            Nanyang Technology University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#204350" title="Click to go to the Author Index">
             Dong, Huixu
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#100042" title="Click to go to the Author Index">
             Chen, I-Ming
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3142" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robotic kitting has attracted considerable attention in logistics and industrial settings. However, existing kitting methods encounter challenges such as low precision and poor efficiency, limiting their widespread applications. To address these issues, we present a novel kitting framework that improves both the precision and computational efficiency of complex kitting tasks. Firstly, our approach introduces a fine-grained orientation estimation technique in the picking module, significantly enhancing orientation precision while effectively decoupling computational load from orientation granularity. This technique combines an SO(2)-equivariant network with a group discretization operation to preciously predict discrete orientation distributions. Secondly, we develop the Hand-Tool Kitting Dataset (HTKD) to evaluate different solutions in handling orientation-sensitive kitting tasks. This dataset comprises a diverse collection of hand tools and synthetically created kits, which reflects the complexities of real-world kitting scenarios. Finally, a series of experiments is conducted to evaluate the performance of the proposed method. The results demonstrate that our approach offers an excellent balance between success rates and computational efficiency in high-precision robotic kitting tasks.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t5_07">
             15:30-16:30, Paper ThPI5T5.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1904'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Kosmos-E: Learning to Follow Instruction for Robotic Grasping
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397238" title="Click to go to the Author Index">
             Wang, Zhi
            </a>
           </td>
           <td class="r">
            Microsoft
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397193" title="Click to go to the Author Index">
             Wu, Xun
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397216" title="Click to go to the Author Index">
             Wu, Xun
            </a>
           </td>
           <td class="r">
            Microsoft
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397230" title="Click to go to the Author Index">
             Dong, Li
            </a>
           </td>
           <td class="r">
            Microsoft
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397226" title="Click to go to the Author Index">
             Wenhui, Wang
            </a>
           </td>
           <td class="r">
            Microsoft
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397249" title="Click to go to the Author Index">
             Ma, Shuming
            </a>
           </td>
           <td class="r">
            Microsoft
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397247" title="Click to go to the Author Index">
             Wei, Furu
            </a>
           </td>
           <td class="r">
            Microsoft
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1904" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#grasping" title="Click to go to the Keyword Index">
               Grasping
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Tuning on instruction-following data has been shown to enhance the capabilities and controllability of language models, but the idea is less explored in the robotic field. In this work, we introduce Kosmos-E, a Multimodal Large Language Model (MLLM) that leverages instruction-following robotic grasping data to enhance capabilities for precise and intricate robotic grasping maneuvers. To achieve this, we craft a large-scale instruction-following robotic grasping dataset, termed Instruct-Grasp, primarily comprising two aspects: (i) grasp a single object following varying levels of granularity descriptions, e.g., different angles and aspects, and (ii) grasp a specific object within a multi-object environment following specific attributes, e.g., color and shape. Extensive experiments show the effectiveness of Kosmos-E on robotic grasping tasks across a variety of environments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t5_08">
             15:30-16:30, Paper ThPI5T5.8
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2509'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Avoiding Object Damage in Robotic Manipulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#377407" title="Click to go to the Author Index">
             Aduh, Erica
            </a>
           </td>
           <td class="r">
            Amazon Robotics
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#212074" title="Click to go to the Author Index">
             Wang, Fan
            </a>
           </td>
           <td class="r">
            Amazon Robotics
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#377569" title="Click to go to the Author Index">
             Randle, Dylan Labatt
            </a>
           </td>
           <td class="r">
            Amazon Robotics
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#377552" title="Click to go to the Author Index">
             Wang, Kaiwen
            </a>
           </td>
           <td class="r">
            Amazon
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#375740" title="Click to go to the Author Index">
             Shah, Priyesh
            </a>
           </td>
           <td class="r">
            Amazon
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#205886" title="Click to go to the Author Index">
             Mitash, Chaitanya
            </a>
           </td>
           <td class="r">
            Amazon Robotics
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#271698" title="Click to go to the Author Index">
             Nambi, Manikantan
            </a>
           </td>
           <td class="r">
            Amazon Robotics
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2509" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#industrial_robots" title="Click to go to the Keyword Index">
               Industrial Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The large-scale deployment of robotic manipulation systems in warehouses has highlighted the rare but costly problem of robot-induced object damage. We present a system that uses a classification model to predict whether an object will get damaged during robotic manipulation. The model uses object attributes retrieved from warehouse information systems as well as attributes available at our robotic workcell. We evaluated different classical machine learning models, as well as a large language model (BERT) and a multimodal-transformer for our task. We show that the multi-modal transformer model that is able to leverage text and image data outperforms models that only rely on categorical and numerical data. Furthermore, our comparative analysis equips the selection the optimal model for an application. We validate our system during an experiment in which the output of the damage prediction system is used to avoid picking objects that are likely to get damaged. In over 50k pick-and-place activities, our system reduces damage rate by 64%.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t5_09">
             15:30-16:30, Paper ThPI5T5.9
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1642'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              APEX: Ambidextrous Dual-Arm Robotic Manipulation Using Collision-Free Generative Diffusion Models
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#276258" title="Click to go to the Author Index">
             Dastider, Apan
            </a>
           </td>
           <td class="r">
            University of Central Florida
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#373815" title="Click to go to the Author Index">
             Fang, Hao
            </a>
           </td>
           <td class="r">
            University of Central Florida
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#222632" title="Click to go to the Author Index">
             Mingjie, Lin
            </a>
           </td>
           <td class="r">
            University of Central Florida
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1642" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#dual_arm_manipulation" title="Click to go to the Keyword Index">
               Dual Arm Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#task_and_motion_planning" title="Click to go to the Keyword Index">
               Task and Motion Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Dexterous manipulation, particularly adept coordinating and grasping, constitutes a fundamental and indispensable capability for robots, facilitating the emulation of human-like behaviors. Integrating this capability into robots empowers them to supplement and even supplant humans in undertaking increasingly intricate tasks in both daily life and industrial settings. Unfortunately, contemporary methodologies encounter serious challenges in devising manipulation trajectories owing to the intricacies of tasks, the expansive robotic manipulation space, and dynamic obstacles. We propose a novel approach, APEX, to address all these difficulties by introducing a collision free latent diffusion model for both robotic motion planning and manipulation. Firstly, we simplify the complexity of real-life ambidextrous dual-arm robotic manipulation tasks by abstracting them as aligning two vectors. Secondly, we devise latent diffusion models to produce a variety of robotic manipulation trajectories. Furthermore, we integrate obstacle information utilizing a classifier-guidance technique, thereby guaranteeing both the feasibility and safety of the generated manipulation trajectories. Lastly, we validate our proposed algorithm through extensive experiments conducted on the hardware platform of ambidextrous dual-arm robots. Our algorithm consistently generates successful and seamless trajectories across diverse tasks, surpassing conventional robotic motion planning algorithms. These results carry significant implications for the future design of diffusion robots, enhancing their capability to tackle more intricate robotic manipulation tasks with increased efficiency and safety. Complete video demonstrations of our experiments can be found in https://sites.google.com/view/apex-dual-arm/home.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t5_10">
             15:30-16:30, Paper ThPI5T5.10
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2678'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Inverse Kinematics of Robotic Manipulators Using a New Learning-By-Example Method
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#291933" title="Click to go to the Author Index">
             Demby's, Jacket
            </a>
           </td>
           <td class="r">
            University of Missouri-Columbia
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398517" title="Click to go to the Author Index">
             Farag, Ramy
            </a>
           </td>
           <td class="r">
            University of Missouri-Columbia
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#107532" title="Click to go to the Author Index">
             DeSouza, Guilherme
            </a>
           </td>
           <td class="r">
            University of Missouri-Columbia
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2678" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#kinematics" title="Click to go to the Keyword Index">
               Kinematics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#machine_learning_for_robot_control" title="Click to go to the Keyword Index">
               Machine Learning for Robot Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Inverse Kinematics (IK) is one of the most fundamental challenges in robotics. It refers to the process of determining the joint configurations required to achieve the desired position and orientation (pose) of a robot end-effector. Although numerous Data-Driven (DD) IK solvers have demonstrated encouraging results, they have not achieved the same accuracy when compared to other IK methods for complex robot configurations (e.g., numerical methods for higher Degrees of Freedom (DoF)). In this work, we propose a new Learning-by-Example method, and show that such a scheme considerably improves the IK learning results when compared to other DD learners. In our approach, the network input incorporates an example of joint-pose pair along with the query pose to predict the desired robot joint configuration. We show that the example joint-pose pair does not need to be too close to the query -- i.e. example and query can be as far as 20 degrees apart in the joint configuration space. Furthermore, we investigate the utilization of residual and dense skip connections in Multilayer Perceptron for DDIK solvers and employ the resulting networks for two redundant robotic manipulators: a 7-DoF-7R commensurate robot and a 7-DoF-2RP4R incommensurate robot. Our experimental results show that the resulting DDIK solver can reliably predict IK solutions with accuracy better than 1 mm in position and 1 deg in orientation.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t5_11">
             15:30-16:30, Paper ThPI5T5.11
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('881'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Pseudo-Rigid Body Networks: Learning Interpretable Deformable Object Dynamics from Partial Observations
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#286894" title="Click to go to the Author Index">
             Mamedov, Shamil
            </a>
           </td>
           <td class="r">
            KU Leuven
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#192175" title="Click to go to the Author Index">
             Geist, Andreas Ren
            </a>
           </td>
           <td class="r">
            Max Planck Institute for Intelligent Systems
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#114656" title="Click to go to the Author Index">
             Swevers, Jan
            </a>
           </td>
           <td class="r">
            KU Leuven
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#133317" title="Click to go to the Author Index">
             Trimpe, Sebastian
            </a>
           </td>
           <td class="r">
            RWTH Aachen University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab881" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#model_learning_for_control" title="Click to go to the Keyword Index">
               Model Learning for Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#modeling__control__and_learning_for_soft_robots" title="Click to go to the Keyword Index">
               Modeling, Control, and Learning for Soft Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Accurately predicting deformable linear object (DLO) dynamics is challenging, especially when the task requires a model that is both human-interpretable and computationally efficient. In this work, we draw inspiration from the pseudo-rigid body method (PRB) and model a DLO as a serial chain of rigid bodies whose internal state is unrolled through time by a dynamics network. This dynamics network is trained jointly with a physics-informed encoder that maps observed motion variables to the DLO's hidden state. To encourage the state to acquire a physically meaningful representation, we leverage the forward kinematics of the PRB model as a decoder. We demonstrate in robot experiments that the proposed DLO dynamics model provides physically interpretable predictions from partial observations while being on par with black-box models regarding prediction accuracy. The project code is available at: tinyurl.com/prb-networks
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t5_12">
             15:30-16:30, Paper ThPI5T5.12
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('370'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Object Segmentation from Open-Vocabulary Manipulation Instructions Based on Optimal Transport Polygon Matching with Multimodal Foundation Models
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#390891" title="Click to go to the Author Index">
             Nishimura, Takayuki
            </a>
           </td>
           <td class="r">
            Keio University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#350315" title="Click to go to the Author Index">
             Kuyo, Katsuyuki
            </a>
           </td>
           <td class="r">
            Keio University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#295707" title="Click to go to the Author Index">
             Kambara, Motonari
            </a>
           </td>
           <td class="r">
            Keio University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#117516" title="Click to go to the Author Index">
             Sugiura, Komei
            </a>
           </td>
           <td class="r">
            Keio University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab370" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We consider the task of generating segmentation masks for the target object from an object manipulation instruction, allowing users to give open vocabulary instructions to domestic service robots. The conventional segmentation generation approaches often fail to account for objects outside the camera's field of view and cases where the order of vertices differs but still represents the same polygon, which leads to erroneous mask generation. In this study, we propose a novel methodology which performs generating segmentation from open vocabulary instructions. We implement a novel loss function using optimal transport to prevent significant loss where the order of vertices differs but still represents the same polygon. To evaluate our approach, we constructed a new dataset based on the REVERIE dataset and the Matterport3D dataset. Results demonstrate the effectiveness of the proposed method against existing mask generation methods; Remarkably, our best model showed a +16.32% improvement on the dataset compared to a representative polygon-based method.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t5_13">
             15:30-16:30, Paper ThPI5T5.13
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('448'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Channel-Wise Motion Features for Efficient Motion Segmentation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392092" title="Click to go to the Author Index">
             Inoue, Riku
            </a>
           </td>
           <td class="r">
            Honda R&amp;D Co., Ltd
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392428" title="Click to go to the Author Index">
             Tsuchiya, Masamitsu
            </a>
           </td>
           <td class="r">
            Honda R&amp;D Co.Ltd
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#253119" title="Click to go to the Author Index">
             Yasui, Yuji
            </a>
           </td>
           <td class="r">
            Honda R&amp;D Co., Ltd
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab448" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_transportation" title="Click to go to the Keyword Index">
               Computer Vision for Transportation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             For safety-critical robotics applications such as autonomous driving, it is important to detect all required objects accurately in real-time. Motion segmentation offers a solution by identifying dynamic objects from the scene in a class-agnostic manner. Recently, various motion segmentation models have been proposed, most of which jointly use subnetworks to estimate Depth, Pose, Optical Flow, and Scene Flow. As a result, the overall computational cost of the model increases, hindering real-time performance. In this paper, we propose a novel cost-volume-based motion feature representation, Channel-wise Motion Features. By extracting depth features of each instance in the feature map and capturing the scene's 3D motion information, it offers enhanced efficiency. The only subnetwork used to build Channel-wise Motion Features is the Pose Network, and no others are required. Our method not only achieves about 4 times the FPS of state-of-the-art models in the KITTI Dataset and Cityscapes of the VCAS-Motion Dataset, but also demonstrates equivalent accuracy while reducing the parameters to about 25%.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t5_14">
             15:30-16:30, Paper ThPI5T5.14
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1761'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              InverseMatrixVT3D: An Efficient Projection Matrix-Based Approach for 3D Occupancy Prediction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397176" title="Click to go to the Author Index">
             Ming, Zhenxing
            </a>
           </td>
           <td class="r">
            The University of Sydney
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#219093" title="Click to go to the Author Index">
             Berrio Perez, Julie Stephany
            </a>
           </td>
           <td class="r">
            ACFR - the University of Sydney
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#159718" title="Click to go to the Author Index">
             Shan, Mao
            </a>
           </td>
           <td class="r">
            The University of Sydney
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#103694" title="Click to go to the Author Index">
             Worrall, Stewart
            </a>
           </td>
           <td class="r">
            University of Sydney
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1761" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_transportation" title="Click to go to the Keyword Index">
               Computer Vision for Transportation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper introduces InverseMatrixVT3D, an efficient method for transforming multi-view image features into 3D feature volumes for 3D semantic occupancy prediction. Existing methods for constructing 3D volumes often rely on depth estimation, device-specific operators, or transformer queries, which hinders the widespread adoption of 3D occupancy models. In contrast, our approach leverages two projection matrices to store the static mapping relationships and matrix multiplications to efficiently generate global Bird's Eye View (BEV) features and local 3D feature volumes. Specifically, we achieve this by performing matrix multiplications between multi-view image feature maps and two sparse projection matrices. We introduce a sparse matrix handling technique for the projection matrices to optimize GPU memory usage. Moreover, a global-local attention fusion module is proposed to integrate the global BEV features with the local 3D feature volumes to obtain the final 3D volume. We also employ a multi-scale supervision mechanism to enhance performance further. Extensive experiments performed on the nuScenes and SemanticKITTI datasets reveal that our approach not only stands out for its simplicity and effectiveness but also achieves the top performance in detecting vulnerable road users (VRU), crucial for autonomous driving and road safety. The code has been made available at: https://github.com/DanielMing123/InverseMatrixVT3D
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t5_15">
             15:30-16:30, Paper ThPI5T5.15
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1786'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Density-Aware Domain Generalization for LiDAR Semantic Segmentation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#317987" title="Click to go to the Author Index">
             Kim, Jaeyeul
            </a>
           </td>
           <td class="r">
            DGIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#320488" title="Click to go to the Author Index">
             Woo, Jungwan
            </a>
           </td>
           <td class="r">
            DGIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#241049" title="Click to go to the Author Index">
             Shin, Ukcheol
            </a>
           </td>
           <td class="r">
            CMU(Carnegie Mellon University)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#179403" title="Click to go to the Author Index">
             Oh, Jean
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#212740" title="Click to go to the Author Index">
             Im, Sunghoon
            </a>
           </td>
           <td class="r">
            DGIST
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1786" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_transportation" title="Click to go to the Keyword Index">
               Computer Vision for Transportation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             3D LiDAR-based perception has made remarkable advancements, leading to the widespread adoption of LiDAR in autonomous driving systems. Despite these technological strides, variations in LiDAR sensors and environmental conditions can significantly deteriorate the performance of perception models, primarily due to changes in the density of point clouds. Recent studies in domain generalization have aimed to mitigate this challenge; however, they often rely on the availability of sequential data and ego-motion, which limits their applicability. To address these limitations, we propose two novel methods that enable network operation in a density-aware fashion without any constraints, thereby ensuring consistent performance despite fluctuations in point cloud density. First, we design the network to be density-aware by utilizing the kernel occupancy information from the 3D sparse convolution as geometric features. Subsequently, we further enhance density awareness by incorporating voxel-wise density prediction as an auxiliary task in a self-supervised manner. Our method demonstrates superior performance over current state-of-the-art approaches, achieving this without the need for specific data prerequisites. Our approach is compatible with a variety of 3D backbone architectures, enhancing domain generalization performance by 18.4% while adding a minimal computational overhead of only 7ms.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t5_16">
             15:30-16:30, Paper ThPI5T5.16
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('545'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              QuerySOD: A Small Object Detection Algorithm Based on Sparse Convolutional Network and Query Mechanism
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#111763" title="Click to go to the Author Index">
             Cao, Zhengcai
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#344116" title="Click to go to the Author Index">
             Li, Junnian
            </a>
           </td>
           <td class="r">
            Beijing Univ. of Chemical Tech
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#374147" title="Click to go to the Author Index">
             Niu, Jie
            </a>
           </td>
           <td class="r">
            Beijing University of Chemical Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#112540" title="Click to go to the Author Index">
             Zhou, MengChu
            </a>
           </td>
           <td class="r">
            New Jersey Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab545" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#marine_robotics" title="Click to go to the Keyword Index">
               Marine Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Although remarkable advances have been achieved in generic object detection, small object detection (SOD) remains challenging owing to small objects information loss and noisy representation caused by their non-uniform distribution. Their limited width and height, scale variations, and redundant computation make SOD hard. To overcome them, this work proposes a new SOD method based on sparse convolutional network (SCNet) and Query Mechanism called QuerySOD. First, an extended feature pyramid network is constructed for extracting feature maps of small objects with more regional details. Then, a Sparse Head is neatly designed by using SCNet for accelerating the interfering speed and obtaining weights of each layer. After that, a Query Mechanism is innovatively introduced for harvesting the benefit of sparse value feature maps from the Sparse Head. QuerySOD is evaluated on public benchmarks including COCO and VisDrone. Finally, we apply it on Jinghai unmanned survey vehicles and receive excellent SOD performance from this real-world application.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t5_17">
             15:30-16:30, Paper ThPI5T5.17
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2742'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Learning-On-The-Drive: Self-Supervised Adaptive Long-Range Perception for High-Speed Offroad Driving
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#364420" title="Click to go to the Author Index">
             Chen, Eric
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#215359" title="Click to go to the Author Index">
             Ho, Cherie
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#364421" title="Click to go to the Author Index">
             Maulimov, Mukhtar
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#200000" title="Click to go to the Author Index">
             Wang, Chen
            </a>
           </td>
           <td class="r">
            University at Buffalo
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#104304" title="Click to go to the Author Index">
             Scherer, Sebastian
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2742" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#sensor_fusion" title="Click to go to the Keyword Index">
               Sensor Fusion
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Autonomous offroad driving is essential for applications like emergency rescue, military operations, and agriculture. Despite progress, systems struggle with high-speed vehicles exceeding 10m/s due to the need for accurate long-range (&gt;50m) perception for safe navigation. Current approaches are limited by sensor constraints; LiDAR-based methods offer precise short-range data but are noisy beyond 30m, while visual models provide dense long-range measurements but falter with unseen scenarios. To overcome these issues, we introduce
             <i>
              ALTER
             </i>
             , a
             <i>
              learning-on-the-drive
             </i>
             perception framework that leverages both sensor types. ALTER uses a
             <i>
              self-supervised
             </i>
             visual model to learn and adapt from near-range LiDAR measurements, improving long-range prediction in new environments without manual labeling. It also includes a model selection module for better sensor failure response and adaptability to known environments. Testing in two real-world settings showed on average 43.4% better traversability prediction than LiDAR-only and 164% over non-adaptive state-of-the-art (SOTA) visual semantic methods after 45 seconds of online learning.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thpi5t6">
             <b>
              ThPI5T6
             </b>
            </a>
           </td>
           <td class="r">
            Room 6
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thpi5t6" title="Click to go to the Program at a Glance">
             <b>
              Learning IV
             </b>
            </a>
           </td>
           <td class="r">
            Teaser Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#206366" title="Click to go to the Author Index">
             Bardaro, Gianluca
            </a>
           </td>
           <td class="r">
            Politecnico Di Milano
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#308158" title="Click to go to the Author Index">
             Fu, Yanwei
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t6_01">
             15:30-16:30, Paper ThPI5T6.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1555'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              GenChIP: Generating Robot PolicyCode forHigh-Precision and Contact-Rich Manipulation Tasks
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#275689" title="Click to go to the Author Index">
             Burns, Kaylee
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#193710" title="Click to go to the Author Index">
             Jain, Ajinkya
            </a>
           </td>
           <td class="r">
            Intrinsic Innovation LLC
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395987" title="Click to go to the Author Index">
             Go, Keegan
            </a>
           </td>
           <td class="r">
            Intrinsic Innovation LLC
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#235958" title="Click to go to the Author Index">
             Xia, Fei
            </a>
           </td>
           <td class="r">
            Google Inc
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396631" title="Click to go to the Author Index">
             Stark, Michael
            </a>
           </td>
           <td class="r">
            Intrinsic Innovation LLC
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#102004" title="Click to go to the Author Index">
             Schaal, Stefan
            </a>
           </td>
           <td class="r">
            Google X
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#154015" title="Click to go to the Author Index">
             Hausman, Karol
            </a>
           </td>
           <td class="r">
            Google Brain
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1555" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#ai_based_methods" title="Click to go to the Keyword Index">
               AI-Based Methods
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#big_data_in_robotics_and_automation" title="Click to go to the Keyword Index">
               Big Data in Robotics and Automation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#compliance_and_impedance_control" title="Click to go to the Keyword Index">
               Compliance and Impedance Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Large Language Models (LLMs) have been successful at generating robot policy code, but so far these results have been limited to high-level tasks that do not require accurate movement. It is an open question how well such approaches work for tasks that require reasoning over contacts forces and working within tight success tolerances. We find that, with the right action space, LLMs are capable of successfully generating policies for a variety of contact-rich and high-precision manipulation tasks. Specifically, we reparameterize the action space to include compliance with constraints on the interaction forces and stiffnesses involved in reaching a target pose. We validate this approach on subtasks derived from the Functional Manipulation Benchmark (FMB) and NIST Task Board Benchmarks, where policy generation with an LLM in this action space improves success rates over generation in non-compliant action spaces by greater than 3x and 4x, respectively.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t6_02">
             15:30-16:30, Paper ThPI5T6.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1897'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              LLaKey: Follow My Basic Action Instructions to Your Next Key State
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#393375" title="Click to go to the Author Index">
             Zhao, Zheyi
            </a>
           </td>
           <td class="r">
            Guangdong Laboratory of Artificial Intelligence and Digital Econ
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#374365" title="Click to go to the Author Index">
             He, Ying
            </a>
           </td>
           <td class="r">
            Shenzhen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#393394" title="Click to go to the Author Index">
             Yu, Fei
            </a>
           </td>
           <td class="r">
            Guangming Lab
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#376116" title="Click to go to the Author Index">
             Li, Pengteng
            </a>
           </td>
           <td class="r">
            Shenzhen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#393371" title="Click to go to the Author Index">
             Zhuo, Fan
            </a>
           </td>
           <td class="r">
            Guangdong Laboratory of Artificial Intelligence and Digital Econ
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#302867" title="Click to go to the Author Index">
             Sun, Xilong
            </a>
           </td>
           <td class="r">
            Kuban State University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1897" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#ai_based_methods" title="Click to go to the Keyword Index">
               AI-Based Methods
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#manipulation_planning" title="Click to go to the Keyword Index">
               Manipulation Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In 3D object manipulation, collecting expert data for end-to-end imitation learning becomes a mainstream method. Though successful, previous works neglect the guiding role of language in action execution. These methods lack the understanding of action semantics, in which multiple action sequences are guided by a category of instructions, resulting in overlearned object semantics and vague action semantics. To address the above limitation, we introduce a novel framework named LLaKey, which breaks down skill commands into more detailed action instructions based on key states for fine-grained action control. Specifically, LLaKey first leverages the knowledge encoded in pre-trained large-scale models to fine-tune an action instruction conductor. Then, these instructions are executed by a downstream action model. Comprehensive experiments show that LLaKey significantly surpasses baselines with a relative improvement of 15% in nine complex and varied skill tasks, demonstrating the superiority of our method.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t6_03">
             15:30-16:30, Paper ThPI5T6.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3233'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              LANCAR: Leveraging Language for Context-Aware Robot Locomotion in Unstructured Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#365912" title="Click to go to the Author Index">
             Shek, Chak Lam
            </a>
           </td>
           <td class="r">
            University of Maryland, College Park
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#367351" title="Click to go to the Author Index">
             Wu, Xiyang
            </a>
           </td>
           <td class="r">
            University of Maryland
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#376285" title="Click to go to the Author Index">
             Suttle, Wesley A.
            </a>
           </td>
           <td class="r">
            DEVCOM ARL
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#377807" title="Click to go to the Author Index">
             Busart, Carl
            </a>
           </td>
           <td class="r">
            US Army Research Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#399092" title="Click to go to the Author Index">
             Zaroukian, Erin
            </a>
           </td>
           <td class="r">
            DEVCOM ARL
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#106235" title="Click to go to the Author Index">
             Manocha, Dinesh
            </a>
           </td>
           <td class="r">
            University of Maryland
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#128263" title="Click to go to the Author Index">
             Tokekar, Pratap
            </a>
           </td>
           <td class="r">
            University of Maryland
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#274898" title="Click to go to the Author Index">
             Bedi, Amrit Singh
            </a>
           </td>
           <td class="r">
            University of Maryland, College Park
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3233" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#ai_based_methods" title="Click to go to the Keyword Index">
               AI-Based Methods
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#machine_learning_for_robot_control" title="Click to go to the Keyword Index">
               Machine Learning for Robot Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Navigating robots through unstructured terrains is challenging, primarily due to the dynamic environmental changes. While humans adeptly navigate such terrains by using context from their observations, creating a similar context-aware navigation system for robots is difficult. The essence of the issue lies in the acquisition and interpretation of context information, a task complicated by the inherent ambiguity of human language. In this work, we introduce LANCAR, which addresses this issue by combining a context translator with reinforcement learning (RL) agents for context-aware locomotion. LANCAR allows robots to comprehend context information through Large Language Models (LLMs) sourced from human observers and convert this information into actionable context embeddings. These embeddings, combined with the robot's sensor data, provide a complete input for the RL agent's policy network. We provide an extensive evaluation of LANCAR under different levels of context ambiguity and compare with alternative methods. The experimental results showcase the superior generalizability and adaptability across different terrains. Notably, LANCAR shows at least a 7.4% increase in episodic reward over the best alternatives, highlighting its potential to enhance robotic navigation in unstructured environments. More details and experiment videos could be found in http://raaslab.org/projects/LLM_Context_Estimation/.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t6_04">
             15:30-16:30, Paper ThPI5T6.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1043'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Ensuring Safety in LLM-Driven Robotics: A Cross-Layer Sequence Supervision Mechanism
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395233" title="Click to go to the Author Index">
             Wang, Ziming
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#241402" title="Click to go to the Author Index">
             Liu, Qingchen
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#244245" title="Click to go to the Author Index">
             Qin, Jiahu
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#244239" title="Click to go to the Author Index">
             Li, Man
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1043" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#human_centered_robotics" title="Click to go to the Keyword Index">
               Human-Centered Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#task_planning" title="Click to go to the Keyword Index">
               Task Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Integrating Large Language Models (LLMs) into robotics significantly enhances autonomous task planning. However, ensuring that multi-step task plans (action sequence) generated by LLMs comply with pre-defined safety constraints during planning and execution remains a challenge, limiting their adaptability in complex environments. To address this issue, a mechanism that can monitor and adjust the plan generated by the LLM-driven task planner and guide the motion planner to avoid potential risks during action execution is required. Therefore, this paper proposes a cross-layer sequence supervision mechanism. Specifically, we employ linear temporal logic syntax to express safety constraints and convert them into a set of nondeterministic Bchi automatons to build a cross-layer safety supervisor. For the task planning layer, the safety supervisor provides a closed-loop correction mechanism that can identify violations in the task plan in real time and guide LLM-driven planners to correct this plan to ensure compliance. For the motion planning layer, the safety supervisor introduces virtual ``obstacle" information into the task plan to form the task plan tuple. Based on this plan tuple, the motion planner can proactively prevent unsafe behaviors during action execution. Extensive experimentation demonstrates significant improvements in safety with this cross-layer supervision mechanism, highlighting its potential to enhance LLM-driven robotic technology.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t6_05">
             15:30-16:30, Paper ThPI5T6.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1220'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              NARRATE: Versatile Language Architecture for Optimal Control in Robotics
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395978" title="Click to go to the Author Index">
             Ismail, Seif
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395979" title="Click to go to the Author Index">
             Arbues, Antonio
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395996" title="Click to go to the Author Index">
             Cotterell, Ryan
            </a>
           </td>
           <td class="r">
            ETH Zrich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#321088" title="Click to go to the Author Index">
             Zurbrgg, Ren
            </a>
           </td>
           <td class="r">
            ETH Zrich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#315882" title="Click to go to the Author Index">
             Amo Alonso, Carmen
            </a>
           </td>
           <td class="r">
            Caltech
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1220" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#control_architectures_and_programming" title="Click to go to the Keyword Index">
               Control Architectures and Programming
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The impressive capabilities of Large Language Models (LLMs) have led to various efforts in enabling robots to be controlled through natural language instructions, opening exciting possibilities for human-robot interaction. The goal is for the motor-control task to be performed accurately, efficiently and safely while also enjoying the flexibility imparted by LLMs to specify and adjust the task through natural language. In this work, we demonstrate how a careful layering of an LLM in combination with a Model Predictive Control (MPC) formulation allows for accurate and flexible robotic control via natural language while taking into consideration safety constraints. In particular, we rely on the LLM to effectively frame constraints and objective functions as mathematical expressions, which are later used in the motor-control module via MPC. The transparency of the optimization formulation allows for interpretability of the task and enables adjustments through human feedback. We demonstrate the validity of our method through extensive experiments on long-horizon reasoning, contact-rich, and multi-object interaction tasks. Our evaluations show that NARRATE outperforms current existing methods on these benchmarks and effectively transfers to the real world on two different embodiments. Videos, Code and Prompts at narrate-mpc.github.io
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t6_06">
             15:30-16:30, Paper ThPI5T6.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1434'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              CoT-TL: Low-Resource Temporal Knowledge Representation of Planning Instructions Using Chain-Of-Thought Reasoning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#378688" title="Click to go to the Author Index">
             Manas, Kumar
            </a>
           </td>
           <td class="r">
            Freie Universitt Berlin
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#378682" title="Click to go to the Author Index">
             Zwicklbauer, Stefan
            </a>
           </td>
           <td class="r">
            Continetal AG
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#378690" title="Click to go to the Author Index">
             Paschke, Adrian
            </a>
           </td>
           <td class="r">
            Fraunhofer FOKUS and Freie University Berlin
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1434" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#formal_methods_in_robotics_and_automation" title="Click to go to the Keyword Index">
               Formal Methods in Robotics and Automation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#autonomous_agents" title="Click to go to the Keyword Index">
               Autonomous Agents
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Autonomous agents often face the challenge of interpreting uncertain natural language instructions for planning tasks. Representing these instructions as Linear Temporal Logic (LTL) enables planners to synthesize actionable plans. We introduce CoT-TL, a data-efficient in-context learning framework for translating natural language specifications into LTL representations. CoT-TL addresses the limitations of large language models, which typically rely on extensive fine-tuning data, by extending chain-of-thought reasoning and semantic roles to align with the requirements of formal logic creation. This approach enhances the transparency and rationale behind LTL generation, fostering user trust. CoT-TL achieves state-of-the-art accuracy across three diverse datasets in low-data scenarios, outperforming existing methods without fine-tuning or intermediate translations. To improve reliability and minimize hallucinations, we incorporate model checking to validate the syntax of the generated LTL output. We further demonstrate CoT-TL's effectiveness through ablation studies and evaluations on unseen LTL structures and formulas in a new dataset. Finally, we validate CoT-TL's practicality by integrating it into a QuadCopter for multi-step drone planning based on natural language instructions.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t6_07">
             15:30-16:30, Paper ThPI5T6.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1836'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              MuTT: A Multimodal Trajectory Transformer for Robot Skills
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396314" title="Click to go to the Author Index">
             Kienle, Claudius
            </a>
           </td>
           <td class="r">
            ArtiMinds Robotics GmbH
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#284885" title="Click to go to the Author Index">
             Alt, Benjamin
            </a>
           </td>
           <td class="r">
            ArtiMinds Robotics
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#287097" title="Click to go to the Author Index">
             Celik, Onur
            </a>
           </td>
           <td class="r">
            KIT
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#279685" title="Click to go to the Author Index">
             Becker, Philipp
            </a>
           </td>
           <td class="r">
            Karlsruhe Institute of Technology (KIT)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#181134" title="Click to go to the Author Index">
             Katic, Darko
            </a>
           </td>
           <td class="r">
            Karlsruhe Institute for Technology (KIT)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#132630" title="Click to go to the Author Index">
             Jkel, Rainer
            </a>
           </td>
           <td class="r">
            Karlsruhe Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#113579" title="Click to go to the Author Index">
             Neumann, Gerhard
            </a>
           </td>
           <td class="r">
            Karlsruhe Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1836" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             High-level robot skills represent an increasingly popular paradigm in robot programming. However, configuring the skills' parameters for a specific task remains a manual and time-consuming endeavor. Existing approaches for learning or optimizing these parameters often require numerous real-world executions or do not work in dynamic environments. To address these challenges, we propose MuTT, a novel encoder-decoder transformer architecture designed to predict environment-aware executions of robot skills by integrating vision, trajectory, and robot skill parameters. Notably, we pioneer the fusion of vision and trajectory, introducing a novel trajectory projection. Furthermore, we illustrate MuTT's efficacy as a predictor when combined with a model-based robot skill optimizer. This approach facilitates the optimization of robot skill parameters for the current environment, without the need for real-world executions during optimization. Designed for compatibility with any representation of robot skills, MuTT demonstrates its versatility across three comprehensive experiments, showcasing superior performance across two different skill representations.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t6_08">
             15:30-16:30, Paper ThPI5T6.8
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2495'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Vision-Language Model-Based Physical Reasoning for Robot Liquid Perception
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398254" title="Click to go to the Author Index">
             Lai, Wenqiang
            </a>
           </td>
           <td class="r">
            Shenzhen Institute of Artificial Intelligence and Robotics for S
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#153981" title="Click to go to the Author Index">
             Zhang, Tianwei
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#113090" title="Click to go to the Author Index">
             Lam, Tin Lun
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong, Shenzhen
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#212292" title="Click to go to the Author Index">
             Gao, Yuan
            </a>
           </td>
           <td class="r">
            Shenzhen Institute of Artificial Intelligence and Robotics for S
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2495" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#recognition" title="Click to go to the Keyword Index">
               Recognition
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             There is a growing interest in applying large language models (LLMs) in robotic tasks, due to their remarkable reasoning ability and extensive knowledge learned from vast training corpora. Grounding LLMs in the physical world remains an open challenge as they can only process textual input. Recent advancements in large vision-language models (LVLMs) have enabled a more comprehensive understanding of the physical world by incorporating visual input, which provides richer contextual information than language alone. In this work, we proposed a novel paradigm that leveraged GPT-4V(ision), the state-of-the-art LVLM by OpenAI, to enable embodied agents to perceive liquid objects via image-based environmental feedback. Specifically, we exploited the physical understanding of GPT-4V to interpret the visual representation (e.g., time-series plot) of non-visual feedback (e.g., F/T sensor data), indirectly enabling multimodal perception beyond vision and language using images as proxies. We evaluated our method using 10 common household liquids with containers of various geometry and material. Without any training or fine-tuning, we demonstrated that our method can enable the robot to indirectly perceive the physical response of liquids and estimate their viscosity. We also showed that by jointly reasoning over the visual and physical attributes learned through interactions, our method could recognize liquid objects in the absence of strong visual cues (e.g., container labels with legible text or symbols), increasing the accuracy from 69.0%achieved by the best-performing vision-only variantto 86.0%.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t6_09">
             15:30-16:30, Paper ThPI5T6.9
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2747'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Multi-Modal Representation Learning with Tactile Data
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#334741" title="Click to go to the Author Index">
             Chi, Hyung-gun
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#255940" title="Click to go to the Author Index">
             Mercat, Jean
            </a>
           </td>
           <td class="r">
            1991
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#234213" title="Click to go to the Author Index">
             Barreiros, Jose
            </a>
           </td>
           <td class="r">
            Toyota Research Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#119715" title="Click to go to the Author Index">
             Ramani, Karthik
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#107892" title="Click to go to the Author Index">
             Kollar, Thomas
            </a>
           </td>
           <td class="r">
            Toyota Research Institute
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2747" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#data_sets_for_robot_learning" title="Click to go to the Keyword Index">
               Data Sets for Robot Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#force_and_tactile_sensing" title="Click to go to the Keyword Index">
               Force and Tactile Sensing
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#multi_modal_perception_for_hri" title="Click to go to the Keyword Index">
               Multi-Modal Perception for HRI
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Advancements in embodied language models like PALM-E and RT-2 have significantly enhanced language-conditioned robotic manipulation. However, these advances remain predominantly focused on vision and language, often overlooking the pivotal role of tactile feedback which is advantageous in contact-rich interactions. Our research introduces a novel approach that synergizes tactile information with vision and language. We present the Multi-Modal Wand (MMWand) dataset enriched with linguistic descriptions and tactile data. By integrating tactile feedback, we aim to bridge the divide between human linguistic understanding and robotic sensory interpretation. Our multi-modal representation model is trained on these datasets by employing the multi-modal embedding alignment principle from ImageBind which has shown promising results, emphasizing the potential of tactile data in robotic applications. The validation of our approach in downstream robotics tasks, such as texture-based object classification, cross-modality retrieval, and the dense reward function for visuomotor control, attests to its effectiveness. Our contributions underscore the importance of tactile feedback in multi-modal robotic learning and its potential to enhance robotic tasks.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t6_10">
             15:30-16:30, Paper ThPI5T6.10
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1749'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              TempBEV: Improving Learned BEV Encoders with Combined Image and BEV Space Temporal Aggregation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#320668" title="Click to go to the Author Index">
             Monninger, Thomas
            </a>
           </td>
           <td class="r">
            Mercedes-Benz AG, University of Stuttgart
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396161" title="Click to go to the Author Index">
             Dokkadi, Vandana
            </a>
           </td>
           <td class="r">
            University of Massachusetts Amherst
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#345459" title="Click to go to the Author Index">
             Anwar, Md Zafar
            </a>
           </td>
           <td class="r">
            Penn State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#299137" title="Click to go to the Author Index">
             Staab, Steffen
            </a>
           </td>
           <td class="r">
            University of Stuttgart
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1749" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#sensor_fusion" title="Click to go to the Keyword Index">
               Sensor Fusion
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Autonomous driving requires an accurate representation of the environment. A strategy toward high accuracy is to fuse data from several sensors. Learned Bird's-Eye View (BEV) encoders can achieve this by mapping data from individual sensors into one joint latent space. For cost-efficient camera-only systems, this provides an effective mechanism to fuse data from multiple cameras with different views. Accuracy can further be improved by aggregating sensor information over time. This is especially important in monocular camera systems to account for the lack of explicit depth and velocity measurements, such that decision-critical information about distance distances and motions of other objects is easily accessible. Thereby, the effectiveness of developed BEV encoders crucially depends on the operators used to aggregate temporal information and on the used latent representation spaces. We analyze BEV encoders proposed in the literature and compare their effectiveness, quantifying the effects of aggregation operators and latent representations. While most existing approaches aggregate temporal information either in image or in BEV latent space, our analyses and performance comparisons suggest that these latent representations exhibit complementary strengths. Therefore, we develop a novel temporal BEV encoder, TempBEV, which integrates aggregated temporal information from both latent spaces. We consider subsequent image frames as stereo through time and leverage methods from optical flow estimation for temporal stereo encoding. Empirical evaluation on the NuScenes dataset shows a significant improvement by TempBEV over the baseline for 3D object detection and BEV segmentation. The ablation uncovers a strong synergy of joint temporal aggregation in the image and BEV latent space. These results indicate the overall effectiveness of our approach and make a strong case for aggregating temporal information in both image and BEV latent spaces.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t6_11">
             15:30-16:30, Paper ThPI5T6.11
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2027'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Polaris: Open-Ended Interactive Robotic Manipulation Via Syn2Real Visual Grounding and Large Language Models
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#374143" title="Click to go to the Author Index">
             Wang, Tianyu
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#296392" title="Click to go to the Author Index">
             Lin, Haitao
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#388694" title="Click to go to the Author Index">
             Yu, Junqiu
            </a>
           </td>
           <td class="r">
            FudanUniversity
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#308158" title="Click to go to the Author Index">
             Fu, Yanwei
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2027" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#data_sets_for_robotic_vision" title="Click to go to the Keyword Index">
               Data Sets for Robotic Vision
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper investigates the task of the open-ended interactive robotic manipulation on table-top scenarios. While recent Large Language Models (LLMs) enhance robots' comprehension of user instructions, their lack of visual grounding constrains their ability to physically interact with the environment. This is because the robot needs to locate the target object for manipulation within the physical workspace. To this end, we introduce an interactive robotic manipulation framework called Polaris, which integrates perception and interaction by utilizing GPT-4 alongside grounded vision models. For precise manipulation, it is essential that such grounded vision models produce detailed object pose for the target object, rather than merely identifying pixels belonging to them in the image. Consequently, we propose a novel Synthetic-to-Real (Syn2Real) pose estimation pipeline. This pipeline utilizes rendered synthetic data for training and is then transferred to real-world manipulation tasks. The real-world performance demonstrates the efficacy of our proposed pipeline and underscores its potential for extension to more general categories. Moreover, real-robot experiments have showcased the impressive performance of our framework in grasping and executing multiple manipulation tasks. This indicates its potential to generalize to scenarios beyond the tabletop.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t6_12">
             15:30-16:30, Paper ThPI5T6.12
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('554'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              BTGenBot: Behavior Tree Generation for Robotic Tasks with Lightweight LLMs
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#393084" title="Click to go to the Author Index">
             Izzo, Riccardo Andrea
            </a>
           </td>
           <td class="r">
            Politecnico Di Milano
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#206366" title="Click to go to the Author Index">
             Bardaro, Gianluca
            </a>
           </td>
           <td class="r">
            Politecnico Di Milano
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#103628" title="Click to go to the Author Index">
             Matteucci, Matteo
            </a>
           </td>
           <td class="r">
            Politecnico Di Milano
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab554" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#behavior_based_systems" title="Click to go to the Keyword Index">
               Behavior-Based Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a novel approach to generating behavior trees for robots using lightweight large language models (LLMs) with a maximum of 7 billion parameters. The study demonstrates that it is possible to achieve satisfying results with compact LLMs when fine-tuned on a specific dataset. The key contributions of this research include the creation of a fine-tuning dataset based on existing behavior trees using GPT-3.5 and a comprehensive comparison of multiple LLMs (namely llama2, llama-chat, and code-llama) across nine distinct tasks. To be thorough, we evaluated the generated behavior trees using static syntactical analysis, a validation system, a simulated environment, and a real robot. Furthermore, this work opens the possibility of deploying such solutions directly on the robot, enhancing its practical applicability. Findings from this study demonstrate the potential of LLMs with a limited number of parameters in generating effective and efficient robot behaviors.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t6_13">
             15:30-16:30, Paper ThPI5T6.13
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1744'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Multi-Modal Motion Prediction Using Temporal Ensembling with Learning-Based Aggregation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#340323" title="Click to go to the Author Index">
             Hong, Kai-Yin
            </a>
           </td>
           <td class="r">
            National Yang Ming Chiao Tung University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#103458" title="Click to go to the Author Index">
             Wang, Chieh-Chih
            </a>
           </td>
           <td class="r">
            National Yang Ming Chiao Tung University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#249645" title="Click to go to the Author Index">
             Lin, Wen-Chieh
            </a>
           </td>
           <td class="r">
            National Yang Ming Chiao Tung University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1744" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#intention_recognition" title="Click to go to the Keyword Index">
               Intention Recognition
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#ai_based_methods" title="Click to go to the Keyword Index">
               AI-Based Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Recent years have seen a shift towards learning-based methods for trajectory prediction, with challenges remaining in addressing uncertainty and capturing multi-modal distributions. This paper introduces Temporal Ensembling with Learning-based Aggregation, a meta-algorithm designed to mitigate the issue of missing behaviors in trajectory prediction, which leads to inconsistent predictions across consecutive frames. Unlike conventional model ensembling, temporal ensembling leverages predictions from nearby frames to enhance spatial coverage and prediction diversity. By confirming predictions from multiple frames, temporal ensembling compensates for occasional errors in individual frame predictions. Furthermore, trajectory-level aggregation, often utilized in model ensembling, is insufficient for temporal ensembling due to a lack of consideration of traffic context and its tendency to assign candidate trajectories with incorrect driving behaviors to final predictions. We further emphasize the necessity of learning-based aggregation by utilizing mode queries within a DETR-like architecture for our temporal ensembling, leveraging the characteristics of predictions from nearby frames. Our method, validated on the Argoverse 2 dataset, shows notable improvements: a 4% reduction in minADE, a 5% decrease in minFDE, and a 1.16% reduction in the miss rate compared to the strongest baseline, QCNet, highlighting its efficacy and potential in autonomous driving.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t6_14">
             15:30-16:30, Paper ThPI5T6.14
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2631'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              The Power of the Senses: Generalizable Manipulation from Vision and Touch through Masked Multimodal Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#195795" title="Click to go to the Author Index">
             Sferrazza, Carmelo
            </a>
           </td>
           <td class="r">
            UC Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398478" title="Click to go to the Author Index">
             Seo, Younggyo
            </a>
           </td>
           <td class="r">
            Dyson
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398483" title="Click to go to the Author Index">
             Liu, Hao
            </a>
           </td>
           <td class="r">
            UC Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#241050" title="Click to go to the Author Index">
             Lee, Youngwoon
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#107568" title="Click to go to the Author Index">
             Abbeel, Pieter
            </a>
           </td>
           <td class="r">
            UC Berkeley
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2631" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#representation_learning" title="Click to go to the Keyword Index">
               Representation Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#force_and_tactile_sensing" title="Click to go to the Keyword Index">
               Force and Tactile Sensing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Humans rely on the synergy of their senses for most essential tasks. For tasks requiring object manipulation, we seamlessly and effectively exploit the complementarity of our senses of vision and touch. This paper draws inspiration from such capabilities and aims to find a systematic approach to fuse visual and tactile information in a reinforcement learning setting. We propose Masked Multimodal Learning (M3L), which jointly learns a policy and visual-tactile representations based on masked autoencoding. The representations jointly learned from vision and touch improve sample efficiency, and unlock generalization capabilities beyond those achievable through each of the senses separately. Remarkably, representations learned in a multimodal setting also benefit vision-only policies at test time. We evaluate M3L on three simulated environments with both visual and tactile observations: robotic insertion, door opening, and dexterous in-hand manipulation, demonstrating the benefits of learning a multimodal policy. Videos of the experiments and the open-source code are available at https://sferrazza.cc/m3l_site.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t6_15">
             15:30-16:30, Paper ThPI5T6.15
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1989'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Prompt-Driven Temporal Domain Adaptation for Nighttime UAV Tracking
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#160488" title="Click to go to the Author Index">
             Fu, Changhong
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#393299" title="Click to go to the Author Index">
             Wang, Yiheng
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#339398" title="Click to go to the Author Index">
             Yao, Liangliang
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#287784" title="Click to go to the Author Index">
             Zheng, Guangze
            </a>
           </td>
           <td class="r">
            The University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#324459" title="Click to go to the Author Index">
             Zuo, Haobo
            </a>
           </td>
           <td class="r">
            University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#131357" title="Click to go to the Author Index">
             Pan, Jia
            </a>
           </td>
           <td class="r">
            University of Hong Kong
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1989" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#transfer_learning" title="Click to go to the Keyword Index">
               Transfer Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#data_sets_for_robotic_vision" title="Click to go to the Keyword Index">
               Data Sets for Robotic Vision
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Nighttime UAV tracking under low-illuminated scenarios has achieved great progress by domain adaptation (DA). However, previous DA training-based works are deficient in narrowing the discrepancy of temporal contexts for UAV trackers. To address the issue, this work proposes a prompt-driven temporal domain adaptation training framework to fully utilize temporal contexts for challenging nighttime UAV tracking, i.e., TDA. Specifically, the proposed framework aligns the distribution of temporal contexts from daytime and nighttime domains by training the temporal feature generator against the discriminator. The temporal-consistent discriminator progressively extracts shared domain-specific features to generate coherent domain discrimination results in the time series. Additionally, to obtain high-quality training samples, a prompt-driven object miner is employed to precisely locate objects in unannotated nighttime videos. Moreover, a new benchmark for long-term nighttime UAV tracking is constructed. Exhaustive evaluations on both public and self-constructed nighttime benchmarks demonstrate the remarkable performance of the tracker trained in TDA framework, i.e., TDA-Track. Real-world tests at nighttime also show its practicality. The code and demo videos are available at https://github.com/vision4robotics/TDA-Track.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t6_16">
             15:30-16:30, Paper ThPI5T6.16
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1105'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Do One Thing and Do It Well: Delegate Responsibilities in Classical Planning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#237502" title="Click to go to the Author Index">
             Lai, Tin
            </a>
           </td>
           <td class="r">
            University of Sydney
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#205050" title="Click to go to the Author Index">
             Morere, Philippe
            </a>
           </td>
           <td class="r">
            University of Sydney
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1105" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#learning_categories_and_concepts" title="Click to go to the Keyword Index">
               Learning Categories and Concepts
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#autonomous_agents" title="Click to go to the Keyword Index">
               Autonomous Agents
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#task_planning" title="Click to go to the Keyword Index">
               Task Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We propose a novel framework and algorithm for solving classical planning problems with an implicit hierarchical solver based on the principle of delegation. This framework, the Markov Intent Process, features a collection of skills that are each specialised to perform a single task well. Skills are aware of their intended effects and are able to analyse planning goals to delegate planning to the best-suited skill. This principle dynamically creates a hierarchy of plans, in which each skill plans for sub-goals for which it is specialised. Our method performs robustly in noisy environments with non-deterministic action effects and features on-demand execution - skill policies are only evaluated when needed. Plans are only generated at the highest level, then expanded and optimised when the latest state information is available. The high-level plan retains the initial planning intent and previously computed skills, effectively reducing the computation needed to adapt to environmental changes. We show this planning approach is experimentally very competitive to classic planning and reinforcement learning techniques on a variety of domains, both in terms of solution length and planning time.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t6_17">
             15:30-16:30, Paper ThPI5T6.17
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('869'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              ODTFormer: Efficient Obstacle Detection and Tracking with Stereo Cameras Based on Transformer
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#391376" title="Click to go to the Author Index">
             Ding, Tianye
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#324814" title="Click to go to the Author Index">
             Li, Hongyu
            </a>
           </td>
           <td class="r">
            Brown University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#239579" title="Click to go to the Author Index">
             Jiang, Huaizu
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab869" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#visual_tracking" title="Click to go to the Keyword Index">
               Visual Tracking
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Obstacle detection and tracking represent a critical component in robot autonomous navigation. In this paper, we propose ODTFormer, a Transformer-based model that addresses both obstacle detection and tracking problems. For the detection task, our approach leverages deformable attention to construct a 3D cost volume, which is decoded progressively in the form of voxel occupancy grids. We further track the obstacles by matching the voxels between consecutive frames. The entire model can be optimized in an end-to-end manner. Through extensive experiments on DrivingStereo and KITTI benchmarks, our model achieves state-of-the-art performance in the obstacle detection task. We also report comparable accuracy to state-of-the-art obstacle tracking models while requiring only a fraction of their computation cost, typically ten-fold to twenty-fold less. Our code is available on https://github.com/neu-vi/ODTFormer
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thpi5t7">
             <b>
              ThPI5T7
             </b>
            </a>
           </td>
           <td class="r">
            Room 7
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thpi5t7" title="Click to go to the Program at a Glance">
             <b>
              Perception III (Semantic Scene Understanding)
             </b>
            </a>
           </td>
           <td class="r">
            Teaser Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#152911" title="Click to go to the Author Index">
             Parasuraman, Ramviyas
            </a>
           </td>
           <td class="r">
            University of Georgia
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#125090" title="Click to go to the Author Index">
             Abu-Khalaf, Jumana
            </a>
           </td>
           <td class="r">
            Edith Cowan University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t7_01">
             15:30-16:30, Paper ThPI5T7.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('78'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Object-Oriented Material Classification and 3D Clustering for Improved Semantic Perception and Mapping in Mobile Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#386985" title="Click to go to the Author Index">
             Ravipati, Siva Krishna
            </a>
           </td>
           <td class="r">
            University of Georgia
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#332036" title="Click to go to the Author Index">
             Latif, Ehsan
            </a>
           </td>
           <td class="r">
            University of Georgia
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#386986" title="Click to go to the Author Index">
             Bhandarkar, Suchendra
            </a>
           </td>
           <td class="r">
            University of Georgia
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#152911" title="Click to go to the Author Index">
             Parasuraman, Ramviyas
            </a>
           </td>
           <td class="r">
            University of Georgia
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab78" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Classification of different object surface material types can play a significant role in the decision-making algorithms for mobile robots and autonomous vehicles. RGB-based scene-level semantic segmentation has been well-addressed in the literature. However, improving material recognition using the depth modality and its integration with SLAM algorithms for 3D semantic mapping could unlock new potential benefits in the robotics perception pipeline. To this end, we propose a complementarity-aware deep learning approach for RGB-D-based material classification built on top of an object-oriented pipeline. The approach further integrates the ORB-SLAM2 method for 3D scene mapping with multiscale clustering of the detected material semantics in the point cloud map generated by the visual SLAM algorithm. Extensive experimental results with existing public datasets and newly contributed real-world robot datasets demonstrate a significant improvement in material classification and 3D clustering accuracy compared to state-of-the-art approaches for 3D semantic scene mapping.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t7_02">
             15:30-16:30, Paper ThPI5T7.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('502'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Transcrib3D: 3D Referring Expression Resolution through Large Language Models
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#313318" title="Click to go to the Author Index">
             Fang, Jiading
            </a>
           </td>
           <td class="r">
            Toyota Technological Institute at Chicago
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#376034" title="Click to go to the Author Index">
             Tan, Xiangshan
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#240274" title="Click to go to the Author Index">
             Lin, Shengjie
            </a>
           </td>
           <td class="r">
            TTI-Chicago
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#269740" title="Click to go to the Author Index">
             Vasiljevic, Igor
            </a>
           </td>
           <td class="r">
            Toyota Research Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#141133" title="Click to go to the Author Index">
             Guizilini, Vitor
            </a>
           </td>
           <td class="r">
            Toyota Research Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#375476" title="Click to go to the Author Index">
             Mei, Hongyuan
            </a>
           </td>
           <td class="r">
            Toyota Technological Institute at Chicago
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#117786" title="Click to go to the Author Index">
             Ambrus, Rares
            </a>
           </td>
           <td class="r">
            Toyota Research Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#308464" title="Click to go to the Author Index">
             Shakhnarovich, Gregory
            </a>
           </td>
           <td class="r">
            Toyota Technological Institute at Chicago
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#114908" title="Click to go to the Author Index">
             Walter, Matthew
            </a>
           </td>
           <td class="r">
            Toyota Technological Institute at Chicago
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab502" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             If robots are to work effectively alongside people, they must be able to interpret natural language references to objects in their 3D environment. Understanding 3D referring expressions is challenging---it requires the ability to both parse the 3D structure of the scene as well as to correctly ground free-form language in the presence of distraction and clutter. We introduce Transcrib3D, an approach that brings together 3D detection methods and the emergent reasoning capabilities of large language models (LLMs), using text as the unifying medium, which allows us to sidestep the need for learning complex multi-modality ``adapters'' from limited 3D annotated data. Transcrib3D demonstrates its effectiveness in experiments, achieving state-of-the-art results on 3D referring benchmarks, with a great leap in performance from previous multi-modality baselines. We implement our method on real robots to perform pick-and-place tasks given queries that contain challenging referring expressions. We also provide an analysis of common failure scenarios to inform potential enhancements in future work.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t7_03">
             15:30-16:30, Paper ThPI5T7.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('837'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Visual Preference Inference: An Image Sequence-Based Preference Reasoning in Tabletop Object Manipulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#366938" title="Click to go to the Author Index">
             Lee, Joonhyung
            </a>
           </td>
           <td class="r">
            Korea University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#312179" title="Click to go to the Author Index">
             Park, Sangbeom
            </a>
           </td>
           <td class="r">
            Korea University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392949" title="Click to go to the Author Index">
             Kwon, Yongin
            </a>
           </td>
           <td class="r">
            Electronics and Telecommunications Research Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392948" title="Click to go to the Author Index">
             Lee, Jemin
            </a>
           </td>
           <td class="r">
            Electronics and Telecommunications Research Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#393610" title="Click to go to the Author Index">
             Ahn, Minwook
            </a>
           </td>
           <td class="r">
            Neubla Korea Corporation
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#163618" title="Click to go to the Author Index">
             Choi, Sungjoon
            </a>
           </td>
           <td class="r">
            Korea University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab837" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In robotic object manipulation, human preferences can often be influenced by the visual attributes of objects, such as color and shape. These properties play a crucial role in operating a robot to interact with objects and align with human intention. In this paper, we focus on the problem of inferring underlying human preferences from a sequence of raw visual observations in tabletop manipulation environments with a variety of object types, named Visual Preference Inference (VPI). To facilitate visual reasoning in the context of manipulation, we introduce the Chain-of-Visual-Residuals (CoVR) method. CoVR employs a prompting mechanism that describes the difference between the consecutive images (i.e., visual residuals) and incorporates such texts with a sequence of images to infer the user's preference. This approach significantly enhances the ability to understand and adapt to dynamic changes in its visual environment during manipulation tasks. Furthermore, we incorporate such texts along with a sequence of images to infer the user's preferences. Our method outperforms baseline methods in terms of extracting human preferences from visual sequences in both simulation and real-world environments. Code and videos are available at: href{https://joonhyung-lee.github.io/vpi/}{https://joonhyu ng-lee.github.io/vpi/}
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t7_04">
             15:30-16:30, Paper ThPI5T7.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1369'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Language-Driven Navigation Strategy Integrating Semantic Maps and Large Language Models
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396216" title="Click to go to the Author Index">
             Zhong, Zhengjun
            </a>
           </td>
           <td class="r">
            Shenzhen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#374365" title="Click to go to the Author Index">
             He, Ying
            </a>
           </td>
           <td class="r">
            Shenzhen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#376116" title="Click to go to the Author Index">
             Li, Pengteng
            </a>
           </td>
           <td class="r">
            Shenzhen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#393394" title="Click to go to the Author Index">
             Yu, Fei
            </a>
           </td>
           <td class="r">
            Guangming Lab
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398640" title="Click to go to the Author Index">
             Ma, Fei
            </a>
           </td>
           <td class="r">
            Guangdong Laboratory of Artificial Intelligence and Digital Econ
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1369" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Accurate perception of semantic and spatial information is crucial for robots performing language-driven navigation tasks. Existing approaches utilize visual-language models to extract semantic information from the environment and construct maps. However, constrained by the generalization and accuracy of these models themselves, the constructed maps may not be accurate and comprehensive, thereby affecting the accuracy of navigation tasks. Inspired by foundational models' outstanding classification and segmentation capabilities, this study introduces a semantic map constructed using foundational models. We leverage a foundational model to semantically segment objects in the robot's video stream and fuse semantics onto the map. Furthermore, this map is used in conjunction with large language models (LLMs) that receive natural language instructions to complete the navigation task. A substantial number of experiments in a simulated environment demonstrate that our method outperforms existing ones in language-driven navigation tasks.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t7_05">
             15:30-16:30, Paper ThPI5T7.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1851'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Context-Enhanced Full-Resolution Floor Plan Segmentation Network for Topological Semantic Mapping
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#111763" title="Click to go to the Author Index">
             Cao, Zhengcai
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395234" title="Click to go to the Author Index">
             Sun, Yiyang
            </a>
           </td>
           <td class="r">
            Beijing University of Chemical Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395238" title="Click to go to the Author Index">
             Ma, Zhe
            </a>
           </td>
           <td class="r">
            Beijing University of Chemical Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#112540" title="Click to go to the Author Index">
             Zhou, MengChu
            </a>
           </td>
           <td class="r">
            New Jersey Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1851" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_medical_robotics" title="Click to go to the Keyword Index">
               Computer Vision for Medical Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Topological semantic maps provide a practical solution to enhance indoor navigation for the Partially Sighted or Visually Impaired (PSVI). Segmenting indoor floor plans and extracting boundaries are key to constructing these maps. The existing methods exhibit low accuracy in segmentation. To achieve desired high segmentation accuracy, we introduce a Context-Enhanced Full-Resolution Network (CEFRN) for floor plan segmentation. It is designed to harness the shallow detailed features and inter-category contextual dependencies inherent in floor plans. CEFRN integrates modified residual blocks to capture the low-stage full-resolution features while maintaining its compactness. A position attention module is employed to refine the deep-stage contextual information. We also propose a two-dimensional deep supervision method to merge features from both stages, which significantly boosts the feature representation ability of CEFRN. Finally, a practical topological semantic mapping method for PSVI indoor navigation is introduced. Experimental results demonstrate that CEFRN's segmentation accuracy well exceeds the state-of-the-art methods'. It can be used to well support accurate topological semantic mapping.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t7_06">
             15:30-16:30, Paper ThPI5T7.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2036'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              DVT: Decoupled Dual-Branch View Transformation for Monocular Bird's Eye View Semantic Segmentation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#314030" title="Click to go to the Author Index">
             Du, Jiayuan
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392284" title="Click to go to the Author Index">
             Pan, Xianghui
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397741" title="Click to go to the Author Index">
             Shen, Mengjiao
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#323995" title="Click to go to the Author Index">
             Su, Shuai
            </a>
           </td>
           <td class="r">
            Tongji University, China
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397719" title="Click to go to the Author Index">
             Yang, Jingwei
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#134932" title="Click to go to the Author Index">
             Liu, Chengju
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#118155" title="Click to go to the Author Index">
             Chen, Qijun
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2036" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Monocular Bird's Eye View (BEV) semantic segmentation is critical for autonomous driving for its inherent advantages in spatial representation and downstream tasks. However, it is challenging to simultaneously learn view transformation and pixel-wise classification. Previous works suffer from non-flat region distortion, distant depth ambiguity, and visual occlusion. To address these aforementioned concerns, we propose dual-branch view transformation (DVT), a novel framework for monocular BEV semantic segmentation. Our method consists of: (i) A dual-branch view transformation to decouple features into flat region and non-flat region and process them independently. (ii) A depth-aware weighting method to make the model pay more attention to the distant depth. (iii) An auxiliary task to introduce more inductive biases to alleviate the inaccuracy caused by visual occlusion. Furthermore, we design a class-aware weighting method to address the class and size imbalance of datasets. Experimental results on nuScenes and KITTI-360 datasets demonstrate that DVT outperforms previous state-of-the-art (SOTA). Our codes are available at https://github.com/MrPicklesGG/DVT.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t7_07">
             15:30-16:30, Paper ThPI5T7.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2148'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Indoor Scene Change Understanding (SCU): Segment, Describe, and Revert Any Change
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#375673" title="Click to go to the Author Index">
             Khan, Mariia
            </a>
           </td>
           <td class="r">
            Edith Cowan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#340195" title="Click to go to the Author Index">
             Qiu, Yue
            </a>
           </td>
           <td class="r">
            National Institute of Advanced Industrial Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#372006" title="Click to go to the Author Index">
             Cong, Yuren
            </a>
           </td>
           <td class="r">
            Leibniz University Hannover
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#204884" title="Click to go to the Author Index">
             Rosenhahn, Bodo
            </a>
           </td>
           <td class="r">
            Institute of Information Processing, Leibniz Universitt Hannove
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#375658" title="Click to go to the Author Index">
             Suter, David
            </a>
           </td>
           <td class="r">
            Edith Cowan University, School of Science, Centre of AI and Mach
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#125090" title="Click to go to the Author Index">
             Abu-Khalaf, Jumana
            </a>
           </td>
           <td class="r">
            Edith Cowan University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2148" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Understanding of scene changes is crucial for embodied AI applications, such as visual room rearrangement, where the agent must revert changes by restoring the objects to their original locations or states. Visual changes between two scenes, pre- and post-rearrangement, encompass two tasks: scene change detection (locating changes) and image difference captioning (describing changes). While previous methods, focused on sequential 2D images, have addressed these tasks separately, it is essential to emphasize the significance of their combination. Therefore, we propose a new Scene Change Understanding (SCU) task for simultaneous change detection and description. Moreover, we go beyond change language description generation and aim to generate rearrangement instructions for the robotic agent to revert changes. To solve this task, we propose a novel method - EmbSCU, which allows to compare instance-level change object masks (for 53 frequently seen indoor object classes) before and after changes and generate rearrangement language instructions for the agent. EmbSCU is built on our Segment Any Object Model (SAOMv2) - a fine-tuned version of Segment Anything Model (SAM), adapted to obtain instance-level object masks for both foreground and background objects in indoor embodied environments. EmbSCU is evaluated on our own dataset of sequential 2D image pairs before and after changes, collected from the Ai2Thor simulator. The proposed framework achieves promising results in both change detection and change description. Moreover, EmbSCU demonstrates positive generalization results on real-world scenes without using any real-life data during training. The dataset and the code are available here.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t7_08">
             15:30-16:30, Paper ThPI5T7.8
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2188'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Weakly Scene Segmentation Using Efficient Transformer
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#338683" title="Click to go to the Author Index">
             Huang, Hao
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#241148" title="Click to go to the Author Index">
             Yuan, Shuaihang
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#338399" title="Click to go to the Author Index">
             Wen, Congcong
            </a>
           </td>
           <td class="r">
            New York University Abu Dhabi
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#276514" title="Click to go to the Author Index">
             Hao, Yu
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#200546" title="Click to go to the Author Index">
             Fang, Yi
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2188" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Existing methods for large-scale point cloud scene semantic segmentation require manually annotated dense point-wise labels, which is expensive, tedious, and error-prone. Therefore, collecting point cloud scenes containing billions of labeled points is impractical in the real world. In this paper, we propose a novel weak supervision method to semantically segment large-scale indoor scenes in which only 1 points are required labels. Specifically, we design an efficient point neighbor Transformer to learn the geometry of local point cloud patches. To reduce the quadratic complexity of self-attention computation in Transformer, especially when applied to large-scale point clouds, we propose to approximate self-attention matrix using low-rank and sparse decomposition. Built upon the point neighbor Transformer as basic blocks, we design Low-rank Sparse Transformer network, dubbed as LST-Net, for weakly large-scale point cloud scene semantic segmentation. Experimental results on two widely used indoor point cloud scene segmentation benchmarks show a significant gain against existing weakly supervised methods, and the results are also comparable to those of fully supervised methods. Our code will be released upon acceptance.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t7_09">
             15:30-16:30, Paper ThPI5T7.9
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2241'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              DiffPrompter: Differentiable Implicit Visual Prompts for Semantic-Segmentation in Adverse Conditions
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#339069" title="Click to go to the Author Index">
             Kalwar, Sanket
            </a>
           </td>
           <td class="r">
            International Institute of Information Technology, Hyderabad
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#376927" title="Click to go to the Author Index">
             Ungarala, Sri Mihir Devapi
            </a>
           </td>
           <td class="r">
            IIIT Hyderabad
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#376328" title="Click to go to the Author Index">
             Jain, Shruti
            </a>
           </td>
           <td class="r">
            The International Institute of Information Technology - Hyderaba
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#360574" title="Click to go to the Author Index">
             Monis, Aaron
            </a>
           </td>
           <td class="r">
            IIIT Hyderabad
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#339105" title="Click to go to the Author Index">
             Konda, Krishna
            </a>
           </td>
           <td class="r">
            ZF TCI
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#196783" title="Click to go to the Author Index">
             Garg, Sourav
            </a>
           </td>
           <td class="r">
            University of Adelaide
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#102906" title="Click to go to the Author Index">
             Krishna, Madhava
            </a>
           </td>
           <td class="r">
            IIIT Hyderabad
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2241" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_transportation" title="Click to go to the Keyword Index">
               Computer Vision for Transportation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Semantic segmentation in adverse weather scenarios is a critical task for autonomous driving systems. While foundation models have shown promise, the need for specialized adaptors becomes evident for handling more challenging scenarios. We introduce DiffPrompter, a novel differentiable visual and latent prompting mechanism aimed at expanding the learning capabilities of existing adaptors in foundation models. Our proposed del-HFC (High Frequency Components) based image processing block excels particularly in adverse weather conditions, where conventional methods often fall short. Furthermore, we investigate the advantages of jointly training visual and latent prompts, demonstrating that this combined approach significantly enhances performance in out-of-distribution scenarios. Our differentiable visual prompts leverage parallel and series architectures to generate prompts, effectively improving object segmentation tasks in adverse conditions. Through a comprehensive series of experiments and evaluations, we provide empirical evidence to support the efficacy of our approach.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t7_10">
             15:30-16:30, Paper ThPI5T7.10
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2316'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Leveraging Computation of Expectation Models for Commonsense Affordance Estimation on 3D Scene Graphs
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#340572" title="Click to go to the Author Index">
             Valdes Saucedo, Mario Alberto
            </a>
           </td>
           <td class="r">
            Lulea University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#325063" title="Click to go to the Author Index">
             Stathoulopoulos, Nikolaos
            </a>
           </td>
           <td class="r">
            Lule University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#313126" title="Click to go to the Author Index">
             Patel, Akash
            </a>
           </td>
           <td class="r">
            Lule University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#199367" title="Click to go to the Author Index">
             Kanellakis, Christoforos
            </a>
           </td>
           <td class="r">
            LTU
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#105882" title="Click to go to the Author Index">
             Nikolakopoulos, George
            </a>
           </td>
           <td class="r">
            Lule University of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2316" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#learning_categories_and_concepts" title="Click to go to the Keyword Index">
               Learning Categories and Concepts
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This article studies the commonsense object affordance concept for enabling close-to-human task planning and task optimization of embodied robotic agents in urban environments. The focus of the object affordance is on reasoning how to effectively identify objects inherent utility during the task execution, which in this work is enabled through the analysis of contextual relations of sparse information of 3D scene graphs. The proposed framework develops a Correlation Information (CECI) model to learn probability distributions using a Graph Convolutional Network, allowing to extract the commonsense affordance for individual members of a semantic class. The overall framework was experimentally validated in a real-world indoor environment, showcasing the ability of the method to level with human commonsense. % For a video of the article, showcasing the experimental demonstration, please refer to the following link: https://youtu.be/BDCMVx2GiQE
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t7_11">
             15:30-16:30, Paper ThPI5T7.11
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2321'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Learning High-Level Semantic-Relational Concepts for SLAM
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#354774" title="Click to go to the Author Index">
             Millan Romera, Jose Andres
            </a>
           </td>
           <td class="r">
            University of Luxembourg
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#226120" title="Click to go to the Author Index">
             Bavle, Hriday
            </a>
           </td>
           <td class="r">
            University of Luxembourg
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#319058" title="Click to go to the Author Index">
             Shaheer, Muhammad
            </a>
           </td>
           <td class="r">
            University of Luxembourg
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#211264" title="Click to go to the Author Index">
             Oswald, Martin R.
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#115622" title="Click to go to the Author Index">
             Voos, Holger
            </a>
           </td>
           <td class="r">
            University of Luxembourg
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#158182" title="Click to go to the Author Index">
             Sanchez-Lopez, Jose Luis
            </a>
           </td>
           <td class="r">
            University of Luxembourg
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2321" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#representation_learning" title="Click to go to the Keyword Index">
               Representation Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Abstract Recent works on SLAM extend their pose graphs with higher-level semantic concepts like Rooms exploiting relationships between them, to provide, not only a richer representation of the situation/environment but also to improve the accuracy of its estimation. Concretely, our previous work, Situational Graphs (S-Graphs+), a pioneer in jointly leveraging semantic relationships in the factor optimization process, relies on semantic entities such as Planes and Rooms, whose relationship is mathematically defined. Nevertheless, there is no unique approach to finding all the hidden patterns in lower-level factor-graphs that correspond to high-level concepts of different natures. It is currently tackled with ad-hoc algorithms, which limits its graph expressiveness.
             <p>
              To overcome this limitation, in this work, we propose an algorithm based on Graph Neural Networks for learning high-level semantic-relational concepts that can be inferred from the low-level factor graph. Given a set of mapped Planes our algorithm is capable of inferring Room entities relating to the Planes. Additionally, to demonstrate the versatility of our method, our algorithm can infer an additional semantic-relational concept, i.e. Wall, and its relationship with its Planes. We validate our method in both simulated and real datasets demonstrating improved performance over two baseline approaches. Furthermore, we integrate our method into the S-Graphs+ algorithm providing improved pose and map accuracy compared to the baseline while further enhancing the scene representation.
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t7_12">
             15:30-16:30, Paper ThPI5T7.12
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2592'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              EMBOSR: Embodied Spatial Reasoning for Enhanced Situated Question Answering in 3D Scenes
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#276514" title="Click to go to the Author Index">
             Hao, Yu
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#378264" title="Click to go to the Author Index">
             Yang, Fan
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#378249" title="Click to go to the Author Index">
             Fang, Nicholas
            </a>
           </td>
           <td class="r">
            NYU Abu Dhabi
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#338409" title="Click to go to the Author Index">
             Liu, Yu-Shen
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2592" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             3D Embodied Spatial Reasoning, emphasizing an agent's interaction with its surroundings for spatial information inference, is adeptly facilitated by the process of Situated Question Answering in 3D Scenes (SQA3D). SQA3D requires an agent to comprehend its position and orientation within a 3D scene based on a textual situation and then utilize this understanding to answer questions about the surrounding environment in that context. Previous methods in this field face substantial challenges, including a dependency on constant retraining on limited datasets, which leads to poor performance in unseen scenarios, limited expandability, and inadequate generalization. To address these challenges, we present a new embodied spatial reasoning paradigm for enhanced SQA3D, fusing the capabilities of foundation models with the chain of thought methodology. This approach is designed to elevate adaptability and scalability in a wide array of 3D environments. A new aspect of our model is the integration of a chain of thought reasoning process, which significantly augments the model's capability for spatial reasoning and complex query handling in diverse 3D environments. In our structured experiments, we compare our approach against other methods with varying architectures, demonstrating its efficacy in multiple tasks including SQA3D and 3D captioning. We also assess the informativeness contained in the generated answers for complex queries. Ablation studies further delineate the individual contributions of our method to its overall performance. The results consistently affirm our proposed method's effectiveness and efficiency.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t7_13">
             15:30-16:30, Paper ThPI5T7.13
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2870'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SePaint: Semantic Map Inpainting Via Multinomial Diffusion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#246551" title="Click to go to the Author Index">
             Chen, Zheng
            </a>
           </td>
           <td class="r">
            Indiana University Bloomington
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#355204" title="Click to go to the Author Index">
             Duggirala, Deepak
            </a>
           </td>
           <td class="r">
            Indiana University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#166783" title="Click to go to the Author Index">
             Crandall, David
            </a>
           </td>
           <td class="r">
            Indiana University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#355590" title="Click to go to the Author Index">
             Jiang, Lei
            </a>
           </td>
           <td class="r">
            Indiana University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#141677" title="Click to go to the Author Index">
             Liu, Lantao
            </a>
           </td>
           <td class="r">
            Indiana University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2870" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#environment_monitoring_and_management" title="Click to go to the Keyword Index">
               Environment Monitoring and Management
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Prediction beyond partial observations is crucial for robots to navigate in unknown environments because it can provide extra information regarding the surroundings beyond the current sensing range or resolution. In this work, we consider the inpainting of semantic Bird's-Eye-View maps. We propose SePaint, an inpainting model for semantic data based on generative multinomial diffusion. To maintain semantic consistency, we need to condition the prediction for the missing regions on the known regions. We propose a novel and efficient condition strategy, Look-Back Condition (LB-Con), which performs a one-step look-back operations during the reverse diffusion process. By doing so, we are able to strengthen the harmonization between unknown and known parts, leading to better completion performance. We have conducted extensive experiments on different datasets, showing our proposed model outperforms commonly used interpolation methods in various robotic applications.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t7_14">
             15:30-16:30, Paper ThPI5T7.14
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2898'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Volumetric Mapping with Panoptic Refinement Using Kernel Density Estimation for Mobile Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#350237" title="Click to go to the Author Index">
             Nguyen, Khang
            </a>
           </td>
           <td class="r">
            University of Texas at Arlington
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#351067" title="Click to go to the Author Index">
             Dang, Tuan
            </a>
           </td>
           <td class="r">
            University Taxes at Arlington
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#101285" title="Click to go to the Author Index">
             Huber, Manfred
            </a>
           </td>
           <td class="r">
            University of Texas at Arlington
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2898" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#probability_and_statistical_methods" title="Click to go to the Keyword Index">
               Probability and Statistical Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Reconstructing three-dimensional (3D) scenes with semantic understanding is vital in many robotic applications. Robots need to identify which objects, along with their positions and shapes, to manipulate them precisely with given tasks. Mobile robots, especially, usually use lightweight networks to segment objects on RGB images and then localize them via depth maps; however, they often encounter out-of-distribution scenarios where masks over-cover the objects. In this paper, we address the problem of panoptic segmentation quality in 3D scene reconstruction by refining segmentation errors using non-parametric statistical methods. To enhance mask precision, we map the predicted masks into a depth frame to estimate their distribution via kernel densities. The outliers in depth perception are then rejected without the need for additional parameters in an adaptive manner to out-of-distribution scenarios, followed by 3D reconstruction using projective signed distance functions (SDFs). We validate our method on a synthetic dataset, which shows improvements in both quantitative and qualitative results for panoptic mapping. Through real-world testing, the results furthermore show our method's capability to be deployed on a real-robot system. Our source code is available at https://github.com/mkhangg/refined_panoptic_mapping.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t7_15">
             15:30-16:30, Paper ThPI5T7.15
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3051'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Semantic Layering in Room Segmentation Via LLMs
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#373292" title="Click to go to the Author Index">
             Kim, Taehyeon
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#164680" title="Click to go to the Author Index">
             Min, Byung-Cheol
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3051" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper, we introduce Semantic Layering in Room Segmentation via LLMs (SeLRoS), an advanced method for semantic room segmentation by integrating Large Language Models (LLMs) with traditional 2D map-based segmentation. Unlike previous approaches that solely focus on the geometric segmentation of indoor environments, our work enriches segmented maps with semantic data, including object identification and spatial relationships, to enhance robotic navigation. By leveraging LLMs, we provide a novel framework that interprets and organizes complex information about each segmented area, thereby improving the accuracy and contextual relevance of room segmentation. Furthermore, SeLRoS overcomes the limitations of existing algorithms by using a semantic evaluation method to accurately distinguish true room divisions from those erroneously generated by furniture and segmentation inaccuracies. The effectiveness of SeLRoS is verified through its application across 30 different 3D environments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t7_16">
             15:30-16:30, Paper ThPI5T7.16
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3355'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              EVSMap: An Efficient Volumetric-Semantic Mapping Approach for Embedded Systems
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#394703" title="Click to go to the Author Index">
             Qiu, Jiyuan
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#316805" title="Click to go to the Author Index">
             Jiang, Chen
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#394722" title="Click to go to the Author Index">
             Zhang, Pengfei
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#316909" title="Click to go to the Author Index">
             Wang, Haowen
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3355" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Despite significant progress in perception tasks such as 3D scene mapping and semantic information extraction using SLAM and deep learning, applying these techniques within computationally constrained embedded systems remains a challenge. In this work, we introduce a novel end-to-end framework for efficient and real-time volumetric-semantic mapping. We have developed a lightweight and robust RGB-D segmentation network for extracting semantic information. Through the introduction of three distinct modulesCFIM, DAPPF, and LADour network significantly enhances real-time performance while achieving Mean Intersection over Union (MIoU) scores comparable to state-of-the-art (SOTA) models. Our model reduces the parameters by 8 to 26 times compared to similar networks and improves inference speed by 2 to 3 times. Additionally, we improved a multi-class bayesian updating strategy by refining penalty function to reduce the memory size of the semantic map and enhance the mapping speed. Compared with other volumetric-semantic mapping approaches, our work maintains the same level of detail in semantic information representation, while increasing mapping speed by 1.3 to 9.6 times and reducing memory size of the map by up to 2.6 times. Finally, we applied our work to real-world mobile robot exploration scenarios, demonstrating the efficiency of the proposed framework.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thpi5t8">
             <b>
              ThPI5T8
             </b>
            </a>
           </td>
           <td class="r">
            Room 8
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thpi5t8" title="Click to go to the Program at a Glance">
             <b>
              Robot Motion Planning IV
             </b>
            </a>
           </td>
           <td class="r">
            Teaser Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#370306" title="Click to go to the Author Index">
             Zhang, Liding
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t8_01">
             15:30-16:30, Paper ThPI5T8.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1568'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Optimal Robotic Assembly Sequence Planning (ORASP): A Sequential Decision-Making Approach
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#371749" title="Click to go to the Author Index">
             Nagpal, Kartik
            </a>
           </td>
           <td class="r">
            University of California Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#277963" title="Click to go to the Author Index">
             Mehr, Negar
            </a>
           </td>
           <td class="r">
            University of California Berkeley
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1568" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#assembly" title="Click to go to the Keyword Index">
               Assembly
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#planning__scheduling_and_coordination" title="Click to go to the Keyword Index">
               Planning, Scheduling and Coordination
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The optimal robotic assembly planning problem entails determining the sequence of actions for a robot to feasibly assemble a product from its components which minimizes a given objective. This problem is made especially challenging as the number of potential sequences increase exponentially with respect to the number of parts in the assembly. Additionally, the optimal sequence must also consider and satisfy a selection of constraints such as attachment precedence or a maximum robot carry weight. Traditionally, robotic assembly planning problems have been solved using heuristics, but these methods are specific to a given robot or cost structure. In this paper, we propose a novel approach to robotic assembly planning that reformulates assembly sequencing as a shortest-path problem, and we propose a novel sequential decision-making framework. We formulate assembly sequencing as a Markov Decision Process and use Dynamic Programming (DP) to find optimal assembly policies that far outperform the state-of-the-art in terms of speed. We further exploit the deterministic nature of assembly planning to introduce a class of optimal Graph Exploration Assembly Planners (GEAPs) and even propose our own ORASP Search Method. We further showcase how we can produce high-reward assembly plans for larger structures using our deep Reinforcement Learning (RL) method. We evaluate this method on large robotic assembly problems such as the assembly of the Hubble Space Telescope, the International Space Station, and the James Webb Space Telescope. We further discuss how our DP, GEAP, and RL methods are capable of finding optimal solutions under a variety of different objective functions and how we translate any form of precedence constraints to branch pruning and further improve performance. We have published our code at https://github.com/labicon/ORASP-Code.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t8_02">
             15:30-16:30, Paper ThPI5T8.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3208'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Ontology Based AI Planning and Scheduling for Robotic Assembly
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#358280" title="Click to go to the Author Index">
             Zhao, Jingyun
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#143931" title="Click to go to the Author Index">
             Vogel-Heuser, Birgit
            </a>
           </td>
           <td class="r">
            Technical University Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398987" title="Click to go to the Author Index">
             Ao, Jicong
            </a>
           </td>
           <td class="r">
            Technical University Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#299533" title="Click to go to the Author Index">
             Wu, Yansong
            </a>
           </td>
           <td class="r">
            Technische Universitt Mnchen
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#370306" title="Click to go to the Author Index">
             Zhang, Liding
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#358279" title="Click to go to the Author Index">
             Fandi, Bi
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#322267" title="Click to go to the Author Index">
             Hujo, Dominik
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#195330" title="Click to go to the Author Index">
             Bing, Zhenshan
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#212176" title="Click to go to the Author Index">
             Wu, Fan
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#105608" title="Click to go to the Author Index">
             Knoll, Alois
            </a>
           </td>
           <td class="r">
            Tech. Univ. Muenchen TUM
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#108317" title="Click to go to the Author Index">
             Haddadin, Sami
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#358282" title="Click to go to the Author Index">
             Vojanec, Bernd
            </a>
           </td>
           <td class="r">
            WITTENSTEIN SE
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#319042" title="Click to go to the Author Index">
             Markert, Timo
            </a>
           </td>
           <td class="r">
            Resense GmbH
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#349204" title="Click to go to the Author Index">
             Kraft, Andr
            </a>
           </td>
           <td class="r">
            BMW AG, Germany
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3208" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#planning__scheduling_and_coordination" title="Click to go to the Keyword Index">
               Planning, Scheduling and Coordination
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#task_planning" title="Click to go to the Keyword Index">
               Task Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#intelligent_and_flexible_manufacturing" title="Click to go to the Keyword Index">
               Intelligent and Flexible Manufacturing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The rising demand for customized products necessitates the integration of multiple robotic systems, underscoring the need for advanced production planning and scheduling. This paper introduces an ontology-based, artificial intelligence-enhanced method for dynamic task planning and scheduling, aimed at improving the efficiency of production process, reducing machine downtime, and consequently increasing throughput in assembly operations. Designed to generate and execute feasible production plans dynamically, this method minimizes manual planning and scheduling efforts. We evaluate its effectiveness using two gear assembly use cases with various robot skills, highlighting its flexibility in planning and scheduling and its contributions to the evolution of smart manufacturing. The method's adaptability suggests its applicability across diverse smart factory environments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t8_03">
             15:30-16:30, Paper ThPI5T8.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3447'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Using Graphs of Convex Sets to Guide Nonconvex Trajectory Optimization
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392002" title="Click to go to the Author Index">
             von Wrangel, David
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#105841" title="Click to go to the Author Index">
             Tedrake, Russ
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3447" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#collision_avoidance" title="Click to go to the Keyword Index">
               Collision Avoidance
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Collision-free motion planning with trajectory optimization is inherently nonconvex. Some of this nonconvexity is fundamental: the robot might need to make a discrete decision to go left around an obstacle or right around an obstacle. Some of the nonconvexity is potentially more benign: we might want to penalize high-order derivatives of our continuous trajectories in order to encourage smoothness. Recently, Graphs of Convex Sets (GCS) have been applied to trajectory optimization, addressing the fundamental nonconvexity with efficient online optimization over a roadmap represented by an approximate convex decomposition of the configuration space. In this paper, we explore some of the most useful nonconvex costs and constraints and the suitability of combining convex global optimization using GCS with nonconvex trajectory optimization for rounding the local solutions. We find that for many applications, this combination can lead to a small number of nonconvex optimizations finding extremely good solutions to the nonconvex trajectory optimization problem.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t8_04">
             15:30-16:30, Paper ThPI5T8.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3498'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Demonstration to Adaptation: A User-Guided Framework for Sequential and Real-Time Planning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#266688" title="Click to go to the Author Index">
             Cai, Kuanqi
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#238518" title="Click to go to the Author Index">
             Laha, Riddhiman
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#340384" title="Click to go to the Author Index">
             Gong, Yuhe
            </a>
           </td>
           <td class="r">
            Karlsruhe Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#296908" title="Click to go to the Author Index">
             Chen, Lingyun
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#370306" title="Click to go to the Author Index">
             Zhang, Liding
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#156255" title="Click to go to the Author Index">
             Figueredo, Luis
            </a>
           </td>
           <td class="r">
            University of Nottingham (UoN)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#108317" title="Click to go to the Author Index">
             Haddadin, Sami
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3498" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#human_centered_robotics" title="Click to go to the Keyword Index">
               Human-Centered Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper introduces a comprehensive user-guided planning framework designed for robots operating in dynamic, human-centered environments  where the ability to execute sequential tasks flexibly and adaptively is paramount. Our planner enables robots to (i) encode object-centric constraints and user preferences via multiple demonstrations,(ii) transfer geometric features and implicit relaxations to novel scenarios while reacting to unforeseen events, and (iii) adapt to changing task conditions in real-time, including the real- time replanning and tracking of moving targets. Our approach relies on C1 screw linear interpolation, which generates smooth paths satisfying the underlying geometric constraints that characterize the task. The prescribed path is combined with a hierarchical quadratic programming-based controller which explores the user demonstrationss stochastic variability to relax task constraints while ensuring real-time whole-body collision avoidance. Our framework continuously checks for dynamic changes in task targets, ensuring appropriate planning or control actions, tending to the prescribed screw path. This comprehensive approach is deployed in different task conditions which are available at https://youtu.be/hvJ3pGUyLAI.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t8_05">
             15:30-16:30, Paper ThPI5T8.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2197'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Self-Reconfiguration Strategies for Space-Distributed Spacecraft
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#366815" title="Click to go to the Author Index">
             Liu, Tianle
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#367335" title="Click to go to the Author Index">
             Wang, Zhixiang
            </a>
           </td>
           <td class="r">
            Northwestern Polytechnical University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#201278" title="Click to go to the Author Index">
             Zhang, Yongwei
            </a>
           </td>
           <td class="r">
            National University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#237215" title="Click to go to the Author Index">
             Wang, Ziwei
            </a>
           </td>
           <td class="r">
            Lancaster University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#377374" title="Click to go to the Author Index">
             Liu, Zihao
            </a>
           </td>
           <td class="r">
            Northwestern Polytechnical University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#135120" title="Click to go to the Author Index">
             Zhang, Yizhai
            </a>
           </td>
           <td class="r">
            Northwestern Polytechnical University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#179293" title="Click to go to the Author Index">
             Huang, Panfeng
            </a>
           </td>
           <td class="r">
            Northwestern Polytechnical University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2197" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mobile_manipulation" title="Click to go to the Keyword Index">
               Mobile Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#ai_based_methods" title="Click to go to the Keyword Index">
               AI-Based Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper proposes a distributed on-orbit spacecraft assembly algorithm, where future spacecraft can assemble modules with different functions on orbit to form a spacecraft structure with specific functions. This form of spacecraft organization has the advantages of reconfigurability, fast mission response and easy maintenance. Reasonable and efficient on-orbit self-reconfiguration algorithms play a crucial role in realizing the benefits of distributed spacecraft. This paper adopts the framework of imitation learning combined with reinforcement learning for strategy learning of module handling order. A robot arm motion algorithm is then designed to execute the handling sequence. We achieve the self-reconfiguration handling task by creating a map on the surface of the module, completing the path point planning of the robotic arm using A*. The joint planning of the robotic arm is then accomplished through forward and reverse kinematics. Finally, the results are presented in Unity3D.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t8_06">
             15:30-16:30, Paper ThPI5T8.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('243'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Grasping Trajectory Optimization with Point Clouds
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#206428" title="Click to go to the Author Index">
             Xiang, Yu
            </a>
           </td>
           <td class="r">
            University of Texas at Dallas
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#378433" title="Click to go to the Author Index">
             Allu, Sai Haneesh
            </a>
           </td>
           <td class="r">
            The University of Texas at Dallas
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#391231" title="Click to go to the Author Index">
             Peddi, Rohith
            </a>
           </td>
           <td class="r">
            University of Texas at Dallas
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#215201" title="Click to go to the Author Index">
             Summers, Tyler
            </a>
           </td>
           <td class="r">
            University of Texas at Dallas
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#391232" title="Click to go to the Author Index">
             Gogate, Vibhav
            </a>
           </td>
           <td class="r">
            University of Texas at Dallas
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab243" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#manipulation_planning" title="Click to go to the Keyword Index">
               Manipulation Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#grasping" title="Click to go to the Keyword Index">
               Grasping
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We introduce a new trajectory optimization method for robotic grasping based on a point-cloud representation of robots and task spaces. In our method, robots are represented by 3D points on their link surfaces. The task space of a robot is represented by a point cloud that can be obtained from depth sensors. Using the point-cloud representation, goal reaching in grasping can be formulated as point matching, while collision avoidance can be efficiently achieved by querying the signed distance values of the robot points in the signed distance field of the scene points. Consequently, a constrained nonlinear optimization problem is formulated to solve the joint motion and grasp planning problem. The advantage of our method is that the point-cloud representation is general to be used with any robot in any environment. We demonstrate the effectiveness of our method by performing experiments on a tabletop scene and a shelf scene for grasping with a Fetch mobile manipulator and a Franka Panda arm.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t8_07">
             15:30-16:30, Paper ThPI5T8.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('833'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              UNO Push: Unified Nonprehensile Object Pushing Via Non-Parametric Estimation and Model Predictive Control
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#355249" title="Click to go to the Author Index">
             Wang, Gaotian
            </a>
           </td>
           <td class="r">
            Rice University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#236218" title="Click to go to the Author Index">
             Ren, Kejia
            </a>
           </td>
           <td class="r">
            Rice University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#155700" title="Click to go to the Author Index">
             Hang, Kaiyu
            </a>
           </td>
           <td class="r">
            Rice University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab833" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#manipulation_planning" title="Click to go to the Keyword Index">
               Manipulation Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Nonprehensile manipulation through precise pushing is an essential skill that has been commonly challenged by perception and physical uncertainties, such as those associated with contacts, object geometries, and physical properties. For this, we propose a unified framework that jointly addresses system modeling, action generation, and control. While most existing approaches either heavily rely on a priori system information for analytic modeling, or leverage a large dataset to learn dynamic models, our framework approximates a system transition function via non-parametric learning only using a small number of exploratory actions (ca. 10). The approximated function is then integrated with model predictive control to provide precise pushing manipulation. Furthermore, we show that the approximated system transition functions can be robustly transferred across novel objects while being online updated to continuously improve the manipulation accuracy. Through extensive experiments on a real robot platform with a set of novel objects and comparing against a state-of-the-art baseline, we show that the proposed unified framework is a light-weight and highly effective approach to enable precise pushing manipulation all by itself. Our evaluation results illustrate that the system can robustly ensure millimeter-level precision and can straightforwardly work on any novel object.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t8_08">
             15:30-16:30, Paper ThPI5T8.8
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1828'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Combining Sampling and Gradient-Based Planning for Contact-Rich Manipulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#354119" title="Click to go to the Author Index">
             Rozzi, Filippo
            </a>
           </td>
           <td class="r">
            Politecnico Di Milano
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#159606" title="Click to go to the Author Index">
             Roveda, Loris
            </a>
           </td>
           <td class="r">
            SUPSI-IDSIA
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#152432" title="Click to go to the Author Index">
             Haninger, Kevin
            </a>
           </td>
           <td class="r">
            Fraunhofer IPK
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1828" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#manipulation_planning" title="Click to go to the Keyword Index">
               Manipulation Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#contact_modeling" title="Click to go to the Keyword Index">
               Contact Modeling
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#compliance_and_impedance_control" title="Click to go to the Keyword Index">
               Compliance and Impedance Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Planning for contact-rich manipulation involves discontinuous dynamics, which presents challenges to planning methods. Sampling-based planners have higher sample complexity in high-dimensional problems and cannot efficiently handle state constraints such as force limits. Gradient-based solvers can suffer from local optima and their convergence rate is often worse on non-smooth problems. We propose a planning method that is both sampling- and gradient-based, using the Cross-entropy method to initialize a gradient-based solver, providing better initialization to the gradient-based method and allowing explicit handling of state constraints. The sampling-based planner also allows direct integration of a particle filter, which is here used for online contact mode estimation. The approach is shown to improve performance in MuJoCo environments and the effects of problem stiffness and planing horizon are investigated. The estimator and planner are then applied to an impedance-controlled robot, showing a reduction in solve time in contact transitions to only gradient-based.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t8_09">
             15:30-16:30, Paper ThPI5T8.9
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2262'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Clutter-Aware Spill-Free Liquid Transport Via Learned Dynamics
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#355151" title="Click to go to the Author Index">
             Abderezaei, Ava
            </a>
           </td>
           <td class="r">
            University of Colorado Boulder
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#250297" title="Click to go to the Author Index">
             Pasricha, Anuj
            </a>
           </td>
           <td class="r">
            University of Colorado Boulder
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392014" title="Click to go to the Author Index">
             Klausenstock, Alex
            </a>
           </td>
           <td class="r">
            University of Colorado Boulder
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#168176" title="Click to go to the Author Index">
             Roncone, Alessandro
            </a>
           </td>
           <td class="r">
            University of Colorado Boulder
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2262" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#manipulation_planning" title="Click to go to the Keyword Index">
               Manipulation Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#ai_based_methods" title="Click to go to the Keyword Index">
               AI-Based Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this work, we present a novel algorithm to perform spill-free handling of open-top liquid-filled containers that operates in cluttered environments. By allowing liquid-filled containers to be tilted at higher angles and enabling motion along all axes of end-effector orientation, our work extends the reachable space and enhances maneuverability around obstacles, broadening the range of feasible scenarios. Our key contributions include: i) generating spill-free paths through the use of RRT* with an informed sampler that leverages container properties to avoid spill-inducing states (such as an upside-down container), ii) parameterizing the resulting path to generate spill-free trajectories through the implementation of a time parameterization algorithm, coupled with a transformer-based machine-learning model capable of classifying trajectories as spill-free or not. We validate our approach in real-world, obstacle-rich task settings using containers of various shapes and fill levels and demonstrate an extended solution space that is at least 3x larger than an existing approach.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t8_10">
             15:30-16:30, Paper ThPI5T8.10
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2609'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              ContactHandover: Contact-Guided Robot-To-Human Object Handover
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395088" title="Click to go to the Author Index">
             Wang, Zixi
            </a>
           </td>
           <td class="r">
            Columbia University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#382979" title="Click to go to the Author Index">
             Liu, Zeyi
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#295426" title="Click to go to the Author Index">
             Ouporov, Nicolas
            </a>
           </td>
           <td class="r">
            Columbia University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#193621" title="Click to go to the Author Index">
             Song, Shuran
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2609" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#manipulation_planning" title="Click to go to the Keyword Index">
               Manipulation Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#human_centered_robotics" title="Click to go to the Keyword Index">
               Human-Centered Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robot-to-human object handover is an important step in many human robot collaboration tasks. A successful handover requires the robot to maintain a stable grasp on the object while making sure the human receives the object in a natural and easy-to-use manner. We propose ContactHandover, a robot to human handover system that consists of two phases: a contact-guided grasping phase and an object delivery phase. During the grasping phase, ContactHandover predicts both 6-DoF robot grasp poses and a 3D affordance map of human contact points on the object. The robot grasp poses are re-ranked by penalizing those that block human contact points, and the robot executes the highest ranking grasp. During the delivery phase, the robot end effector pose is computed by maximizing human contact points close to the human while minimizing the human arm joint torques and displacements. We evaluate our system on 27 diverse household objects and show that our system achieves better visibility and reachability of human contacts to the receiver compared to several baselines.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t8_11">
             15:30-16:30, Paper ThPI5T8.11
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2743'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Task-Driven Manipulation with Reconfigurable Parallel Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398532" title="Click to go to the Author Index">
             Morton, Daniel
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#101950" title="Click to go to the Author Index">
             Cutkosky, Mark
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#123466" title="Click to go to the Author Index">
             Pavone, Marco
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2743" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#manipulation_planning" title="Click to go to the Keyword Index">
               Manipulation Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#parallel_robots" title="Click to go to the Keyword Index">
               Parallel Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             ReachBot, a proposed robotic platform, employs extendable booms as limbs for mobility in challenging environments, such as martian caves. When attached to the environment, ReachBot acts as a parallel robot, with reconfiguration driven by the ability to detach and re-place the booms. This ability enables manipulation-focused scientific objectives: for instance, through operating tools, or handling and transporting samples. To achieve these capabilities, we develop a two-part solution, optimizing for robustness against task uncertainty and stochastic failure modes. First, we present a mixed-integer stance planner to determine the positioning of ReachBot's booms to maximize the task wrench space about the nominal point(s). Second, we present a convex tension planner to determine boom tensions for the desired task wrenches, accounting for the probabilistic nature of microspine grasping. We demonstrate improvements in key robustness metrics from the field of dexterous manipulation, and show a large increase in the volume of the manipulation workspace. Finally, we employ Monte-Carlo simulation to validate the robustness of these methods, demonstrating good performance across a range of randomized tasks and environments, and generalization to cable-driven morphologies. We make our code available at our project webpage, https://stanfordasl.github.io/reachbot_manipulation/
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t8_12">
             15:30-16:30, Paper ThPI5T8.12
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3131'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A General Formulation for Path Constrained Time-Optimized Trajectory Planning with Environmental and Object Contacts
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#312636" title="Click to go to the Author Index">
             Mahalingam, Dasharadhan
            </a>
           </td>
           <td class="r">
            Stony Brook University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#269340" title="Click to go to the Author Index">
             Patankar, Aditya
            </a>
           </td>
           <td class="r">
            Stony Brook University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#238518" title="Click to go to the Author Index">
             Laha, Riddhiman
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#327372" title="Click to go to the Author Index">
             Lakshminarayanan, Srinivasan
            </a>
           </td>
           <td class="r">
            TUM
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#108317" title="Click to go to the Author Index">
             Haddadin, Sami
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#107599" title="Click to go to the Author Index">
             Chakraborty, Nilanjan
            </a>
           </td>
           <td class="r">
            Stony Brook University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3131" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#manipulation_planning" title="Click to go to the Keyword Index">
               Manipulation Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             A typical manipulation task consists of a manip- ulator equipped with a gripper to grasp and move an object with constraints on the motion of the hand-held object, which may be due to the nature of the task itself or from object- environment contacts. In this paper, we study the problem of computing joint torques and grasping forces for time-optimal motion of an object, while ensuring that the grasp is not lost and any constraints on the motion of the object, either due to dynamics, environment contact, or no-slip requirements, are also satisfied. We present a second order cone program (SOCP) formulation of the time-optimal trajectory planning problem that considers nonlinear friction cone constraints at the hand-object and object-environment contacts. Since SOCPs are convex optimization problems that can be solved optimally in polynomial time using interior point methods, we can solve the trajectory optimization problem efficiently. We present simulation results on three examples, including a non-prehensile manipulation task which shows the generality and effectiveness of our approach
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t8_13">
             15:30-16:30, Paper ThPI5T8.13
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3279'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Trajectory Planning for Non-Prehensile Object Transportation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#296908" title="Click to go to the Author Index">
             Chen, Lingyun
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#373773" title="Click to go to the Author Index">
             Yu, Haoyu
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#370306" title="Click to go to the Author Index">
             Zhang, Liding
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#239366" title="Click to go to the Author Index">
             Naceri, Abdeldjallil
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#292159" title="Click to go to the Author Index">
             Swikir, Abdalla
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#108317" title="Click to go to the Author Index">
             Haddadin, Sami
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3279" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#manipulation_planning" title="Click to go to the Keyword Index">
               Manipulation Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Non-prehensile transportation of unstable objects presents a challenging task in robotics. To ensure the success of the transportation, it is necessary to consider both the object's stability via contact dynamics and the motion constraints of the robot. We propose two novel trajectory planning methods derived from sampling and dynamic programming algorithms, tested on a 7-DoF Franka Emika robot against common strategies like Model Predictive Control (MPC) and S-curve planning, particularly under the constraint of a non-rotating tray. The results demonstrate the effectiveness of our methodologies in improving transportation speed. This research contributes to advancements in robotic manipulation techniques by tackling non-prehensile manipulation of dynamically unstable objects.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t8_14">
             15:30-16:30, Paper ThPI5T8.14
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1537'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              The Effectiveness of State Representation Model in Multi-Agent Proximal Policy Optimization for Multi-Agent Path Finding
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396770" title="Click to go to the Author Index">
             Chung, Jaehoon
            </a>
           </td>
           <td class="r">
            University of Victoria
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#306759" title="Click to go to the Author Index">
             Fayyad, Jamil
            </a>
           </td>
           <td class="r">
            The University of British Colombia
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#339132" title="Click to go to the Author Index">
             Ghafarian Tamizi, Mehran
            </a>
           </td>
           <td class="r">
            University of Victoria
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#103596" title="Click to go to the Author Index">
             Najjaran, Homayoun
            </a>
           </td>
           <td class="r">
            University of Victoria
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1537" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#autonomous_agents" title="Click to go to the Keyword Index">
               Autonomous Agents
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#model_learning_for_control" title="Click to go to the Keyword Index">
               Model Learning for Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Multi-agent pathfinding plays a crucial role in various robot applications. Recently, deep reinforcement learning methods have been adopted to solve large-scale planning problems in a decentralized manner. Nonetheless, such approaches pose challenges such as non-stationarity and partial observability. In this paper, we address these challenges by integrating a state representation model into a multi-agent proximal policy optimization framework. To do so, we propose to utilize a world model which extracts representation features from the global map and leverages this information to enhance the training process. Our approach involves decoupling the feature extractor from the agent training process, enabling a more accurate representation of the global state that remains unbiased by the actions of the agents. Furthermore, our modularized approach offers the flexibility to replace the representation model with another model or modify tasks within the global map, without the retraining of the agents. We demonstrated the effectiveness of our approach by comparing three multi-agent proximal policy optimization frameworks. Our experimental results demonstrate that our approach improves the average episode reward compared to the other approaches.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t8_15">
             15:30-16:30, Paper ThPI5T8.15
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2124'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Camera-Based Belief Space Planning in Discrete Partially-Observable Domains
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#375937" title="Click to go to the Author Index">
             Freund, Janis Eric
            </a>
           </td>
           <td class="r">
            Technical University of Berlin
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#225245" title="Click to go to the Author Index">
             Phiquepal, Camille
            </a>
           </td>
           <td class="r">
            University of Stuttgart
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#160156" title="Click to go to the Author Index">
             Orthey, Andreas
            </a>
           </td>
           <td class="r">
            Realtime Robotics Inc
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#102561" title="Click to go to the Author Index">
             Toussaint, Marc
            </a>
           </td>
           <td class="r">
            TU Berlin
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2124" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#reactive_and_sensor_based_planning" title="Click to go to the Keyword Index">
               Reactive and Sensor-Based Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#nonholonomic_motion_planning" title="Click to go to the Keyword Index">
               Nonholonomic Motion Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robots often have to operate in discrete partially observable worlds, where the state of the world is only observable at runtime. To react to different world states, robots need contingencies. To find contingencies, prior work developed the path tree optimization (PTO) method, which computes motion contingencies by constructing a tree of motion paths in belief space. In this paper, we extend upon PTO by enabling camera-based belief space planning through an extension of the open motion planning library (OMPL). By leveraging this extension, we develop an improved camera-based state sampler and an efficient open-source implementation of PTO. This version of PTO supports a virtual camera, non-euclidean state spaces, and different state samplers. We evaluate this improved version of PTO on four realistic scenarios with a virtual camera in up to 10-dimensional state spaces. In our evaluations, we compare PTO both with a default and with the new camera-based state sampler. The results indicate that the camera-based state sampler improves success rates in 3 out of 4 scenarios while having a significant lower memory footprint. Our work thus makes an important step in advancing belief-space planning and provides researchers with an open source tool to use, modify, and benchmark belief-space planning methods.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t8_16">
             15:30-16:30, Paper ThPI5T8.16
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1714'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Path-Parameterised RRTs for Underactuated Systems
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#380512" title="Click to go to the Author Index">
             Abood, Damian
            </a>
           </td>
           <td class="r">
            University of Sydney
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#109851" title="Click to go to the Author Index">
             Manchester, Ian
            </a>
           </td>
           <td class="r">
            University of Sydney
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1714" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#underactuated_robots" title="Click to go to the Keyword Index">
               Underactuated Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#dynamics" title="Click to go to the Keyword Index">
               Dynamics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present a sample-based motion planning algorithm specialised to a class of underactuated systems using path parameterisation. The structure this class presents under a path parameterisation enables the trivial computation of dynamic feasibility along a path. Using this, a specialised state-based steering mechanism within an RRT motion planning algorithm is developed, enabling the generation of both geometric paths and their time parameterisations without introducing excessive computational overhead. We find with two systems that our algorithm computes feasible trajectories with higher rates of success and lower mean computation times compared to existing approaches.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thpi5t9">
             <b>
              ThPI5T9
             </b>
            </a>
           </td>
           <td class="r">
            Room 9
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thpi5t9" title="Click to go to the Program at a Glance">
             <b>
              Navigation IV
             </b>
            </a>
           </td>
           <td class="r">
            Teaser Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#143360" title="Click to go to the Author Index">
             Mahmoudian, Nina
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t9_01">
             15:30-16:30, Paper ThPI5T9.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2255'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Exploring Latent Pathways: Enhancing the Interpretability of Autonomous Driving with a Variational Autoencoder
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392650" title="Click to go to the Author Index">
             Bairouk, Anass
            </a>
           </td>
           <td class="r">
            Capgemini
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#390767" title="Click to go to the Author Index">
             Maras, Mirjana
            </a>
           </td>
           <td class="r">
            Capgemini
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398034" title="Click to go to the Author Index">
             Herlin, Simon
            </a>
           </td>
           <td class="r">
            Capgemini
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#201560" title="Click to go to the Author Index">
             Amini, Alexander
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#245389" title="Click to go to the Author Index">
             Blanchon, Marc
            </a>
           </td>
           <td class="r">
            Capgemini Engineering
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#233504" title="Click to go to the Author Index">
             Hasani, Ramin
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology (MIT)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398037" title="Click to go to the Author Index">
             Chareyre, Patrick
            </a>
           </td>
           <td class="r">
            Capgemini
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#101388" title="Click to go to the Author Index">
             Rus, Daniela
            </a>
           </td>
           <td class="r">
            MIT
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2255" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#acceptability_and_trust" title="Click to go to the Keyword Index">
               Acceptability and Trust
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_transportation" title="Click to go to the Keyword Index">
               Computer Vision for Transportation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Autonomous driving presents a complex challenge, which is usually addressed with artificial intelligence models that are end-to-end or modular in nature. Within the landscape of modular approaches, a bio-inspired neural circuit policy model has emerged as an innovative control module, offering a compact and inherently interpretable system to infer a steering wheel command from abstract visual features. Here, we take a leap forward by integrating a variational autoencoder with the neural circuit policy controller, forming a solution that directly generates steering commands from input camera images. By substituting the traditional convolutional neural network approach to feature extraction with a variational autoencoder, we enhance the system's interpretability, enabling a more transparent and understandable decision-making process.
             <p>
              In addition to the architectural shift toward a variational autoencoder, this study introduces the automatic latent perturbation tool, a novel contribution designed to probe and elucidate the latent features within the variational autoencoder. The automatic latent perturbation tool automates the interpretability process, offering granular insights into how specific latent variables influence the overall model's behavior. Through a series of numerical experiments, we demonstrate the interpretative power of the variational autoencoder-neural circuit policy model and the utility of the automatic latent perturbation tool in making the inner workings of autonomous driving systems more transparent.
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t9_02">
             15:30-16:30, Paper ThPI5T9.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2357'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Synergistic Reinforcement and Imitation Learning for Vision-Driven Autonomous Flight of UAV Along River
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#354431" title="Click to go to the Author Index">
             Wang, Zihan
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#290947" title="Click to go to the Author Index">
             Li, Jianwen
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#143360" title="Click to go to the Author Index">
             Mahmoudian, Nina
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2357" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Vision-driven autonomous flight and obstacle avoidance of Unmanned Aerial Vehicles (UAVs) along complex riverine environments for tasks like rescue and surveillance requires a robust navigation policy, which is yet difficult to obtain due to the shortage of trainable riverine environment simulators. To easily verify the vision-based navigation controller performance for the river following task before real-world deployment, we developed a trainable photo-realistic dynamics-free riverine simulation environment using Unity. In this paper, we address the shortcomings that vanilla Reinforcement Learning (RL) algorithm encounters in learning a navigation policy within this partially observable, non-Markovian environment. We propose a synergistic approach that integrates RL and Imitation Learning (IL). Initially, an IL expert is trained on manually collected demonstrations, which then guides the RL policy training process. Concurrently, experiences generated by the RL agent are utilized to re-train the IL expert, enhancing its ability to generalize to unseen data. By leveraging the strengths of both RL and IL, this framework achieves a faster convergence rate and higher performance compared to pure RL, pure IL, and RL combined with static IL algorithms. The results validate the efficacy of the proposed method in terms of both task completion and efficiency. The code and trainable environments are available.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t9_03">
             15:30-16:30, Paper ThPI5T9.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2922'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              AdvDiffuser: Generating Adversarial Safety-Critical Driving Scenarios Via Guided Diffusion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#277780" title="Click to go to the Author Index">
             Xie, Yuting
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#336518" title="Click to go to the Author Index">
             Guo, Xianda
            </a>
           </td>
           <td class="r">
            School of Computer Science, Wuhan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397576" title="Click to go to the Author Index">
             Wang, Cong
            </a>
           </td>
           <td class="r">
            Institute of Automation, Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398720" title="Click to go to the Author Index">
             Kunhua, Liu
            </a>
           </td>
           <td class="r">
            Qingdao University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#167297" title="Click to go to the Author Index">
             Chen, Long
            </a>
           </td>
           <td class="r">
            Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2922" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#simulation_and_animation" title="Click to go to the Keyword Index">
               Simulation and Animation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#autonomous_agents" title="Click to go to the Keyword Index">
               Autonomous Agents
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#autonomous_vehicle_navigation" title="Click to go to the Keyword Index">
               Autonomous Vehicle Navigation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Safety-critical scenarios are infrequent in natural driving environments but hold significant importance for the training and testing of autonomous driving systems. The prevailing approach involves generating safety-critical scenarios automatically in simulation by introducing adversarial adjustments to natural environments. These adjustments are often tailored to specific tested systems, thereby disregarding their transferability across different systems. In this paper, we propose AdvDiffuser, an adversarial framework for generating safety-critical driving scenarios through guided diffusion. By incorporating a diffusion model to capture plausible collective behaviors of background vehicles and a lightweight guide model to effectively handle adversarial scenarios, AdvDiffuser facilitates transferability. Experimental results on the nuScenes dataset demonstrate that AdvDiffuser, trained on offline driving logs, can be applied to various tested systems with minimal warm-up episode data and outperform other existing methods in terms of realism, diversity, and adversarial performance.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t9_04">
             15:30-16:30, Paper ThPI5T9.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1235'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              OSM vs HD Maps: Map Representations for Trajectory Prediction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#328756" title="Click to go to the Author Index">
             Liao, Jing-Yan
            </a>
           </td>
           <td class="r">
            University of California, San Diego
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#379140" title="Click to go to the Author Index">
             Doshi, Parth Jaydip
            </a>
           </td>
           <td class="r">
            UCSD
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#379122" title="Click to go to the Author Index">
             Zhang, Zihan
            </a>
           </td>
           <td class="r">
            University of California San Diego
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#276913" title="Click to go to the Author Index">
             Paz, David
            </a>
           </td>
           <td class="r">
            University of California, San Diego
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#102379" title="Click to go to the Author Index">
             Christensen, Henrik Iskov
            </a>
           </td>
           <td class="r">
            UC San Diego
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1235" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#autonomous_agents" title="Click to go to the Keyword Index">
               Autonomous Agents
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#ai_based_methods" title="Click to go to the Keyword Index">
               AI-Based Methods
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#autonomous_vehicle_navigation" title="Click to go to the Keyword Index">
               Autonomous Vehicle Navigation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             High Definition (HD) Maps have long been favored for their precise depictions of static road elements. However, their accessibility constraints and vulnerability to rapid environmental changes impede the widespread deployment of highly map-reliant autonomous driving tasks, such as motion forecasting. In this context, we propose to leverage OpenStreetMap (OSM) as a promising alternative to HD Maps for long-term motion forecasting. The contributions of this work are threefold: firstly, we extend the application of OSM to long-horizon forecasting, doubling the forecasting horizon compared to previous studies. Secondly, through an expanded observation landscape and the integration of intersection priors, our OSM-based approach exhibits competitive performance, narrowing the gap with HD-map-based models. Lastly, we conduct an exhaustive context-aware analysis, providing deeper insights in motion forecasting across diverse scenarios as well as conducting class-aware comparisons. This research not only advances long-term motion forecasting with coarse map representations but additionally offers a scalable solution within the domain of autonomous driving.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t9_05">
             15:30-16:30, Paper ThPI5T9.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1331'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              FDNet: Feature Decoupling Framework for Trajectory Prediction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396153" title="Click to go to the Author Index">
             Li, Yuhang
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396272" title="Click to go to the Author Index">
             Li, Changsheng
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396273" title="Click to go to the Author Index">
             Fan, Baoyu
            </a>
           </td>
           <td class="r">
            Nankai University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396223" title="Click to go to the Author Index">
             Li, Rongqing
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396159" title="Click to go to the Author Index">
             Zhang, Ziyue
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#243946" title="Click to go to the Author Index">
             Ren, Dongchun
            </a>
           </td>
           <td class="r">
            Meituan
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396280" title="Click to go to the Author Index">
             Yuan, Ye
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#340111" title="Click to go to the Author Index">
             Wang, Guoren
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1331" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#autonomous_agents" title="Click to go to the Keyword Index">
               Autonomous Agents
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Trajectory prediction plays a significant role in autonomous driving, with current challenges primarily focused on capturing complex interactions in traffic scenes. Previous methods usually directly encode non-interactive and interactive information together, and then decode them for trajectory prediction. However, given the complexity inherent property in the trajectory generation process (e.g., the generation of trajectory points are influenced by the interactions among multiple moving agents, as well as the interactions between agents and the static environment), previous approaches fail to precisely capture separate variations of the trajectory generation process. In this paper, we propose a general and plug-and-play feature decoupling framework for trajectory prediction called FDNet, which can learn the interactive and non-interactive factors in the latent space to capture separate variations of the trajectory generation process. At its core, FDNet is comprised of a Non-interactive Feature Extraction Module to extract non-interactive features, and an Interactive Feature Decoupling Module to decouple interactive features. Extensive experiments conducted on Argoverse and nuScenes demonstrate that FDNet significantly improves the performance of existing methods.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t9_06">
             15:30-16:30, Paper ThPI5T9.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1570'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Active Learning-Augmented Intention-Aware Obstacle Avoidance of Autonomous Surface Vehicles in High-Traffic Waters
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#277458" title="Click to go to the Author Index">
             Jeong, Mingi
            </a>
           </td>
           <td class="r">
            Dartmouth College
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#377767" title="Click to go to the Author Index">
             Chadda, Arihant
            </a>
           </td>
           <td class="r">
            IQT Labs
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#156679" title="Click to go to the Author Index">
             Quattrini Li, Alberto
            </a>
           </td>
           <td class="r">
            Dartmouth College
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1570" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#marine_robotics" title="Click to go to the Keyword Index">
               Marine Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#autonomous_vehicle_navigation" title="Click to go to the Keyword Index">
               Autonomous Vehicle Navigation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#collision_avoidance" title="Click to go to the Keyword Index">
               Collision Avoidance
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper enhances the obstacle avoidance of Autonomous Surface Vehicles (ASVs) for safe navigation in high-traffic waters with an active state estimation of obstacle's passing intention and reducing its uncertainty. We introduce a topological modeling of passing intention of obstacles, which can be applied to varying encounter situations based on the inherent embedding of topological concepts in COLREGs. With a Long Short-Term Memory (LSTM) neural network, we classify the passing intention of obstacles. Then, for determining the ASV maneuver, we propose a multi-objective optimization framework including information gain about the passing obstacle intention and safety. We validate the proposed approach under extensive Monte Carlo simulations (2,400 runs) with a varying number of obstacles, dynamic properties, encounter situations, and different behavioral patterns of obstacles (cooperative, non-cooperative). We also present the results from a real marine accident case study as well as real-world experiments of a real ASV with environmental disturbances, showing successful collision avoidance with our strategy in real-time.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t9_07">
             15:30-16:30, Paper ThPI5T9.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3053'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Map-Based Modular Approach for Zero-Shot Embodied Question Answering
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#390947" title="Click to go to the Author Index">
             Sakamoto, Koya
            </a>
           </td>
           <td class="r">
            Kyoto University, ATR
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#394594" title="Click to go to the Author Index">
             Azuma, Daichi
            </a>
           </td>
           <td class="r">
            Sony Semiconductor Solutions
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#394595" title="Click to go to the Author Index">
             Miyanishi, Taiki
            </a>
           </td>
           <td class="r">
            Advanced Telecommunications Research Institute International
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#394186" title="Click to go to the Author Index">
             Kurita, Shuhei
            </a>
           </td>
           <td class="r">
            RIKEN
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#164056" title="Click to go to the Author Index">
             Kawanabe, Motoaki
            </a>
           </td>
           <td class="r">
            Advanced Telecommunications Research Institutte International
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3053" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#multi_modal_perception_for_hri" title="Click to go to the Keyword Index">
               Multi-Modal Perception for HRI
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Embodied Question Answering (EQA) serves as a benchmark task to evaluate the capability of robots to navigate within novel environments and identify objects in response to human queries. However, existing EQA methods often rely on simulated environments and operate with limited vocabularies. This paper presents a map-based modular approach to EQA, enabling real-world robots to explore and map unknown environments. By leveraging foundation models, our method facilitates answering a diverse range of questions using natural language. We conducted extensive experiments in both virtual and real-world settings, demonstrating the robustness of our approach in navigating and comprehending queries within unknown environments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t9_08">
             15:30-16:30, Paper ThPI5T9.8
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3229'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              LeGo-Drive: Language-Enhanced Goal-Oriented Closed-Loop End-To-End Autonomous Driving
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#379093" title="Click to go to the Author Index">
             Paul, Pranjal
            </a>
           </td>
           <td class="r">
            International Institute of Information Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397433" title="Click to go to the Author Index">
             Garg, Anant
            </a>
           </td>
           <td class="r">
            International Institute of Information Technology, Hyderabad
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#354595" title="Click to go to the Author Index">
             Choudhary, Tushar
            </a>
           </td>
           <td class="r">
            International Institute of Information Technology, Hyderabad
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#123110" title="Click to go to the Author Index">
             Singh, Arun Kumar
            </a>
           </td>
           <td class="r">
            University of Tartu
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#102906" title="Click to go to the Author Index">
             Krishna, Madhava
            </a>
           </td>
           <td class="r">
            IIIT Hyderabad
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3229" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#autonomous_vehicle_navigation" title="Click to go to the Keyword Index">
               Autonomous Vehicle Navigation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Existing Vision-Language Models (VLMs) produce long-term trajectory waypoints or directly control actions based on their perception input and language prompt. However, these VLMs are not explicitly aware of the constraints imposed by the scene or kinematics of the vehicle. As a result, the generated trajectories or control inputs are likely to be unsafe and/or infeasible. In this paper, we introduce LeGo-Drive, which aims to address these issues. Our key idea is to use the VLM to just predict a goal location based on the given language command and perception input, which is then fed to a downstream differentiable trajectory optimizer with learnable components. We train the VLM and the trajectory optimizer in an end-to-end fashion using a loss function that captures the ego-vehicle's ability to reach the predicted goal while satisfying safety and kinematic constraints. The gradients during the back-propagation flow through the optimization layer and make the VLM aware of the planners capabilities, making more feasible goal predictions. We compare our end-to-end approach with a decoupled framework where the planner is just used at the inference time to drive to the VLM-predicted goal location and report a goal reaching Success Rate of 81%. We demonstrate the versatility of LeGo-Drive across various driving scenarios and navigation commands, highlighting its potential for practical deployment in autonomous vehicles.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t9_09">
             15:30-16:30, Paper ThPI5T9.9
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3235'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Monocular Depth Estimation for Drone Obstacle Avoidance in Indoor Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398897" title="Click to go to the Author Index">
             Zheng, Haokun
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398650" title="Click to go to the Author Index">
             Rajadnya, Sidhant
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#127841" title="Click to go to the Author Index">
             Zakhor, Avideh
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3235" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__perception_and_autonomy" title="Click to go to the Keyword Index">
               Aerial Systems: Perception and Autonomy
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#collision_avoidance" title="Click to go to the Keyword Index">
               Collision Avoidance
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Autonomous nano-quadcopters possess large potential for indoor use. Existing works on autonomous flight however rely on large amounts of compute, therefore resulting in heavy and bulky platforms that can only be safely deployed outdoors. We present a monocular depth estimation method for autonomous indoor obstacle avoidance and waypoint navigation of nano-quadcopters demonstrated on the Bitcraze Crazyflie 2.1 which weighs a mere 33g. Our depth estimation model has 1.56 million parameters and is 4 MB, which after quantization becomes 1 MB. We transmit the images via WiFi from the onboard grayscale camera on the Bitcraze to a laptop, which then runs the 1 MB quantized model to generate small-size depth maps. Subsequently, we run our navigation algorithms on a laptop and transmit high-level motion commands back to the drone. We demonstrate obstacle avoidance capability of this end-to-end system through real-world flights in a variety of indoor environments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t9_10">
             15:30-16:30, Paper ThPI5T9.10
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3340'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              TriHelper: Zero-Shot Object Navigation with Dynamic Assistance
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#394915" title="Click to go to the Author Index">
             Zhang, Lingfeng
            </a>
           </td>
           <td class="r">
            The Hong Kong University of Science and Technology (Guangzhou)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#372752" title="Click to go to the Author Index">
             Zhang, Qiang
            </a>
           </td>
           <td class="r">
            The Hong Kong University of Science and Technology (Guangzhou)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#394926" title="Click to go to the Author Index">
             Wang, Hao
            </a>
           </td>
           <td class="r">
            Hong Kong University of Science and Technology(Guang Zhou)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395018" title="Click to go to the Author Index">
             Xiao, Erjia
            </a>
           </td>
           <td class="r">
            The Hong Kong University of Science and Technology (Guangzhou)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397168" title="Click to go to the Author Index">
             Jiang, Zixuan
            </a>
           </td>
           <td class="r">
            HKUST(GZ)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398737" title="Click to go to the Author Index">
             Chen, Honglei
            </a>
           </td>
           <td class="r">
            The Hong Kong University of Science and Technology Guang
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#354054" title="Click to go to the Author Index">
             Xu, Renjing
            </a>
           </td>
           <td class="r">
            The Hong Kong University of Science and Technology (Guangzhou)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3340" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Navigating toward specific objects in unknown environments without additional training, known as Zero-Shot object navigation, poses a significant challenge in the field of robotics, which demands high levels of auxiliary information and strategic planning. Traditional works have focused on holistic solutions, overlooking the specific challenges agents encounter during navigation such as collision, low exploration efficiency, and misidentification of targets. To address these challenges, our work proposes TriHelper, a novel framework designed to assist agents dynamically through three primary navigation challenges: collision, exploration, and detection. Specifically, our framework consists of three innovative components: (i) Collision Helper, (ii) Exploration Helper, and (iii) Detection Helper. These components work collaboratively to solve these challenges throughout the navigation process. Experiments on the Habitat-Matterport 3D (HM3D) and Gibson datasets demonstrate that TriHelper significantly outperforms all existing baseline methods in Zero-Shot object navigation, showcasing superior success rates and exploration efficiency. Our ablation studies further underscore the effectiveness of each helper in addressing their respective challenges, notably enhancing the agent's navigation capabilities. By proposing TriHelper, we offer a fresh perspective on advancing the object navigation task, paving the way for future research in the domain of Embodied AI and visual-based navigation.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t9_11">
             15:30-16:30, Paper ThPI5T9.11
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('474'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Towards Cross-View-Consistent Self-Supervised Surround Depth Estimation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#277439" title="Click to go to the Author Index">
             Ding, Laiyan
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong, Shenzhen
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#277433" title="Click to go to the Author Index">
             Jiang, Hualie
            </a>
           </td>
           <td class="r">
            Insta360 Research
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#307631" title="Click to go to the Author Index">
             Li, Jie
            </a>
           </td>
           <td class="r">
            Shenzhen Polytechnic University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#138646" title="Click to go to the Author Index">
             Chen, Yongquan
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong, Shenzhen
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#277435" title="Click to go to the Author Index">
             Huang, Rui
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong, Shenzhen
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab474" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#omnidirectional_vision" title="Click to go to the Keyword Index">
               Omnidirectional Vision
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Depth estimation is a cornerstone for autonomous driving, yet acquiring per-pixel depth ground truth for supervised learning is challenging. Self-Supervised Surround Depth Estimation (SSSDE) from consecutive images offers an economical alternative. While previous SSSDE methods have proposed different mechanisms to fuse information across images, few of them explicitly consider the cross-view constraints, leading to inferior performance, particularly in overlapping regions. This paper proposes an efficient and consistent pose estimation design and two loss functions to enhance cross-view consistency for SSSDE. For pose estimation, we propose to use only front-view images to reduce training memory and sustain pose estimation consistency. The first loss function is the dense depth consistency loss, which penalizes the difference between predicted depths in overlapping regions. The second one is the multi-view reconstruction consistency loss, which aims to maintain consistency between reconstruction from spatial and spatial-temporal contexts. Additionally, we introduce a novel flipping augmentation to improve the performance further. Our techniques enable a simple neural model to achieve state-of-the-art performance on the DDAD and nuScenes datasets. Last but not least, our proposed techniques can be easily applied to other methods. The code is available at https://github.com/denyingmxd/CVCDepth.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t9_12">
             15:30-16:30, Paper ThPI5T9.12
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('707'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Boosting Generalizability towards Zero-Shot Cross-Dataset Single-Image Indoor Depth by Meta-Initialization
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#274405" title="Click to go to the Author Index">
             Wu, Cho-Ying
            </a>
           </td>
           <td class="r">
            University of Southern California
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#237228" title="Click to go to the Author Index">
             Zhong, Yiqi
            </a>
           </td>
           <td class="r">
            University of Southern California
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#373516" title="Click to go to the Author Index">
             Wang, Junying
            </a>
           </td>
           <td class="r">
            University of Southern California
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#172779" title="Click to go to the Author Index">
             Neumann, Ulrich
            </a>
           </td>
           <td class="r">
            University of Southern California
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab707" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#range_sensing" title="Click to go to the Keyword Index">
               Range Sensing
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Indoor robots rely on depth to perform tasks like navigation or obstacle detection, and single-image depth estimation is widely used to assist perception. Most indoor single-image depth prediction focuses less on model generalizability to unseen datasets, concerned with in-the-wild robustness for system deployment. This work leverages gradient-based meta-learning to gain higher generalizability on zero-shot cross-dataset inference. Unlike the most-studied meta-learning of image classification associated with explicit class labels, no explicit task boundaries exist for continuous depth values tied to highly varying indoor environments regarding object arrangement and scene composition. We propose fine-grained task that treats each RGB-D mini-batch as a task in our meta-learning formulation. We first show that our method on limited data induces a much better prior (max 27.8% in RMSE). Then, finetuning on meta-learned initialization consistently outperforms baselines without the meta approach. Aiming at generalization, we propose zero-shot cross-dataset protocols and validate higher generalizability induced by our meta-initialization, as a simple and useful plugin to many existing depth estimation methods. The work at the intersection of depth and meta-learning potentially drives both research to step closer to practical robotic and machine perception usage.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t9_13">
             15:30-16:30, Paper ThPI5T9.13
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3058'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              LAC-Net: Linear-Fusion Attention-Guided Convolutional Network for Accurate Robotic Grasping under the Occlusion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#377773" title="Click to go to the Author Index">
             Zhang, Jinyu
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#352485" title="Click to go to the Author Index">
             Gu, Yongchong
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#377646" title="Click to go to the Author Index">
             Gao, Jianxiong
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#296392" title="Click to go to the Author Index">
             Lin, Haitao
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#352127" title="Click to go to the Author Index">
             Sun, Qiang
            </a>
           </td>
           <td class="r">
            Shanghai University of International Business and Economics
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#377482" title="Click to go to the Author Index">
             Sun, Xinwei
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#256022" title="Click to go to the Author Index">
             Xue, Xiangyang
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#308158" title="Click to go to the Author Index">
             Fu, Yanwei
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3058" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper addresses the challenge of perceiving complete object shapes through visual perception. While prior studies have demonstrated encouraging outcomes in segmenting the visible parts of objects within a scene, amodal segmentation, in particular, has the potential to allow robots to infer the occluded parts of objects. To this end, this paper introduces a new framework that explores amodal segmentation for robotic grasping in cluttered scenes, thus greatly enhancing robotic grasping abilities. Initially, we use a conventional segmentation algorithm to detect the visible segments of the target object, which provides shape priors for completing the full object mask. Particularly, to explore how to utilize semantic features from RGB images and geometric information from depth images, we propose a Linear-fusion Attention-guided Convolutional Network (LAC-Net). LAC-Net utilizes the linear-fusion strategy to effectively fuse this cross-modal data, and then uses the prior visible mask as attention map to guide the network to focus on target feature locations for further complete mask recovery. Using the amodal mask of the target object provides advantages in selecting more accurate and robust grasp points compared to relying solely on the visible segments. The results on different datasets show that our method achieves state-of-the-art performance. Furthermore, the robot experiments validate the feasibility and robustness of this method in the real world.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t9_14">
             15:30-16:30, Paper ThPI5T9.14
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3179'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Visual-Geometry GP-Based Navigable Space for Autonomous Navigation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#326026" title="Click to go to the Author Index">
             Ali, Mahmoud
            </a>
           </td>
           <td class="r">
            Indiana University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#245868" title="Click to go to the Author Index">
             Pushp, Durgakant
            </a>
           </td>
           <td class="r">
            Indiana University Bloomington
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#246551" title="Click to go to the Author Index">
             Chen, Zheng
            </a>
           </td>
           <td class="r">
            Indiana University Bloomington
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#141677" title="Click to go to the Author Index">
             Liu, Lantao
            </a>
           </td>
           <td class="r">
            Indiana University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3179" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Autonomous navigation in unknown environments is challenging and requires the consideration of both geometric and semantic information to assess the navigability of the environment. In this work, we propose a novel space modeling framework, Visual-Geometry Sparse Gaussian Process (VG-SGP), that simultaneously considers semantics and geometry of the scene. Our proposed approach can overcome the limitation of visual planners that fail to recognize geometry associated with the semantic and the geometric planners that completely overlook the semantic information which is very critical in real-world navigation. The proposed method leverages dual Sparse Gaussian Processes in an integrated manner; the first is trained to forecast geometrically navigable spaces while the second predicts the semantically navigable areas. This integrated model is able to pinpoint the overlapping (geometric and semantic) navigable space. The simulation and real-world experiments demonstrate that the proposed VG-SGP model, coupled with our innovative navigation strategy, outperforms models solely reliant on visual or geometric navigation algorithms, highlighting a superior adaptive behavior.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t9_15">
             15:30-16:30, Paper ThPI5T9.15
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('580'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Privacy-Preserving Map-Free Exploration for Confirming the Absence of a Radioactive Source
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#362771" title="Click to go to the Author Index">
             Lepowsky, Eric
            </a>
           </td>
           <td class="r">
            Princeton University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#315841" title="Click to go to the Author Index">
             Snyder, David
            </a>
           </td>
           <td class="r">
            Princeton University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#375989" title="Click to go to the Author Index">
             Glaser, Alexander
            </a>
           </td>
           <td class="r">
            Princeton University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#148191" title="Click to go to the Author Index">
             Majumdar, Anirudha
            </a>
           </td>
           <td class="r">
            Princeton University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab580" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#search_and_rescue_robots" title="Click to go to the Keyword Index">
               Search and Rescue Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#probability_and_statistical_methods" title="Click to go to the Keyword Index">
               Probability and Statistical Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Performing an inspection task while maintaining the privacy of the inspected site is a challenging balancing act. In this work, we are motivated by the future of nuclear arms control verification, which requires both a high level of privacy and guaranteed correctness. For scenarios with limitations on sensors and stored information due to the potentially secret nature of observable features, we propose a robotic verification procedure that provides map-free exploration to perform a source verification task without requiring, nor revealing, any task-irrelevant, site-specific information. We provide theoretical guarantees on the privacy and correctness of our approach, validated by extensive simulated and hardware experiments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t9_16">
             15:30-16:30, Paper ThPI5T9.16
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('974'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Time-Ordered Ad-Hoc Resource Sharing for Independent Robotic Agents
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#337710" title="Click to go to the Author Index">
             Chakravarty, Arjo
            </a>
           </td>
           <td class="r">
            Intrinsic LLC, Singapore University of Technology and Design
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#349908" title="Click to go to the Author Index">
             Grey, Michael
            </a>
           </td>
           <td class="r">
            Intrinsic, an Alphabet Company
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#191819" title="Click to go to the Author Index">
             Muthugala Arachchige, Viraj Jagathpriya Muthugala
            </a>
           </td>
           <td class="r">
            Singapore University of Technology and Design
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#107677" title="Click to go to the Author Index">
             Elara, Mohan Rajesh
            </a>
           </td>
           <td class="r">
            Singapore University of Technology and Design
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab974" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#planning__scheduling_and_coordination" title="Click to go to the Keyword Index">
               Planning, Scheduling and Coordination
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#formal_methods_in_robotics_and_automation" title="Click to go to the Keyword Index">
               Formal Methods in Robotics and Automation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#intelligent_transportation_systems" title="Click to go to the Keyword Index">
               Intelligent Transportation Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Resource sharing is a crucial part of a multi-robot system. We propose a Boolean satisfiability problem (SAT) based approach to resource sharing. Our key contributions are an algorithm for converting any constrained assignment to a weighted-SAT based optimization. We propose a theorem that allows optimal resource assignment problems to be solved via repeated application of a SAT solver. Additionally we show a way to encode continuous time ordering constraints using Conjunctive Normal Form (CNF). We benchmark our new algorithms and show that they can be used in an ad-hoc setting. We test our algorithms on a fleet of simulated and real world robots and show that the algorithms are able to handle real world situations. Our algorithms and test harnesses are open source and build on Open-RMFs fleet management system.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thpi5t10">
             <b>
              ThPI5T10
             </b>
            </a>
           </td>
           <td class="r">
            Room 10
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thpi5t10" title="Click to go to the Program at a Glance">
             <b>
              Simultaneous Localization and Mapping (SLAM) V
             </b>
            </a>
           </td>
           <td class="r">
            Teaser Session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#115540" title="Click to go to the Author Index">
             Kawasaki, Hiroshi
            </a>
           </td>
           <td class="r">
            Kyushu University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t10_01">
             15:30-16:30, Paper ThPI5T10.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1395'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Inline Photometrically Calibrated Hybrid Visual SLAM
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#356547" title="Click to go to the Author Index">
             Abboud, Nicolas
            </a>
           </td>
           <td class="r">
            American University of Beirut
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#375450" title="Click to go to the Author Index">
             Sayour, Malak
            </a>
           </td>
           <td class="r">
            American University of Beirut
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#100044" title="Click to go to the Author Index">
             Elhajj, Imad
            </a>
           </td>
           <td class="r">
            American University of Beirut
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#102280" title="Click to go to the Author Index">
             Zelek, John S.
            </a>
           </td>
           <td class="r">
            University of Waterloo
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#101924" title="Click to go to the Author Index">
             Asmar, Daniel
            </a>
           </td>
           <td class="r">
            American University of Beirut
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1395" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents an integrated approach to Visual SLAM, merging online sequential photometric calibration within a hybrid direct-indirect visual SLAM (H-SLAM). Photometric calibration helps normalize pixel intensity values under different lighting conditions, and thereby improves the direct component of our H-SLAM. A tangential benefit also results to the indirect component of H-SLAM given that the detected features are more stable across variable lighting conditions. Our proposed photometrically calibrated H-SLAM is tested on several datasets, including the TUM monoVO as well as on a dataset we created. Calibrated H-SLAM outperforms other state of the art direct, indirect, and hybrid Visual SLAM systems in all the experiments. Furthermore, in online SLAM tested at our site, it also significantly outperformed the other SLAM Systems.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t10_02">
             15:30-16:30, Paper ThPI5T10.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1598'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              ESO-SLAM: Tightly-Coupled and Simultaneous Estimation of Self and Multi-Object Pose Via Sensor Fusion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#353445" title="Click to go to the Author Index">
             Li, Wu
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#219010" title="Click to go to the Author Index">
             Zhang, Yunzhou
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#352365" title="Click to go to the Author Index">
             Lv, Yuezhang
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#373900" title="Click to go to the Author Index">
             Wang, TingTing
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#374309" title="Click to go to the Author Index">
             Wang, Sizhan
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395137" title="Click to go to the Author Index">
             Wang, Guiyuan
            </a>
           </td>
           <td class="r">
            Jiangsu Shuguang Optoelectronics Co., Ltd., Yangzhou, China
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1598" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Simultaneous Localization and Mapping (SLAM) is widely used in applications such as robotics and autonomous driving, with methods involving multi-sensor fusion demonstrating excellent performance. However, they simply reject dynamic features and ignore the mutual benefits of self and dynamic objects, which greatly limits their application in actual high-dynamic scenes. To address this issue, we propose ESO-SLAM, a tightly-coupled system for simultaneous self and multi-object pose estimation achieved through sensor fusion. This system employs a multi-probability fusion tracker based on filter to establish more robust object-level data association. Building upon this, we introduce a method that combines 3D Kalman filter velocity priors and camera optical flow decoupling for dynamic point cloud removal, aiming at improving the accuracy of self-pose estimation in odometry. Finally, we jointly refine the poses of the robot and objects using multiple constraint factors within our proposed framework. Experimental results on the KITTI raw dataset demonstrate that our approach achieves better pose accuracy for both self and tracked objects compared to baseline and state-of-the-art techniques. Furthermore, the proposed method exhibits feasibility in real-time performance to ensure its practical application value.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t10_03">
             15:30-16:30, Paper ThPI5T10.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1605'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              BE-SLAM: BEV-Enhanced Dynamic Semantic SLAM with Static Object Reconstruction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#373375" title="Click to go to the Author Index">
             Luo, Jun
            </a>
           </td>
           <td class="r">
            Chongqing University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#273644" title="Click to go to the Author Index">
             Wang, Gang
            </a>
           </td>
           <td class="r">
            Chongqing University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#373399" title="Click to go to the Author Index">
             Liu, Hongliang
            </a>
           </td>
           <td class="r">
            Chongqing University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#287969" title="Click to go to the Author Index">
             Wu, Lang
            </a>
           </td>
           <td class="r">
            Huazhong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#373402" title="Click to go to the Author Index">
             Huang, Tao
            </a>
           </td>
           <td class="r">
            Chongqing University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#347615" title="Click to go to the Author Index">
             Xiao, Dengyu
            </a>
           </td>
           <td class="r">
            Chongqing University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#154410" title="Click to go to the Author Index">
             Pu, Huayan
            </a>
           </td>
           <td class="r">
            Shanghai University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#332222" title="Click to go to the Author Index">
             Luo, Jun
            </a>
           </td>
           <td class="r">
            Chongqing University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1605" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#dynamics" title="Click to go to the Keyword Index">
               Dynamics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The quality of a robots environmental perception determines whether it can achieve more intelligent applications, such as semantic interaction with humans. SLAM, on the other hand, is one of the crucial capabilities for a robot to perceive its environment. However, when only a monocular image is provided, dynamic objects in the environment significantly impact the accuracy of map construction by the robot, leading to erroneous perception results. To address this issue, we propose a Visual SLAM framework based on BEV perception results, named BE-SLAM. With this framework, we can handle dynamic objects, occlusions, and incompletely observed objects. It can construct a stable static map by strengthening trust in static objects. Considering that object-level semantic maps can enhance a robots perception abilities, we also reconstruct static objects in the map and use them to optimize the pose. Through experiments on existing publicly available dataset, we compare BE-SLAM with several existing methods that have shown good performance. The experimental results demonstrate that BE-SLAM performs exceptionally well on high-dynamic sequences and achieves comparable results on static or low-dynamic sequences.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t10_04">
             15:30-16:30, Paper ThPI5T10.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1632'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Low-Texture Robust Hybrid Feature Based Visual Odometry
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#391071" title="Click to go to the Author Index">
             Wang, He
            </a>
           </td>
           <td class="r">
            Shanxi University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#348551" title="Click to go to the Author Index">
             Zhang, Qi
            </a>
           </td>
           <td class="r">
            University of Bath
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#391533" title="Click to go to the Author Index">
             Zheng, Zhiwen
            </a>
           </td>
           <td class="r">
            Shanxi University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#278501" title="Click to go to the Author Index">
             Li, Xiaoli
            </a>
           </td>
           <td class="r">
            Institute for Infocomm Research
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#416024" title="Click to go to the Author Index">
             Tan, Hongye
            </a>
           </td>
           <td class="r">
            Shanxi University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395280" title="Click to go to the Author Index">
             Li, Ru
            </a>
           </td>
           <td class="r">
            ShanXi University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1632" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In low-texture scenes, Visual Odometry (VO) algorithms often encounter challenges stemming from sparse feature sets and reduced accuracy in feature matching. To overcome this, integrating plane features and vanishing point characteristics can provide additional constraints for refining camera poses. Optical flow-based tracking methods may also offer improved matching precision compared to traditional feature-based approaches. Motivated by these challenges, we present a robust Visual Odometry system tailored for low-texture environments. Our system combines a vanishing point-based approach for camera pose optimization with a Manhattan-aided algorithm for matching line segments using optical flow. By incorporating planes and vanishing points as supplementary features for pose estimation, we enhance overall accuracy without significant time overhead. We utilize detected line features to compute vanishing points, improving accuracy without compromising efficiency. In addition, our Manhattan-aided optical flow technique supplements and refines the results of line feature matching, further enhancing the accuracy of vanishing points. Evaluation on various public datasets demonstrates the superior accuracy and robustness of our system compared to state-of-the-art Simultaneous Localization And Mapping (SLAM) and VO methods. Notably, our method effectively addresses issues of failure in low-texture scenes and improves the accuracy of line feature matching compared to baseline methods. We will release our source code upon paper acceptance.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t10_05">
             15:30-16:30, Paper ThPI5T10.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1717'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Grid-Based Submap Joining: An Efficient Algorithm for Simultaneously Optimizing Global Occupancy Map and Local Submap Frames
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#324226" title="Click to go to the Author Index">
             Wang, Yingyu
            </a>
           </td>
           <td class="r">
            University of Technology Sydney
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#141224" title="Click to go to the Author Index">
             Zhao, Liang
            </a>
           </td>
           <td class="r">
            University of Technology Sydney
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#101879" title="Click to go to the Author Index">
             Huang, Shoudong
            </a>
           </td>
           <td class="r">
            University of Technology, Sydney
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1717" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Optimizing robot poses and the map simultaneously has been shown to provide more accurate SLAM results. However, for non-feature based SLAM approaches, directly optimizing all the robot poses and the whole map will greatly increase the computational cost, making SLAM problems difficult to solve in large-scale environments. To solve the 2D non-feature based SLAM problem in large-scale environments more accurately and efficiently, we propose the grid-based submap joining method. Specifically, we first formulate the 2D grid-based submap joining problem as a non-linear least squares (NLLS) form to optimize the global occupancy map and local submap frames simultaneously. We then prove that in solving the NLLS problem using Gauss-Newton (GN) method, the increments of the poses in each iteration are independent of the occupancy values of the global occupancy map. Based on this property, we propose a pose-only GN algorithm equivalent to full GN method to solve the NLLS problem. The proposed submap joining algorithm is very efficient due to the independent property and the pose-only solution. Evaluations using simulations and publicly available practical 2D laser datasets confirm the outperformance of our proposed method compared to the state-of-the-art methods in terms of efficiency and accuracy, as well as the ability to solve the grid-based SLAM problem in very large-scale environments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t10_06">
             15:30-16:30, Paper ThPI5T10.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1907'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              ASML-VDIO: Visual-Depth-Inertial Odometry Using Selected Accurate and Stable Multi-Modal Landmarks in Structural Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#349514" title="Click to go to the Author Index">
             Luo, Xingjian
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#356388" title="Click to go to the Author Index">
             Pang, Chenglin
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#363277" title="Click to go to the Author Index">
             Wu, Xuankang
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#121608" title="Click to go to the Author Index">
             Fang, Zheng
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1907" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#visual_inertial_slam" title="Click to go to the Keyword Index">
               Visual-Inertial SLAM
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In complex indoor structural scenes such as shopping centers and malls, camera pose estimation using pure point features is easy to fail due to the difficulty in extracting sufficient and stable point features from weak textures or dynamic environments. Recent works have attempted to address these challenges by introducing line features. However, the addition of line features increases the number of parameters and landmarks for BA (Bundle Adjustment), leading to efficiency reduction. This is a common issue in multi-modal SLAM (Simultaneous Localization And Mapping). To address this issue, this paper proposes a novel visual-depth-inertial odometry (ASML-VDIO) framework by combining RGB-D and IMU sensors. To improve the efficiency of BA, the proposed landmark classification method classifies 3D landmarks into accurate landmarks and other landmarks based on spatial consistency verification and depth range limitation. Then, accurate landmarks are fixed, and only other landmarks are optimized in the optimization of BA. Furthermore, to remove line features extracted from dynamic objects (pedestrian, shopping-car, etc), we propose a dynamic line removal method that combines geometric constraints and motion constraints of line features. Finally, the method is evaluated on public and author-collected datasets, showing competitive accuracy and robustness in complex indoor structural scenes while 71% speedup on optimization thread with same constraints.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t10_07">
             15:30-16:30, Paper ThPI5T10.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2270'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Efficient Dynamic LiDAR Odometry for Mobile Robots with Structured Point Clouds
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397997" title="Click to go to the Author Index">
             Lichtenfeld, Jonathan
            </a>
           </td>
           <td class="r">
            Technische Universitt Darmstadt
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#185048" title="Click to go to the Author Index">
             Daun, Kevin
            </a>
           </td>
           <td class="r">
            Technische Universitt Darmstadt
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#106791" title="Click to go to the Author Index">
             von Stryk, Oskar
            </a>
           </td>
           <td class="r">
            Technische Universitt Darmstadt
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2270" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We propose a real-time dynamic LiDAR odometry pipeline for mobile robots in Urban Search and Rescue (USAR) scenarios. Existing approaches to dynamic object detection often rely on pretrained learned networks or computationally expensive volumetric maps. To enhance efficiency on computationally limited robots, we reuse data between the odometry and detection module. Utilizing a range image segmentation technique and a novel residual-based heuristic, our method distinguishes dynamic from static objects before integrating them into the point cloud map. The approach demonstrates robust object tracking and improved map accuracy in environments with numerous dynamic objects. Even highly non-rigid objects, such as running humans, are accurately detected at point level without prior downsampling of the point cloud and hence, without loss of information. Evaluation on simulated and real-world data validates its computational efficiency. Compared to a state-of-the-art volumetric method, our approach shows comparable detection performance at a fraction of the processing time, adding only 14 ms to the odometry module for dynamic object detection and tracking. The implementation and a new real-world dataset are available as open-source for further research.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t10_08">
             15:30-16:30, Paper ThPI5T10.8
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2503'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              LA-LIO: Robust Localizability-Aware LiDAR-Inertial Odometry for Challenging Scenes
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#373944" title="Click to go to the Author Index">
             Huang, Junjie
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#219010" title="Click to go to the Author Index">
             Zhang, Yunzhou
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397688" title="Click to go to the Author Index">
             Xu, Qingdong
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397700" title="Click to go to the Author Index">
             Wu, Song
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#336086" title="Click to go to the Author Index">
             Liu, Jun
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395137" title="Click to go to the Author Index">
             Wang, Guiyuan
            </a>
           </td>
           <td class="r">
            Jiangsu Shuguang Optoelectronics Co., Ltd., Yangzhou, China
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395142" title="Click to go to the Author Index">
             Liu, Wei
            </a>
           </td>
           <td class="r">
            Jiangsu Shuguang Optoelectronics Co., Ltd., Yangzhou, China
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2503" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Modern robotic systems are increasingly deployed in complex and diverse environments, reliable localization under challenging conditions becomes crucial for the safe and efficient operation of these systems. The odometry based on LiDAR is prone to system collapse caused by computational divergence under conditions of aggressive motion and information deficiency in spatial geometry. To enhance the robustness of systems in challenging scenes, this work proposes LA-LIO, robust localizability-aware LiDAR inertial odometry. It mainly consists of three parts. Firstly, this paper presents a LiDAR degeneration detection method that enabling stable degeneration assessment. Secondly, a method for segmenting LiDAR point clouds is proposed to alleviate the issue of excessive distortion in point clouds under aggressive motion scenes. The last is an Errors State Kalman Filter (ESKF) method with adaptive weights to utilize as much as possible the existing spatial information to improve the stability of the system in degenerated scenarios. The proposed method is evaluated and compared in multiple experiments, demonstrating the performance and reliability improvements of this approach in challenging environments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t10_09">
             15:30-16:30, Paper ThPI5T10.9
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2578'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Visual Loop Closure Detection with Thorough Temporal and Spatial Context Exploitation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#350319" title="Click to go to the Author Index">
             Li, Jiaxin
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#350947" title="Click to go to the Author Index">
             Wang, Zan
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#269180" title="Click to go to the Author Index">
             Di, Huijun
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#350957" title="Click to go to the Author Index">
             Li, Jian
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#350943" title="Click to go to the Author Index">
             Liang, Wei
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2578" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#recognition" title="Click to go to the Keyword Index">
               Recognition
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Despite advancements in visual SLAM, prevailing visual loop closure detection(LCD) methods primarily rely on computationally intensive image similarity comparisons, neglecting temporal-spatial context during long-term exploration. To address this issue, we propose TOSA, a novel visual loop closure detection algorithm harnessing temporal and spatial context for efficient LCD. Specifically, as the agent explores through time, our approach recurrently updates a latent feature incorporating historical information via an LSTM module. Upon receiving a query frame, TOSA seamlessly fuses the latent feature with the query feature to predict the candidates' distribution, thus averting intensive similarity computation. Additionally, TOSA integrates a candidate refinement strategy that leverages the spatial correlation to enhance selected candidates, further boosting the performance. Extensive experiments across standard datasets showcase the superiority of our method over existing state-of-the-art techniques, demonstrating the effectiveness of using temporal-spatial contexts.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t10_10">
             15:30-16:30, Paper ThPI5T10.10
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2838'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              MM3DGS SLAM: Multi-Modal 3D Gaussian Splatting for SLAM Using Vision, Depth, and Inertial Measurements
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398480" title="Click to go to the Author Index">
             Sun, Lisong C.
            </a>
           </td>
           <td class="r">
            University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#317474" title="Click to go to the Author Index">
             Bhatt, Neel P.
            </a>
           </td>
           <td class="r">
            The University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398544" title="Click to go to the Author Index">
             Liu, Jonathan C.
            </a>
           </td>
           <td class="r">
            The University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398557" title="Click to go to the Author Index">
             Fan, Zhiwen
            </a>
           </td>
           <td class="r">
            The University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#212821" title="Click to go to the Author Index">
             Wang, Zhangyang (Atlas)
            </a>
           </td>
           <td class="r">
            Texas A&amp;M University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#284739" title="Click to go to the Author Index">
             Humphreys, Todd E.
            </a>
           </td>
           <td class="r">
            The University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#131686" title="Click to go to the Author Index">
             Topcu, Ufuk
            </a>
           </td>
           <td class="r">
            The University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2838" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#visual_inertial_slam" title="Click to go to the Keyword Index">
               Visual-Inertial SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Simultaneous localization and mapping is essential for position tracking and scene understanding. 3D Gaussian-based map representations enable photorealistic reconstruction and real-time rendering of scenes using multiple posed cameras. We show for the first time that using 3D Gaussians for map representation with unposed camera images and inertial measurements can enable accurate SLAM. Our method, MM3DGS, addresses the limitations of prior neural radiance field-based representations by enabling faster rendering, scale awareness, and improved trajectory tracking. Our framework enables keyframe-based mapping and tracking utilizing loss functions that incorporate relative pose transformations from pre-integrated inertial measurements, depth estimates, and measures of photometric rendering quality. We also release a multi-modal dataset, UT-MM, collected from a mobile robot equipped with a camera and an inertial measurement unit. Experimental evaluation on several scenes from the dataset shows that MM3DGS achieves nearly 3x improvement in tracking and 5% improvement in photometric rendering quality compared to the current 3DGS SLAM state-of-the-art, while allowing real-time rendering of a high-resolution dense 3D map.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t10_11">
             15:30-16:30, Paper ThPI5T10.11
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3021'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              PickScan: Object Discovery and Reconstruction from Handheld Interactions
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397245" title="Click to go to the Author Index">
             van der Brugge, Vincent Daniel
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#128027" title="Click to go to the Author Index">
             Pollefeys, Marc
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#202511" title="Click to go to the Author Index">
             Tenenbaum, Joshua
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#184588" title="Click to go to the Author Index">
             Jatavallabhula, Krishna Murthy
            </a>
           </td>
           <td class="r">
            MIT
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398543" title="Click to go to the Author Index">
             Tewari, Ayush
            </a>
           </td>
           <td class="r">
            MIT
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3021" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Reconstructing compositional 3D representations of scenes, where each object is represented with its own 3D model, is a highly desirable capability in robotics and augmented reality. However, most existing methods do not allow for user interactions with objects, which are necessary to scan objects fully and to guide object discovery in challenging scenarios, and/or rely heavily on strong appearance priors for object discovery, therefore only working on those classes of objects on which the method has been trained. We address these limitations with a novel interaction-guided and class-agnostic method based on object displacements that allows a user to move around a scene with an RGB-D camera, hold up objects, and finally outputs one 3D model per held-up object. Our main contribution to this end is a novel approach to detecting user-object interactions and extracting the masks of manipulated objects. On a custom-captured dataset, our pipeline discovers manipulated objects with 78.3% precision at 100% recall and reconstructs them with a mean chamfer distance of 0.90cm. Compared to Co-Fusion, the only comparable interaction-based and class-agnostic baseline, this corresponds to a reduction in chamfer distance of 73% while detecting 99% fewer false positives.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t10_12">
             15:30-16:30, Paper ThPI5T10.12
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3034'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Towards Long Term SLAM on Thermal Imagery
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#278420" title="Click to go to the Author Index">
             Keil, Colin
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#283649" title="Click to go to the Author Index">
             Gupta, Aniket
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#278175" title="Click to go to the Author Index">
             Kaveti, Pushyami
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#105079" title="Click to go to the Author Index">
             Singh, Hanumant
            </a>
           </td>
           <td class="r">
            Northeatern University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3034" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#data_sets_for_slam" title="Click to go to the Keyword Index">
               Data Sets for SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#data_sets_for_robot_learning" title="Click to go to the Keyword Index">
               Data Sets for Robot Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Visual SLAM with thermal imagery remains a difficult problem for many state of the art (SOTA) algorithms. Compared with visible spectrum imagery, thermal imagery generally has lower contrast, higher noise, and tends to have lower resolution, making for challenging front-end data association. Thermal imagery also presents a difficult problem for long term relocalization and map reuse, because the relative temperatures of objects in thermal imagery tend to change dramatically from day to night. Feature descriptors typically used for relocalization in SLAM are unable to maintain consistency over these diurnal changes. We show that learned feature descriptors can be used within existing bag of word based localization schemes to dramatically improve place recognition across large temporal gaps in thermal imagery. In order to demonstrate the effectiveness of our trained vocabulary, we have developed a baseline SLAM system, integrating learned features and matching into a classical SLAM algorithm. Our system demonstrates good local tracking on challenging thermal imagery, and relocalization that overcomes dramatic day to night thermal appearance changes. Our code and datasets are available here: https://github.com/neufieldrobotics/IRSLAM Baseline
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t10_13">
             15:30-16:30, Paper ThPI5T10.13
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3103'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Two-Stage Pose Optimization Algorithm Using Color Information for Underwater SLAM with Light-Sectioning-Based 3D Scanning Method
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#338816" title="Click to go to the Author Index">
             Ikeda, Takaki
            </a>
           </td>
           <td class="r">
            Kyushu University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#299890" title="Click to go to the Author Index">
             Iwaguchi, Takafumi
            </a>
           </td>
           <td class="r">
            Kyushu University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#166088" title="Click to go to the Author Index">
             Thomas, Diego
            </a>
           </td>
           <td class="r">
            Kyushu University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#115540" title="Click to go to the Author Index">
             Kawasaki, Hiroshi
            </a>
           </td>
           <td class="r">
            Kyushu University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3103" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#marine_robotics" title="Click to go to the Keyword Index">
               Marine Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#calibration_and_identification" title="Click to go to the Keyword Index">
               Calibration and Identification
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The demand for 3D shape measurement of underwater scene is increasing in various applications. Especially, simultaneous localization and mapping (SLAM) technique utilizing remotely operated vehicle (ROV) attached with 3D sensors has been intensively researched. This paper focuses on solving pose optimization problem for underwater robots with camera/multiple-line-lasers setup, especially for the scene with some textures (color information). To this end, a two-stage pose optimization technique is proposed. In the first stage, due to the sparse nature of the reconstructed shape in the light-sectioning method consisting of several 3D curves, we bundle 10 to 20 consecutive frames to form a block shape, refining significant errors in the initial sensor poses using a novel bundle adjustment algorithm. In the second stage, remaining pose errors are corrected by a block-based matching algorithm utilizing iterative closest point (ICP) algorithm with color information. Through experiments in underwater environment with a real system, it was validated that the proposed method demonstrates superior performance compared to past underwater SLAM techniques.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t10_14">
             15:30-16:30, Paper ThPI5T10.14
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3116'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              NF-SLAM: Effective, Normalizing Flow-Supported Neural Field Representations for Object-Level Visual SLAM in Automotive Applications
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#270849" title="Click to go to the Author Index">
             Cui, Li
            </a>
           </td>
           <td class="r">
            Motovis Intelligent Technologies (Shanghai) Co Ltd
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397307" title="Click to go to the Author Index">
             Ding, Yang
            </a>
           </td>
           <td class="r">
            Motovis Intelligent Technologies (Shanghai) Co Ltd
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#107641" title="Click to go to the Author Index">
             Hartley, Richard
            </a>
           </td>
           <td class="r">
            Australian National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396303" title="Click to go to the Author Index">
             Xie, Zirui
            </a>
           </td>
           <td class="r">
            Motovis Intelligent Technologies (Shanghai) Co Ltd
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#123271" title="Click to go to the Author Index">
             Kneip, Laurent
            </a>
           </td>
           <td class="r">
            ShanghaiTech University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397859" title="Click to go to the Author Index">
             Yu, Zhenghua
            </a>
           </td>
           <td class="r">
            Motovis Intelligent Technologies
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3116" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#intelligent_transportation_systems" title="Click to go to the Keyword Index">
               Intelligent Transportation Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_transportation" title="Click to go to the Keyword Index">
               Computer Vision for Transportation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We propose a novel, vision-only object-level SLAM framework for automotive applications representing 3D shapes by implicit signed distance functions. Our key innovation consists of augmenting the standard neural representation by a normalizing flow network. As a result, achieving strong representation power on the specific class of road vehicles is made possible by compact networks with only 16-dimensional latent codes. Furthermore, the newly proposed architecture exhibits a significant performance improvement in the presence of only sparse and noisy data, which is demonstrated through comparative experiments on synthetic data. The module is embedded into the back-end of a stereo-vision based framework for joint, incremental shape optimization. The loss function is given by a combination of a sparse 3D point-based SDF loss, a sparse rendering loss, and a semantic mask-based silhouette-consistency term. We furthermore leverage semantic information to determine keypoint extraction density in the front-end. Finally, experimental results on real-world data reveal accurate and reliable performance comparable to alternative frameworks that make use of direct depth readings. The proposed method performs well with only sparse 3D points obtained from bundle adjustment, and eventually continues to deliver stable results even under exclusive use of the mask-consistency term.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t10_15">
             15:30-16:30, Paper ThPI5T10.15
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3454'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Object-Based SLAM Using Superquadrics
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#341443" title="Click to go to the Author Index">
             Xing, Yifan
            </a>
           </td>
           <td class="r">
            University of Bristol
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#289496" title="Click to go to the Author Index">
             Samano, Noe
            </a>
           </td>
           <td class="r">
            University of Bristol
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#325800" title="Click to go to the Author Index">
             Fan, Wen
            </a>
           </td>
           <td class="r">
            University of Bristol
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#118860" title="Click to go to the Author Index">
             Calway, Andrew
            </a>
           </td>
           <td class="r">
            University of Bristol
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3454" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#visual_tracking" title="Click to go to the Keyword Index">
               Visual Tracking
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Visual SLAM uses visual information, typically point features, to localise a camera and, at the same time, map the environment. In recent years, there has been interest in using scene-understanding capabilities to enhance the mapping process and object-level SLAM systems have appeared in response. However, most of the previous work is limited to pre-stored object models or pre-trained networks to represent the objects, which limits working scenarios or uses representations with limited scope, such as cubes or quadrics. To address this, we propose to use superquadrics as the object representation and, in this paper, present a proof of principle SLAM system in which object-based mapping is fully integrated with camera tracking via keyframe optimisation. The system was tested on simulated and real datasets, and the results show that the system can achieve lightweight and comparatively good object representation whilst also giving good camera trajectories estimates under certain scenarios.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t10_16">
             15:30-16:30, Paper ThPI5T10.16
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1832'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SwiftBase: A Dataset Based on High-Frequency Visual Measurement for Visual-Inertial Localization in High-Speed Motion Scenes
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#355394" title="Click to go to the Author Index">
             Zou, Zhenghao
            </a>
           </td>
           <td class="r">
            Northwestern Polytechnical University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#198472" title="Click to go to the Author Index">
             Lyu, Yang
            </a>
           </td>
           <td class="r">
            Northwestern Polytechnical University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#251512" title="Click to go to the Author Index">
             Zhao, Chunhui
            </a>
           </td>
           <td class="r">
            Northwestern Polytechnical University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397080" title="Click to go to the Author Index">
             Kao, XiRui
            </a>
           </td>
           <td class="r">
            Northwestern Polytechnical University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397120" title="Click to go to the Author Index">
             Liu, Jiang Bo
            </a>
           </td>
           <td class="r">
            Northwestern Polytechnical University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397107" title="Click to go to the Author Index">
             Chai, Haochen
            </a>
           </td>
           <td class="r">
            Northwestern Polytechnical University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1832" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#data_sets_for_slam" title="Click to go to the Keyword Index">
               Data Sets for SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#visual_inertial_slam" title="Click to go to the Keyword Index">
               Visual-Inertial SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#sensor_fusion" title="Click to go to the Keyword Index">
               Sensor Fusion
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Localizing an aggressively moving platform is a considerable challenge in the SLAM domain. This paper presents a dataset, SwiftBase, crafted to facilitate research into precise localization under such conditions. It includes high-speed cameras with over 200Hz sampling rate, capturing detailed visual data for analyzing rapid external dynamics. The dataset features two IDS high-speed cameras, a low-frequency camera, and a high-precision integrated inertial measurement unit (IMU). Calibration parameters are provided, and sensor data is synchronized using ROS system time. SwiftBaseis recorded in indoor environments, utilizing pulleys and suspension ropes to simulate high-speed conditions, with ground truth data supplied by OptiTrack. SwiftBase has been instrumental in evaluating advanced VI-SLAM algorithms. However, there is still an urgent need for new algorithms capable of robust and real-time tracking in High-Speed localization.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thpi5t11">
             <b>
              ThPI5T11
             </b>
            </a>
           </td>
           <td class="r">
            Room 11
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thpi5t11" title="Click to go to the Program at a Glance">
             <b>
              Multi-Robot Systems and Swarms IV
             </b>
            </a>
           </td>
           <td class="r">
            Teaser Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#200000" title="Click to go to the Author Index">
             Wang, Chen
            </a>
           </td>
           <td class="r">
            University at Buffalo
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t11_01">
             15:30-16:30, Paper ThPI5T11.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2004'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Online Planning for Multi Agent Path Finding in Inaccurate Maps
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397357" title="Click to go to the Author Index">
             Malka Nir, Nir
            </a>
           </td>
           <td class="r">
            Ben-Gurion University of the Negev
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397270" title="Click to go to the Author Index">
             Shani, Guy
            </a>
           </td>
           <td class="r">
            Ben Gurion University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#305571" title="Click to go to the Author Index">
             Stern, Roni
            </a>
           </td>
           <td class="r">
            Ben Gurion University of the Negev, Palo Alto Research Center (P
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2004" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#path_planning_for_multiple_mobile_robots_or_agents" title="Click to go to the Keyword Index">
               Path Planning for Multiple Mobile Robots or Agents
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In multi-agent path finding (MAPF), agents navigate to their target positions without conflict within an environment, typically represented as a graph. Traditionally, the input graph is assumed to be accurate. We investigate MAPF scenarios where the input graph may be inaccurate, containing non-existent edges or missing edges present in the environment. Agents can verify the existence or non-existence of an edge only by moving close to it. To navigate such maps, we propose an online approach where planning and execution are interleaved. As agents gather new information about the environment over time, they replan accordingly. To minimize replanning efforts, we developed methods to identify and replan only for agents affected by observed changes. To scale to larger problems, we defer conflicts resolution expected only in the distant future and adapt single-agent path-finding algorithms to account for map inaccuracies. Experimental results show impressive scalability, solving problems involving over 1000 agents in under 3 minutes.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t11_02">
             15:30-16:30, Paper ThPI5T11.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2065'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SwarmPRM: Probabilistic Roadmap Motion Planning for Large-Scale Swarm Robotic Systems
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397152" title="Click to go to the Author Index">
             Hu, Yunze
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#384425" title="Click to go to the Author Index">
             Yang, Xuru
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#299292" title="Click to go to the Author Index">
             Zhou, Kangjie
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397167" title="Click to go to the Author Index">
             Liu, Qinghang
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397142" title="Click to go to the Author Index">
             Ding, Kang
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#362706" title="Click to go to the Author Index">
             Gao, Han
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#299303" title="Click to go to the Author Index">
             Zhu, Pingping
            </a>
           </td>
           <td class="r">
            Marshall University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#191913" title="Click to go to the Author Index">
             Liu, Chang
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2065" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#path_planning_for_multiple_mobile_robots_or_agents" title="Click to go to the Keyword Index">
               Path Planning for Multiple Mobile Robots or Agents
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#swarm_robotics" title="Click to go to the Keyword Index">
               Swarm Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Large-scale swarm robotic systems consisting of numerous cooperative agents show considerable promise for performing autonomous tasks across various sectors. Nonetheless, traditional motion planning approaches often face a trade-off between scalability and solution quality due to the exponential growth of the joint state space of robots. In response, this work proposes SwarmPRM, a hierarchical, scalable, computationally efficient, and risk-aware sampling-based motion planning approach for large-scale swarm robots. SwarmPRM utilizes a Gaussian Mixture Model (GMM) to represent the swarm's macroscopic state and constructs a Probabilistic Roadmap in Gaussian space, referred to as the Gaussian roadmap, to generate a transport trajectory of GMM. This trajectory is then followed by each robot at the microscopic stage. To enhance trajectory safety, SwarmPRM incorporates the conditional value-at-risk (CVaR) in the collision checking process to impart the property of risk awareness to the constructed Gaussian roadmap. SwarmPRM then crafts a linear programming formulation to compute the optimal GMM transport trajectory within this roadmap. Extensive simulations demonstrate that SwarmPRM outperforms state-of-the-art methods in computational efficiency, scalability, and trajectory quality while offering the capability to adjust the risk tolerance of generated trajectories.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t11_03">
             15:30-16:30, Paper ThPI5T11.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2080'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Multi-Robot Active Graph Exploration with Reduced Pose-SLAM Uncertainty Via Submodular Optimization
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#296723" title="Click to go to the Author Index">
             Bai, Ruofei
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#185927" title="Click to go to the Author Index">
             Yuan, Shenghai
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#238454" title="Click to go to the Author Index">
             Guo, Hongliang
            </a>
           </td>
           <td class="r">
            Agency for Science Technology and Research
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#276650" title="Click to go to the Author Index">
             Yin, Pengyu
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#196715" title="Click to go to the Author Index">
             Yau, Wei-Yun
            </a>
           </td>
           <td class="r">
            I2R
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#115410" title="Click to go to the Author Index">
             Xie, Lihua
            </a>
           </td>
           <td class="r">
            NanyangTechnological University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2080" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#path_planning_for_multiple_mobile_robots_or_agents" title="Click to go to the Keyword Index">
               Path Planning for Multiple Mobile Robots or Agents
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#autonomous_vehicle_navigation" title="Click to go to the Keyword Index">
               Autonomous Vehicle Navigation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper considers the multi-robot active graph exploration problem, where robots need to collaboratively cover a graph environment while maintaining reliable pose estimation in collaborative Simultaneous Localization and Mapping (SLAM). Considering both objectives presents challenges for multi-robot pathfinding, as it involves the expensive covariance propagation for SLAM uncertainty evaluation, especially when considering various combinations of robots' paths. To reduce the computational complexity, we propose an efficient two-stage strategy where exploration paths are first generated for quick coverage, and then enhanced by adding informative loop-closing actions along the paths for reliable pose estimation. We formulate the latter problem as a non-monotone submodular maximization problem by relating SLAM uncertainty with pose graph topology, which (1) facilitates a more efficient evaluation of SLAM uncertainty than covariance inference, and (2) allows the employment of approximation algorithms in submodular optimization to provide suboptimality guarantees. We further introduce ordering heuristics to improve the objective values while preserving the optimality bound. Simulation experiments over randomly generated graph environments verify the effectiveness of our methods to achieve quick coverage and enhanced pose graph reliability, and benchmark the performance of the approximation algorithms and the greedy-based algorithm in the loop edge selection problem. Our implementations will be open-source at url{https://github.com/bairuofei/CGE}.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t11_04">
             15:30-16:30, Paper ThPI5T11.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2233'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Non-Homogeneity Mapless Navigation Based on Hierarchical Safe Reinforcement Learning in Dynamic Complex Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#345289" title="Click to go to the Author Index">
             Qin, Jianmin
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#241402" title="Click to go to the Author Index">
             Liu, Qingchen
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#345842" title="Click to go to the Author Index">
             Ma, Qichao
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397330" title="Click to go to the Author Index">
             Wu, Zipeng
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#244245" title="Click to go to the Author Index">
             Qin, Jiahu
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2233" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#path_planning_for_multiple_mobile_robots_or_agents" title="Click to go to the Keyword Index">
               Path Planning for Multiple Mobile Robots or Agents
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#robot_safety" title="Click to go to the Keyword Index">
               Robot Safety
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Addressing safe and efficient navigation in dynamic, realistic, and complex environments stands as a pivotal inquiry within the realm of robotics. Recently, numerous learning-based methods are introduced into the field of navigation, yielding notable outcomes. In this letter, we propose a hierarchical safe reinforcement learning navigation approach (HSRLN) for mapless navigation. It trains mapless navigation policies for non-homogeneous complex scenarios in a hierarchical manner through a kind of three-stage learning, global planning RL + expert imitation learning (IL) + transfer reinforcement learning (TRL). The innovation of this work is fourfold: a) It effectively reduces the difficulty of training for complex navigation by effectively narrowing the task horizon of RL through a hierarchical framework. b) We designed an imitation learning method based on Relative Driving Safety Index (RDSI) to focus on learning critical expert actions. c) It employs a TRL approach to improve generalization under non-homogeneous assumptions by fine-tuning the policy. d) HSRLN extracted significant features important for navigation decisions from raw observations via velocity obstacle modeling. Experiments indicate that it has a better performance compared to existing hierarchical RL navigation methods (HDRL, SRL-ORCA). Relative to SRL-ORCA, it improves navigation success by 12.1% under the non-homogeneity assumption.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t11_05">
             15:30-16:30, Paper ThPI5T11.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2363'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              IMTSP: Solving Min-Max Multiple Traveling Salesman Problem with Imperative Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392472" title="Click to go to the Author Index">
             Guo, Yifan
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#211685" title="Click to go to the Author Index">
             Ren, Zhongqiang
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#200000" title="Click to go to the Author Index">
             Wang, Chen
            </a>
           </td>
           <td class="r">
            University at Buffalo
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2363" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#path_planning_for_multiple_mobile_robots_or_agents" title="Click to go to the Keyword Index">
               Path Planning for Multiple Mobile Robots or Agents
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper considers a Min-Max Multiple Traveling Salesman Problem (MTSP), where the goal is to find a set of tours, one for each agent, to collectively visit all the cities while minimizing the length of the longest tour. Though MTSP has been widely studied, obtaining near-optimal solutions for large-scale problems is still challenging due to its NP-hardness. Recent efforts in data-driven methods face challenges of the need for hard-to-obtain supervision and issues with high variance in gradient estimations, leading to slow convergence and highly sub-optimal solutions. We address these issues by reformulating MTSP as a bilevel optimization problem, using the concept of imperative learning (IL). This involves introducing an allocation network that decomposes the MTSP into multiple single-agent traveling salesman problems (TSPs). The longest tour from these TSP solutions is then used to self-supervise the allocation network, resulting in a new self-supervised, bilevel, end-to-end learning framework, which we refer to as imperative MTSP (iMTSP). Additionally, to tackle the high-variance gradient issues during the optimization, we introduce a control variate-based gradient estimation algorithm. Our experiments showed that these innovative designs enable our gradient estimator to converge 20 times faster than the advanced reinforcement learning baseline, and find up to 80% shorter tour length compared with Google OR-Tools MTSP solver, especially in large-scale problems (e.g. 1000 cities and 15 agents).
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t11_06">
             15:30-16:30, Paper ThPI5T11.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2564'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Multi-Agent Vulcan: An Information-Driven Multi-Agent Path Finding Approach
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#238516" title="Click to go to the Author Index">
             Olkin, Jake
            </a>
           </td>
           <td class="r">
            MIT
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398366" title="Click to go to the Author Index">
             Parimi, Viraj
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#106049" title="Click to go to the Author Index">
             Williams, Brian
            </a>
           </td>
           <td class="r">
            MIT
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2564" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#path_planning_for_multiple_mobile_robots_or_agents" title="Click to go to the Keyword Index">
               Path Planning for Multiple Mobile Robots or Agents
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#autonomous_agents" title="Click to go to the Keyword Index">
               Autonomous Agents
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Scientists often search for phenomenon of interest while exploring new environments. Autonomous vehicles are deployed to explore such areas where human-operated vehicles would be costly or dangerous. Online control of autonomous vehicles for information-gathering is called adaptive sampling and can be framed as a Partially Observable Markov Decision Process (POMDPs) that uses information gain as its principal objective. While prior work focuses largely on single-agent scenarios, this paper confronts challenges unique to multi-agent adaptive sampling, such as avoiding redundant observations, preventing vehicle collision, and facilitating path planning under limited communication. We start with Multi-Agent Path Finding (MAPF) methods, which address collision avoidance by decomposing the multi-agent path planning problem into a series of single-agent path planning problems. We present an extension to these methods called information-driven MAPF which addresses multi-agent information gain under limited communication. First, we introduce an admissible heuristic that relaxes mutual information gain to an additive function that can be evaluated as a set of independent single agent path planning problems. Second, we extend our approach to a distributed system that is robust to limited communication. When all agents are in range, the group plans jointly to maximize information. When some agents move out of range, communicating subgroups are formed and the subgroups plan independently. Since redundant observations are less likely when vehicles are far apart, this approach only incurs a small loss in information gain, resulting in an approach that gracefully transitions from full to partial communication. We evaluate our method against other adaptive sampling strategies across various scenarios, including real-world robotic applications. Our method was able to locate up to 200% more unique phenomena in certain scenarios, and each agent located its first unique phenomenon faster by up to 50%.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t11_07">
             15:30-16:30, Paper ThPI5T11.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2681'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              K-Robust Conflict-Based Search with Continuous Time for Multi-Robot Coordination
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#358310" title="Click to go to the Author Index">
             Daudt, Guilherme
            </a>
           </td>
           <td class="r">
            Universidade Federal Do Rio Grande Do Sul
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#358303" title="Click to go to the Author Index">
             Deus, Alleff Dymytry
            </a>
           </td>
           <td class="r">
            Institute of Informatics, Universidade Federal Do Rio Grande Do
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#165993" title="Click to go to the Author Index">
             Kolberg, Mariana
            </a>
           </td>
           <td class="r">
            UFRGS
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#148047" title="Click to go to the Author Index">
             Maffei, Renan
            </a>
           </td>
           <td class="r">
            Federal University of Rio Grande Do Sul
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2681" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#path_planning_for_multiple_mobile_robots_or_agents" title="Click to go to the Keyword Index">
               Path Planning for Multiple Mobile Robots or Agents
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Coordinating multiple robots is crucial for various real-life applications. Many Multi-Agent Path Finding (MAPF) algorithms have been proven to be successful in addressing this challenge. Nevertheless, some problems, such as unexpected delays during navigation, commonly arise when handling high- level abstractions, potentially leading to failures or collisions in live executions. This paper proposes k-Robust Continuous- time Conflict-Based Search (kR-CCBS), a novel algorithm that overcomes some of these limitations. Our approach offers path planning with continuous time, leading to more precise routes than discrete time approaches. Additionally, we increase safety by incorporating k-robustness, enabling the system to adapt to agent failures due to delays and minimize collision risks. Comparative evaluations demonstrate that kR-CCBS outperforms similar works in effectiveness while maintaining reasonable costs, making it a promising solution for real-world multi-agent coordination scenarios.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t11_08">
             15:30-16:30, Paper ThPI5T11.8
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3105'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Highly Efficient Observation Process Based on FFT Filtering for Robot Swarm Collaborative Navigation in Unknown Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396259" title="Click to go to the Author Index">
             Li, Chenxi
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#358012" title="Click to go to the Author Index">
             Lu, Weining
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396364" title="Click to go to the Author Index">
             Ma, Zhihao
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396419" title="Click to go to the Author Index">
             Meng, Litong
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#189152" title="Click to go to the Author Index">
             Liang, Bin
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3105" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#path_planning_for_multiple_mobile_robots_or_agents" title="Click to go to the Keyword Index">
               Path Planning for Multiple Mobile Robots or Agents
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#swarm_robotics" title="Click to go to the Keyword Index">
               Swarm Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Collaborative path planning for robot swarms in complex, unknown environments without external positioning is a challenging problem. This requires robots to find safe directions based on real-time environmental observations, and to efficiently transfer and fuse these observations within the swarm. This study presents a filtering method based on Fast Fourier Transform (FFT) to address these two issues. We treat sensors' environmental observations as a digital sampling process. Then, we design two different types of filters for safe direction extraction, as well as for the compression and reconstruction of environmental data. The reconstructed data is mapped to probabilistic domain, achieving efficient fusion of swarm observations and planning decision. The computation time is only on the order of microseconds, and the transmission data in communication systems is in bit-level. The performance of our algorithm in sensor data processing was validated in real world experiments, and the effectiveness in swarm path optimization was demonstrated through extensive simulations.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t11_09">
             15:30-16:30, Paper ThPI5T11.9
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3262'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Cooperative Path Planning for Four-Way Shuttle Vehicles in Storage and Retrieval Systems: A Hierarchically Dynamic Graph Based Approach
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#299256" title="Click to go to the Author Index">
             Han, Xingyao
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#399065" title="Click to go to the Author Index">
             Tan, Yuhong
            </a>
           </td>
           <td class="r">
            MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai J
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#299253" title="Click to go to the Author Index">
             Chen, Siyuan
            </a>
           </td>
           <td class="r">
            Shanghai JiaoTong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#147237" title="Click to go to the Author Index">
             Liu, Zhe
            </a>
           </td>
           <td class="r">
            University of Cambridge
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#103003" title="Click to go to the Author Index">
             Wang, Hesheng
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3262" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#path_planning_for_multiple_mobile_robots_or_agents" title="Click to go to the Keyword Index">
               Path Planning for Multiple Mobile Robots or Agents
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#planning__scheduling_and_coordination" title="Click to go to the Keyword Index">
               Planning, Scheduling and Coordination
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Recently, Shuttle-based Storage and Retrieval Systems (SBS/RSs) have garnered significant attention from both academia and industry, owing to their high spatial utilization and rapid response speed. However, the weak connectivity of roadmaps in densely stored environments increases the likelihood of congestion and deadlocks when multiple four-way shuttles operate simultaneously, thereby imposing greater demands for collaborative path planning. Instead of exhaustively coordinate the shuttle motions during the off-line planning or online local control stages, we solve the cooperative path planning challenge from the perspective of altering the road graph structure dynamically. More specifically, we propose an approach to automatically transfer the typical road graph of SBS/RSs into a hierarchical graph with a reduced size, and then dynamically adjust its edge properties to prohibit any motion conflicts. In this manner, the planning problem of large-scale shuttle groups can be easily resolved and all the potential congestions can be eliminated inherently. Finally, we build a complete multi-shuttle cooperative path planning system adaptable for large-scale problems.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t11_10">
             15:30-16:30, Paper ThPI5T11.10
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3282'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Spatiotemporal Co-Design Enabling Prioritized Multi-Agent Motion Planning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#340801" title="Click to go to the Author Index">
             Huang, Yunshen
            </a>
           </td>
           <td class="r">
            Washington University in St. Louis
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#340797" title="Click to go to the Author Index">
             He, Wenbo
            </a>
           </td>
           <td class="r">
            Washington University in St. Louis
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#169035" title="Click to go to the Author Index">
             Kantaros, Yiannis
            </a>
           </td>
           <td class="r">
            Washington University in St. Louis
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#291218" title="Click to go to the Author Index">
             Zeng, Shen
            </a>
           </td>
           <td class="r">
            Washington University in St. Louis
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3282" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#path_planning_for_multiple_mobile_robots_or_agents" title="Click to go to the Keyword Index">
               Path Planning for Multiple Mobile Robots or Agents
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#planning__scheduling_and_coordination" title="Click to go to the Keyword Index">
               Planning, Scheduling and Coordination
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper introduces an innovative planner for prioritized multi-agent motion planning, employing a sequential integration of spatial and temporal designs. The planner initiates a smooth trajectory in space for each agent, ignoring the presence of other agents. Subsequently, by treating spatial collisions as 2D obstacles from a temporal perspective, the planner dynamically fine-tunes the trajectory-tracking speed of agents to avoid collisions, ensuring optimal time consumption for the last agent to reach the target as well. Additionally, the proposed approach systematically coordinates priority for each agent. The efficacy of the approach is validated through both simulations and comparative experiments with a recent planner.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t11_11">
             15:30-16:30, Paper ThPI5T11.11
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('884'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Decentralized Multi-Robot Navigation Coupled with Spatial-Temporal RetNet Based on Deep Reinforcement Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#327791" title="Click to go to the Author Index">
             Chen, Lin
            </a>
           </td>
           <td class="r">
            Hu Nan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#157693" title="Click to go to the Author Index">
             Wang, Yaonan
            </a>
           </td>
           <td class="r">
            Hunan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#157684" title="Click to go to the Author Index">
             Miao, Zhiqiang
            </a>
           </td>
           <td class="r">
            Hunan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#322278" title="Click to go to the Author Index">
             Feng, Mingtao
            </a>
           </td>
           <td class="r">
            Xidian University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#240150" title="Click to go to the Author Index">
             Wang, Yuanzhe
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#171335" title="Click to go to the Author Index">
             Mo, Yang
            </a>
           </td>
           <td class="r">
            Hunan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#359630" title="Click to go to the Author Index">
             Zhou, Zhen
            </a>
           </td>
           <td class="r">
            Hunan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#103003" title="Click to go to the Author Index">
             Wang, Hesheng
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#100565" title="Click to go to the Author Index">
             Wang, Danwei
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab884" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#autonomous_agents" title="Click to go to the Keyword Index">
               Autonomous Agents
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#agent_based_systems" title="Click to go to the Keyword Index">
               Agent-Based Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Navigating robots through dynamic multi-robot environments, avoiding collisions with both other robots and obstacles, has emerged as a central challenge in robotics. The existing approaches fall short in allowing the policy network to effectively capture spatial-temporal reciprocal collision avoidance in multi-robot environments, comprising both static and dynamic obstacles, resulting in inadequate safety and efficiency in directing robot movement. In this study, we introduce a novel policy neural network called Spatial-Temporal RetNet (STR), designed to encode reciprocal collision avoidance states between robots in spatial and temporal dimensions. The goal is to improve the safety and efficacy of the policy neural network in directing robots to complete assigned tasks. The spatial state encoder module is built upon a parallel RetNet structure, which strengthens the neural network's capacity in extracting reciprocal collision avoidance states between robots in spatial dimensions. This module addresses the limitations of position encoding in transformer-based multi-robot navigation policy neural networks. We design a temporal state encoder utilizing a recurrent RetNet structure. This innovation bolsters the multi-robot navigation policy neural network's capability to capture features in the temporal dimension of multi-robot movements. It addresses the limitations of transformer-based multi-robot navigation policy neural networks, particularly in recurrently inferring information across time dimensions. Simulation experiments were conducted to showcase the superior safety and effectiveness of our proposed method compared to previous state-of-the-art approaches in guiding robots to accomplish tasks.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t11_12">
             15:30-16:30, Paper ThPI5T11.12
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('777'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Consistent Distributed Cooperative Localization: A Coordinate Transformation Approach
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#344468" title="Click to go to the Author Index">
             Tian, Chungeng
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#267966" title="Click to go to the Author Index">
             Hao, Ning
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#255205" title="Click to go to the Author Index">
             He, Fenghua
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#255210" title="Click to go to the Author Index">
             Yao, Haodi
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab777" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#distributed_robot_systems" title="Click to go to the Keyword Index">
               Distributed Robot Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper addresses the consistency issue of multi-robot distributed cooperative localization. We introduce a consistent distributed cooperative localization algorithm conducting state estimation in a transformed coordinate. The core idea involves a linear time-varying coordinated transformation to render the propagation Jacobian independent of the state and make it suitable for a distributed manner. This transformation is seamlessly integrated into a server-based distributed cooperative localization framework, in which each robot estimates its own state while the server maintains the cross-correlations. The transformation ensures the correct observability property of the entire framework. Moreover, the algorithm accommodates various types of robot-to-robot relative measurements, broadening its applicability. Through simulations and real-world dataset experiments, the proposed algorithm has demonstrated better performance in terms of both consistency and accuracy compared to existing algorithms.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t11_13">
             15:30-16:30, Paper ThPI5T11.13
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1009'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Scalability of Platoon-Based Coordination for Mixed Autonomy Intersections
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#318304" title="Click to go to the Author Index">
             Yan, Zhongxia
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#160979" title="Click to go to the Author Index">
             Wu, Cathy
            </a>
           </td>
           <td class="r">
            MIT
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1009" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#distributed_robot_systems" title="Click to go to the Keyword Index">
               Distributed Robot Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#intelligent_transportation_systems" title="Click to go to the Keyword Index">
               Intelligent Transportation Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#transfer_learning" title="Click to go to the Keyword Index">
               Transfer Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             As transportation systems see gradual deployment of connected and automated vehicles (CAVs), there is increasing opportunity for intelligent coordination of CAVs towards system-wide objectives. While numerous previous works have modeled single junctions (eg intersections and merges) and investigated control theory-based strategies for vehicle-based coordination, this work investigates the scalability of vehicular control approaches to large networks of intersections, where interactions among multiple intersections may amplify traffic disturbances. Moreover, this work focuses on mixed autonomy networks where the highly nonlinear behavior of human-driven vehicles (HDVs) complicate overall system dynamics, and where the formation of CAV-led platoons may be advantageous. Two approaches are considered for the studied settings: model predictive control and model-free reinforcement learning, both adapted from previous methods designed for single intersection and/or full autonomy settings. Results in a network of two intersections demonstrate that model predictive control faces significant challenges in low-level nonlinear trajectory optimization as well as high-level crossing scheduling, while the policies derived from model-free reinforcement learning implicitly optimizes for both low-level control and high-level coordination. Scalability analysis in large networks with hundred of intersections reveal that policies derived from model-free reinforcement learning and additional finetuning only suffer mild degradation in performance despite the numerous out-of-distribution traffic conditions that may emerge under large scale.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t11_14">
             15:30-16:30, Paper ThPI5T11.14
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2390'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Leader-Follower Cooperative Manipulation under Spatio-Temporal Constraints
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#315014" title="Click to go to the Author Index">
             Sewlia, Mayank
            </a>
           </td>
           <td class="r">
            KTH Royal Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#164014" title="Click to go to the Author Index">
             Verginis, Christos
            </a>
           </td>
           <td class="r">
            Uppsala University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#103919" title="Click to go to the Author Index">
             Dimarogonas, Dimos V.
            </a>
           </td>
           <td class="r">
            KTH Royal Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2390" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#mobile_manipulation" title="Click to go to the Keyword Index">
               Mobile Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#compliance_and_impedance_control" title="Click to go to the Keyword Index">
               Compliance and Impedance Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this work, we develop a control algorithm for mobile manipulators manipulating an object within a leader-follower framework. Unlike existing literature, we avoid the knowledge of the objects dynamics, and only the leader is aware of the tasks to be executed by the object. The followers are primarily tasked to lift the object and maintain a desired posture while the leader manipulates the object despite its unknown dynamic parameters. We employ a stiffness-based controller for the followers, allowing set-point stabilisation with permissible flexibility and a high-gain prescribed performance controller for the leader to facilitate manipulation from the objects equilibrium state. We present simulation results with two followers and one leader KUKA youbots to validate our proposed framework.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t11_15">
             15:30-16:30, Paper ThPI5T11.15
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2396'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Extending Task and Motion Planning with Feasibility Prediction: Towards Multi-Robot Manipulation Planning of Realistic Objects
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#336843" title="Click to go to the Author Index">
             Ait Bouhsain, Smail
            </a>
           </td>
           <td class="r">
            LAAS-CNRS
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#103296" title="Click to go to the Author Index">
             Alami, Rachid
            </a>
           </td>
           <td class="r">
            CNRS
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#101778" title="Click to go to the Author Index">
             Simeon, Thierry
            </a>
           </td>
           <td class="r">
            LAAS-CNRS
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2396" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#task_and_motion_planning" title="Click to go to the Keyword Index">
               Task and Motion Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The hybrid discrete/continuous nature of task and motion planning (TAMP) results often in a combinatorial explosion. This challenge is even more pronounced in multi-robot TAMP problems due to the increase in dimensionality of the action space. Previous works use action feasibility prediction as a heuristic to accelerate TAMP. However, these methods are limited to box-shaped objects and specific single or dual robot settings. In this paper, we propose a feasibility-enabled multi-robot TAMP algorithm capable of tackling complex multi-robot manipulation problems. Also, we expand on our previous work on action and grasp feasibility prediction by extending its use to mesh-shaped objects. We demonstrate the performance of our method compared to a non feasibility-informed baseline, and show its ability to handle TAMP problems requiring the collaboration of multiple robots.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t11_16">
             15:30-16:30, Paper ThPI5T11.16
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2375'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              PEERNet: An End-To-End Profiling Tool for Real-Time Networked Robotic Systems
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395071" title="Click to go to the Author Index">
             Narayanan, Aditya
            </a>
           </td>
           <td class="r">
            University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397930" title="Click to go to the Author Index">
             Kasibhatla, Pranav
            </a>
           </td>
           <td class="r">
            University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#381980" title="Click to go to the Author Index">
             Choi, Minkyu
            </a>
           </td>
           <td class="r">
            The University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#360821" title="Click to go to the Author Index">
             Li, Po-han
            </a>
           </td>
           <td class="r">
            The University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#291448" title="Click to go to the Author Index">
             Zhao, Ruihan
            </a>
           </td>
           <td class="r">
            UT Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#133556" title="Click to go to the Author Index">
             Chinchali, Sandeep
            </a>
           </td>
           <td class="r">
            The University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2375" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#networked_robots" title="Click to go to the Keyword Index">
               Networked Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#software_tools_for_benchmarking_and_reproducibility" title="Click to go to the Keyword Index">
               Software Tools for Benchmarking and Reproducibility
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#sensor_networks" title="Click to go to the Keyword Index">
               Sensor Networks
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Networked robotic systems balance compute, power, and latency constraints in applications such as self-driving vehicles, drone swarms, and teleoperated surgery. A core problem in this domain is deciding when to offload a computationally expensive task to the cloud, a remote server, at the cost of communication latency. Task offloading algorithms often rely on precise knowledge of system-specific performance metrics, such as sensor data rates, network bandwidth, and machine learning model latency. While these metrics can be modeled during system design, uncertainties in connection quality, server load, and hardware conditions introduce real-time performance variations, hindering overall performance. We introduce PEERNet, an end-to-end and real-time profiling tool for cloud robotics. PEERNet enables performance monitoring on heterogeneous hardware through targeted yet adaptive profiling of system components such as sensors, networks, deep-learning pipelines, and devices. We showcase PEERNets capabilities through networked robotics tasks, such as image-based teleoperation of a Franka Emika Panda arm and querying vision language models using an Nvidia Jetson Orin. PEERNet reveals non-intuitive behavior in robotic systems, such as asymmetric network transmission and bimodal language model output. Our evaluation underscores the effectiveness and importance of benchmarking in networked robotics, demonstrating PEERNets adaptability. We will release the code once accepted.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thpi5t12">
             <b>
              ThPI5T12
             </b>
            </a>
           </td>
           <td class="r">
            Room 12
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thpi5t12" title="Click to go to the Program at a Glance">
             <b>
              Aerial Systems II
             </b>
            </a>
           </td>
           <td class="r">
            Teaser Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#218673" title="Click to go to the Author Index">
             Chen, Kuan-Wen
            </a>
           </td>
           <td class="r">
            National Yang Ming Chiao Tung University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t12_01">
             15:30-16:30, Paper ThPI5T12.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('760'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Biodegradable Gliding Paper Flyers Fabricated through Inkjet Printing
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#393527" title="Click to go to the Author Index">
             Girardi, Luca
            </a>
           </td>
           <td class="r">
            ETH Zrich / Eidg. Forschungsanstalt WSL
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#229393" title="Click to go to the Author Index">
             Wu, Rui
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#344509" title="Click to go to the Author Index">
             Fukatsu, Yuki
            </a>
           </td>
           <td class="r">
            Shibaura Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#171243" title="Click to go to the Author Index">
             Shigemune, Hiroki
            </a>
           </td>
           <td class="r">
            Shibaura Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#148776" title="Click to go to the Author Index">
             Mintchev, Stefano
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab760" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__mechanics_and_control" title="Click to go to the Keyword Index">
               Aerial Systems: Mechanics and Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#soft_robot_materials_and_design" title="Click to go to the Keyword Index">
               Soft Robot Materials and Design
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#biologically_inspired_robots" title="Click to go to the Keyword Index">
               Biologically-Inspired Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Seed-inspired minimalist microflyers are showing potential as dispersal platforms for sensor networks and seeds. Their function relies on the sheer number of low-cost flyers, inevitably raising concerns about the post-operation environmental impact. We propose a biodegradable paper glider platform fabricated through origami inkjet printing. This method can fold origami along printing patterns on different paper varieties. We created a printing pattern that allows 2D paper sheets to self-fold into a 3D flying wing glider with designable wing geometry and center of gravity (CG). The design allows stable and repeatable gliding behavior, proven in our gliding tests. It successfully achieves the dispersal of multiple gliders from a hovering drone, covering an average horizontal distance larger than the release height. We also tested the biodegradation of different paper types compatible with the printing method, showing near-complete degradation after 15 weeks in moist soil. Our study presents a novel, potentially scalable approach for fabricating environmentally friendly microflyers, offering new avenues for remote environmental sensing and automated forest restoration programs.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t12_02">
             15:30-16:30, Paper ThPI5T12.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1048'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Ducted Fan UAV for Safe Aerial Grabbing and Transfer of Multiple Loads Using Electromagnets
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#391085" title="Click to go to the Author Index">
             Yin, Zhong
            </a>
           </td>
           <td class="r">
            South China University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#272256" title="Click to go to the Author Index">
             Pei, Hai-Long
            </a>
           </td>
           <td class="r">
            South China University of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1048" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__mechanics_and_control" title="Click to go to the Keyword Index">
               Aerial Systems: Mechanics and Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#physical_human_robot_interaction" title="Click to go to the Keyword Index">
               Physical Human-Robot Interaction
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#safety_in_hri" title="Click to go to the Keyword Index">
               Safety in HRI
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In recent years, research on aerial grasping, manipulation, and transportation of objects has garnered significant attention. These tasks often require UAVs to operate safely close to environments or objects and to efficiently grasp payloads. However, current widely adopted flying platforms pose safety hazards: unprotected high-speed rotating propellers can cause harm to the surroundings. Additionally, the space for carrying payloads on the fuselage is limited, and the restricted position of the payload also hinders efficient grasping. To address these issues, this paper presents a coaxial ducted fan UAV which is equipped with electromagnets mounted externally on the fuselage, enabling safe grasping and transfer of multiple loads in midair without complex additional actuators. It also has the capability to achieve direct human-UAV cargo transfer in the air. The forces acting on the loads during magnetic attachment and their influencing factors were analyzed. An ADRC controller is utilized to counteract disturbances during grasping and achieve attitude control. Finally, flight tests are conducted to verify the UAV's ability to directly grasp multiple loads from human hands in flight while maintaining attitude tracking.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t12_03">
             15:30-16:30, Paper ThPI5T12.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1547'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Control of Unknown Quadrotors from a Single Throw
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396790" title="Click to go to the Author Index">
             Blaha, Till Martin
            </a>
           </td>
           <td class="r">
            Delft University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#183399" title="Click to go to the Author Index">
             Smeur, Ewoud
            </a>
           </td>
           <td class="r">
            TU Delft
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#131960" title="Click to go to the Author Index">
             Remes, Bart
            </a>
           </td>
           <td class="r">
            Delft University of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1547" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__mechanics_and_control" title="Click to go to the Keyword Index">
               Aerial Systems: Mechanics and Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#model_learning_for_control" title="Click to go to the Keyword Index">
               Model Learning for Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a method to recover quadrotor Unmanned Air Vehicles (UAVs) from a throw, when no control parameters are known before the throw. We leverage the availability of high-frequency rotor speed feedback available in racing drone hardware and software to find control effectiveness values and fit a motor model using recursive least squares (RLS) estimation. Furthermore, we propose an excitation sequence that provides large actuation commands while guaranteeing to stay within gyroscope sensing limits. After 450ms of excitation, an Incremental Nonlinear Dynamic Inversion (INDI) attitude controller uses the 52 fitted parameters to arrest rotational motion and recover an upright attitude. Finally, a Nonlinear Dynamic Inversion (NDI) position controller drives the craft to a position setpoint. The proposed algorithm runs efficiently on microcontrollers found in common UAV flight controllers, and was shown to recover an agile quadrotor every time in live experiments with as low as 3.5m throw height, demonstrating robustness against initial rotations and noise. We also demonstrate control of randomized quadrotors in simulated throws, where the parameter fitting Root-Mean-Square (RMS) error is typically within 10% of the true value.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t12_04">
             15:30-16:30, Paper ThPI5T12.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1887'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              AirCrab: A Hybrid Aerial-Ground Manipulator with an Active Wheel
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#224101" title="Click to go to the Author Index">
             Cao, Muqing
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397208" title="Click to go to the Author Index">
             Zhao, Jiayan
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#353500" title="Click to go to the Author Index">
             Xu, Xinhang
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#115410" title="Click to go to the Author Index">
             Xie, Lihua
            </a>
           </td>
           <td class="r">
            NanyangTechnological University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1887" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__mechanics_and_control" title="Click to go to the Keyword Index">
               Aerial Systems: Mechanics and Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#dynamics" title="Click to go to the Keyword Index">
               Dynamics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mobile_manipulation" title="Click to go to the Keyword Index">
               Mobile Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Inspired by the behavior of birds, we present AirCrab, a hybrid aerial ground manipulator (HAGM) with a single active wheel and a 3-degree of freedom (3-DoF) manipulator. AirCrab leverages a single point of contact with the ground to reduce position drift and improve manipulation accuracy. The single active wheel enables locomotion on narrow surfaces without adding significant weight to the robot. To realize accurate attitude maintenance using propellers on the ground, we design a control allocation method for AirCrab that prioritizes attitude control and dynamically adjusts the thrust input to reduce energy consumption. Experiments verify the effectiveness of the proposed control method and the gain in manipulation accuracy with ground contact. A series of operations to complete the letters NTU demonstrates the capability of the robot to perform challenging hybrid aerial-ground manipulation missions.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t12_05">
             15:30-16:30, Paper ThPI5T12.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2319'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Energy-Optimal Planning of Waypoint-Based UAV Missions - Does Minimum Distance Mean Minimum Energy?
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#374902" title="Click to go to the Author Index">
             Michel, Nicolas
            </a>
           </td>
           <td class="r">
            University of California, Davis
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#374660" title="Click to go to the Author Index">
             Patnaik, Ayush
            </a>
           </td>
           <td class="r">
            University of California, Davis
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#116325" title="Click to go to the Author Index">
             Kong, Zhaodan
            </a>
           </td>
           <td class="r">
            University of California, Davis
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#374910" title="Click to go to the Author Index">
             Lin, Xinfan
            </a>
           </td>
           <td class="r">
            University of California, Davis
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2319" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__mechanics_and_control" title="Click to go to the Keyword Index">
               Aerial Systems: Mechanics and Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Multirotor unmanned aerial vehicle is a prevailing type of aerial robots with wide real-world applications. The energy efficiency of the robot is a critical aspect of its performance, determining the range and duration of the missions that can be performed. This paper studies the energy-optimal planning of the multirotor, which aims at finding the optimal ordering of waypoints with the minimum energy consumption for missions in 3D space. The study is performed based on a previously developed model capturing first-principle energy dynamics of the multirotor. We found that in majority of the cases (up to 95%) the solutions of the energy-optimal planning are different from those of the traditional traveling salesman problem which minimizes the total distance. The difference can be as high as 14.9%, with the average at 1.6%-3.3% and 90th percentile at 3.7%-6.5% depending on the range and number of waypoints in the mission. We then identified and explained the key features of the minimum-energy order by correlating to the underlying flight energy dynamics. It is shown that instead of minimizing the distance, coordination of vertical and horizontal motion to promote aerodynamic efficiency is the key to optimizing energy consumption.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t12_06">
             15:30-16:30, Paper ThPI5T12.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2588'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Learning to Walk and Fly with Adversarial Motion Priors
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#251910" title="Click to go to the Author Index">
             L'Erario, Giuseppe
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#309929" title="Click to go to the Author Index">
             Hanover, Drew
            </a>
           </td>
           <td class="r">
            University of Zurich - Robotics and Perception Group
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#308767" title="Click to go to the Author Index">
             Romero, Angel
            </a>
           </td>
           <td class="r">
            University of Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#277846" title="Click to go to the Author Index">
             Song, Yunlong
            </a>
           </td>
           <td class="r">
            University of Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#196567" title="Click to go to the Author Index">
             Nava, Gabriele
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#293937" title="Click to go to the Author Index">
             Viceconte, Paolo Maria
            </a>
           </td>
           <td class="r">
            Lab0 SRL
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#161571" title="Click to go to the Author Index">
             Pucci, Daniele
            </a>
           </td>
           <td class="r">
            Italian Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#105662" title="Click to go to the Author Index">
             Scaramuzza, Davide
            </a>
           </td>
           <td class="r">
            University of Zurich
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2588" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__mechanics_and_control" title="Click to go to the Keyword Index">
               Aerial Systems: Mechanics and Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#humanoid_and_bipedal_locomotion" title="Click to go to the Keyword Index">
               Humanoid and Bipedal Locomotion
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robot multimodal locomotion encompasses the ability to transition between walking and flying, representing a significant challenge in robotics. This work presents an approach that enables automatic smooth transitions between legged and aerial locomotion. Leveraging the concept of Adversarial Motion Priors, our method allows the robot to imitate motion datasets and accomplish the desired task without the need for complex reward functions. The robot learns walking patterns from human-like gaits and aerial locomotion patterns from motions obtained using trajectory optimization. Through this process, the robot adapts the locomotion scheme based on environmental feedback using reinforcement learning, with the spontaneous emergence of mode-switching behavior. The results highlight the potential for achieving multimodal locomotion in aerial humanoid robotics through automatic control of walking and flying modes, paving the way for applications in diverse domains such as search and rescue, surveillance, and exploration missions. This research contributes to advancing the capabilities of aerial humanoid robots in terms of versatile locomotion in various environments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t12_07">
             15:30-16:30, Paper ThPI5T12.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1022'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Tracking Control with Uncertainty Smoothing Estimation under Aggressive Maneuvers of Aerial Vehicles
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395134" title="Click to go to the Author Index">
             Zhang, Hao
            </a>
           </td>
           <td class="r">
            ChongQing University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#285518" title="Click to go to the Author Index">
             Jiang, Tao
            </a>
           </td>
           <td class="r">
            Chongqing University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395030" title="Click to go to the Author Index">
             Tan, Senqi
            </a>
           </td>
           <td class="r">
            China North Artificial Intelligence &amp; Innovation Research Instit
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395037" title="Click to go to the Author Index">
             Ye, Jianchuan
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395041" title="Click to go to the Author Index">
             Zheng, Zhi
            </a>
           </td>
           <td class="r">
            Chongqing University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1022" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__mechanics_and_control" title="Click to go to the Keyword Index">
               Aerial Systems: Mechanics and Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#robust_adaptive_control" title="Click to go to the Keyword Index">
               Robust/Adaptive Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Aggressive maneuvering is crucial for aerial vehicles to execute adversarial and penetration missions. However, this challenges the accurate tracking control of drones due to uncertainties induced by high-speed flight. Therefore, firstly, a highly dynamic tracking control framework is proposed to actualize the accurate tracking of aggressive trajectories with velocities up to 15 m/s (i.e., 54 km/h and acceleration of 2 g. Secondly, in order to mitigate the impact of conjoint effects on uncertainty estimation during aggressive flights and to ensure that uncertainty is smoothly compensated, a novel adaptive nonlinear extended state observer (ANESO) with noise suppression and peak attenuation capabilities is designed. Finally, extensive comparative simulation and real-world practical experimental results certify the superiority of the proposed control strategy in tracking aggressive trajectories.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t12_08">
             15:30-16:30, Paper ThPI5T12.8
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2674'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Quaternion-Based Sliding Mode Control for Six Degrees of Freedom Flight Control of Quadrotors
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397953" title="Click to go to the Author Index">
             Yazdanshenas, Amin
            </a>
           </td>
           <td class="r">
            Toronto Metropolitan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#272189" title="Click to go to the Author Index">
             Faieghi, Reza
            </a>
           </td>
           <td class="r">
            Toronto Metropolitan University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2674" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__mechanics_and_control" title="Click to go to the Keyword Index">
               Aerial Systems: Mechanics and Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#robust_adaptive_control" title="Click to go to the Keyword Index">
               Robust/Adaptive Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#motion_control" title="Click to go to the Keyword Index">
               Motion Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Despite extensive research on sliding mode control (SMC) design for quadrotors, the existing approaches suffer from certain limitations. Euler angle-based SMC formulations suffer from poor performance in high-pitch or -roll maneuvers. Quaternion-based SMC approaches have unwinding issues and complex architecture. Coordinate-free methods are slow and only almost globally stable. This paper presents a new six degrees of freedom SMC flight controller to address the above limitations. We use a cascaded architecture with a position controller in the outer loop and a quaternion-based attitude controller in the inner loop. The position controller generates the desired trajectory for the attitude controller using a coordinate-free approach. The quaternion-based attitude controller uses the natural characteristics of the quaternion hypersphere, featuring a simple structure while providing global stability and avoiding unwinding issues. We compare our controller with three other common control methods conducting challenging maneuvers like flip-over and high-speed trajectory tracking in the presence of model uncertainties and disturbances. Our controller consistently outperforms the benchmark approaches with less control effort and actuator saturation, offering highly effective and efficient flight control.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t12_09">
             15:30-16:30, Paper ThPI5T12.9
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('513'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Photometric Consistency for Precise Drone Rephotography
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#374802" title="Click to go to the Author Index">
             Chang, Hsuan-Jui
            </a>
           </td>
           <td class="r">
            National Yang Ming Chiao Tung University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#373032" title="Click to go to the Author Index">
             Huang, Tzu-Chun
            </a>
           </td>
           <td class="r">
            Internet of Things Laboratory, Chunghwa Telecom Laboratories
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#391679" title="Click to go to the Author Index">
             Xu, Hao-Liang
            </a>
           </td>
           <td class="r">
            National Yang Ming Chiao Tung University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#218673" title="Click to go to the Author Index">
             Chen, Kuan-Wen
            </a>
           </td>
           <td class="r">
            National Yang Ming Chiao Tung University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab513" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__perception_and_autonomy" title="Click to go to the Keyword Index">
               Aerial Systems: Perception and Autonomy
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#machine_learning_for_robot_control" title="Click to go to the Keyword Index">
               Machine Learning for Robot Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper proposes a precise drone rephotography system for fixed-domain patrolling scenarios. The proposed system integrates computer-vision-based localization and finetuned pixel-level dense flow prediction to achieve consistent and precise rephotography images with viewpoints that closely align with those of target images. The proposed Keypoints Alignment Through Dense Flow Prediction (KADFP) model effectively handles challenges arising from lighting variations and background differences. Moreover, a novel flight procedure is implemented in the proposed system. This procedure involves using an Interleaved Drone Controller to alternate between translation and rotation adjustments to ensure smooth flight dynamics during rephotography. Experiments indicated that the proposed system provided considerably more precise rephotography results (error of 4.72 pixels indoors) than did an existing localization approach (error of 35.56 pixels).
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t12_10">
             15:30-16:30, Paper ThPI5T12.10
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2598'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Active Scout: Multi-Target Tracking Using Neural Radiance Fields in Dense Urban Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#286528" title="Click to go to the Author Index">
             Hsu, Christopher D.
            </a>
           </td>
           <td class="r">
            DEVCOM Army Research Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#148833" title="Click to go to the Author Index">
             Chaudhari, Pratik
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2598" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__perception_and_autonomy" title="Click to go to the Keyword Index">
               Aerial Systems: Perception and Autonomy
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#visual_tracking" title="Click to go to the Keyword Index">
               Visual Tracking
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#surveillance_robotic_systems" title="Click to go to the Keyword Index">
               Surveillance Robotic Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We study pursuit-evasion games in highly occluded urban environments, e.g. tall buildings in a city, where a scout (quadrotor) tracks multiple dynamic targets on the ground. We show that we can build a neural radiance field (NeRF) representation of the city---online---using RGB and depth images from different vantage points. This representation is used to calculate the information gain to both explore unknown parts of the city and track the targets---thereby giving a completely first-principles approach to actively tracking dynamic targets. We demonstrate, using a custom-built simulator using Open Street Maps data of Philadelphia and New York City, that we can explore and locate 20 stationary targets within 300 steps. This is slower than a greedy baseline, which does not use active perception. But for dynamic targets that actively hide behind occlusions, we show that our approach maintains, at worst, a tracking error of 200m; the greedy baseline can have a tracking error as large as 600m. We observe a number of interesting properties in the scout's policies, e.g., it switches its attention to track a different target periodically, as the quality of the NeRF representation improves over time, the scout also becomes better in terms of target tracking.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t12_11">
             15:30-16:30, Paper ThPI5T12.11
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2738'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Semantics from Space: Satellite-Guided Thermal Semantic Segmentation Annotation for Aerial Field Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#279493" title="Click to go to the Author Index">
             Lee, Connor
            </a>
           </td>
           <td class="r">
            California Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#390920" title="Click to go to the Author Index">
             Soedarmadji, Saraswati
            </a>
           </td>
           <td class="r">
            California Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#267936" title="Click to go to the Author Index">
             Anderson, Matthew
            </a>
           </td>
           <td class="r">
            Caltech
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#216119" title="Click to go to the Author Index">
             Clark, Anthony
            </a>
           </td>
           <td class="r">
            Pomona College
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#118282" title="Click to go to the Author Index">
             Chung, Soon-Jo
            </a>
           </td>
           <td class="r">
            Caltech
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2738" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__perception_and_autonomy" title="Click to go to the Keyword Index">
               Aerial Systems: Perception and Autonomy
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present a new method to automatically generate semantic segmentation annotations for thermal imagery captured from an aerial vehicle by utilizing satellite-derived data products alongside onboard global positioning and attitude estimates. This new capability overcomes the challenge of developing thermal semantic perception algorithms for field robots due to the lack of annotated thermal field datasets and the time and costs of manual annotation, enabling precise and rapid annotation of thermal data from field collection efforts at a massively-parallelizable scale. By incorporating a thermal-conditioned refinement step with visual foundation models, our approach can produce highly-precise semantic segmentation labels using low-resolution satellite land cover data for little-to-no cost. It achieves 98.5% of the performance from using costly high-resolution options and demonstrates between 70-160% improvement over popular zero-shot semantic segmentation methods based on large vision-language models currently used for generating annotations for RGB imagery. Code will be made available at: https://github.com/connorlee77/aerial-auto-segment
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t12_12">
             15:30-16:30, Paper ThPI5T12.12
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2775'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Neural Control Barrier Functions for Safe Navigation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398043" title="Click to go to the Author Index">
             Harms, Marvin Chayton
            </a>
           </td>
           <td class="r">
            NTNU
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#272076" title="Click to go to the Author Index">
             Kulkarni, Mihir
            </a>
           </td>
           <td class="r">
            NTNU: Norwegian University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#238472" title="Click to go to the Author Index">
             Khedekar, Nikhil Vijay
            </a>
           </td>
           <td class="r">
            NTNU
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#250696" title="Click to go to the Author Index">
             Jacquet, Martin
            </a>
           </td>
           <td class="r">
            NTNU
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#132933" title="Click to go to the Author Index">
             Alexis, Kostas
            </a>
           </td>
           <td class="r">
            NTNU - Norwegian University of Science and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2775" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__perception_and_autonomy" title="Click to go to the Keyword Index">
               Aerial Systems: Perception and Autonomy
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__mechanics_and_control" title="Click to go to the Keyword Index">
               Aerial Systems: Mechanics and Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Autonomous robot navigation can be particularly demanding, especially when the surrounding environment is not known and safety of the robot is crucial. This work relates to the synthesis of Control Barrier Functions (CBFs) through data for safe navigation in unknown environments. A novel methodology to jointly learn CBFs and corresponding safe controllers, in simulation, inspired by the State Dependent Riccati Equation (SDRE) is proposed. The CBF is used to obtain admissible commands from any nominal, possibly unsafe controller. An approach to apply the CBF inside a safety filter without the need for a consistent map or position estimate is developed. Subsequently, the resulting reactive safety filter is deployed on a multirotor platform integrating a LiDAR sensor both in simulation and real-world experiments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t12_13">
             15:30-16:30, Paper ThPI5T12.13
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2841'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Preserving Relative Localization of FoV-Limited Drone Swarm Via Active Mutual Observation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398625" title="Click to go to the Author Index">
             Guo, Lianjie
            </a>
           </td>
           <td class="r">
            Huzhou Institute, Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392773" title="Click to go to the Author Index">
             Gongye, Zaitian
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#394199" title="Click to go to the Author Index">
             Xu, Ziyi
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#288151" title="Click to go to the Author Index">
             Wang, Yingjian
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#273022" title="Click to go to the Author Index">
             Zhou, Xin
            </a>
           </td>
           <td class="r">
            ZHEJIANG UNIVERSITY
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#204347" title="Click to go to the Author Index">
             Zhou, Jinni
            </a>
           </td>
           <td class="r">
            Hong Kong University of Science and Technology (Guangzhou)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#200893" title="Click to go to the Author Index">
             Gao, Fei
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2841" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__perception_and_autonomy" title="Click to go to the Keyword Index">
               Aerial Systems: Perception and Autonomy
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#swarm_robotics" title="Click to go to the Keyword Index">
               Swarm Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Relative state estimation is crucial for vision-based swarms to estimate and compensate for the unavoidable drift of visual odometry. For autonomous drones equipped with the most compact sensor setting --- a stereo camera that provides a limited field of view (FoV), the demand for mutual observation for relative state estimation conflicts with the demand for environment observation. To balance the two demands for FoV-limited swarms by acquiring mutual observations with a safety guarantee, this paper proposes an active localization correction system, which plans camera orientations via a yaw planner during the flight. The yaw planner manages the contradiction by calculating suitable timing and yaw angle commands based on the evaluation of localization uncertainty estimated by the Kalman Filter. Simulation validates the scalability of our algorithm. In real-world experiments, we reduce positioning drift by up to 65% and managed to maintain a given formation in a 100m long-range flight, from which the accuracy, efficiency and robustness of the proposed system are verified.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t12_14">
             15:30-16:30, Paper ThPI5T12.14
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3026'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Targeted Image Transformation for Improving Robustness in Long Range Aircraft Detection
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#328823" title="Click to go to the Author Index">
             Martin, Rebecca
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398869" title="Click to go to the Author Index">
             Fung, Clement
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#296446" title="Click to go to the Author Index">
             Keetha, Nikhil Varma
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398868" title="Click to go to the Author Index">
             Bauer, Lujo
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#104304" title="Click to go to the Author Index">
             Scherer, Sebastian
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3026" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__perception_and_autonomy" title="Click to go to the Keyword Index">
               Aerial Systems: Perception and Autonomy
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In the field of aviation, the Detect and Avoid (DAA) problem deals with incorporating collision avoidance capabilities into current autopilot navigation systems. As an application of the Small Object Detection (SOD) problem, DAA presents the difficulties of a low signal-to-noise ratio and far range detection. Visual DAA is also susceptible to changing weather and lighting conditions at deployment. While current literature has presented many solutions for this, prior work has yet to study the robustness of the learning-based models for DAA. In this work, we show that standard techniques for improving robustness for object detection do not produce the desired results for DAA given the SOD constraints. We present targeted transformations, a zero-shot technique that can significantly improve robustness with minimal impact on accuracy. We demonstrate how to construct these transformations and evaluate our method on the current SOTA model for DAA, showing a 53.6% increase in recall. This makes our pipeline more robust to changes in lighting and environmental factors, and better able to detect potential threats. In the future, we hope to automate the transformation selection process, making it easier to adopt in different use cases.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t12_15">
             15:30-16:30, Paper ThPI5T12.15
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2670'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Explainable Artificial Intelligence for Autonomous UAV Navigation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398452" title="Click to go to the Author Index">
             Dissanayaka, Didula
            </a>
           </td>
           <td class="r">
            Memorial University of Newfoundland
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#160824" title="Click to go to the Author Index">
             Wanasinghe, Thumeera Ruwansiri
            </a>
           </td>
           <td class="r">
            Memorial University of Newfoundland
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#111047" title="Click to go to the Author Index">
             Gosine, Raymond G.
            </a>
           </td>
           <td class="r">
            Memorial University of Newfoundland
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2670" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__perception_and_autonomy" title="Click to go to the Keyword Index">
               Aerial Systems: Perception and Autonomy
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Unmanned Aerial Vehicles (UAVs) with limited computational, perception and power resources face significant challenges when navigating autonomously in unfamiliar environments. While artificial intelligence (AI)-assisted algorithms have been used to address these limitations, transparency of the underlying AI models remains a concern, hindering user trust. To address this limitation, this research study proposes a novel, explainable AI-based navigation approach for UAVs to navigate them through unknown environments autonomously. The soft actor-critic (SAC) algorithm and multilayer perceptron (MLP) policies integrated deep reinforcement learning algorithm is developed to derive control actions. This controller is integrated with a novel moving-window gradient-based explainable artificial intelligence (XAI) framework to shed light on the UAVs decision-making process. The proposed XAI algorithm provides granular insights into how various factors, such as image segments and UAV state features, influence the UAVs actions. It lays the groundwork for a novel visual explanation approach that segments input depth images to highlight critical navigational cues, augmented by a dynamic color map for precise obstacle identification. Additionally, the study introduces comprehensive textual explanations to provide an in-depth understanding of the UAVs decision processes, thereby improving the models transparency and explainability. The simulation results indicate that the proposed DRL model achieves over 95% success rate. Moreover, evaluations conducted in two distinct environments demonstrate the model's capability to generate effective and reliable explanations.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thpi5t12_16">
             15:30-16:30, Paper ThPI5T12.16
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3298'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Novel Variable Step-Size Path Planning Framework with Step Consistent Markov Decision Process for Large Scale UAV Swarm
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392839" title="Click to go to the Author Index">
             Xu, Dan
            </a>
           </td>
           <td class="r">
            National University of Defense Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392858" title="Click to go to the Author Index">
             Guo, Yunxiao
            </a>
           </td>
           <td class="r">
            National University of Defense Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392838" title="Click to go to the Author Index">
             Long, Han
            </a>
           </td>
           <td class="r">
            National University of Defense Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#164497" title="Click to go to the Author Index">
             Wang, Chang
            </a>
           </td>
           <td class="r">
            National University of Defense Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3298" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__perception_and_autonomy" title="Click to go to the Keyword Index">
               Aerial Systems: Perception and Autonomy
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#machine_learning_for_robot_control" title="Click to go to the Keyword Index">
               Machine Learning for Robot Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In recent years, Deep Reinforcement Learning (DRL) has been a key approach to solving Unmanned Aerial Vehicle (UAV) swarm path planning problems. However, traditional DRL methods often face challenges in the initial learning stage and struggle to learn from variable step-size tasks. This paper introduces a novel training framework for large-scale UAV swarm variable step-size path planning: Rapidly-exploring Variable Step-size Deep Reinforcement Learning (RVSDRL). This framework involves common training on the ground local server and decentralized training on distributed UAVs. In the common training stage, we generate rapidly-exploring random graph samples to accelerate the common agent explore environment. In the decentralized training stages, we utilize the priority replay mechanism to improve efficiency. To enhance convergence stability, we restrict the returns of the equivalent paths and propose the Step-size Consistent Markov Decision Process (SCMDP) path planning model. Our method is compared with traditional methods, and the experiments demonstrate its superior performance in complex obstacle environments.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct1">
             <b>
              ThCT1
             </b>
            </a>
           </td>
           <td class="r">
            Room 1
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thct1" title="Click to go to the Program at a Glance">
             <b>
              SLAM III
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#185927" title="Click to go to the Author Index">
             Yuan, Shenghai
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct1_01">
             16:30-16:45, Paper ThCT1.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('960'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              I2EKF-LO: A Dual-Iteration Extended Kalman Filter Based LiDAR Odometry
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#332249" title="Click to go to the Author Index">
             Yu, Wenlu
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#326560" title="Click to go to the Author Index">
             Xu, Jie
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#376671" title="Click to go to the Author Index">
             Zhao, Chengwei
            </a>
           </td>
           <td class="r">
            Hangzhou Guochen Robot Technology Company Limited
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#142000" title="Click to go to the Author Index">
             Zhao, Lijun
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#210732" title="Click to go to the Author Index">
             Nguyen, Thien-Minh
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#185927" title="Click to go to the Author Index">
             Yuan, Shenghai
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#379331" title="Click to go to the Author Index">
             Bai, Mingming
            </a>
           </td>
           <td class="r">
            College of Control Science and Engineering, Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#115410" title="Click to go to the Author Index">
             Xie, Lihua
            </a>
           </td>
           <td class="r">
            NanyangTechnological University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab960" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             LiDAR odometry is a pivotal technology in the fields of autonomous driving and autonomous mobile robotics. However, most of the current works focus on nonlinear optimization methods, and still existing many challenges in using the traditional Iterative Extended Kalman Filter (IEKF) framework to tackle the problem: IEKF only iterates over the observation equation, relying on a rough estimate of the initial state, which is insufficient to fully eliminate motion distortion in the input point cloud; the system process noise is difficult to be determined during state estimation of the complex motions; and the varying motion models across different sensor carriers. To address these issues, we propose the Dual-Iteration Extended Kalman Filter (I2EKF) and the LiDAR odometry based on I2EKF (I2EKF-LO). This approach not only iterates over the observation equation but also leverages state updates to iteratively mitigate motion distortion in LiDAR point clouds. Moreover, it dynamically adjusts process noise based on the confidence level of prior predictions during state estimation and establishes motion models for different sensor carriers to achieve accurate and efficient state estimation. Comprehensive experiments demonstrate that I^2EKF-LO achieves outstanding levels of accuracy and computational efficiency in the realm of LiDAR odometry. Additionally, to foster community development, our code is open-sourced.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct1_02">
             16:45-17:00, Paper ThCT1.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1124'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Physically-Based Photometric Bundle Adjustment in Non-Lambertian Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#390928" title="Click to go to the Author Index">
             Hu, Junpeng
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392726" title="Click to go to the Author Index">
             Cheng, Lei
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392780" title="Click to go to the Author Index">
             Yan, Haodong
            </a>
           </td>
           <td class="r">
            The Hong Kong University of Science and Technology (Guangzhou)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#285402" title="Click to go to the Author Index">
             Gladkova, Mariia
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#333989" title="Click to go to the Author Index">
             Huang, Tianyu
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#100055" title="Click to go to the Author Index">
             Liu, Yunhui
            </a>
           </td>
           <td class="r">
            Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#150816" title="Click to go to the Author Index">
             Cremers, Daniel
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#210132" title="Click to go to the Author Index">
             Li, Haoang
            </a>
           </td>
           <td class="r">
            Hong Kong University of Science and Technology (Guangzhou)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1124" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Photometric bundle adjustment (PBA) is widely used in estimating the camera pose and 3D geometry by assuming a Lambertian world. However, the assumption of photometric consistency is often violated since the non-diffuse reflection is common in real-world environments. The photometric inconsistency significantly affects the reliability of existing PBA methods. To solve this problem, we propose a novel physically-based PBA method. Specifically, we introduce the physically-based weights regarding material, illumination, and light path. These weights distinguish the pixel pairs with different levels of photometric inconsistency. We also design corresponding models for material estimation based on sequential images and illumination estimation based on point clouds. In addition, we establish the first SLAM-related dataset of non-Lambertian scenes with complete ground truth of illumination and material. Extensive experiments demonstrated that our PBA method outperforms existing approaches in accuracy.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct1_03">
             17:00-17:15, Paper ThCT1.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1944'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              HSS-SLAM: Human-In-The-Loop Semantic SLAM Represented by Superquadrics
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#372886" title="Click to go to the Author Index">
             Li, Yulong
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#219010" title="Click to go to the Author Index">
             Zhang, Yunzhou
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#373217" title="Click to go to the Author Index">
             Zhao, Bin
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#372689" title="Click to go to the Author Index">
             Zhang, Zhiyao
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#353395" title="Click to go to the Author Index">
             Shen, You
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#373883" title="Click to go to the Author Index">
             Zhang, Tengda
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#374404" title="Click to go to the Author Index">
             Chen, Guolu
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1944" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#human_factors_and_human_in_the_loop" title="Click to go to the Keyword Index">
               Human Factors and Human-in-the-Loop
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The advancement of object detection algorithms has catalyzed the development of object-level semantic SLAM. However, due to missed and false detections, object-level semantic SLAM fails to represent the objects within the scene adequately. Therefore, this paper proposes a novel object-level semantic SLAM termed HSS-SLAM. We incorporate human-in-the-loop into our method, establishing an interaction module to facilitate human editing and rectifying semantic information. Additionally, to minimize the manual correction workload, a lightweight and intuitive method for semantic extension is proposed, augmenting the semantic richness of the global map with a few operations. Furthermore, our method adopts superquadrics for object representation, enabling detailed descriptions of various object shapes. This mitigates the limitation of conventional semantic mapping, where objects are difficult to distinguish due to the reliance on a single-shape representation. Subsequently, precise estimation of superquadric parameters and camera poses is achieved through joint optimization. Extensive experiments conducted on TUM RGB-D and Scenes V2 datasets demonstrate that the proposed approach exhibits competitive performance, surpassing current methods in both object representation and camera localization accuracy.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct1_04">
             17:15-17:30, Paper ThCT1.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2047'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              High-Fidelity SLAM Using Gaussian Splatting with Rendering-Guided Densification and Regularized Optimization
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#377965" title="Click to go to the Author Index">
             Sun, Shuo
            </a>
           </td>
           <td class="r">
            Orebro University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#191501" title="Click to go to the Author Index">
             Mielle, Malcolm
            </a>
           </td>
           <td class="r">
            Schindler
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#104036" title="Click to go to the Author Index">
             Lilienthal, Achim J.
            </a>
           </td>
           <td class="r">
            Orebro University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#101909" title="Click to go to the Author Index">
             Magnusson, Martin
            </a>
           </td>
           <td class="r">
            rebro University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2047" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We propose a dense RGBD SLAM system based on 3D Gaussian Splatting that provides metrically accurate pose tracking and visually realistic reconstruction. To this end, we first propose a Gaussian densification strategy based on the rendering loss to map unobserved areas and refine reobserved areas. Second, we introduce extra regularization parameters to alleviate the forgetting problem in the continuous mapping problem, where parameters tend to overfit the latest frame and result in decreasing rendering quality for previous frames. Both mapping and tracking are performed with Gaussian parameters by minimizing re-rendering loss in a differentiable way. Compared to recent neural and concurrently developed gaussian splatting RGBD SLAM baselines, our method achieves state-of-the-art results on the synthetic dataset Replica and competitive results on the real-world dataset TUM.The code is released on https://github.com/ljjTYJR/HF-SLAM
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct2">
             <b>
              ThCT2
             </b>
            </a>
           </td>
           <td class="r">
            Room 2
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thct2" title="Click to go to the Program at a Glance">
             <b>
              Simulation and Animation
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#170047" title="Click to go to the Author Index">
             Courtecuisse, Hadrien
            </a>
           </td>
           <td class="r">
            AVR, CNRS Strasbourg
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct2_01">
             16:30-16:45, Paper ThCT2.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1484'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Solving Dynamic Cosserat Rods with Frictional Contact Using the Shooting Method and Implicit Surfaces
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#354311" title="Click to go to the Author Index">
             Jilani, Radhouane
            </a>
           </td>
           <td class="r">
            INRIA
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#354303" title="Click to go to the Author Index">
             Villard, Pierre-Frederic
            </a>
           </td>
           <td class="r">
            Universit De Lorraine
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#169751" title="Click to go to the Author Index">
             Kerrien, Erwan
            </a>
           </td>
           <td class="r">
            INRIA
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1484" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#simulation_and_animation" title="Click to go to the Keyword Index">
               Simulation and Animation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#contact_modeling" title="Click to go to the Keyword Index">
               Contact Modeling
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#surgical_robotics__steerable_catheters_needles" title="Click to go to the Keyword Index">
               Surgical Robotics: Steerable Catheters/Needles
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We propose a method for solving the strong form of the dynamic Cosserat rod clamped-free boundary value problem (BVP) under frictional contact. The shooting method is employed for BVP solving, and contact response is computed using the penalty method. The proposed method is tailored for accurate and rapid simulation of catheterization-like medical procedures. Our contact scenarios involve complex shapes and real patient geometry, modeled with implicit surfaces as blobby models. The results show that using blobby models for contact detection is orders of magnitude faster than using triangle meshes. This approach provides smoother contact forces, accelerating the convergence of the shooting method. We have made our code available as open source at https://gitlab.inria.fr/rjilani/IROS2024/.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct2_02">
             16:45-17:00, Paper ThCT2.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2093'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              High Rate Mechanical Coupling of Interacting Objects in the Context of Needle Insertion Simulation with Haptic Feedback
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#355587" title="Click to go to the Author Index">
             Martin, Claire
            </a>
           </td>
           <td class="r">
            INRIA
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#115421" title="Click to go to the Author Index">
             Duriez, Christian
            </a>
           </td>
           <td class="r">
            INRIA
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#170047" title="Click to go to the Author Index">
             Courtecuisse, Hadrien
            </a>
           </td>
           <td class="r">
            AVR, CNRS Strasbourg
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2093" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#simulation_and_animation" title="Click to go to the Keyword Index">
               Simulation and Animation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#haptics_and_haptic_interfaces" title="Click to go to the Keyword Index">
               Haptics and Haptic Interfaces
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#contact_modeling" title="Click to go to the Keyword Index">
               Contact Modeling
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Needle-based procedures such as biopsies or radiofrequency ablation (RFA) of tumors are often considered to diagnose and treat liver cancer for their low invasiveness but raise difficulties for practitioners related to needle placement and visibility of internal anatomical structures. Efforts are being conducted to build real-time needle insertion simulators with both visual and haptic rendering, facing challenges related to model accuracy and real-time computational performance. This work focuses on the contact model involved in needle-tissue interactions in order to improve the realism of the resulting haptic rendering. We present a novel method to update the compliant coupling at high rates of a complete contact system involving the mechanics of a large object and the complete model of a flexible needle. These updates allow to adapt the contact directions to the needle deformations in the haptic thread, with the aim of improving the resulting haptic feedback. Updates of contact directions and the related mechanical system according to high-rate deformations decrease force feedback artifacts associated with low-rate mechanics while maintaining high-rate performances for the haptic loop.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct2_03">
             17:00-17:15, Paper ThCT2.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3204'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Text-To-Drive: Diverse Driving Behavior Synthesis Via Large Language Models
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398926" title="Click to go to the Author Index">
             Nguyen, Phat
            </a>
           </td>
           <td class="r">
            University of Massachusetts Amherst
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#220292" title="Click to go to the Author Index">
             Wang, Tsun-Hsuan
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#246645" title="Click to go to the Author Index">
             Hong, Zhang-Wei
            </a>
           </td>
           <td class="r">
            National Tsing Hua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#124153" title="Click to go to the Author Index">
             Karaman, Sertac
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#101388" title="Click to go to the Author Index">
             Rus, Daniela
            </a>
           </td>
           <td class="r">
            MIT
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3204" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#simulation_and_animation" title="Click to go to the Keyword Index">
               Simulation and Animation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#ai_based_methods" title="Click to go to the Keyword Index">
               AI-Based Methods
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#autonomous_vehicle_navigation" title="Click to go to the Keyword Index">
               Autonomous Vehicle Navigation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Generating varied scenarios through simulation is crucial for training and evaluating safety-critical systems, such as autonomous vehicles. Yet, the task of modeling the trajectories of other vehicles to simulate diverse and meaningful close interactions remains prohibitively costly. Adopting language descriptions to generate driving behaviors emerges as a promising strategy, offering a scalable and intuitive method for human operators to simulate a wide range of driving interactions. However, the scarcity of large-scale annotated language-trajectory data makes this approach challenging. To address this gap, we propose Text-to-Drive (T2D) to synthesize diverse driving behaviors via Large Language Models (LLMs). We introduce a knowledge-driven approach that operates in two stages. In the first stage, we employ the embedded knowledge of LLMs to generate diverse language descriptions of driving behaviors for a scene. Then, we leverage LLM's reasoning capabilities to synthesize these behaviors in simulation. At its core, T2D employs an LLM to construct a state chart that maps low-level states to high-level abstractions. This strategy aids in downstream tasks such as summarizing low-level observations, assessing behavioral alignment, and shaping the auxiliary reward, all without needing human supervision. With our knowledge-driven approach, we demonstrate that T2D generates more diverse trajectories compared to other baselines and offers a natural language interface that allows for incorporating human preference.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct2_04">
             17:15-17:30, Paper ThCT2.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('661'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              TriVis: Versatile, Reliable, and High-Performance Tool for Computing Visibility in Polygonal Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#316650" title="Click to go to the Author Index">
             Mikula, Jan
            </a>
           </td>
           <td class="r">
            Czech Technical University in Prague
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#119465" title="Click to go to the Author Index">
             Kulich, Miroslav
            </a>
           </td>
           <td class="r">
            Czech Technical University in Prague
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#115576" title="Click to go to the Author Index">
             Preucil, Libor
            </a>
           </td>
           <td class="r">
            Czech Technical University in Prague, CIIRC
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab661" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#software_tools_for_robot_programming" title="Click to go to the Keyword Index">
               Software Tools for Robot Programming
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#computational_geometry" title="Click to go to the Keyword Index">
               Computational Geometry
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#range_sensing" title="Click to go to the Keyword Index">
               Range Sensing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Visibility is a fundamental concept in computational geometry, with numerous applications in surveillance, robotics, and games. This software paper presents TiVis, a C++ library developed by the authors for computing numerous visibility-related queries in highly complex polygonal environments. Adapting the triangular expansion algorithm, TiVis stands out as a versatile, high-performance, more reliable and easy-to-use alternative to current solutions that is also free of heavy dependencies. Through evaluation on a challenging dataset, TiVis has been benchmarked against existing visibility libraries. The results demonstrate that TiVis outperforms the competing solutions by at least an order of magnitude in query times, while exhibiting more reliable runtime behavior. TiVis is freely available for private, research, and institutional use at https://github.com/janmikulacz/trivis.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct3">
             <b>
              ThCT3
             </b>
            </a>
           </td>
           <td class="r">
            Room 3
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thct3" title="Click to go to the Program at a Glance">
             <b>
              Manipulation and Grasping III
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#117857" title="Click to go to the Author Index">
             Sycara, Katia
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct3_01">
             16:30-16:45, Paper ThCT3.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1234'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Touch-GS: Visual-Tactile Supervised 3D Gaussian Splatting
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#310782" title="Click to go to the Author Index">
             Swann, Aiden
            </a>
           </td>
           <td class="r">
            Stanford
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#290028" title="Click to go to the Author Index">
             Strong, Matthew
            </a>
           </td>
           <td class="r">
            University of Colorado Boulder
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#224987" title="Click to go to the Author Index">
             Do, Won Kyung
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#313693" title="Click to go to the Author Index">
             Sznaier Camps, Gadiel
            </a>
           </td>
           <td class="r">
            Stanford
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#106233" title="Click to go to the Author Index">
             Schwager, Mac
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#191308" title="Click to go to the Author Index">
             Kennedy, Monroe
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1234" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#dexterous_manipulation" title="Click to go to the Keyword Index">
               Dexterous Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#force_and_tactile_sensing" title="Click to go to the Keyword Index">
               Force and Tactile Sensing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this work, we propose a novel method to supervise 3D Gaussian Splatting (3DGS) scenes using optical tactile sensors. Optical tactile sensors have become widespread in their use in robotics for manipulation and object representation; however, raw optical tactile sensor data is unsuitable to directly supervise a 3DGS scene. Our representation leverages a Gaussian Process Implicit Surface to implicitly represent the object, combining many touches into a unified representation with uncertainty. We merge this model with a monocular depth estimation network, which is aligned in a two stage process, coarsely aligning with a depth camera and then finely adjusting to match our touch data. For every training image, our method produces a corresponding fused depth and uncertainty map. Utilizing this additional information, we propose a new loss function, variance-weighted depth supervised loss, for training the 3DGS scene model. We leverage the DenseTact optical tactile sensor and RealSense RGB-D camera to show that combining touch and vision in this manner leads to quantitatively and qualitatively better results than vision or touch alone in few-view scene synthesis on opaque, reflective and transparent objects.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct3_02">
             16:45-17:00, Paper ThCT3.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1599'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              ToolEENet: Tool Affordance 6D Pose Estimation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396714" title="Click to go to the Author Index">
             Wang, Yunlong
            </a>
           </td>
           <td class="r">
            Universit Hamburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#287254" title="Click to go to the Author Index">
             Zhang, Lei
            </a>
           </td>
           <td class="r">
            University of Hamburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#271406" title="Click to go to the Author Index">
             Tu, Yuyang
            </a>
           </td>
           <td class="r">
            Universitat Hamburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#317305" title="Click to go to the Author Index">
             Zhang, Hui
            </a>
           </td>
           <td class="r">
            University of Hamburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#309543" title="Click to go to the Author Index">
             Bai, Kaixin
            </a>
           </td>
           <td class="r">
            University of Hamburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#137632" title="Click to go to the Author Index">
             Chen, Zhaopeng
            </a>
           </td>
           <td class="r">
            University of Hamburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#106951" title="Click to go to the Author Index">
             Zhang, Jianwei
            </a>
           </td>
           <td class="r">
            University of Hamburg
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1599" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#data_sets_for_robot_learning" title="Click to go to the Keyword Index">
               Data Sets for Robot Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The exploration of robotic dexterous hands uti- lizing tools has recently attracted considerable attention. A significant challenge in this field is the precise awareness of a tools pose when it is grasped, as occlusion by the hand often degrades the quality of estimation. Additionally, the overall pose of the tool often fails to accurately represent the contact interaction, thereby limiting the effectiveness of vision-guided contact-dependent activities. To overcome this limitation, we present the innovative TOOLEE dataset, which, as far as we know, is the first to feature the affordance segmentation of a tools end-effector (EE) along with its defined 6D pose based on its usage. Furthermore, we propose the ToolEENet framework for the accurate 6D pose estimation of the tools EE. This framework begins by segmenting the tools EE from raw RGB-D data, then using a diffusion model-based pose estimator for 6D pose estimation at a category-specific level. Addressing the issue of symmetry in pose estimation, we introduce a symmetry-aware pose representation that enhances the consistency of pose estimation. Our approach excels in this field, demonstrating high levels of precision and generalization. Furthermore, it shows great promise for application in contact- based manipulation scenarios.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct3_03">
             17:00-17:15, Paper ThCT3.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2538'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              ShapeGrasp: Zero-Shot Task-Oriented Grasping with Large Language Models through Geometric Decomposition
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#386132" title="Click to go to the Author Index">
             Li, Samuel
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#386133" title="Click to go to the Author Index">
             Bhagat, Sarthak
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#185939" title="Click to go to the Author Index">
             Campbell, Joseph
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#275067" title="Click to go to the Author Index">
             Xie, Yaqi
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398302" title="Click to go to the Author Index">
             Kim, Woojun
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#117857" title="Click to go to the Author Index">
             Sycara, Katia
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#192216" title="Click to go to the Author Index">
             Stepputtis, Simon
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2538" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#grasping" title="Click to go to the Keyword Index">
               Grasping
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Task-oriented grasping of unfamiliar objects is a necessary skill for robots in dynamic in-home environments. Inspired by the human capability to grasp such objects through intuition about their shape and structure, we present a novel zero-shot task-oriented grasping method leveraging a geometric decomposition of the target object into simple, convex shapes that we represent in a graph structure, including geometric attributes and spatial relationships. Our approach employs minimal essential information  the objects name and the intended task  to facilitate zero-shot task-oriented grasping. We utilize the commonsense reasoning capabilities of large language models to dynamically assign semantic meaning to each decomposed part and subsequently reason over the utility of each part for the intended task. Through extensive experiments on a real-world robotics platform, we demonstrate that our grasping approachs decomposition and reasoning pipeline is capable of selecting the correct part in 92% of the cases and successfully grasping the object in 82% of the tasks we evaluate.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct4">
             <b>
              ThCT4
             </b>
            </a>
           </td>
           <td class="r">
            Room 4
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thct4" title="Click to go to the Program at a Glance">
             <b>
              Soft Robot Applications I
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#146365" title="Click to go to the Author Index">
             Cha, Youngsu
            </a>
           </td>
           <td class="r">
            Korea University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct4_01">
             16:30-16:45, Paper ThCT4.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('376'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Vine Robots That Evert through Bending
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#229393" title="Click to go to the Author Index">
             Wu, Rui
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#148776" title="Click to go to the Author Index">
             Mintchev, Stefano
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab376" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#soft_robot_applications" title="Click to go to the Keyword Index">
               Soft Robot Applications
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#soft_sensors_and_actuators" title="Click to go to the Keyword Index">
               Soft Sensors and Actuators
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#soft_robot_materials_and_design" title="Click to go to the Keyword Index">
               Soft Robot Materials and Design
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Despite the elegantly simple operation principle, the design of everting vine robots still presents two complexities: a tip-device required for buckle-free retraction; a pressurised base chamber to maintain inflation while allowing the everted body to be winded onto a motorised reel. We create a new type of vine robots with a unique eversion method: instead of using a single everting tube, our design comprises multiple interconnected tubes that bend to induce an overall eversion. Our design eliminates the pressurised base and the tip-device, while allowing buckle-free retraction. It also enables new designs and functionalities beyond conventional vine robots, including drone-based vine robots capable of cantilevered operations, everting grippers that grasp then transport objects into the robot body, and solid-state vine robots without the need for inflation.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct4_02">
             16:45-17:00, Paper ThCT4.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('508'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Tunable Stiffness Glove for Tremor Suppression Based on 3D Printed Structured Fabrics
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#348618" title="Click to go to the Author Index">
             Chen, Yu
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#228781" title="Click to go to the Author Index">
             Li, Junwei
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#346228" title="Click to go to the Author Index">
             Yang, Xudong
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#344256" title="Click to go to the Author Index">
             Wang, Yifan
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab508" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#soft_robot_applications" title="Click to go to the Keyword Index">
               Soft Robot Applications
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#soft_robot_materials_and_design" title="Click to go to the Keyword Index">
               Soft Robot Materials and Design
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#prosthetics_and_exoskeletons" title="Click to go to the Keyword Index">
               Prosthetics and Exoskeletons
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Tremors, common symptoms of Parkinsons disease (PD) and essential tremor (ET), significantly impact the quality of life for affected individuals. Traditional treatments, including pharmaceutical medications and invasive surgical procedures, often come with limitations and side effects, prompting the need for alternative solutions. In this paper, the Tunable Stiffness Glove (TSG) is developed as a non-invasive exoskeleton to suppress wrist tremors. By employing chain mail fabrics with tunable stiffness, the TSG permits natural wrist movement when in a soft state, while transitioning to a jammed state upon the application of negative pressure, effectively suppressing tremors in two directions. Evaluation of the TSGs efficacy was conducted through comprehensive three-point bending tests and human trials, utilizing commercial mechanical tester, inertial measurement unit (IMU), and electromyography (EMG) sensors. Weighing a mere 92 grams, the TSG demonstrated remarkable tremor suppression rates of 74.86%5.52% (in the flexion-extension direction) and 66.80%15.47% (in the adduction-abduction direction). Future enhancements aim to optimize the design for increased damping force and integrate sophisticated control strategies for improved user-exoskeleton interaction.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct4_03">
             17:00-17:15, Paper ThCT4.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1024'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Design and Control of a Novel Soft-Rigid Lower Limb Exoskeleton Robot
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#327429" title="Click to go to the Author Index">
             Wang, Yuxuan
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#393357" title="Click to go to the Author Index">
             Yuan, Shaoke
            </a>
           </td>
           <td class="r">
            Shanghaijiaotong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#393355" title="Click to go to the Author Index">
             Pu, Zihan
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#214791" title="Click to go to the Author Index">
             Wang, Jiangbei
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#209862" title="Click to go to the Author Index">
             Yanqiong, Fei
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1024" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#soft_robot_applications" title="Click to go to the Keyword Index">
               Soft Robot Applications
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#wearable_robotics" title="Click to go to the Keyword Index">
               Wearable Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#soft_sensors_and_actuators" title="Click to go to the Keyword Index">
               Soft Sensors and Actuators
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a study on the design and control of a novel soft-rigid lower limb exoskeleton robot. First, based on anatomy, a novel exoskeleton structure design is proposed that applies Curl Pneumatic Artificial Muscles (CPAMs) to lower limb joints to actuate lower limb movement, and transmit force and motion through rigid parts. A phenomenological characteristic model of the CPAM based on experimental tests and fitting methods is constructed for exoskeleton control. Then, a feedforward-feedback hybrid control method based on four CPAM characteristic models is proposed to improve the control accuracy and stability of the exoskeleton. A human-in-the-loop control method based on human-robot hybrid dynamics modeling and human intentions and states is proposed to improve the human-robot interaction performance of exoskeletons. Experimental results show that feedforward-feedback hybrid control can reduce the maximum tracking error of the exoskeleton to 3.4% for hip, 2.9% for knee, and 4.7% for ankle joint. The exoskeleton can achieve intentional control based on EMG signals. With the assistance of the exoskeleton, the muscle activity of the human lower limbs is reduced by an average of 32.2%. The proposed soft-rigid lower limb exoskeleton robot has the advantages of being lightweight, having good flexibility, being comfortable wearing and having good human-computer interaction, which can improve efficiency. In the future, it will provide more effective intelligent rehabilitation equipment for patients with lower limb movement disorders.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct4_04">
             17:15-17:30, Paper ThCT4.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2354'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SoftNeRF: A Self-Modeling Soft Robot Plugin for Various Tasks
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#379522" title="Click to go to the Author Index">
             Shan, Jiwei
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#379525" title="Click to go to the Author Index">
             Li, Yirui
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392899" title="Click to go to the Author Index">
             Feng, Qiyu
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#415483" title="Click to go to the Author Index">
             Li, Ditao
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#256999" title="Click to go to the Author Index">
             Han, Lijun
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#103003" title="Click to go to the Author Index">
             Wang, Hesheng
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2354" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#soft_robot_applications" title="Click to go to the Keyword Index">
               Soft Robot Applications
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#bioinspired_robot_learning" title="Click to go to the Keyword Index">
               Bioinspired Robot Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#embodied_cognitive_science" title="Click to go to the Keyword Index">
               Embodied Cognitive Science
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Building a self-model for robots, enabling them to simulate their physical selves and predict future states without direct interaction with the physical world, is crucial for robot motion planning and control. Existing self-modeling methods primarily focus on rigid robots and typically require significant time, effort, and resources to gather training data. In this study, we introduce SoftNeRF, a self-supervised visual self-model designed for soft robots. We use a hybrid neural shape representation based on the Signed Distance Function (SDF) to capture both the geometry and complex nonlinear motion of soft robots. By leveraging differentiable rendering, our method learns a self-model from readily available RGB images, similar to how humans understand their physical state through reflection. To improve training efficiency and model accuracy, we propose an error-guided adaptive sampling strategy. SoftNeRF can serve as a plug-in for various downstream tasks, even when trained with data unrelated to those tasks. We demonstrate SoftNeRF's ability to support shape prediction and motion planning for robots in both simulated and real-world environments. Furthermore, SoftNeRF excels in detecting and recovering from damage, thereby enhancing machine resilience. Code is available at: https://github.com/IRMVLab/soft-nerf.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct5">
             <b>
              ThCT5
             </b>
            </a>
           </td>
           <td class="r">
            Room 5
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thct5" title="Click to go to the Program at a Glance">
             <b>
              Mechanism Design II
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#201182" title="Click to go to the Author Index">
             Borisov, Ivan
            </a>
           </td>
           <td class="r">
            ITMO University
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#101894" title="Click to go to the Author Index">
             Stefanini, Cesare
            </a>
           </td>
           <td class="r">
            Scuola Superiore Sant'Anna
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct5_01">
             16:30-16:45, Paper ThCT5.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('727'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Design and Preliminary Validation of a Multi-Mode Quadrotor Aerial-Aquatic Vehicle with Tilting Mechanism
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#393693" title="Click to go to the Author Index">
             Liu, Mengyao
            </a>
           </td>
           <td class="r">
            Nanjing University of Aeronautics and Astronautics
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#363095" title="Click to go to the Author Index">
             Chen, Bai
            </a>
           </td>
           <td class="r">
            Nanjing University of Aeronautics and Astronautics
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#347616" title="Click to go to the Author Index">
             Wang, Lingyu
            </a>
           </td>
           <td class="r">
            Nanjing University of Aeronautics and Astronautics
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#306701" title="Click to go to the Author Index">
             Mao, Zebing
            </a>
           </td>
           <td class="r">
            Yamaguchi University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#189351" title="Click to go to the Author Index">
             Shen, Yayi
            </a>
           </td>
           <td class="r">
            Tokyo Institution of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab727" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#mechanism_design" title="Click to go to the Keyword Index">
               Mechanism Design
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#field_robots" title="Click to go to the Keyword Index">
               Field Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#marine_robotics" title="Click to go to the Keyword Index">
               Marine Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents the development of a quadrotor vehicle capable of movement between air and water environments. The design challenges, including torque matching in the propulsion system, balance maintenance in different mediums, and waterproofing requirements, are discussed. A prototype is constructed to demonstrate the feasibility of the proposed design. Through the tilting mechanism in four rotors, the vehicle can not only fly in air like a normal quadrotor vehicle, but also propel in water with two configured rear thrusters. Moreover, the four tilted rotors cooperate in aligning the center of gravity and buoyancy, addressing limitations observed in traditional aerial-aquatic vehicles. The prototype has been verified through experiments in both air and water.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct5_02">
             16:45-17:00, Paper ThCT5.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2085'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Frozen Assets: Leveraging Ice, Water, and Phase Transitions in Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#378580" title="Click to go to the Author Index">
             Wilhelm, Aaron
            </a>
           </td>
           <td class="r">
            Cornell University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#325559" title="Click to go to the Author Index">
             Wilhelm, Andrew
            </a>
           </td>
           <td class="r">
            Cornell University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#383097" title="Click to go to the Author Index">
             Caldern-Aceituno, Lydia Isabela
            </a>
           </td>
           <td class="r">
            Cornell University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#105917" title="Click to go to the Author Index">
             Napp, Nils
            </a>
           </td>
           <td class="r">
            Cornell University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#122376" title="Click to go to the Author Index">
             Petersen, Kirstin Hagelskjaer
            </a>
           </td>
           <td class="r">
            Cornell University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#169741" title="Click to go to the Author Index">
             Helbling, E. Farrell
            </a>
           </td>
           <td class="r">
            Cornell University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2085" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#mechanism_design" title="Click to go to the Keyword Index">
               Mechanism Design
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#soft_robot_materials_and_design" title="Click to go to the Keyword Index">
               Soft Robot Materials and Design
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robots are especially useful in cold, remote, and inhospitable environments such as polar regions and extraterrestrial settings. Due to subfreezing temperatures and limited resources in these environments, robots made of ice are particularly advantageous. In this paper we demonstrate how the solid and liquid phases of water, and transitions between these phases, can be leveraged into common robot designs for modular robots, robot arms, rovers, and soft robots. We explore how robots can utilize structural elements made of ice and exploit the phase change between ice and water to augment their capabilities. Additionally, we do a scaling analysis of ice structural elements to provide insight on their performance at different length scales and ambient temperatures.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct5_03">
             17:00-17:15, Paper ThCT5.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2282'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Synergizing Morphological Computation and Generative Design: Automatic Synthesis of Tendon-Driven Grippers
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#354564" title="Click to go to the Author Index">
             Zharkov, Kirill
            </a>
           </td>
           <td class="r">
            ITMO University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#354565" title="Click to go to the Author Index">
             Chaikovskii, Mikhail
            </a>
           </td>
           <td class="r">
            ITMO University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#354562" title="Click to go to the Author Index">
             Osipov, Yefim
            </a>
           </td>
           <td class="r">
            ITMO University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#376940" title="Click to go to the Author Index">
             Alshaowa, Rahaf
            </a>
           </td>
           <td class="r">
            ITMO University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#201182" title="Click to go to the Author Index">
             Borisov, Ivan
            </a>
           </td>
           <td class="r">
            ITMO University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#166925" title="Click to go to the Author Index">
             Kolyubin, Sergey
            </a>
           </td>
           <td class="r">
            ITMO University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2282" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#mechanism_design" title="Click to go to the Keyword Index">
               Mechanism Design
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#tendon_wire_mechanism" title="Click to go to the Keyword Index">
               Tendon/Wire Mechanism
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#grippers_and_other_end_effectors" title="Click to go to the Keyword Index">
               Grippers and Other End-Effectors
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The design process of robotic systems is a complex journey that involves multiple phases. Throughout this process, the aim is to tackle various criteria simultaneously, even though they often contradict each other. The ultimate goal is to uncover the optimal solution that resolves these conflicting factors. Within this paper we propose a design methodology to generate linkage mechanisms for robots with morphological computation. We use a graph grammar and a heuristic search algorithm to create robot mechanism graphs that are converted into simulation models for testing the design output. To verify the design methodology we have applied it to a relatively simple quasi-static problem of object grasping. Designing a fully actuated gripper may seem simple, but we found a way to automatically design an underactuated tendon-driven gripper that can grasp a wide range of objects. This is possible because of its structure, not because of sophisticated planning or learning. To test the applicability of the proposed method in real engineering practice, we used it to create physical prototypes. Simulation results together with results of testing of physical prototypes are given at the end of the paper. The framework is open source and the link to GitHub is given in the paper.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct5_04">
             17:15-17:30, Paper ThCT5.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1737'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A High-Performance Anthropomorphic Robotic Arm for Household Applications
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#189793" title="Click to go to the Author Index">
             Liu, Tianliang
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#189711" title="Click to go to the Author Index">
             Yang, Sicheng
            </a>
           </td>
           <td class="r">
            Tencent
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#361354" title="Click to go to the Author Index">
             Li, Jingchen
            </a>
           </td>
           <td class="r">
            Tencent
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#292090" title="Click to go to the Author Index">
             Chen, Xiangchi
            </a>
           </td>
           <td class="r">
            Tencent
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#249449" title="Click to go to the Author Index">
             Wang, Shuai
            </a>
           </td>
           <td class="r">
            Tencent
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#203731" title="Click to go to the Author Index">
             Teng, Xiao
            </a>
           </td>
           <td class="r">
            Keppel-NUS Corporate Lab, National University of Singapore
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#174513" title="Click to go to the Author Index">
             Lee, Wang Wei
            </a>
           </td>
           <td class="r">
            Tencent
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#181507" title="Click to go to the Author Index">
             Li, Xiong
            </a>
           </td>
           <td class="r">
            Tencent
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#115743" title="Click to go to the Author Index">
             Zheng, Yu
            </a>
           </td>
           <td class="r">
            Tencent
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1737" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#tendon_wire_mechanism" title="Click to go to the Keyword Index">
               Tendon/Wire Mechanism
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mechanism_design" title="Click to go to the Keyword Index">
               Mechanism Design
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#redundant_robots" title="Click to go to the Keyword Index">
               Redundant Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Anthropomorphic robotic arms, mimicking the structure and function of human arms, show great potential for helping people in various tedious and repetitive household tasks. However, such arms mostly consist of multiple serial links controlled independently by actuators at joints with high reduction ratios, posing challenges in household services in terms of load capacity, responsiveness, and safety. In this paper, we propose a high-performance anthropomorphic arm called TRX-Arm based on differential cable transmission, characterized by features of high load capacity, high dynamics, and inherent compliance. TRX-Arm is composed of three deferential cable-driven coupling joints and one independent roll joint. Thanks to the cable differential transmission, the joints are capable of achieving doubled torque and stiffness without replacing motors. To enhance safety in human-robot interaction, the actuators including motors, reducer, belt, and pulley are mounted at the shoulder near the base and drive the joints remotely using cables, thereby minimizing the inertia of the whole arm. The workspace of TRX-Arm has a volume of 1.56 m2, much larger than that of the human arm. Real experiments show its capabilities including high repeatability and load capacity as well as high dynamic behavior of a dual- arm robot platform built with TRX-Arms.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct6">
             <b>
              ThCT6
             </b>
            </a>
           </td>
           <td class="r">
            Room 6
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thct6" title="Click to go to the Program at a Glance">
             <b>
              Aerial Systems: Perception and Autonomy II
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct6_01">
             16:30-16:45, Paper ThCT6.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1983'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Trajectory Optimization with Global Yaw Parameterization for Field-Of-View Constrained Autonomous Flight
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#286879" title="Click to go to the Author Index">
             Wu, Yuwei
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#283123" title="Click to go to the Author Index">
             Tao, Yuezhan
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#239282" title="Click to go to the Author Index">
             Spasojevic, Igor
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#104342" title="Click to go to the Author Index">
             Kumar, Vijay
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1983" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#autonomous_vehicle_navigation" title="Click to go to the Keyword Index">
               Autonomous Vehicle Navigation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Trajectory generation for quadrotors with limited field-of-view sensors has numerous applications such as aerial exploration, coverage, inspection, videography, and target tracking. Most previous works simplify the task of optimizing yaw trajectories by either aligning the heading of the robot with its velocity, or potentially restricting the feasible space of candidate trajectories by using a limited yaw domain to circumvent angular singularities. In this paper, we propose a novel global yaw parameterization method for trajectory optimization that allows a 360-degree yaw variation as demanded by the underlying algorithm. This approach effectively bypasses inherent singularities by including supplementary quadratic constraints and transforming the final decision variables into the desired state representation. This method significantly reduces the needed control effort, and improves optimization feasibility. Furthermore, we apply the method to several examples of different applications that require jointly optimizing over both the yaw and position trajectories. Ultimately, we present a comprehensive numerical analysis and evaluation of our proposed method in both simulation and real-world experiments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct6_02">
             16:45-17:00, Paper ThCT6.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2303'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Drones Guiding Drones: Cooperative Navigation of a Less-Equipped Micro Aerial Vehicle in Cluttered Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#290671" title="Click to go to the Author Index">
             Pritzl, Vaclav
            </a>
           </td>
           <td class="r">
            Czech Technical University in Prague
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#223249" title="Click to go to the Author Index">
             Vrba, Matous
            </a>
           </td>
           <td class="r">
            Faculty of Electrical Engineering, Czech Technical University In
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#286696" title="Click to go to the Author Index">
             Stasinchuk, Yurii
            </a>
           </td>
           <td class="r">
            Czech Technical University in Prague
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#257250" title="Click to go to the Author Index">
             Kratky, Vit
            </a>
           </td>
           <td class="r">
            Czech Technical University in Prague
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#321026" title="Click to go to the Author Index">
             Horyna, Jiri
            </a>
           </td>
           <td class="r">
            Czech Technical University in Prague
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#120246" title="Click to go to the Author Index">
             Stepan, Petr
            </a>
           </td>
           <td class="r">
            Czech Technical University in Prague, Faculty of Electrical Engi
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#113384" title="Click to go to the Author Index">
             Saska, Martin
            </a>
           </td>
           <td class="r">
            Czech Technical University in Prague
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2303" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__perception_and_autonomy" title="Click to go to the Keyword Index">
               Aerial Systems: Perception and Autonomy
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Reliable deployment of Unmanned Aerial Vehicles (UAVs) in cluttered unknown environments requires accurate sensors for Global Navigation Satellite System (GNSS)-denied localization and obstacle avoidance. Such a requirement limits the usage of cheap and micro-scale vehicles with constrained payload capacity if industrial-grade reliability and precision are required. This paper investigates the possibility of offloading the necessity to carry heavy sensors to another member of the UAV team while preserving the desired capability of the smaller robot intended for exploring narrow passages. A novel cooperative guidance framework offloading the sensing requirements from a minimalistic secondary UAV to a superior primary UAV is proposed. The primary UAV constructs a dense occupancy map of the environment and plans collision-free paths for both UAVs to ensure reaching the desired secondary UAVs goals even in areas not accessible by the bigger robot. The primary UAV guides the secondary UAV to follow the planned path while tracking the UAV using Light Detection and Ranging (LiDAR)-based relative localization. The proposed approach was verified in real-world experiments with a heterogeneous team of a 3D LiDAR-equipped primary UAV and a micro-scale camera-equipped secondary UAV moving autonomously through unknown cluttered GNSS-denied environments with the proposed framework running fully on board the UAVs.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct6_03">
             17:00-17:15, Paper ThCT6.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2220'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              OmniNxt: A Fully Open-Source and Compact Aerial Robot with Omnidirectional Visual Perception
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#356325" title="Click to go to the Author Index">
             Liu, Peize
            </a>
           </td>
           <td class="r">
            The Hong Kong University of Science and Technology, Robotic Inst
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#340168" title="Click to go to the Author Index">
             Feng, Chen
            </a>
           </td>
           <td class="r">
            Hong Kong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#390852" title="Click to go to the Author Index">
             Xu, Yang
            </a>
           </td>
           <td class="r">
            The Hong Kong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397949" title="Click to go to the Author Index">
             Ning, Yan
            </a>
           </td>
           <td class="r">
            Hong Kong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#268067" title="Click to go to the Author Index">
             Xu, Hao
            </a>
           </td>
           <td class="r">
            HKUST
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#142354" title="Click to go to the Author Index">
             Shen, Shaojie
            </a>
           </td>
           <td class="r">
            Hong Kong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2220" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__perception_and_autonomy" title="Click to go to the Keyword Index">
               Aerial Systems: Perception and Autonomy
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#embedded_systems_for_robotic_and_automation" title="Click to go to the Keyword Index">
               Embedded Systems for Robotic and Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Adopting omnidirectional Field of View (FoV) cameras in aerial robots vastly improves perception ability, significantly advancing aerial robotics's capabilities in inspection, reconstruction, and rescue tasks. However, such sensors also elevate system complexity, textit{e.g.}, hardware design, and corresponding algorithm, which limits researchers from utilizing aerial robots with omnidirectional FoV in their research. To bridge this gap, we propose OmniNxt, a fully open-source aerial robotics platform with omnidirectional perception. We design a high-performance flight controller Nxt-FC and a multi-fisheye camera set for OmniNxt. Meanwhile, the compatible software is carefully devised, which empowers OmniNxt to achieve accurate localization and real-time dense mapping with limited computation resource occupancy. We conducted extensive real-world experiments to validate the superior performance of OmniNxt in practical applications. All the hardware and software are open-access atfootnote[3]{url{https://github.com/HKUST-Aerial-Robotics/OmniNxt}}, and we provide docker images of each crucial module in the proposed system. Project page:https://hkust-aerial-robotics.github.io/OmniNxt.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct6_04">
             17:15-17:30, Paper ThCT6.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2315'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Structure-Invariant Range-Visual-Inertial Odometry
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397958" title="Click to go to the Author Index">
             Alberico, Ivan
            </a>
           </td>
           <td class="r">
            University of Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#206699" title="Click to go to the Author Index">
             Delaune, Jeff
            </a>
           </td>
           <td class="r">
            Jet Propulsion Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#276700" title="Click to go to the Author Index">
             Cioffi, Giovanni
            </a>
           </td>
           <td class="r">
            University of Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#105662" title="Click to go to the Author Index">
             Scaramuzza, Davide
            </a>
           </td>
           <td class="r">
            University of Zurich
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2315" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__perception_and_autonomy" title="Click to go to the Keyword Index">
               Aerial Systems: Perception and Autonomy
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The Mars Science Helicopter (MSH) mission aims to deploy the next generation of unmanned helicopters on Mars, targeting landing sites in highly irregular terrain such as Valles Marineris, the largest canyons in the Solar system with elevation variances of up to 8000 meters.
             <p>
              Unlike its predecessor, the Mars 2020 mission, which relied on a state estimation system assuming planar terrain, MSH requires a novel approach due to the complex topography of the landing site.
              <p>
               This paper introduces a novel range-visual-inertial odometry system tailored for the unique challenges of the MSH mission. Our system extends the state-of-the-art xVIO framework by fusing consistent range information with visual and inertial measurements, preventing metric scale drift in the absence of visual-inertial excitation (mono camera and constant velocity descent), and enabling landing on any terrain structure, without requiring any planar terrain assumption. Through extensive testing in image-based simulations using actual terrain structure and textures collected in Mars orbit, we demonstrate that our range-VIO approach estimates terrain-relative velocity meeting the stringent mission requirements, and outperforming existing methods.
              </p>
             </p>
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct7">
             <b>
              ThCT7
             </b>
            </a>
           </td>
           <td class="r">
            Room 7
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thct7" title="Click to go to the Program at a Glance">
             <b>
              Medical Robotics II
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#100111" title="Click to go to the Author Index">
             Arai, Fumihito
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct7_01">
             16:30-16:45, Paper ThCT7.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('446'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Advanced Handheld Micro-Surgical System Using an Hall Sensor and a Magnet Trocar for Retinal Microsurgery
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#337159" title="Click to go to the Author Index">
             Lee, Myung Ho
            </a>
           </td>
           <td class="r">
            DGIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#214102" title="Click to go to the Author Index">
             Im, Jintaek
            </a>
           </td>
           <td class="r">
            DGIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#199203" title="Click to go to the Author Index">
             Song, Cheol
            </a>
           </td>
           <td class="r">
            DGIST
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab446" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#sensor_based_control" title="Click to go to the Keyword Index">
               Sensor-based Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#human_performance_augmentation" title="Click to go to the Keyword Index">
               Human Performance Augmentation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Diseases affecting the retina, such as retinal detachment, diabetic retinopathy, and macular degeneration, are significant contributors to blindness globally, with a substantial risk of vision loss among those afflicted. Surgical treatment of these conditions is complex due to the delicate nature of retinal tissue and the challenges posed by involuntary hand movements. While existing methods aim to compensate for hand tremors using sensor-based systems, they are hindered by limitations in accurately tracking retinal surface movement during surgery, particularly in response to patient movements under anesthesia. To address these issues, this study proposes a novel handheld micro-surgical tool equipped with a 1-degree of freedom (DOF) mechanism and a 3-axis Hall sensor to mitigate physiological hand tremors effectively. By utilizing magnetic flux density measurements, the tool can pinpoint the position of a magnet embedded within the surgical instrument, enabling precise tremor compensation without reliance on a global coordinate system. The design incorporates a piezoelectric (PZT) linear actuator and a Hall sensor for compactness and sensitivity. Optimization of the magnets dimensions through simulation ensures optimal sensor performance. Experimental validation using artificial and ex-vivo porcine eye models demonstrates the tools effectiveness in reducing hand tremors, suggesting potential enhancements in the safety and accuracy of retinal surgeries. For the desired positions from 4000 m to 1000 m, the RMS error of the synthetic eye model and porcine eye decreased from 71.10 m to 33.27 m and 71.36 m to 33.39 m, respectively.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct7_02">
             16:45-17:00, Paper ThCT7.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('542'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              An Octopus-Inspired-Configuration Sensor Array Concept Toward Torso-Oriented Magnetic Localization Task and Simulation Verification
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#340836" title="Click to go to the Author Index">
             Sun, Yichong
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#379323" title="Click to go to the Author Index">
             Chan, Wai Shing
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#265996" title="Click to go to the Author Index">
             Li, Yehui
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#271250" title="Click to go to the Author Index">
             Zhang, Heng
            </a>
           </td>
           <td class="r">
            The University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#286469" title="Click to go to the Author Index">
             Huang, Yisen
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#393044" title="Click to go to the Author Index">
             Hu, Haochen
            </a>
           </td>
           <td class="r">
            Multi-Scale Medical Robotics Center
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#176430" title="Click to go to the Author Index">
             Chiu, Philip, Wai-yan
            </a>
           </td>
           <td class="r">
            Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#152176" title="Click to go to the Author Index">
             Li, Zheng
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab542" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#surgical_robotics__planning" title="Click to go to the Keyword Index">
               Surgical Robotics: Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In response to torso-oriented magnetic localization tasks that require the system to have interactivity and flexibility with guaranteed accuracy, a novel bio-inspired magnetic sensor array configuration is proposed in this paper. Precisely, the ideas of the natural characteristics of octopus flexible tentacles and the wrap morphology are integrated into the design of the magnetic localization system based on the sensor array method. It is worth mentioning that such a design enhances the interactivity and flexibility of the localization system compared to the general planar sensor array strategy. Apart from the concept introduction, the geometry analysis of the proposed configuration is presented based on the constant curvature model. Besides, the magnetic localization algorithm for the system is presented by constructing a magnetic tracking optimization function. Eventually, the proposed concept and developed algorithm are examined in the sensor-array-simulation environment to manifest their effectiveness and applicability. The experimental results indicate that the octopus-inspired-configuration sensor array achieves a mean accuracy at a centimeter-level in our cases, and has better accuracy with a mean value of e  as 0.0178 m and (SQR) _ave as 0.0883 for the center interest space compared to general planar configuration one. Moreover, the effect of the configuration error is analyzed. These results verify the feasibility and superiority of the proposed concept and hold significant practical significance in addressing the challenge associated with magnetic localization tasks toward the clinical application scenarios.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct7_03">
             17:00-17:15, Paper ThCT7.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1939'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Toward Micro Eye Movement Detection in Practice: Stand-Alone Eye Tracker with High Resolution and Wide Measurement Range
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392936" title="Click to go to the Author Index">
             Yokoyama, Keiko
            </a>
           </td>
           <td class="r">
            NEC
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#179716" title="Click to go to the Author Index">
             Sueishi, Tomohiro
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#191114" title="Click to go to the Author Index">
             Inoue, Michiaki
            </a>
           </td>
           <td class="r">
            NEC Corporation
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#209566" title="Click to go to the Author Index">
             Yachida, Shoji
            </a>
           </td>
           <td class="r">
            NEC
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#353947" title="Click to go to the Author Index">
             Hosoi, Toshinori
            </a>
           </td>
           <td class="r">
            NEC Corporation
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#101392" title="Click to go to the Author Index">
             Ishikawa, Masatoshi
            </a>
           </td>
           <td class="r">
            University of Tokyo
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1939" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#visual_servoing" title="Click to go to the Keyword Index">
               Visual Servoing
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#visual_tracking" title="Click to go to the Keyword Index">
               Visual Tracking
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Detecting the micromovements of eyes that reflect a person's inner state can be an essential step in many applications, but most eye trackers need to fix the subject's head using a chin rest to obtain sufficient data quality. We propose a stand-alone eye tracker that utilizes two 500 fps cameras, a pair of rotating mirrors for gaze control, a liquid lens for focus control and an intensity-controllable light source, and describe how the proposed system works in real-time. The experimental results show that our system covers more than twice as wide a measurement range in the depth direction as the conventional eye tracker while achieving sufficient data quality to analyze microsaccades with an amplitude of down to 0.2 deg. We also examine the practical use of the proposed system for microsaccade detection.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct7_04">
             17:15-17:30, Paper ThCT7.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2521'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Hybrid Vision/Force Control Strategy for Handheld Robotic Devices Enhancing Probe-Based Confocal Laser Endomicroscopy
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#269747" title="Click to go to the Author Index">
             Choi, Ingu
            </a>
           </td>
           <td class="r">
            HL Mando
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#253231" title="Click to go to the Author Index">
             Kim, Eunchan
            </a>
           </td>
           <td class="r">
            KIST, Hanyang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#132088" title="Click to go to the Author Index">
             Yang, Sungwook
            </a>
           </td>
           <td class="r">
            Korea Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2521" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#force_control" title="Click to go to the Keyword Index">
               Force Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_medical_robotics" title="Click to go to the Keyword Index">
               Computer Vision for Medical Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We introduce a novel hybrid vision/force control strategy for robotic devices designed to obtain clear and consistent images using probe-based confocal laser endomicroscopy (pCLE). Due to the variable nature of tissue characteristics encountered during pCLE imaging, the conventional approach of pre-setting forces or focus metrics for either force or vision control is often impractical and inadequate. To address this, our strategy employs a blur metric called the CR score to assess the level of blur in pCLE images, enabling the attainment of clear and focused images. At the onset of a pCLE scan, the system autonomously determines the target CR score for vision control, in tandem with a real-time peak detection algorithm. Concurrently, force control is applied judiciously to prevent excessive force on the tissue, adjusting to ensure minimal force is applied, thus preserving image focus. This innovative approach facilitates seamless transitions between vision and force control, depending on the imaging conditions, thereby ensuring the acquisition of consistent pCLE images with minimal force. Our method marks a notable improvement over conventional PID force control techniques. By dynamically adjusting target forces and minimizing force application during operation, we not only enhance the precision and quality of pCLE imaging but also eliminate the dependency on manual pre-settings.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct8">
             <b>
              ThCT8
             </b>
            </a>
           </td>
           <td class="r">
            Room 8
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thct8" title="Click to go to the Program at a Glance">
             <b>
              Localization VI
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#225701" title="Click to go to the Author Index">
             Mock, Alexander
            </a>
           </td>
           <td class="r">
            Osnabrck University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct8_01">
             16:30-16:45, Paper ThCT8.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1862'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Cross-Modal Visual Relocalization in Prior LiDAR Maps Utilizing Intensity Textures
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#372190" title="Click to go to the Author Index">
             Shen, Qiyuan
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#240029" title="Click to go to the Author Index">
             Zhao, Hengwang
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#300932" title="Click to go to the Author Index">
             Yan, Weihao
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#204568" title="Click to go to the Author Index">
             Qin, Tong
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#146916" title="Click to go to the Author Index">
             Yang, Ming
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1862" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Cross-modal localization has drawn increasing attention in recent years, while the visual relocalization in prior LiDAR maps is less studied. Related methods usually suffer from inconsistency between the 2D texture and 3D geometry, neglecting the intensity features in the LiDAR point cloud. In this paper, we propose a cross-modal visual relocalization system in prior LiDAR maps utilizing intensity textures, which consists of three main modules: map projection, coarse retrieval, and fine relocalization. In the map projection module, we construct the database of intensity channel map images leveraging the dense characteristic of panoramic projection. The coarse retrieval module retrieves the top-K most similar map images to the query image from the database, and retains the top-Kresults by covisibility clustering. The fine relocalization module applies a two-stage 2D-3D association and a covisibility inlier selection method to obtain robust correspondences for 6DoF pose estimation. The experimental results on our self-collected datasets demonstrate the effectiveness in both place recognition and pose estimation tasks.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct8_02">
             16:45-17:00, Paper ThCT8.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2056'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              EVIT: Event-Based Visual-Inertial Tracking in Semi-Dense Maps Using Windowed Nonlinear Optimization
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#310616" title="Click to go to the Author Index">
             Yuan, Runze
            </a>
           </td>
           <td class="r">
            Shanghaitech University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#336746" title="Click to go to the Author Index">
             Liu, Tao
            </a>
           </td>
           <td class="r">
            ShanghaiTech University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397303" title="Click to go to the Author Index">
             Dai, Zijia
            </a>
           </td>
           <td class="r">
            Shanghaitech University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#298736" title="Click to go to the Author Index">
             Zuo, Yi-Fan
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#123271" title="Click to go to the Author Index">
             Kneip, Laurent
            </a>
           </td>
           <td class="r">
            ShanghaiTech University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2056" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#visual_inertial_slam" title="Click to go to the Keyword Index">
               Visual-Inertial SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Event cameras are an interesting visual exteroceptive sensor that reacts to brightness changes rather than integrating absolute image intensities. Owing to this design, the sensor exhibits strong performance in situations of challenging dynamics and illumination conditions. While event-based simultaneous tracking and mapping remains a challenging problem, a number of recent works have pointed out the sensor's suitability for prior map-based tracking. By making use of cross-modal registration paradigms, the camera's ego-motion can be tracked across a large spectrum of illumination and dynamics conditions on top of accurate maps that have been created a priori by more traditional sensors. The present paper follows up on a recently introduced event-based geometric semi-dense tracking paradigm, and proposes the addition of inertial signals in order to robustify the estimation. More specifically, the added signals provide strong cues for pose initialization as well as regularization during windowed, multi-frame tracking. As a result, the proposed framework achieves increased performance under challenging illumination conditions as well as a reduction of the rate at which intermediate event representations need to be registered in order to maintain stable tracking across highly dynamic sequences. Our evaluation focuses on a diverse set of real world sequences and comprises a comparison of our proposed method against a purely event-based alternative running at different rates.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct8_03">
             17:00-17:15, Paper ThCT8.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2067'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              MICP-L: Mesh-Based ICP for Robot Localization Using Hardware-Accelerated Ray Casting
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#225701" title="Click to go to the Author Index">
             Mock, Alexander
            </a>
           </td>
           <td class="r">
            Osnabrck University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#117454" title="Click to go to the Author Index">
             Wiemann, Thomas
            </a>
           </td>
           <td class="r">
            Fulda University of Applied Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#212868" title="Click to go to the Author Index">
             Ptz, Sebastian
            </a>
           </td>
           <td class="r">
            German Research Center for Artificial Intelligence
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#105108" title="Click to go to the Author Index">
             Hertzberg, Joachim
            </a>
           </td>
           <td class="r">
            University of Osnabrueck
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2067" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#range_sensing" title="Click to go to the Keyword Index">
               Range Sensing
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#sensor_fusion" title="Click to go to the Keyword Index">
               Sensor Fusion
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Triangle mesh maps are a versatile 3D environment representation for robots to navigate in challenging indoor and outdoor environments exhibiting tunnels, hills and varying slopes. To make use of these mesh maps, methods are needed to accurately localize robots in such maps to perform essential tasks like path planning and navigation. We present Mesh ICP Localization (MICP-L), a novel and computationally efficient method for registering one or more range sensors to a triangle mesh map to continuously localize a robot in 6D, even in GPS-denied environments. We accelerate the computation of ray casting correspondences (RCC) between range sensors and mesh maps by supporting different parallel computing devices like multicore CPUs, GPUs and the latest NVIDIA RTX hardware. By additionally transforming the covariance computation into a reduction operation, we can optimize the initial guessed poses in parallel on CPUs or GPUs, making our implementation applicable in real-time on many architectures. We demonstrate the robustness of our localization approach with datasets from agricultural, aerial, and automotive domains.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct8_04">
             17:15-17:30, Paper ThCT8.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2178'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              DSLO: Deep Sequence LiDAR Odometry Based on Inconsistent Spatio-Temporal Propagation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#377264" title="Click to go to the Author Index">
             Zhang, Huixin
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#234155" title="Click to go to the Author Index">
             Wang, Guangming
            </a>
           </td>
           <td class="r">
            University of Cambridge
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#310729" title="Click to go to the Author Index">
             Wu, Xinrui
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#295784" title="Click to go to the Author Index">
             Xu, Chenfeng
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#266445" title="Click to go to the Author Index">
             Ding, Mingyu
            </a>
           </td>
           <td class="r">
            UC Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#100071" title="Click to go to the Author Index">
             Tomizuka, Masayoshi
            </a>
           </td>
           <td class="r">
            University of California
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#170266" title="Click to go to the Author Index">
             Zhan, Wei
            </a>
           </td>
           <td class="r">
            Univeristy of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#103003" title="Click to go to the Author Index">
             Wang, Hesheng
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2178" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper introduces a 3D point cloud sequence learning model based on inconsistent spatio-temporal propagation for LiDAR odometry, termed DSLO. It consists of a pyramid structure with a spatial information reuse strategy, a sequential pose initialization module, a gated hierarchical pose refinement module, and a temporal feature propagation module. First, spatial features are encoded using a point feature pyramid, with features reused in successive pose estimations to reduce computational overhead. Second, a sequential pose initialization method is introduced, leveraging the high-frequency sampling characteristic of LiDAR to initialize the LiDAR pose. Then, a gated hierarchical pose refinement mechanism refines poses from coarse to fine by selectively retaining or discarding motion information from different layers based on gate estimations. Finally, temporal feature propagation is proposed to incorporate the historical motion information from point cloud sequences, and address the spatial inconsistency issue when transmitting motion information embedded in point clouds between frames. Experimental results on the KITTI odometry dataset and Argoverse dataset demonstrate that DSLO outperforms state-of-the-art methods, achieving at least a 15.67% improvement on RTE and a 12.64% improvement on RRE, while also achieving a 34.69% reduction in runtime compared to baseline methods. Our implementation will be available at https://github.com/IRMVLab/DSLO.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct9">
             <b>
              ThCT9
             </b>
            </a>
           </td>
           <td class="r">
            Room 9
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thct9" title="Click to go to the Program at a Glance">
             <b>
              Motion and Path Planning VI
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#103188" title="Click to go to the Author Index">
             Kyriakopoulos, Kostas
            </a>
           </td>
           <td class="r">
            New York University - Abu Dhabi
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#102420" title="Click to go to the Author Index">
             Bekris, Kostas E.
            </a>
           </td>
           <td class="r">
            Rutgers, the State University of New Jersey
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct9_01">
             16:30-16:45, Paper ThCT9.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1952'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Fast Motion and Foothold Planning Framework for Legged Robot on Discrete Terrain
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392156" title="Click to go to the Author Index">
             Yu, Jiyu
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#256804" title="Click to go to the Author Index">
             Wang, Dongqi
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397539" title="Click to go to the Author Index">
             Chen, Zhenghan
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#276081" title="Click to go to the Author Index">
             Chen, Ci
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397558" title="Click to go to the Author Index">
             Wu, Shuangpeng
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#156231" title="Click to go to the Author Index">
             Wang, Yue
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#113216" title="Click to go to the Author Index">
             Xiong, Rong
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1952" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#whole_body_motion_planning_and_control" title="Click to go to the Keyword Index">
               Whole-Body Motion Planning and Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Legged robot proved their capability to cross complex terrain in recent research, yet the autonomy of robots on discrete terrain still needs to be enhanced since it requires a full stack framework. This paper introduces a real-time motion and foothold planning framework tailored for legged robots navigating uneven terrains, such as stepping stones. Our approach addresses the critical challenges of determining feasible global paths and local footholds to enhance autonomous mobility across complex landscapes. By using a sampling-based global path planner integrated with terrain segmentation and the robot's kinematic model, our framework swiftly generates viable navigation paths. Concurrently, it utilizes a Mixed Integer Programming (MIP) methodology for real-time foothold optimization, ensuring the robot's stability and safety through dynamic terrain interaction. Finally, an execution layer including Model Predictive Control (MPC) and Whole-Body Control (WBC) generates the robots' motion. Simulation and real-world experiments demonstrate that our framework improves legged robots' adaptability on discrete terrains.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct9_02">
             16:45-17:00, Paper ThCT9.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2276'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Robot Active Vision-Based Path Planning for Localization Improvement in Indoor Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397658" title="Click to go to the Author Index">
             Barlakas, Sotirios
            </a>
           </td>
           <td class="r">
            Centre for Research and Technology Hellas / Information Technolo
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#320236" title="Click to go to the Author Index">
             Alexiou, Dimitrios
            </a>
           </td>
           <td class="r">
            Centre for Research and Technology Hellas (CERTH)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#299441" title="Click to go to the Author Index">
             Tsiakas, Kosmas
            </a>
           </td>
           <td class="r">
            Centre for Research and Technology Hellas (CERTH)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397667" title="Click to go to the Author Index">
             Katsatos, Dimitrios
            </a>
           </td>
           <td class="r">
            Centre for Research &amp; Technology Hellas (CERTH)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#155741" title="Click to go to the Author Index">
             Kostavelis, Ioannis
            </a>
           </td>
           <td class="r">
            Center for Research and Technology Hellas
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#191969" title="Click to go to the Author Index">
             Giakoumis, Dimitris
            </a>
           </td>
           <td class="r">
            Centre for Research and Technology Hellas
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#129384" title="Click to go to the Author Index">
             Gasteratos, Antonios
            </a>
           </td>
           <td class="r">
            Democritus University of Thrace
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#191967" title="Click to go to the Author Index">
             Tzovaras, Dimitrios
            </a>
           </td>
           <td class="r">
            Centre for Research and Technology Hellas
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2276" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#reactive_and_sensor_based_planning" title="Click to go to the Keyword Index">
               Reactive and Sensor-Based Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Reliable and robust navigation of autonomous mobile robots in indoor environments faces significant challenges due to the absence of GPS, visual degradation, repetitive structures, illumination variations, and low texture. These factors adversely affect localization systems. Current robots often use a uniform navigation approach, regardless of the varying localization uncertainties within different indoor environments. In this paper, we propose a holistic, active vision-based path planning method that produces efficient trajectories, aiming to minimize localization error and enhance navigation performance. Specifically, we utilize a 3D model of an indoor environment to derive an Artificial Potential Field (APF) with its associated localizability scores that encapsulate both visual features' richness and fiducial markers' placement. APF is employed to direct a Kinematically Constrained Bi-directional Rapidly Exploring Random Tree (KB-RRT) planner towards the calculation of optimal paths, prioritizing high localization areas. Subsequently, we use an online weight-adaptive MPC-based approach that, apart from robust path planning and obstacle avoidance, guides the robot towards areas with the most robust visual features in order to further refine the localization error. The proposed framework has been extensively tested in both simulation and real-world experiments with a mobile robot in a visually challenging indoor environment.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct9_03">
             17:00-17:15, Paper ThCT9.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2281'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              GSRM: Building Roadmaps for Query-Efficient and Near-Optimal Path Planning Using a Reaction Diffusion System
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#182639" title="Click to go to the Author Index">
             Henkel, Christian
            </a>
           </td>
           <td class="r">
            Robert Bosch GmbH
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#102561" title="Click to go to the Author Index">
             Toussaint, Marc
            </a>
           </td>
           <td class="r">
            TU Berlin
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#183179" title="Click to go to the Author Index">
             Hoenig, Wolfgang
            </a>
           </td>
           <td class="r">
            TU Berlin
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2281" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#autonomous_vehicle_navigation" title="Click to go to the Keyword Index">
               Autonomous Vehicle Navigation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#planning__scheduling_and_coordination" title="Click to go to the Keyword Index">
               Planning, Scheduling and Coordination
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Mobile robots frequently navigate on roadmaps, i.e., graphs where edges represent safe motions, in applications such as healthcare, hospitality, and warehouse automation. Often the environment is quasi-static, i.e., it is sufficient to construct a roadmap once and then use it for any future planning queries. Roadmaps are typically used with graph search algorithm to find feasible paths for the robots. Therefore, the roadmap should be well-connected, and graph searches should produce near-optimal solutions with short solution paths while simultaneously be computationally efficient to execute queries quickly.
             <p>
              We propose a new method to construct roadmaps based on the Gray-Scott reaction diffusion system and Delaunay triangulation. Our approach, GSRM, produces roadmaps with evenly distributed vertices and edges that are well-connected even in environments with challenging narrow passages. Empirically, we compare to classical roadmaps generated by 8-connected grids, probabilistic roadmaps (PRM, SPARS2), and optimized roadmap graphs (ORM). Our results show that GSRM consistently produces superior roadmaps that are well-connected, have high query efficiency, and result in short solution paths.
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct9_04">
             17:15-17:30, Paper ThCT9.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2563'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              IDb-RRT: Sampling-Based Kinodynamic Motion Planning with Motion Primitives and Trajectory Optimization
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#212772" title="Click to go to the Author Index">
             Ortiz-Haro, Joaquim
            </a>
           </td>
           <td class="r">
            TU Berlin
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#183179" title="Click to go to the Author Index">
             Hoenig, Wolfgang
            </a>
           </td>
           <td class="r">
            TU Berlin
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#274858" title="Click to go to the Author Index">
             Hartmann, Valentin
            </a>
           </td>
           <td class="r">
            ETH Zrich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#102561" title="Click to go to the Author Index">
             Toussaint, Marc
            </a>
           </td>
           <td class="r">
            TU Berlin
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#107817" title="Click to go to the Author Index">
             Righetti, Ludovic
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2563" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Rapidly-exploring Random Trees (RRT) and its variations have emerged as a robust and efficient tool for finding collision-free paths in robotic systems. However, adding dynamic constraints makes the motion planning problem significantly harder, as it requires solving two-value boundary problems (computationally expensive) or propagating random control inputs (uninformative). Alternatively, Iterative Discontinuity Bounded A* (iDb-A*), introduced in our previous study, combines search and optimization iteratively. The search step connects short trajectories (motion primitives) while allowing a bounded discontinuity between the motion primitives, which is later repaired in the trajectory optimization step. Building upon these foundations, in this paper, we present iDb-RRT, a sampling-based kinodynamic motion planning algorithm that combines motion primitives and trajectory optimization within the RRT framework. iDb-RRT is probabilistically complete and can be implemented in forward or bidirectional mode. We have tested our algorithm across a benchmark suite comprising 30 problems, spanning 8 different systems, and shown that iDb-RRT can find solutions up to 10x faster than previous methods, especially in complex scenarios that require long trajectories or involve navigating through narrow passages.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct10">
             <b>
              ThCT10
             </b>
            </a>
           </td>
           <td class="r">
            Room 10
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thct10" title="Click to go to the Program at a Glance">
             <b>
              Data Sets for Robotic Vision II
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#197869" title="Click to go to the Author Index">
             Erich, Floris Marc Arden
            </a>
           </td>
           <td class="r">
            National Institute of Advanced Industrial Science and Technology
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct10_01">
             16:30-16:45, Paper ThCT10.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('88'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              <i>
               PEGASUS
              </i>
              : Physically Enhanced Gaussian Splatting Simulation System for 6DoF Object Pose Dataset Generation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#387267" title="Click to go to the Author Index">
             Meyer, Lukas
            </a>
           </td>
           <td class="r">
            Friedrich-Alexander-Universitt Erlangen-Nrnberg
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#197869" title="Click to go to the Author Index">
             Erich, Floris Marc Arden
            </a>
           </td>
           <td class="r">
            National Institute of Advanced Industrial Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#225225" title="Click to go to the Author Index">
             Yoshiyasu, Yusuke
            </a>
           </td>
           <td class="r">
            CNRS-AIST JRL
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#293605" title="Click to go to the Author Index">
             Stamminger, Marc
            </a>
           </td>
           <td class="r">
            Universitt Erlangen-Nrnberg
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#100205" title="Click to go to the Author Index">
             Ando, Noriaki
            </a>
           </td>
           <td class="r">
            National Institute of Advanced Industrial Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#109212" title="Click to go to the Author Index">
             Domae, Yukiyasu
            </a>
           </td>
           <td class="r">
            The National Institute of Advanced Industrial Science and Techno
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab88" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#data_sets_for_robotic_vision" title="Click to go to the Keyword Index">
               Data Sets for Robotic Vision
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#simulation_and_animation" title="Click to go to the Keyword Index">
               Simulation and Animation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We introduce Physically Enhanced Gaussian Splatting Simulation System (
             <i>
              PEGASUS
             </i>
             ) for 6DOF object pose dataset generation, a versatile dataset generator based on 3D Gaussian Splatting.
             <p>
              Environment and object representations can be easily obtained using commodity cameras to reconstruct with Gaussian Splatting.
              <i>
               PEGASUS
              </i>
              allows the	composition of new scenes by merging the respective underlying Gaussian Splatting point cloud of an environment with one or multiple objects. Leveraging a physics engine enables the simulation of natural object placement within a scene through interaction between meshes extracted for the objects and the environment. Consequently, an extensive amount of new scenes - static or dynamic - can be created by combining different environments and objects. By rendering scenes from various perspectives, diverse data points such as RGB images, depth maps, semantic masks, and 6DoF object poses can be extracted.
              <p>
               Our study demonstrates that training on data generated by
               <i>
                PEGASUS
               </i>
               enables pose estimation networks to successfully transfer from synthetic data to real-world data. Moreover, we introduce the Ramen dataset, comprising 30 Japanese cup noodle items. This dataset includes spherical scans that captures images from both object hemisphere and the Gaussian Splatting reconstruction, making them compatible with
               <i>
                PEGASUS
               </i>
               .
              </p>
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct10_02">
             16:45-17:00, Paper ThCT10.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('404'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Two Teachers Are Better Than One: Leveraging Depth in Training Only for Unsupervised Obstacle Segmentation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#246107" title="Click to go to the Author Index">
             Eum, Sungmin
            </a>
           </td>
           <td class="r">
            U.S. Army Research Laboratory, Booz Allen Hamilton Inc
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#183483" title="Click to go to the Author Index">
             Lee, Hyungtae
            </a>
           </td>
           <td class="r">
            US Army Research Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#184470" title="Click to go to the Author Index">
             Kwon, Heesung
            </a>
           </td>
           <td class="r">
            DEVCOM Army Research Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#150496" title="Click to go to the Author Index">
             Osteen, Philip
            </a>
           </td>
           <td class="r">
            U.S. Army Research Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#385014" title="Click to go to the Author Index">
             Harrison, Andre
            </a>
           </td>
           <td class="r">
            U.S. Army Research Laboratory
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab404" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#data_sets_for_robotic_vision" title="Click to go to the Keyword Index">
               Data Sets for Robotic Vision
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present a novel unsupervised obstacle segmentation architecture that follows a novel Relation Distillation (RD) paradigm. Our architecture design was inspired by a self-supervised teacher-student approach that relies on the Semantic Distillation originally devised for representation learning. While the teacher in the Semantic Distillation considers a single patch at a time, the teacher within RD takes a 'pair of patches' instead to transfer the local Semantic Co-occurrence Localization (SCooL) relationship that focuses more on the segmentation-boosting signals. To further improve the proposed architecture, we introduce the utilization of another teacher that leverages the depth information which inherently separates the entities at different physical distances, often tied with the boundaries of the obstacles. As the depth is distilled towards the student network only at the time of training, it adds zero computational/hardware cost at run-time. As no relevant public dataset is available, we have curated the Avoiding Obstacles In unstructured Driving (AvOID) dataset as a new testbed for unsupervised obstacle segmentation. We have validated that both the Relation Distillation and depth contribute to boosting the no-annotation segmentation performance on AvOID and KITTI-Obstacles.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct10_03">
             17:00-17:15, Paper ThCT10.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('584'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Visual Place Recognition in Unstructured Driving Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392746" title="Click to go to the Author Index">
             Rai, Utkarsh
            </a>
           </td>
           <td class="r">
            CVIT, IIIT Hyderababd
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392736" title="Click to go to the Author Index">
             Gangisetty, Shankar
            </a>
           </td>
           <td class="r">
            IIIT Hyderabad
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#104143" title="Click to go to the Author Index">
             Abdul Hafez, A. H.
            </a>
           </td>
           <td class="r">
            Hasan Kalyoncu Uiversity
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#297233" title="Click to go to the Author Index">
             Subramanian, Anbumani
            </a>
           </td>
           <td class="r">
            Intel / IIIT-Hyderabad
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#106597" title="Click to go to the Author Index">
             Jawahar, C.V.
            </a>
           </td>
           <td class="r">
            IIIT, Hyderabad
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab584" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#data_sets_for_robotic_vision" title="Click to go to the Keyword Index">
               Data Sets for Robotic Vision
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#intelligent_transportation_systems" title="Click to go to the Keyword Index">
               Intelligent Transportation Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The problem of determining geolocation through visual inputs, known as Visual Place Recognition (VPR), has attracted significant attention in recent years owing to its potential applications in autonomous self-driving systems. The rising interest in these applications poses unique challenges, particularly the necessity for datasets encompassing unstructured environmental conditions to facilitate the development of robust VPR methods. In this paper, we address the VPR challenges by proposing an Indian driving VPR dataset that caters to the semantic diversity of unstructured driving environments like occlusions due to dynamic environments, variations in traffic density, viewpoint variability, and variability in lighting conditions. In unstructured driving environments, GPS signals are unreliable often affecting the vehicle to accurately determine location. To address this challenge, we develop an interactive image-to-image tagging annotation tool to annotate large datasets with ground truth annotations for VPR training. Evaluation of the state-of-the-art methods on our dataset shows a significant performance drop of up to 15%, defeating a large number of standard VPR datasets. We also provide an exhaustive quantitative and qualitative experimental analysis of frontal-view, multi-view, and sequence-matching methods. We believe that our dataset will open new challenges for the VPR research community to build robust models. Project Page: https://cvit.iiit.ac.in/research/projects/cvit-projects/iddvpr
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct10_04">
             17:15-17:30, Paper ThCT10.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('851'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              OTVIC: A Dataset with Online Transmission for Vehicle-To-Infrastructure Cooperative 3D Object Detection
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#388620" title="Click to go to the Author Index">
             Zhu, He
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#286826" title="Click to go to the Author Index">
             Wang, Yunkai
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#388842" title="Click to go to the Author Index">
             Kong, Quyu
            </a>
           </td>
           <td class="r">
            Alibaba Cloud
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#379112" title="Click to go to the Author Index">
             Wei, Yufei
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#388839" title="Click to go to the Author Index">
             Xia, Xunlong
            </a>
           </td>
           <td class="r">
            Alibaba Cloud
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#388865" title="Click to go to the Author Index">
             Deng, Bing
            </a>
           </td>
           <td class="r">
            Alibaba Cloud
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#113216" title="Click to go to the Author Index">
             Xiong, Rong
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#156231" title="Click to go to the Author Index">
             Wang, Yue
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab851" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#data_sets_for_robotic_vision" title="Click to go to the Keyword Index">
               Data Sets for Robotic Vision
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Vehicle-to-infrastructure cooperative 3D object detection (VIC3D) is a task that leverages both vehicle and roadside sensors to jointly perceive the surrounding environment. However, considering the high speed of vehicles, the real-time requirements, and the limitations of communication bandwidth, roadside devices transmit the results of perception rather than raw sensor data or feature maps in our real-world scenarios. And affected by various environmental factors, the transmission delay is dynamic. To meet the needs of practical applications, we present OTVIC, which is the first multi-modality and multi-view dataset with online transmission from real scenes for vehicle-to-infrastructure cooperative 3D object detection. The ego-vehicle receives the results of infrastructure perception in real-time, collected from a section of highway in Chengdu, China. Moreover, we propose LfFormer, which is a novel end-to-end multi-modality late fusion framework with transformer for VIC3D task as a baseline based on OTVIC. Experiments prove our fusion framework's effectiveness and robustness.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct11">
             <b>
              ThCT11
             </b>
            </a>
           </td>
           <td class="r">
            Room 11
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thct11" title="Click to go to the Program at a Glance">
             <b>
              Multi-Robot Systems VI
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#270580" title="Click to go to the Author Index">
             Bray, Edward
            </a>
           </td>
           <td class="r">
            University of Technology Sydney
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#164680" title="Click to go to the Author Index">
             Min, Byung-Cheol
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct11_01">
             16:30-16:45, Paper ThCT11.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2888'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Learning Coordinated Maneuver in Adversarial Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#350374" title="Click to go to the Author Index">
             Hu, Zechen
            </a>
           </td>
           <td class="r">
            George Mason University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#354437" title="Click to go to the Author Index">
             Limbu, Manshi
            </a>
           </td>
           <td class="r">
            George Mason University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#235890" title="Click to go to the Author Index">
             Shishika, Daigo
            </a>
           </td>
           <td class="r">
            George Mason University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#177144" title="Click to go to the Author Index">
             Xiao, Xuesu
            </a>
           </td>
           <td class="r">
            George Mason University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#280358" title="Click to go to the Author Index">
             Wang, Xuan
            </a>
           </td>
           <td class="r">
            George Mason University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2888" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#planning__scheduling_and_coordination" title="Click to go to the Keyword Index">
               Planning, Scheduling and Coordination
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#path_planning_for_multiple_mobile_robots_or_agents" title="Click to go to the Keyword Index">
               Path Planning for Multiple Mobile Robots or Agents
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper aims to solve the coordination of a team of robots traversing a route in the presence of adversaries with random positions. Our goal is to minimize the overall cost of the team, which is determined by (i) the accumulated risk when robots stay in adversary-impacted zones and (ii) the mission completion time. During traversal, robots can reduce their speed and act as a guard (the slower, the better), which will decrease the risks certain adversary incurs. This leads to a trade-off between the robots guarding behaviors and their travel speeds. The formulated problem is highly non-convex and cannot be efficiently solved by existing algorithms. We employ reinforcement learning techniques by developing new encoding and policy-generating methods. Simulations demonstrate that our learning methods can efficiently produce team coordination behaviors. We discuss the reasoning behind these behaviors and explain why they reduce the overall team cost.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct11_02">
             16:45-17:00, Paper ThCT11.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3270'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Multi-Goal Path Planning in Cluttered Environments with PRM-Guided Self-Organising Maps
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#399017" title="Click to go to the Author Index">
             Davis, Benjamin R.
            </a>
           </td>
           <td class="r">
            University of Technology, Sydney
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#270580" title="Click to go to the Author Index">
             Bray, Edward
            </a>
           </td>
           <td class="r">
            University of Technology Sydney
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#185051" title="Click to go to the Author Index">
             Best, Graeme
            </a>
           </td>
           <td class="r">
            University of Technology Sydney
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3270" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#path_planning_for_multiple_mobile_robots_or_agents" title="Click to go to the Keyword Index">
               Path Planning for Multiple Mobile Robots or Agents
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We consider the problem of multi-robot, multi-goal path planning in cluttered environments, motivated by scenarios including surveillance, object search, and package delivery in crowded office spaces and urban environments. While many solutions have been proposed for related vehicle routing problems, they typically do not generalise well for cluttered environments with obstacles due to the introduction of non-Euclidean point-to-point distances. We consider a Self-Organising Map (SOM) algorithm due to its versatility in optimising waypoints within region-based goals. Since standard SOM heavily relies on Euclidean distance-based operations, we propose a generalised SOM with several new innovations: Probabilistic Roadmap (PRM)-guided adaptation and winner selection rules, a two-level path representation for effective routing between goals, and caching operations to overcome the increased computational demands. We present simulation experiments in office and maze environments with one to three robots that show that our approach significantly outperforms standard SOM algorithms as it explicitly reasons over collision avoidance. These results demonstrate the viability of our PRM-guided SOM algorithm for tasks including surveillance in cluttered environments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct11_03">
             17:00-17:15, Paper ThCT11.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3295'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Learning from Demonstration Framework for Multi-Robot Systems Using Interaction Keypoints and Soft Actor-Critic Methods
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#243484" title="Click to go to the Author Index">
             Venkatesh, L.N Vishnunandan
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#164680" title="Click to go to the Author Index">
             Min, Byung-Cheol
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3295" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Learning from Demonstration (LfD) is a promising approach to enable Multi-Robot Systems (MRS) to acquire complex skills and behaviors. However, the intricate interactions and coordination challenges in MRS pose significant hurdles for effective LfD. In this paper, we present a novel LfD framework specifically designed for MRS, which leverages visual demonstrations to capture and learn from robot-robot and robot-object interactions. Our framework introduces the concept of Interaction Keypoints (IKs) to transform the visual demonstrations into a representation that facilitates the inference of various skills necessary for the task. The robots then execute the task using sensorimotor actions and reinforcement learning (RL) policies when required. A key feature of our approach is the ability to handle unseen contact-based skills that emerge during the demonstration. In such cases, RL is employed to learn the skill using a classifier-based reward function, eliminating the need for manual reward engineering and ensuring adaptability to environmental changes. We evaluate our framework across a range of mobile robot tasks, covering both behavior-based and contact-based domains. The results demonstrate the effectiveness of our approach in enabling robots to learn complex multi-robot tasks and behaviors from visual demonstrations.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct11_04">
             17:15-17:30, Paper ThCT11.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1387'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Energy Sharing Mechanism for Freeform Robots Utilizing Conductive Spherical Sliding Surfaces
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#320874" title="Click to go to the Author Index">
             Li, Xin-Zhuo
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong, Shenzhen
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#226856" title="Click to go to the Author Index">
             Tu, Yuxiao
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong, Shenzhen
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#267777" title="Click to go to the Author Index">
             Liang, Guanqi
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong, Shenzhen
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#266582" title="Click to go to the Author Index">
             Wu, Di
            </a>
           </td>
           <td class="r">
            Central South University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#113090" title="Click to go to the Author Index">
             Lam, Tin Lun
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong, Shenzhen
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1387" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#cellular_and_modular_robots" title="Click to go to the Keyword Index">
               Cellular and Modular Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#swarm_robotics" title="Click to go to the Keyword Index">
               Swarm Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Energy sharing among modular robots enables sustainable operation of the system by maintaining energy balance among the modules. In this paper, we propose a novel energy sharing mechanism for FreeSN, a modular self-reconfigurable robot consisting of node and strut modules. Utilizing the feature that our modules are connected in a face-to-face manner, our method successfully establishes an energy sharing channel at almost any point on a sphere by placing transmission intermediaries at the interfacing face between modules, which is facilitated by the combination of brush contact and shell decomposition. Such mechanism also allows the utilization of the node module's inner space for extra energy storage. A prototype of this energy sharing system has been implemented on FreeSN and rigorously tested. Our findings indicate that energy sharing is reliably established between modules; for strut modules positioned randomly on a node module's surface, the probability of forming a valid connection is 56.6%. With orientation adjustment, a connection is achievable at nearly any position on the sphere, barring a few exceptional points. As a result, the operational endurance of the strut modules, which provide all the driving forces in the system, is markedly enhanced. This technique also holds potential for broader application across other freeform robotic platforms that incorporate conductive spherical surfaces for sliding connections.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct12">
             <b>
              ThCT12
             </b>
            </a>
           </td>
           <td class="r">
            Room 12
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thct12" title="Click to go to the Program at a Glance">
             <b>
              Model Learning for Control
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#189210" title="Click to go to the Author Index">
             Zuo, Guoyu
            </a>
           </td>
           <td class="r">
            Beijing University of Technology
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct12_01">
             16:30-16:45, Paper ThCT12.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2378'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Arm-Constrained Curriculum Learning for Loco-Manipulation of a Wheel-Legged Robot
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#394464" title="Click to go to the Author Index">
             Wang, Zifan
            </a>
           </td>
           <td class="r">
            The Hong Kong University of Science and Technology (Guangzhou)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#339530" title="Click to go to the Author Index">
             Jia, Yufei
            </a>
           </td>
           <td class="r">
            Department of Electronic Engineering, Tsinghua University, China
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#269171" title="Click to go to the Author Index">
             Shi, Lu
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397158" title="Click to go to the Author Index">
             Wang, Haoyu
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#340582" title="Click to go to the Author Index">
             Zhao, Haizhou
            </a>
           </td>
           <td class="r">
            Xi'an Jiaotong-Liverpool University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397137" title="Click to go to the Author Index">
             Li, Xueyang
            </a>
           </td>
           <td class="r">
            Discovver Robotics
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#204347" title="Click to go to the Author Index">
             Zhou, Jinni
            </a>
           </td>
           <td class="r">
            Hong Kong University of Science and Technology (Guangzhou)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#182083" title="Click to go to the Author Index">
             Ma, Jun
            </a>
           </td>
           <td class="r">
            The Hong Kong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#165639" title="Click to go to the Author Index">
             Zhou, Guyue
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2378" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#multi_contact_whole_body_motion_planning_and_control" title="Click to go to the Keyword Index">
               Multi-Contact Whole-Body Motion Planning and Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Incorporating a robotic manipulator into a wheel-legged robot enhances its agility and expands its potential for practical applications. However, the presence of potential instability and uncertainties presents additional challenges for control objectives. In this paper, we introduce an arm-constrained curriculum learning architecture to tackle the issues introduced by adding the manipulator. Firstly, we develop an arm-constrained reinforcement learning algorithm to ensure safety and reliability in control performance after equipping the manipulator. Additionally, to address discrepancies in reward settings between the arm and the base, we propose a reward-aware curriculum learning method. The policy is first trained in Isaac gym and transferred to the physical robot to complete grasping tasks, including the door-opening task, fan-twitching task and the relay-baton-picking and following task. The results demonstrate that our proposed approach effectively controls the arm-equipped wheel-legged robot to master grasping abilities including the dynamic grasping skills, allowing it to chase and catch a moving object while in motion. Please refer to our website (https://acodedog.github.io/wheel-legged-loco-manipulation/) for the code and supplemental videos.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct12_02">
             16:45-17:00, Paper ThCT12.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1682'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Control-Oriented Reinforcement Active Modeling Scheme for Hysteresis Compensation of Flexible Endoscopic Robot
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#288627" title="Click to go to the Author Index">
             Ren, Fan
            </a>
           </td>
           <td class="r">
            Nankai University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#281207" title="Click to go to the Author Index">
             Wang, Xiangyu
            </a>
           </td>
           <td class="r">
            Nankai University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#114198" title="Click to go to the Author Index">
             Fang, Yongchun
            </a>
           </td>
           <td class="r">
            Nankai University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#162608" title="Click to go to the Author Index">
             Qin, Yanding
            </a>
           </td>
           <td class="r">
            Nankai University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#122638" title="Click to go to the Author Index">
             Wang, Hongpeng
            </a>
           </td>
           <td class="r">
            Nankai University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#119052" title="Click to go to the Author Index">
             Yu, Ningbo
            </a>
           </td>
           <td class="r">
            Nankai University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#102226" title="Click to go to the Author Index">
             Han, Jianda
            </a>
           </td>
           <td class="r">
            Nankai University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1682" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#model_learning_for_control" title="Click to go to the Keyword Index">
               Model Learning for Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#surgical_robotics__steerable_catheters_needles" title="Click to go to the Keyword Index">
               Surgical Robotics: Steerable Catheters/Needles
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Hysteresis has posed significant challenges to the modeling and control of flexible endoscopic robots, which impedes the advancement of automated endoscopic operation. Despite numerous hysteresis modeling approaches aimed at improving accuracy, there are still several unresolved issues, such as inappropriate model selection and non-ideal assumption of noise. Focusing on these challenges, a novel reinforcement active modeling (RAM) scheme is proposed in this paper. By incorporating reinforcement learning, this method augments an Extended Kalman Filter (EKF)-based active modeling strategy, which improves the insensitivity and generalization ability to non-Gaussian noise that is not introduced in training. Finally, a series of comparative experiments are conducted on the self-built flexible endoscopic robot to validate the improvement achieved by the proposed scheme. Compared with some widely-applied methods, the proposed scheme achieved at least 63.8% improvement in the root mean square error (RMSE) in modeling accuracy under Gaussian noise conditions, and at least 36.5% improvement in RMSE under Poisson noise conditions.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct12_03">
             17:00-17:15, Paper ThCT12.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2018'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Bridging the Sim-To-Real Gap with Bayesian Inference
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#218792" title="Click to go to the Author Index">
             Rothfuss, Jonas
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#324290" title="Click to go to the Author Index">
             Sukhija, Bhavya
            </a>
           </td>
           <td class="r">
            ETH Zrich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397660" title="Click to go to the Author Index">
             Treven, Lenart
            </a>
           </td>
           <td class="r">
            ETH Zrich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#151679" title="Click to go to the Author Index">
             Dorfler, Florian
            </a>
           </td>
           <td class="r">
            ETH Zrich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#147028" title="Click to go to the Author Index">
             Coros, Stelian
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#115203" title="Click to go to the Author Index">
             Krause, Andreas
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2018" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#model_learning_for_control" title="Click to go to the Keyword Index">
               Model Learning for Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#probabilistic_inference" title="Click to go to the Keyword Index">
               Probabilistic Inference
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present SIM-FSVGD for learning robot dynamics from data. As opposed to traditional methods, SIM-FSVGD leverages low-fidelity physical priors, e.g., in the form of simulators, to regularize the training of neural network models. While learning accurate dynamics already in the low data regime, SIM-FSVGD scales and excels also when more data is available. We empirically show that learning with implicit physical priors results in accurate mean model estimation as well as precise uncertainty quantification. We demonstrate the effectiveness of SIM-FSVGD in bridging the sim-to-real gap on a high-performance RC racecar system. Using model-based RL, we demonstrate a highly dynamic parking maneuver with drifting, using less than half the data compared to the state of the art.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct12_04">
             17:15-17:30, Paper ThCT12.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2865'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Cascaded Broad Learning System for Manipulator Motion Control
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#189210" title="Click to go to the Author Index">
             Zuo, Guoyu
            </a>
           </td>
           <td class="r">
            Beijing University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#374937" title="Click to go to the Author Index">
             Dong, Shuaifeng
            </a>
           </td>
           <td class="r">
            Beijing University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#275065" title="Click to go to the Author Index">
             Zhou, Jiyong
            </a>
           </td>
           <td class="r">
            Beijing University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#196054" title="Click to go to the Author Index">
             Yu, Shuangyue
            </a>
           </td>
           <td class="r">
            Beijing University of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2865" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#motion_control" title="Click to go to the Keyword Index">
               Motion Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#model_learning_for_control" title="Click to go to the Keyword Index">
               Model Learning for Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#industrial_robots" title="Click to go to the Keyword Index">
               Industrial Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Intelligent control methods have led to a significant simplification of the robotic arm modeling and control tuning process, and thus they have been widely used. To further improve the precision of robotic arm motion control, this paper proposes a robotic arm motion control strategy based on a cascaded feature-enhanced elastic-net broad learning system (CFE-EN-BLS). This will fully extract data features to improve motion control accuracy. Moreover, ElasticNet regression is introduced to reduce feature redundancy. Finally, Lyapunov stability theory is introduced to constrain the learning parameters of the proposed learning method to enhance the convergence of the control strategy. The simulation and experiment show that the proposed control strategy can realize high-precision trajectory tracking control of the robotic arm.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct13">
             <b>
              ThCT13
             </b>
            </a>
           </td>
           <td class="r">
            Room 13
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thct13" title="Click to go to the Program at a Glance">
             <b>
              Computer Vision for Automation I
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#102317" title="Click to go to the Author Index">
             Triebel, Rudolph
            </a>
           </td>
           <td class="r">
            German Aerospace Center (DLR)
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct13_01">
             16:30-16:45, Paper ThCT13.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('188'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Visual Quality Inspection Planning: A Model-Based Framework for Generating Optimal and Feasible Inspection Poses
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#390437" title="Click to go to the Author Index">
             Staderini, Vanessa
            </a>
           </td>
           <td class="r">
            AIT Austrian Institute of Technology GmbH
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#217384" title="Click to go to the Author Index">
             Glck, Tobias
            </a>
           </td>
           <td class="r">
            AIT Austrian Institute of Technology GmbH
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#390435" title="Click to go to the Author Index">
             Schneider, Philipp
            </a>
           </td>
           <td class="r">
            Center for Vision, Automation &amp; Control, AIT Austrian Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#115936" title="Click to go to the Author Index">
             Kugi, Andreas
            </a>
           </td>
           <td class="r">
            TU Wien
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab188" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#kinematics" title="Click to go to the Keyword Index">
               Kinematics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Automatic visual quality inspection is pivotal in both computer vision and robotics. It plays a crucial role in manufacturing, where robotic systems are increasingly employed to enhance the speed and efficiency of visual quality assessments. Several inspection planning methodologies have been developed; however, they often address the inspection challenge from a singular perspective of robotics or computer vision. This work introduces a comprehensive approach that synergistically integrates principles from both domains. We present an innovative algorithm designed to generate optimal inspection poses by considering the interplay between the inspected object's geometry and the kinematics of the robotic setup used for inspection. This is accomplished by taking advantage of the concept of visibility. The effectiveness of our algorithm is demonstrated through simulations and experiments, demonstrating complete coverage for diverse geometries and materials with a small number of inspection poses. Moreover, we benchmark our framework against box constraints and workspace sampling techniques to generate feasible inspection poses. The results indicate superior performance in achieving extensive coverage and reducing the number of required optimal inspection poses, enhancing the overall inspection process.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct13_02">
             16:45-17:00, Paper ThCT13.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('196'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Progressive Query Refinement Framework for Bird's-Eye-View Semantic Segmentation from Surrounding Images
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#386153" title="Click to go to the Author Index">
             Choi, Dooseop
            </a>
           </td>
           <td class="r">
            ETRI
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#390600" title="Click to go to the Author Index">
             Kang, Jungyu
            </a>
           </td>
           <td class="r">
            Electronics and Telecommunications Research Institute (ETRI)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#390625" title="Click to go to the Author Index">
             An, Taeg-Hyun
            </a>
           </td>
           <td class="r">
            Electronics and Telecommunications Research Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#188052" title="Click to go to the Author Index">
             An, Kyounghwan
            </a>
           </td>
           <td class="r">
            ETRI
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#390596" title="Click to go to the Author Index">
             KyoungWook, Min
            </a>
           </td>
           <td class="r">
            ETRI
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab196" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Expressing images with Multi-Resolution (MR) features has been widely adopted in many computer vision tasks. In this paper, we introduce the MR concept into Bird's-Eye-View (BEV) semantic segmentation for autonomous driving. This introduction enhances our model's ability to capture both global and local characteristics of driving scenes through our proposed residual learning. Specifically, given a set of MR BEV query maps, the lowest resolution query map is initially updated using a View Transformation (VT) encoder. This updated query map is then upscaled and merged with a higher resolution query map to undergo further updates in a subsequent VT encoder. This process is repeated until the resolution of the updated query map reaches the target. Finally, the lowest resolution map is added to the target resolution to generate the final query map. During training, we enforce both the lowest and final query maps to align with the ground-truth BEV semantic map to help our model effectively capture the global and local characteristics. We also propose a visual feature interaction network that promotes the interaction between features across images and feature levels, thus highly contributing to the performance improvement. We evaluate our model on a large-scale real-world dataset. The experimental results show that our model outperforms the SOTA models in terms of IoU metric. We will make our code publicly available in the near future.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct13_03">
             17:00-17:15, Paper ThCT13.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('290'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Adv3D: Generating 3D Adversarial Examples for 3D Object Detection in Driving Scenarios with NeRF
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#390093" title="Click to go to the Author Index">
             Li, Leheng
            </a>
           </td>
           <td class="r">
            HKUST(GZ)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#379253" title="Click to go to the Author Index">
             Lian, Qing
            </a>
           </td>
           <td class="r">
            HKUST
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#344888" title="Click to go to the Author Index">
             Chen, Yingcong
            </a>
           </td>
           <td class="r">
            Hong Kong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab290" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#simulation_and_animation" title="Click to go to the Keyword Index">
               Simulation and Animation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Deep neural networks (DNNs) have been proven extremely susceptible to adversarial examples, which raises special safety-critical concerns for DNN-based autonomous driving stacks i.e., 3D object detection). Although there are extensive works on image-level attacks, most are restricted to 2D pixel spaces, and such attacks are not always physically realistic in our 3D world. Here we present Adv3D, the first exploration of modeling adversarial examples as Neural Radiance Fields (NeRFs) in driving scenarios. Advances in NeRF provide photorealistic appearances and 3D accurate generation, yielding a more realistic and realizable adversarial example. We train our adversarial NeRF by minimizing the surrounding objects' confidence predicted by 3D detectors on the training set. Then we evaluate Adv3D on the unseen validation set and show that it can cause a large performance reduction when rendering NeRF in any sampled pose. To enhance physical effectiveness, we propose primitive-aware sampling and semantic-guided regularization that enable 3D patch attacks with camouflage adversarial texture. Experimental results demonstrate that our method surpasses the mesh baseline and generalizes well to different poses, scenes, and 3D detectors. Finally, we provide a defense method to our attacks that improves both the robustness and clean performance of 3D detectors.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct13_04">
             17:15-17:30, Paper ThCT13.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('401'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Outlier-Robust Geometric Perception: A Novel Thresholding-Based Estimator with Intra-Class Variance Maximization
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#302563" title="Click to go to the Author Index">
             Sun, Lei
            </a>
           </td>
           <td class="r">
            East China University of Science and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab401" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Geometric perception problems are fundamental tasks in robotics and computer vision. In real-world applications, they often encounter the inevitable issue of outliers, preventing traditional algorithms from making correct estimates. In this paper, we present a novel general-purpose robust estimator TIVM (Thresholding with Intra-class Variance Maximization) that can collaborate with standard non-minimal solvers to efficiently reject outliers for geometric perception problems. First, we introduce the technique of intra-class variance maximization to design a dynamic 2-group thresholding method on the measurement residuals, aiming to distinctively separate inliers from outliers. Then, we develop an iterative framework that robustly optimizes the model by approaching the pure-inlier group using a multi-layered dynamic thresholding strategy as subroutine, in which a self-adaptive mechanism for layer-number tuning is further employed to minimize the user-defined parameters. We validate the proposed estimator on 3 classic geometric perception problems: rotation averaging, point cloud registration and category-level perception, and experiments show that it is robust against 70-90% of outliers and can converge typically in only 3-15 iterations, much faster than state-of-the-art robust solvers such as RANSAC, GNC and ADAPT. Furthermore, another highlight is that: our estimator can retain approximately the same level of robustness even when the inlier-noise statistics of the problem are fully unknown.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt1">
             <b>
              ThDT1
             </b>
            </a>
           </td>
           <td class="r">
            Room 1
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thdt1" title="Click to go to the Program at a Glance">
             <b>
              SLAM IV
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt1_01">
             17:30-17:45, Paper ThDT1.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2341'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              AS-LIO: Spatial Overlap Guided Adaptive Sliding Window LiDAR-Inertial Odometry for Aggressive FOV Variation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392979" title="Click to go to the Author Index">
             Zhang, Tianxiang
            </a>
           </td>
           <td class="r">
            Wuhan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392990" title="Click to go to the Author Index">
             Zhang, Xuanxuan
            </a>
           </td>
           <td class="r">
            Wuhan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392285" title="Click to go to the Author Index">
             Liao, Zongbo
            </a>
           </td>
           <td class="r">
            WuHan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#283337" title="Click to go to the Author Index">
             Xia, Xin
            </a>
           </td>
           <td class="r">
            University of California, Los Angeles
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#318825" title="Click to go to the Author Index">
             Li, You
            </a>
           </td>
           <td class="r">
            Wuhan University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2341" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             LiDAR-Inertial Odometry (LIO) demonstrates outstanding accuracy and stability in general low-speed and smooth motion scenarios. However, in high-speed and intense motion scenarios, such as sharp turns, two primary challenges arise: firstly, due to the limitations of IMU frequency, the error in estimating significantly non-linear motion states escalates; secondly, drastic changes in the Field of View (FOV) may diminish the spatial overlap between LiDAR frame and pointcloud map (or between frames), leading to insufficient data association and constraint degradation.
             <p>
              To address these issues, we propose a novel Adaptive Sliding window LIO framework (AS-LIO) guided by the Spatial Overlap Degree (SOD). Initially, we assess the SOD between the LiDAR frames and the registered map, directly evaluating the adverse impact of current FOV variation on pointcloud alignment. Subsequently, we design an adaptive sliding window to manage the continuous LiDAR stream and control state updates, dynamically adjusting the update step according to the SOD. This strategy enables our odometry to adaptively adopt higher update frequency to precisely characterize trajectory during aggressive FOV variation, thus effectively reducing the non-linear error in positioning. Meanwhile, the historical constraints within the sliding window reinforce the frame-to-map data association, ensuring the robustness of state estimation. Experiments show that our AS-LIO framework can quickly perceive and respond to challenging FOV change, outperforming other state-of-the-art LIO frameworks in terms of accuracy and robustness.
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt1_02">
             17:45-18:00, Paper ThDT1.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2524'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              DDS-SLAM: Dense Semantic Neural SLAM for Deformable Endoscopic Scenes
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#379522" title="Click to go to the Author Index">
             Shan, Jiwei
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#379525" title="Click to go to the Author Index">
             Li, Yirui
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392892" title="Click to go to the Author Index">
             Yang, Lujia
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392899" title="Click to go to the Author Index">
             Feng, Qiyu
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#256999" title="Click to go to the Author Index">
             Han, Lijun
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#103003" title="Click to go to the Author Index">
             Wang, Hesheng
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2524" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Estimating camera motion and continuously reconstructing dense scenes in deformable environments presents a complex and open challenge. Many existing approaches tend to rely on assumptions about the scene's topology or the nature of deformable motion. However, these assumptions do not hold true in medical endoscopy applications. To address these challenges, we introduce DDS-SLAM, a novel dense deformable semantic neural SLAM that achieves accurate camera tracking, continuous dense scene reconstruction, and high-quality image rendering in deformable scenes. First, we propose a novel hybrid neural scene representation method capable of capturing both natural and artificial deformations. Additionally, by leveraging the 2D semantic information of the scene, we introduce a semantic loss function based on semantic distance fields. This approach guides network optimization at a higher level, thereby enhancing system performance. Furthermore, we validate our method through a series of experiments conducted on several representative medical datasets, demonstrating its superiority over other state-of-the-art approaches. The code is available at: https://github.com/IRMVLab/DDS-SLAM.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt1_03">
             18:00-18:15, Paper ThDT1.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3171'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              MUP-LIO: Mapping Uncertainty-Aware Point-Wise Lidar Inertial Odometry
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398982" title="Click to go to the Author Index">
             Yao, Hekai
            </a>
           </td>
           <td class="r">
            Dalian University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#197302" title="Click to go to the Author Index">
             Zhang, Xuetao
            </a>
           </td>
           <td class="r">
            Dalian University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#350160" title="Click to go to the Author Index">
             Sun, Gang
            </a>
           </td>
           <td class="r">
            Dalian University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#168878" title="Click to go to the Author Index">
             Liu, Yisha
            </a>
           </td>
           <td class="r">
            Dalian Maritime University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#122338" title="Click to go to the Author Index">
             Zhang, Xuebo
            </a>
           </td>
           <td class="r">
            Nankai University,
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#140896" title="Click to go to the Author Index">
             Zhuang, Yan
            </a>
           </td>
           <td class="r">
            Dalian University of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3171" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper proposes a mapping uncertainty-aware point-wise Lidar Inertial Odometry (LIO), which synthesizes the point-wise point-to-plane match and map refreshment into a probabilistic model. As a result, it can address the issue of mismatching during point registration and remove in-frame motion distortion of Lidar sensors. Specifically, the uncertainty-aware map is designed to embody the uncertainty of map geometric features (points and planes), which comes from the Lidar point measurement and pose estimation. Then the map can be modeled in a probabilistic form. In addition, the proposed framework refreshes map at each Lidar point measurement to timely revise geometric features and provide non-delayed map. On the basis, the probabilistic point-to-plane match method is designed to seek a corresponding plane for each Lidar point in point registration, which can enhance the effectiveness of match and provide adaptive observation noises for more accurate state estimation. Comparative experiments on various public datasets are conducted to demonstrate the superior performance of the proposed framework in terms of higher accuracy and better robustness.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt1_04">
             18:15-18:30, Paper ThDT1.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3225'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              STL-SLAM: A Structured-Constrained RGB-D SLAM Approach to Texture-Limited Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#373931" title="Click to go to the Author Index">
             Dong, Juan
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#378745" title="Click to go to the Author Index">
             Lu, Maobin
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#202850" title="Click to go to the Author Index">
             Chen, Chen
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#379113" title="Click to go to the Author Index">
             Deng, Fang
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#116497" title="Click to go to the Author Index">
             Chen, Jie
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3225" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#visual_tracking" title="Click to go to the Keyword Index">
               Visual Tracking
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Most RGB-D-based SLAM methods assume texture-rich environments, making them susceptible to significant tracking errors or complete failures in the absence of texture features. Moreover, many existing methods encounter substantial rotation estimation errors, leading to long-term drift in tracking. This paper proposes a novel structuredconstrained RGB-D SLAM method (STL-SLAM) for texturelimited environments. Compared to the existing methods, STLSLAM can deal with environments without abundant texture information and significantly reduce long-term drift caused by rotation estimation errors.We assess the distribution complexity of pixels in an image by calculating the information entropy and pre-processing accordingly. We also present an efficient Manhattan Frames (MF) detection strategy based on orthogonal planes and lines. If MF is detected, we decouple rotation and translation, estimate drift-free rotation based on the Manhattan World (MW) coordinate system, and then estimate translation by minimizing the re-projection error of point, line, and plane features. In non-Manhattan Frames, the 6-DoF pose estimation is performed holistically, with the incorporation of structural constraints of parallel and perpendicular planes, as well as parallel and vertical lines, into the optimization process. Finally, we evaluate our method on public datasets and in real-world environments, which shows that our proposed method achieves superior performance compared to its counterparts.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt2">
             <b>
              ThDT2
             </b>
            </a>
           </td>
           <td class="r">
            Room 2
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thdt2" title="Click to go to the Program at a Glance">
             <b>
              Modeling, Tracking and Simulating Humans
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#244803" title="Click to go to the Author Index">
             Bombieri, Nicola
            </a>
           </td>
           <td class="r">
            University of Verona
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#105102" title="Click to go to the Author Index">
             Lee, Jangwon
            </a>
           </td>
           <td class="r">
            Sungkyunkwan University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt2_01">
             17:30-17:45, Paper ThDT2.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2594'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Autonomous Behavior Planning for Humanoid Loco-Manipulation through Grounded Language Model
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#303065" title="Click to go to the Author Index">
             Wang, Jin
            </a>
           </td>
           <td class="r">
            Italian Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#208752" title="Click to go to the Author Index">
             Laurenzi, Arturo
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#105401" title="Click to go to the Author Index">
             Tsagarakis, Nikos
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2594" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#behavior_based_systems" title="Click to go to the Keyword Index">
               Behavior-Based Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#humanoid_robot_systems" title="Click to go to the Keyword Index">
               Humanoid Robot Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Enabling humanoid robots to perform autonomously loco-manipulation in unstructured environments is crucial and highly challenging for achieving embodied intelligence. This involves robots being able to plan their actions and behaviors in long-horizon tasks while using multi-modality to perceive deviations between task execution and high-level planning. Recently, large language models (LLMs) have demonstrated powerful planning and reasoning capabilities for comprehension and processing of semantic information through robot control tasks, as well as the usability of analytical judgment and decision-making for multi-modal inputs. To leverage the power of LLMs towards humanoid loco-manipulation, we propose a novel language-model based framework that enables robots to autonomously plan behaviors and low-level execution under given textual instructions, while observing and correcting failures that may occur during task execution. To systematically evaluate this framework in grounding LLMs, we created the robot 'action' and 'sensing' behavior library for task planning, and conducted mobile manipulation tasks and experiments in both simulated and real environments using the CENTAURO robot, and verified the effectiveness and application of this approach in robotic tasks with autonomous behavioral planning. Video: https://youtu.be/mmnaxthEX34
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt2_02">
             17:45-18:00, Paper ThDT2.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2763'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Real-Time Filter for Human Pose Estimation Based on Denoising Diffusion Models for Edge Devices
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398019" title="Click to go to the Author Index">
             Bozzini, Chiara
            </a>
           </td>
           <td class="r">
            University of Verona
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#295090" title="Click to go to the Author Index">
             Boldo, Michele
            </a>
           </td>
           <td class="r">
            University of Verona
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#295086" title="Click to go to the Author Index">
             Martini, Enrico
            </a>
           </td>
           <td class="r">
            Universit Di Verona
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#244803" title="Click to go to the Author Index">
             Bombieri, Nicola
            </a>
           </td>
           <td class="r">
            University of Verona
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2763" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#modeling_and_simulating_humans" title="Click to go to the Keyword Index">
               Modeling and Simulating Humans
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#human_and_humanoid_motion_analysis_and_synthesis" title="Click to go to the Keyword Index">
               Human and Humanoid Motion Analysis and Synthesis
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#human_detection_and_tracking" title="Click to go to the Keyword Index">
               Human Detection and Tracking
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Human Pose Estimation (HPE) is increasingly utilized across various sectors, from healthcare to Industry 5.0. To address the inherent inaccuracies in CNN-based HPE systems, filtering models are commonly employed to refine and improve inference results. However, state-of-the-art filtering models often require substantial computational resources, limiting their applicability in resource-constrained environments. To overcome this limitation, we propose a real-time filtering approach based on denoising diffusion models (DM) specifically optimized for edge devices. Through a micro-benchmarking process, we analyze the DM adaptability to different types and levels of noise and determine the optimal setup for specific application scenarios. We present a real-time filter that takes advantage of the DM setup with two configurations to address different application scenarios. Using a widespread edge device, we evaluate the model's effectiveness in handling both synthetic and real noise generated by state-of-the-art HPE systems. The results demonstrate a significant improvement in real-time filtering performance with minimal computational overhead. The code is available on github.com/PARCO-LAB/LUT-DM-filters.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt2_03">
             18:00-18:15, Paper ThDT2.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2290'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SFTrack: A Robust Scale and Motion Adaptive Algorithm for Tracking Small and Fast Moving Objects
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#364132" title="Click to go to the Author Index">
             Song, Inpyo
            </a>
           </td>
           <td class="r">
            Sungkyunkwan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#105102" title="Click to go to the Author Index">
             Lee, Jangwon
            </a>
           </td>
           <td class="r">
            Sungkyunkwan University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2290" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#human_detection_and_tracking" title="Click to go to the Keyword Index">
               Human Detection and Tracking
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper addresses the problem of multi-object tracking in Unmanned Aerial Vehicle (UAV) footage. It plays a critical role in various UAV applications, including traffic monitoring systems and real-time suspect tracking by the police. However, this task is highly challenging due to the fast motion of UAVs, as well as the small size of target objects in the videos caused by the high-altitude and wide-angle views of drones. In this study, we thus introduce a simple yet more effective method compared to previous work to overcome these challenges. Our approach involves a new tracking strategy, which initiates the tracking of target objects from low-confidence detections commonly encountered in UAV application scenarios. Additionally, we propose revisiting traditional appearance-based matching algorithms to improve the association of low-confidence detections. To evaluate the effectiveness of our method, we conducted benchmark evaluations on two UAV-specific datasets (VisDrone2019, UAVDT) and one general object tracking dataset (MOT17). The results demonstrate that our approach surpasses current state-of-the-art methodologies, highlighting its robustness and adaptability in diverse tracking environments. Furthermore, we have improved the annotation of the UAVDT dataset by rectifying several errors and addressing omissions found in the original annotations. We will provide this refined version of the dataset to facilitate better benchmarking in the field.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt2_04">
             18:15-18:30, Paper ThDT2.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1355'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Hybrid Human Tracking System Using UWB Sensors and Monocular Visual Data Fusion for Human Following Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#285604" title="Click to go to the Author Index">
             Zhang, Dingzhi
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#368702" title="Click to go to the Author Index">
             Birner, Lukas
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#317718" title="Click to go to the Author Index">
             Pancheri, Felix
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#330076" title="Click to go to the Author Index">
             Rehekampff, Christoph
            </a>
           </td>
           <td class="r">
            Technische Universitt Mnchen
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#105286" title="Click to go to the Author Index">
             Burschka, Darius
            </a>
           </td>
           <td class="r">
            Technische Universitaet Muenchen
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#101537" title="Click to go to the Author Index">
             Lueth, Tim C.
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1355" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#human_detection_and_tracking" title="Click to go to the Keyword Index">
               Human Detection and Tracking
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#sensor_fusion" title="Click to go to the Keyword Index">
               Sensor Fusion
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#embedded_systems_for_robotic_and_automation" title="Click to go to the Keyword Index">
               Embedded Systems for Robotic and Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The ability to follow people can benefit the human-robot interaction of mobile robots. This work proposes a hybrid human tracking system for human following robots, integrating sensor fusion of Ultra-Wideband (UWB) and monocular visual positioning to enhance tracking accuracy and precision. At the same time, UWB and the visual positioning system can operate independently, thereby creating a redundancy in the system. Based on our previous study of UWB-positioning, this article elaborates on a visual positioning system that employs human detection using a pre-trained Convolutional Neural Network (CNN), coupled with data fusion process based on experimental assessments. The hybrid human tracking system achieves a 2D Euclidean accuracy RMS of 7.4 cm, demonstrating sufficient accuracy for human following and improving the following performance in real-world experiments compared to our previous study.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt3">
             <b>
              ThDT3
             </b>
            </a>
           </td>
           <td class="r">
            Room 3
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thdt3" title="Click to go to the Program at a Glance">
             <b>
              Deep Learning in Grasping and Manipulation II
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#144407" title="Click to go to the Author Index">
             Walas, Krzysztof, Tadeusz
            </a>
           </td>
           <td class="r">
            Poznan University of Technology
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt3_01">
             17:30-17:45, Paper ThDT3.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('417'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Tactile Active Inference Reinforcement Learning for Efficient Robotic Manipulation Skill Acquisition
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#377374" title="Click to go to the Author Index">
             Liu, Zihao
            </a>
           </td>
           <td class="r">
            Northwestern Polytechnical University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#208237" title="Click to go to the Author Index">
             Liu, Xing
            </a>
           </td>
           <td class="r">
            Northwestern Polytechnical University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#214164" title="Click to go to the Author Index">
             Liu, Zhengxiong
            </a>
           </td>
           <td class="r">
            Northwestern Polytechnical University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#135120" title="Click to go to the Author Index">
             Zhang, Yizhai
            </a>
           </td>
           <td class="r">
            Northwestern Polytechnical University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#102722" title="Click to go to the Author Index">
             Huang, Panfeng
            </a>
           </td>
           <td class="r">
            Northwestern Polytechnical University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab417" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#force_and_tactile_sensing" title="Click to go to the Keyword Index">
               Force and Tactile Sensing
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robotic manipulation holds the potential to replace humans in the execution of tedious or dangerous tasks. However, control-based approaches are not suitable due to the difficulty of formally describing open-world manipulation in reality, and the inefficiency of existing learning methods. Therefore, applying manipulation in a wide range of scenarios presents significant challenges. In this study, we propose a novel framework for skill learning in robotic manipulation called Tactile Active Inference Reinforcement Learning (Tactile-AIRL), aimed at achieving efficient learning. To enhance the performance of reinforcement learning (RL), we introduce active inference, which integrates model-based techniques and intrinsic curiosity into the RL process. This integration improves the algorithm's training efficiency and adaptability to sparse rewards. Additionally, we have designed universal tactile static and dynamic features based on vision-based tactile sensors, making our framework scalable to many manipulation tasks learning involving tactile feedback. Simulation results demonstrate that our method achieves significantly high training efficiency in objects pushing tasks. It enables agents to excel in both dense and sparse reward tasks with just few interaction episodes, surpassing the SAC baseline. Furthermore, we conduct physical experiments on a gripper screwing task using our method, which showcases the algorithm's rapid learning capability and its potential for practical applications.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt3_02">
             17:45-18:00, Paper ThDT3.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('975'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              RelationGrasp: Object-Oriented Prompt Learning for Simultaneously Grasp Detection and Manipulation Relationship in Open Vocabulary
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#374958" title="Click to go to the Author Index">
             Liu, Songting
            </a>
           </td>
           <td class="r">
            NUS
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#104493" title="Click to go to the Author Index">
             Teo, Tat Joo
            </a>
           </td>
           <td class="r">
            Singapore Institute of Manufacturing Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#276163" title="Click to go to the Author Index">
             Lin, Zhiping
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#173622" title="Click to go to the Author Index">
             Zhu, Haiyue
            </a>
           </td>
           <td class="r">
            Agency for Science, Technology and Research (A*STAR)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab975" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#grasping" title="Click to go to the Keyword Index">
               Grasping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Autonomous robotic grasping under complex, clustered, and unstructured environments is a fundamental but challenging task. To achieve human-like rationality in dealing with the grasping task, the agent requires hybrid intelligence from multilateral aspects. This paper introduces RelationGrasp, a unified framework employing a transformer encoder-decoder structure to simultaneously achieve open-vocabulary object detection, manipulation relationship inference, and grasp pose detection. A unique object-oriented prompt learning mechanism is designed to seamlessly bridge the grasp pose and manipulation relationship branches, delivering high fidelity of object-grasp affiliation for object-aware grasping and grasp sequence planning. By formulating the relationship detection as an adjacency matrix regression task under multi-task learning, our framework significantly increases the relationship accuracy with reduced computational overhead. Moreover, to facilitate the robust and adaptive deployment of the proposed RelationGrasp to novel environments, we propose a consistency-based self-supervised adaptation strategy to adapt the pre-trained network to new scenarios and improve grasp accuracy on unseen objects. Our proposed network achieved state-of-the-art performance on various public dataset such as VMRD, OCID, etc., in both grasp detection and manipulation relationship classification, and real-world robot experiments has also been conducted to show the practical usages.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt3_03">
             18:00-18:15, Paper ThDT3.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2379'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Adapting Skills to Novel Grasps: A Self-Supervised Approach
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#286276" title="Click to go to the Author Index">
             Papagiannis, Georgios
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#299665" title="Click to go to the Author Index">
             Dreczkowski, Kamil
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#312452" title="Click to go to the Author Index">
             Vosylius, Vitalis
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#137912" title="Click to go to the Author Index">
             Johns, Edward
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2379" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper, we study the problem of adapting manipulation trajectories involving grasped objects (e.g. tools) defined for a single grasp pose to novel grasp poses. A common approach to address this is to define a new trajectory for each possible grasp explicitly, but this is highly inefficient. Instead, we propose a method to adapt such trajectories directly while only requiring a period of self-supervised data collection, during which a camera observes the robot's end-effector moving with the object rigidly grasped. Importantly, our method requires no prior knowledge of the grasped object (such as a 3D CAD model), it can work with RGB images, depth images, or both, and it requires no camera calibration. Through a series of real-world experiments involving 1360 evaluations, we find that self-supervised RGB data consistently outperforms alternatives that rely on depth images including several state-of-the-art pose estimation methods. Compared to the best-performing baseline, our method results in an average of 28.5% higher success rate when adapting manipulation trajectories to novel grasps on several everyday tasks. The appendix accompanying the paper and videos of the experiments are available on our webpage at www.robot-learning.uk/adapting-skills.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt3_04">
             18:15-18:30, Paper ThDT3.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2964'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              ManiFoundation Model for General-Purpose Robotic Manipulation of Contact Synthesis with Arbitrary Objects and Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#351843" title="Click to go to the Author Index">
             Xu, Zhixuan
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#285264" title="Click to go to the Author Index">
             Gao, Chongkai
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398809" title="Click to go to the Author Index">
             Liu, Zixuan
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#354078" title="Click to go to the Author Index">
             Yang, Gang
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#378532" title="Click to go to the Author Index">
             Tie, Chenrui
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397675" title="Click to go to the Author Index">
             Zheng, Haozhuo
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#352050" title="Click to go to the Author Index">
             Zhou, Haoyu
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#372765" title="Click to go to the Author Index">
             Weikun, Peng
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398814" title="Click to go to the Author Index">
             Wang, Debang
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398947" title="Click to go to the Author Index">
             Hu, Tianrun
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#379928" title="Click to go to the Author Index">
             Chen, Tianyi
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397697" title="Click to go to the Author Index">
             Yu, Zhouliang
            </a>
           </td>
           <td class="r">
            The Hong Kong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#217322" title="Click to go to the Author Index">
             Shao, Lin
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2964" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#big_data_in_robotics_and_automation" title="Click to go to the Keyword Index">
               Big Data in Robotics and Automation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             To substantially enhance robot intelligence, there is a pressing need to develop a large model that enables general-purpose robots to proficiently undertake a broad spectrum of manipulation tasks, akin to the versatile task-planning ability exhibited by LLMs. The vast diversity in objects, robots, and manipulation tasks presents huge challenges. Our work introduces a comprehensive framework to develop a foundation model for general robotic manipulation that formalizes a manipulation task as contact synthesis. Specifically, our model takes as input object and robot manipulator point clouds, object physical attributes, target motions, and manipulation region masks. It outputs contact points on the object and associated contact forces or post-contact motions for robots to achieve the desired manipulation task. We perform extensive experiments both in the simulation and real-world settings, manipulating articulated rigid objects, rigid objects, and deformable objects that vary in dimensionality, ranging from one-dimensional objects like ropes to two-dimensional objects like cloth and extending to three-dimensional objects such as plasticine. Our model achieves average success rates of around 90%. Supplementary materials and videos are available on our project website at https://manifoundationmodel.github.io/.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt4">
             <b>
              ThDT4
             </b>
            </a>
           </td>
           <td class="r">
            Room 4
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thdt4" title="Click to go to the Program at a Glance">
             <b>
              Soft Robot Applications II
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#152432" title="Click to go to the Author Index">
             Haninger, Kevin
            </a>
           </td>
           <td class="r">
            Fraunhofer IPK
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt4_01">
             17:30-17:45, Paper ThDT4.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3492'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Embedded 3d Printing of Silicone for Soft Actuator with Stiffness Gradient and Programmable Workspace
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#394596" title="Click to go to the Author Index">
             Xiao, Fei
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong, Shenzhen
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#399231" title="Click to go to the Author Index">
             Wei, Zhuoheng
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong, Shenzhen
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#313912" title="Click to go to the Author Index">
             Wang, Hao
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong, Shenzhen
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#253047" title="Click to go to the Author Index">
             Li, Jisen
            </a>
           </td>
           <td class="r">
            Shenzhen Institute of Artificial Intelligence and Robotics for S
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#176251" title="Click to go to the Author Index">
             Zhu, Jian
            </a>
           </td>
           <td class="r">
            Chinese University of Hong Kong, Shenzhen
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3492" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#additive_manufacturing" title="Click to go to the Keyword Index">
               Additive Manufacturing
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#soft_robot_materials_and_design" title="Click to go to the Keyword Index">
               Soft Robot Materials and Design
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#soft_robot_applications" title="Click to go to the Keyword Index">
               Soft Robot Applications
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Soft pneumatic actuators can accomplish various customizable deformation/motion through the distribution of cavities and gradients in stiffness. However, traditional manufacturing methods, say molding, struggle to produce soft actuators with both complex cavities and desirable stiffness distributions. Regular 3D printing methods usually need extra printheads for support materials to fabricate soft actuators with cavities. In addition, the printing quality and fidelity of the whole structure cannot be uniform due to the effect of gravity, especially for a soft actuator with overhang features. To fabricate a soft actuator of uniform fidelity but desirable stiffness distributions, we propose an embedded 3D printing approach with only one active mixing printhead. By adjusting the mixing ratio of the dual-component silicone, we can achieve desinated stiffness gradients, ranging from 30.2 kPa to 198 kPa. With this approach, we successfully fabricate soft pneumatic actuators with overhang features, which exhibit programmable elongation and radial expansion. Additionally, we fabricate soft bending actuators which can achieve programmable workspaces due to their predetermined stiffness distribution.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt4_02">
             17:45-18:00, Paper ThDT4.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1820'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Soft Finger Rotational Stability for Precision Grasps
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#361162" title="Click to go to the Author Index">
             Jang, Hun
            </a>
           </td>
           <td class="r">
            Ulsan National Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#377352" title="Click to go to the Author Index">
             Petrichenko, Valentyn
            </a>
           </td>
           <td class="r">
            Fraunhofer IPK
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#109405" title="Click to go to the Author Index">
             Bae, Joonbum
            </a>
           </td>
           <td class="r">
            Korea University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#152432" title="Click to go to the Author Index">
             Haninger, Kevin
            </a>
           </td>
           <td class="r">
            Fraunhofer IPK
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1820" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#modeling__control__and_learning_for_soft_robots" title="Click to go to the Keyword Index">
               Modeling, Control, and Learning for Soft Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#grasping" title="Click to go to the Keyword Index">
               Grasping
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#soft_robot_applications" title="Click to go to the Keyword Index">
               Soft Robot Applications
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Soft robotic fingers can safely grasp fragile or variable form objects, but their force capacity is limited, especially with less contact area: precision grasps and when objects are smaller or not spherical. Although most research focuses on improving force capacity through mechanical design modifications, optimizing grasping parameters for precision tasks remains crucial. To address this problem, this paper proposes an analytical rotational stability model for soft fingers precision grasping, considering grip failure involving slip and dynamic rotational stability. Comprehensive experiments across various objects, grip condition and types of fingers (PneuNet and commercial fingers) are conducted by examining the relationship between grasp parameters and model variables including coulomb friction, bulk stiffness, and dynamic stability. The findings demonstrate the models utility in identifying optimal grip parameters that enhance the force capacity of soft fingers without causing dynamic instability. This research contributes to the development of more effective and stable soft robotic fingers for precision grasping tasks.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt4_03">
             18:00-18:15, Paper ThDT4.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2166'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Strain-Based Modeling of Rod-Driven Soft Continuum Robots with Co-Located Embedded Sensors
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#301999" title="Click to go to the Author Index">
             Wang, Peiyi
            </a>
           </td>
           <td class="r">
            National University of Singapore (NUS)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#268726" title="Click to go to the Author Index">
             Feliu, Daniel
            </a>
           </td>
           <td class="r">
            Khalifa University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#238245" title="Click to go to the Author Index">
             Guo, Sheng
            </a>
           </td>
           <td class="r">
            Beijing Jiaotong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#150250" title="Click to go to the Author Index">
             Renda, Federico
            </a>
           </td>
           <td class="r">
            Khalifa University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#101847" title="Click to go to the Author Index">
             Laschi, Cecilia
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2166" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#modeling__control__and_learning_for_soft_robots" title="Click to go to the Keyword Index">
               Modeling, Control, and Learning for Soft Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#soft_sensors_and_actuators" title="Click to go to the Keyword Index">
               Soft Sensors and Actuators
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#soft_robot_applications" title="Click to go to the Keyword Index">
               Soft Robot Applications
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Rod-driven soft robots (RDSR) with a well-balanced performance in terms of perception, precision, and intelligence have a great potential for application. Mathematical description and predicted sensing of deformable soft bodies are crucial to achieve controllable and intelligent behaviors of these robots. In this work, we propose a kinetostatic model for RDSR embedded with co-located sensors based on the Geometric Variable Strain (GVS) approach where local deformations, actuation lengths and external interactions are included. This approach allows us to estimate the shape of RDSR and predict the strain variation of soft bodies under internal and external interactions. Simulations and experimental results show that tip position errors are not greater than 1.8% with respect to the whole body length under different loads (0, 100, 200, 300 gf). The maximum error of predicted sensor length change is up to 2 mm and its percentage relative to the actual length does not exceed 4%. The results demonstrate the accuracy and effectiveness of the proposed model.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt4_04">
             18:15-18:30, Paper ThDT4.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('304'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              S-BUN: Soft Bifunctional Utility Module for Robot Sensing and Signaling
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#391352" title="Click to go to the Author Index">
             Mahuttanatan, Suksakaow
            </a>
           </td>
           <td class="r">
            University of the Arts London, Central Saint Martins
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#275162" title="Click to go to the Author Index">
             Asawalertsak, Naris
            </a>
           </td>
           <td class="r">
            Vidyasirimedhi Institute of Science and Technology (VISTEC)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#391396" title="Click to go to the Author Index">
             Paripurana, Jinjuta
            </a>
           </td>
           <td class="r">
            Kamnoetvidya Science Academy
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#345638" title="Click to go to the Author Index">
             Tarapongnivat, Kanut
            </a>
           </td>
           <td class="r">
            Vidyasirimedhi Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#309041" title="Click to go to the Author Index">
             Chuthong, Thirawat
            </a>
           </td>
           <td class="r">
            Vidyasirimedhi Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#120785" title="Click to go to the Author Index">
             Manoonpong, Poramate
            </a>
           </td>
           <td class="r">
            Vidyasirimedhi Institute of Science and Technology (VISTEC)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab304" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#soft_robot_materials_and_design" title="Click to go to the Keyword Index">
               Soft Robot Materials and Design
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#soft_sensors_and_actuators" title="Click to go to the Keyword Index">
               Soft Sensors and Actuators
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#soft_robot_applications" title="Click to go to the Keyword Index">
               Soft Robot Applications
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Conventional approaches in robotics for perceiving the environment and signaling the robots state or intention for human-robot interaction involve the use of separate sensing and signaling systems. This can sometimes result in high costs and complex system installations. In this study, we propose an alternative approach that integrates both robot sensing and signaling mechanisms into a single utility module (called S-BUN, Soft Bifunctional Utility module for robot sensing aNd signaling). Soft material (Ecoflex 00-10 silicone) is used to form its bun-like structure with a central cavity filled with a NaCl solution. Inspired by honeycombs, the modules surface incorporates a hexagonal pattern to enhance structural robustness. The design of S-BUN enables it to function as a sensor for both noncontact proximity and touch sensing, utilizing the NaCl solution. Additionally, it serves as a signaling mechanism to indicate the robots state through active inflation and deflation dynamics of the module, simulating lifelike breathing patterns. Through our experiments, we present S-BUNs capabilities in proximity and touch sensing, including its ability to discern various touch intensities. Finally, we demonstrate the application of S-BUN in the context of reactive behavioral control for a crawling robot and in human-robot interaction scenarios.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt5">
             <b>
              ThDT5
             </b>
            </a>
           </td>
           <td class="r">
            Room 5
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thdt5" title="Click to go to the Program at a Glance">
             <b>
              Robot Design
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#101894" title="Click to go to the Author Index">
             Stefanini, Cesare
            </a>
           </td>
           <td class="r">
            Scuola Superiore Sant'Anna
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt5_01">
             17:30-17:45, Paper ThDT5.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2886'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Design of a Soft Shell for a Spherical Exploration Robot Traversing Varying Terrain
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#340759" title="Click to go to the Author Index">
             Dravid, Meghali Prashant
            </a>
           </td>
           <td class="r">
            Texas A&amp;M University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#340728" title="Click to go to the Author Index">
             Oevermann, Micah
            </a>
           </td>
           <td class="r">
            Texas A&amp;M University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#383260" title="Click to go to the Author Index">
             McDougall, David
            </a>
           </td>
           <td class="r">
            Texas A&amp;M University, College Station
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#383261" title="Click to go to the Author Index">
             Dugas, David
            </a>
           </td>
           <td class="r">
            Texas A&amp;M University, College Station
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#337083" title="Click to go to the Author Index">
             Ambrose, Robert
            </a>
           </td>
           <td class="r">
            Texas A&amp;M University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2886" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#soft_robot_applications" title="Click to go to the Keyword Index">
               Soft Robot Applications
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#soft_robot_materials_and_design" title="Click to go to the Keyword Index">
               Soft Robot Materials and Design
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#space_robotics_and_automation" title="Click to go to the Keyword Index">
               Space Robotics and Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Exploration robots are typically equipped with either wheels or legs for mobility. However, these conventional designs inherently possess certain limitations, such as restricted ability to navigate through steep slopes, soft soil or muddy watery terrain. Despite the advancements in robotics technology, these challenges persist in both terrestrial and extraterrestrial environments. To overcome these obstacles, we have proposed a novel solution in the form of a spherical robot with a soft shell. This paper provides an overview of the construction of this spherical robot and the design of its soft shell for adapting to varying terrain. Furthermore, we analyze and characterize its performance across various terrain types as the shell pressure is changed. By employing this approach, the paper establishes the suitability and efficiency of the spherical robot in traversing different terrain by adapting the shell pressure.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt5_02">
             17:45-18:00, Paper ThDT5.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3081'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Design of a Fully Actuated Drone with Non-Isotropic Wrench Shape
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395588" title="Click to go to the Author Index">
             Park, Seongsu
            </a>
           </td>
           <td class="r">
            KAIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#149446" title="Click to go to the Author Index">
             Kim, Min Jun
            </a>
           </td>
           <td class="r">
            KAIST
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3081" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#methods_and_tools_for_robot_system_design" title="Click to go to the Keyword Index">
               Methods and Tools for Robot System Design
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper proposes a novel design framework for a fully actuated drone with a non-isotropic wrench shape. Conventional fully actuated drones face challenges related to high energy consumption during aerial contact manipulation, particularly when force exertion along a specific direction is required. This challenge arises from the balanced tilting of propellers, which leads to an isotropic wrench shape. To address this limitation, we explicitly define the required wrench set (RW) for aerial contact manipulation and integrate it into an optimization problem. To ensure the generation of the RW, we employ the hyperplane shifting method, commonly used for verifying wrench feasibility in cable-driven robots. The optimization aims to minimize hovering energy consumption while ensuring wrench feasibility. Consequently, the proposed design demonstrates a significant improvement over the typical fully actuated drone, with hovering and contact force efficiency more than doubled and nearly 1.4 times higher, respectively. The effectiveness of our design is also validated through simulation.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt5_03">
             18:00-18:15, Paper ThDT5.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3173'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Novel Design of Reconfigurable Tracked Robot with Geometry-Changing Tracks
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396905" title="Click to go to the Author Index">
             Xuan, Chice
            </a>
           </td>
           <td class="r">
            Huzhou Institute of Zhejiang University, Huzhou
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397008" title="Click to go to the Author Index">
             Lu, Jiadong
            </a>
           </td>
           <td class="r">
            Zhejiang Universisy, Huzhou Institute of Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397022" title="Click to go to the Author Index">
             Tian, Zhihao
            </a>
           </td>
           <td class="r">
            Nanjing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396344" title="Click to go to the Author Index">
             Li, Jiacheng
            </a>
           </td>
           <td class="r">
            Huzhou Institute of Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#337145" title="Click to go to the Author Index">
             Zhang, Mengke
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396941" title="Click to go to the Author Index">
             Xie, Hanbin
            </a>
           </td>
           <td class="r">
            Huzhou Institute of Zhejiang University, Huzhou
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396958" title="Click to go to the Author Index">
             Qiu, Jianxiong
            </a>
           </td>
           <td class="r">
            Zhejiang Zhongyan Industry Co. Ltd
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#213352" title="Click to go to the Author Index">
             Xu, Chao
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#178926" title="Click to go to the Author Index">
             Cao, Yanjun
            </a>
           </td>
           <td class="r">
            Zhejiang University, Huzhou Institute of Zhejiang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3173" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#search_and_rescue_robots" title="Click to go to the Keyword Index">
               Search and Rescue Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mechanism_design" title="Click to go to the Keyword Index">
               Mechanism Design
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#wheeled_robots" title="Click to go to the Keyword Index">
               Wheeled Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Tracked robots with reconfigurable mechanisms exhibit great maneuverability due to their adaptability to complex ground conditions. Reconfigurable tracked robots with geometry-changing tracks show further obstacle-crossing capabilities with compact dimensions. However, existing systems face deployment limitations due to either complex transmission mechanisms or unsustainable designs when maintain the tension in the tracks. To address these challenges, we introduce a novel design of a reconfigurable tracked robot with geometry-changing tracks, which achieves strong terrain traversability with good mechanical properties. We achieve the elliptical trajectory of key planetary wheels through a novel Quad-slider Elliptical Trammel Mechanism (Qs-ETM), allowing the tracks to maintain fixed tension while changing their geometry. Furthermore, the combination of direct drive motors significantly enhances its mechanical properties and agility. A detailed analysis of the kinematic and dynamic characteristics is conducted, and is proved with a series of simulations. We built a fully functional prototype of the design and tested it in real-world experiments to validate its advantages. The result shows that our design can reduce the torque required by up to 68.3% and the shear stress of the flipper by up to 67.1%.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt5_04">
             18:15-18:30, Paper ThDT5.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3189'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Self-Assessment of Robotic Laboratory and Equipment Readiness Using Large Language Models and Robotic Data Capture
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#354647" title="Click to go to the Author Index">
             Ili, Stefan
            </a>
           </td>
           <td class="r">
            EPFL
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#194194" title="Click to go to the Author Index">
             Hughes, Josie
            </a>
           </td>
           <td class="r">
            EPFL
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3189" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#process_control" title="Click to go to the Keyword Index">
               Process Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#manufacturing__maintenance_and_supply_chains" title="Click to go to the Keyword Index">
               Manufacturing, Maintenance and Supply Chains
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#intelligent_and_flexible_manufacturing" title="Click to go to the Keyword Index">
               Intelligent and Flexible Manufacturing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This study explores the potential of automating robotic laboratory readiness assessment by integrating Large Language Models (LLMs) with robotic data acquisition. It investigates the capability of LLMs to detect equipment motion and operational status using visual and auditory information. Despite the challenges LLMs face in spatial analysis, this study also investigates LLM grounding methods to ensure accurate workspace assessment. By inspecting a robotic cooking setup with camera-equipped robotic arm, LLMs can detect the motion of custom equipment via color-coded marks, and identify the operational status of kitchen appliances from a single image without any physical augmentations. Additionally, device operation perceived through the emission of loud noises can be assessed by post-processing sound recordings and analyzing loudness and sound frequency metrics presented in a visual plot form. For simple spatial tasks like saucepan positioning, LLM provides accurate assessments when grounded with a single image, while complex workspace safety assessment task requires extensive knowledge of past experiences. By reviewing status of each checklist item, the LLM can decide whether experiment needs to be halted or requires human intervention, offering a set of troubleshooting steps. These findings demonstrate feasibility of the self-assessment approach for robotic laboratory systems, paving the way for future deployments.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt6">
             <b>
              ThDT6
             </b>
            </a>
           </td>
           <td class="r">
            Room 6
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thdt6" title="Click to go to the Program at a Glance">
             <b>
              Aerial Systems: Perception and Autonomy III
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#204568" title="Click to go to the Author Index">
             Qin, Tong
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt6_01">
             17:30-17:45, Paper ThDT6.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1008'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SCP: Soft Conditional Prompt Learning for Aerial Video Action Recognition
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#325883" title="Click to go to the Author Index">
             Wang, Xijun
            </a>
           </td>
           <td class="r">
            University of Maryland, College Park
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#339326" title="Click to go to the Author Index">
             Xian, Ruiqi
            </a>
           </td>
           <td class="r">
            University of Maryland-College Park
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#269556" title="Click to go to the Author Index">
             Guan, Tianrui
            </a>
           </td>
           <td class="r">
            University of Maryland
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#395084" title="Click to go to the Author Index">
             Liu, Fuxiao
            </a>
           </td>
           <td class="r">
            University of Maryland
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#106235" title="Click to go to the Author Index">
             Manocha, Dinesh
            </a>
           </td>
           <td class="r">
            University of Maryland
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1008" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__perception_and_autonomy" title="Click to go to the Keyword Index">
               Aerial Systems: Perception and Autonomy
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present a new learning approach, Soft Conditional Prompt Learning (SCP), which leverages the strengths of prompt learning for aerial video action recognition. Our approach is designed to predict the action of each agent by helping the models focus on the descriptions or instructions associated with actions in the input videos for aerial/robot visual perception. Our formulation supports various prompts, including learnable prompts, auxiliary visual information, and large vision models to improve recognition performance. We also present a soft conditional prompt method that learns to dynamically generate prompts from a pool of prompt experts under different video inputs. By sharing the same objective with the task, our proposed SCP can optimize prompts that guide the model's predictions while explicitly learning input-invariant (prompt experts pool) and input-specific (data-dependent) prompt knowledge. In practice, we observe a 3.17-10.2% accuracy improvement on the aerial video datasets (Okutama, NECDrone), which consist of scenes with single-agent and multi-agent actions. We further evaluate our approach on ground camera videos to verify the effectiveness and generalization and achieve a 1.0-3.6% improvement on dataset SSV2. We integrate our method into the ROS2 as well.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt6_02">
             17:45-18:00, Paper ThDT6.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2701'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SOAR: Simultaneous Exploration and Photographing with Heterogeneous UAVs for Fast Autonomous Reconstruction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#393952" title="Click to go to the Author Index">
             Zhang, Mingjie
            </a>
           </td>
           <td class="r">
            Northwestern Polytechnical University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#340168" title="Click to go to the Author Index">
             Feng, Chen
            </a>
           </td>
           <td class="r">
            Hong Kong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#397571" title="Click to go to the Author Index">
             Li, Zengzhi
            </a>
           </td>
           <td class="r">
            North China Electric Power University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#375205" title="Click to go to the Author Index">
             Zheng, Guiyong
            </a>
           </td>
           <td class="r">
            Xidian University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#371295" title="Click to go to the Author Index">
             Luo, Yiming
            </a>
           </td>
           <td class="r">
            The University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#234281" title="Click to go to the Author Index">
             Wang, Zhu
            </a>
           </td>
           <td class="r">
            North China Electric Power University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#204347" title="Click to go to the Author Index">
             Zhou, Jinni
            </a>
           </td>
           <td class="r">
            Hong Kong University of Science and Technology (Guangzhou)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#142354" title="Click to go to the Author Index">
             Shen, Shaojie
            </a>
           </td>
           <td class="r">
            Hong Kong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#225160" title="Click to go to the Author Index">
             Zhou, Boyu
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2701" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__perception_and_autonomy" title="Click to go to the Keyword Index">
               Aerial Systems: Perception and Autonomy
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Unmanned Aerial Vehicles (UAVs) have gained significant popularity in scene reconstruction. This paper presents SOAR, a LiDAR-Visual heterogeneous multi-UAV system specifically designed for fast autonomous reconstruction of complex environments. Our system comprises a LiDAR-equipped explorer with a large field-of-view (FoV), alongside photographers equipped with cameras. To ensure rapid acquisition of the scenes surface geometry, we employ a surface frontier-based exploration strategy for the explorer. As the surface is progressively explored, we identify the uncovered areas and generate viewpoints incrementally. These viewpoints are then assigned to photographers through solving a Consistent Multiple Depot Multiple Traveling Salesman Problem (Consistent-MDMTSP), which optimizes scanning efficiency while ensuring task consistency. Finally, photographers utilize the assigned viewpoints to determine optimal coverage paths for acquiring images. We present extensive benchmarks in the realistic simulator, which validates the performance of SOAR compared with classical and state-of-the-art methods. For more details, please see our project page at sysu-star.github.io/SOAR.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt6_03">
             18:00-18:15, Paper ThDT6.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3009'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Data-Driven Koopman Operator-Based Error-State Kalman Filter for Enhanced State Estimation of Quadrotors in Agile Flight
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#256410" title="Click to go to the Author Index">
             Huang, Peng
            </a>
           </td>
           <td class="r">
            Barkhausen Institut
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#374599" title="Click to go to the Author Index">
             Zheng, Ketong
            </a>
           </td>
           <td class="r">
            Technische Universitt Dresden
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#257133" title="Click to go to the Author Index">
             Fettweis, Gerhard
            </a>
           </td>
           <td class="r">
            Technische Universitt Dresden
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3009" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__perception_and_autonomy" title="Click to go to the Keyword Index">
               Aerial Systems: Perception and Autonomy
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#sensor_fusion" title="Click to go to the Keyword Index">
               Sensor Fusion
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Highly dynamic maneuvers pose a challenge to conventional state estimators of quadrotors in rapidly tracking the pose. This paper proposes a data-driven Koopman operator-based error-state Kalman filter (K-ESKF) to enhance pose estimation in agile flight. Our method uses the Koopman operator theory to transform the full-state nonlinear quadrotor dynamics into a lifted bilinear control system driven by accelerations and angular rates. A deep neural network (DNN) is used to represent the Koopman observable functions. Our proposed K-ESKF extends the propagation step of a standard error-state Kalman filter (ESKF) using the lifted bilinear control system. An open-source quadrotor dataset, NeuroBEM, is used for training and evaluating the DNN and for testing the K-ESKF. The learned Koopman bilinear system demonstrates a 60% less attitude errors compared to the first-order Euler method in terms of model accuracy. Using real trajectories from the dataset, our proposed K-ESKF can estimate the pose as accurately as the ESKF during normal flight. More importantly, our proposed approach outperforms the ESKF by achieving about 50% less attitude and velocity estimation errors in a highly agile flight. During drastic attitude and velocity changes, the K-ESKF can still estimate the pose while the ESKF loses tracking.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt6_04">
             18:15-18:30, Paper ThDT6.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3421'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Greedy Perspectives: Multi-Drone View Planning for Collaborative Perception in Cluttered Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#351169" title="Click to go to the Author Index">
             Suresh, Krishna
            </a>
           </td>
           <td class="r">
            Olin College of Engineering
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#377989" title="Click to go to the Author Index">
             Rauniyar, Aditya
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#180547" title="Click to go to the Author Index">
             Corah, Micah
            </a>
           </td>
           <td class="r">
            Colorado School of Mines
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#104304" title="Click to go to the Author Index">
             Scherer, Sebastian
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3421" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__perception_and_autonomy" title="Click to go to the Keyword Index">
               Aerial Systems: Perception and Autonomy
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Deployment of teams of aerial robots could enable large-scale filming of dynamic groups of people (actors) in complex environments for applications in areas such as team sports and cinematography. Toward this end, methods for submodular maximization via sequential greedy planning can enable scalable optimization of camera views across teams of robots but face challenges with efficient coordination in cluttered environments. Obstacles can produce occlusions and increase chances of inter-robot collision which can violate requirements for near-optimality guarantees. To coordinate teams of aerial robots in filming groups of people in dense environments, a more general view-planning approach is required. We explore how collision and occlusion impact performance in filming applications through the development of a multi-robot multi-actor view planner with an occlusion-aware objective for filming groups of people and compare with a formation planner and a greedy planner that ignores inter-robot collisions. We evaluate our approach based on five test environments and complex multi-actor behaviors. Compared with a formation planner, our sequential planner generates 14% greater view reward for filming the actors in three scenarios and comparable performance to formation planning on two others. We also observe near identical view rewards for sequential planning both with and without inter-robot collision constraints which indicates that robots are able to avoid collisions without impairing performance in the perception task. Overall, we demonstrate effective coordination of teams of aerial robots in environments cluttered with obstacles that may cause collisions or occlusions and for filming groups that may split, merge, or spread apart.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt7">
             <b>
              ThDT7
             </b>
            </a>
           </td>
           <td class="r">
            Room 7
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thdt7" title="Click to go to the Program at a Glance">
             <b>
              Medical Robotics III
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#170047" title="Click to go to the Author Index">
             Courtecuisse, Hadrien
            </a>
           </td>
           <td class="r">
            AVR, CNRS Strasbourg
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#172565" title="Click to go to the Author Index">
             Stilli, Agostino
            </a>
           </td>
           <td class="r">
            University College London
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt7_01">
             17:30-17:45, Paper ThDT7.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2556'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Real-Time Robotic Flexible Needle Insertion in Deformable Living Organs Using Isolated Objective Constraint
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#300481" title="Click to go to the Author Index">
             Ha, Thuc Long
            </a>
           </td>
           <td class="r">
            ICUBE - Universite De Strasbourg
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#103579" title="Click to go to the Author Index">
             Bert, Julien
            </a>
           </td>
           <td class="r">
            LaTIM, INSERM, CHRU Brest
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#170047" title="Click to go to the Author Index">
             Courtecuisse, Hadrien
            </a>
           </td>
           <td class="r">
            AVR, CNRS Strasbourg
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2556" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#simulation_and_animation" title="Click to go to the Keyword Index">
               Simulation and Animation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents an innovative approach for executing robotic needle insertion within deformable living organs. The objective is to maintain the insertion pivot point on the skin, which remains stationary. At the same time, the organs undergo displacement and deformation due to respiration. Therefore, real-time control and precise needle steering are crucial. The proposed method relies on isolated objective constraints to ensure the objectives while steering the needle along a predefined trajectory. The needle insertion process benefits from Finite Element (FE) models to simulate the environment and address the inverse problem to drive the robot's end effector (EE) by re-evaluating the objective functions in the constraint space for each time step. So, the desired motion of the robot's EE could be calculated at a small cost for non-linear functions in real-time, resulting in better precision and reducing stress caused to the organs.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt7_02">
             17:45-18:00, Paper ThDT7.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2761'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Continuum Robot Shape Estimation Using Magnetic Ball Chains
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#217704" title="Click to go to the Author Index">
             Pittiglio, Giovanni
            </a>
           </td>
           <td class="r">
            Worcester Polytechnic Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#293299" title="Click to go to the Author Index">
             Donder, Abdulhamit
            </a>
           </td>
           <td class="r">
            Boston Children's Hospital &amp; Harvard Medical School
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#101790" title="Click to go to the Author Index">
             Dupont, Pierre
            </a>
           </td>
           <td class="r">
            Children's Hospital Boston, Harvard Medical School
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2761" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#surgical_robotics__steerable_catheters_needles" title="Click to go to the Keyword Index">
               Surgical Robotics: Steerable Catheters/Needles
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#flexible_robotics" title="Click to go to the Keyword Index">
               Flexible Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Shape sensing of medical continuum robots is important both for closed-loop control as well as for enabling the clinician to visualize the robot inside the body. There is a need for inexpensive, but accurate shape sensing technologies. This paper proposes the use of magnetic ball chains as a means of generating shape-specific magnetic fields that can be detected by an external array of Hall effect sensors. Such a ball chain, encased in a flexible polymer sleeve, could be inserted inside the lumen of any continuum robot to provide real-time shape feedback. The sleeve could be removed, as needed, during the procedure to enable use of the entire lumen. To investigate this approach, a shape-sensing model for a steerable catheter tip is derived and an observability and sensitivity analysis are presented. Experiments show maximum estimation errors of 7.1% and mean of 2.9% of the tip position with respect to total length.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt7_03">
             18:00-18:15, Paper ThDT7.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2908'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              An MR Safe Double-Arch Needle Insertion Robot with Scissor-Folding Mechanism for Abdominal Percutaneous Interventions
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#240563" title="Click to go to the Author Index">
             Liang, Ziting
            </a>
           </td>
           <td class="r">
            University College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#359432" title="Click to go to the Author Index">
             Lu, Chuang
            </a>
           </td>
           <td class="r">
            University College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396803" title="Click to go to the Author Index">
             Yang, Haoqian
            </a>
           </td>
           <td class="r">
            University College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#313794" title="Click to go to the Author Index">
             Hashem, Ryman
            </a>
           </td>
           <td class="r">
            University of College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#175862" title="Click to go to the Author Index">
             Abdelaziz, Mohamed Essam Mohamed Kassem
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#192606" title="Click to go to the Author Index">
             Lindenroth, Lukas
            </a>
           </td>
           <td class="r">
            King's College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#321992" title="Click to go to the Author Index">
             Bandula, Steve
            </a>
           </td>
           <td class="r">
            Wellcome/EPSRC Centre for Interventional and Surgical Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#128031" title="Click to go to the Author Index">
             Stoyanov, Danail
            </a>
           </td>
           <td class="r">
            University College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#172565" title="Click to go to the Author Index">
             Stilli, Agostino
            </a>
           </td>
           <td class="r">
            University College London
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2908" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#surgical_robotics__steerable_catheters_needles" title="Click to go to the Keyword Index">
               Surgical Robotics: Steerable Catheters/Needles
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#surgical_robotics__planning" title="Click to go to the Keyword Index">
               Surgical Robotics: Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Tumors affecting abdominal organs rank among the deadliest malignancies. In this context, Magnetic Resonance Imaging (MRI) serves as an effective diagnostic tool with a strong potential to support image-guided minimally invasive interventions for treating these tumours, offering an ionizing-radiation-free medical modality. MRI provides exceptional soft tissue contrast and multi-angle imaging, enabling accurate intraoperative localisation of target tumours within these vital organs. Nevertheless, MRI-guided minimally invasive interventions still encounter significant challenges due to the strong magnetic field environment and the narrow and deep bore of MRI machines. This paper proposes a novel MR safe 5-degrees-of-freedom (DoFs) parallel table-mounted double-arch needle insertion robot with a scissor-folding mechanism (SFM) for abdominal interventions. The proposed robot is designed to fit a standard 70-cm MRI bore. Initial evaluation experiments indicate mean errors of 3.14 mm for the proposed robotic arch and 2.23 mm for the full needle insertion robot, respectively. Additionally, preliminary testing of the system in an MRI environment resulted in unaltered MRI imaging output, with negligible artefacts associated with the presence of the robot within the bore.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt7_04">
             18:15-18:30, Paper ThDT7.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2926'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Spatial Spinal Fixation: A Transformative Approach Using a Unique Robot-Assisted Steerable Drilling System and Flexible Pedicle Screw
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#309709" title="Click to go to the Author Index">
             Sharma, Susheela
            </a>
           </td>
           <td class="r">
            University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#354989" title="Click to go to the Author Index">
             Kulkarni, Yash
            </a>
           </td>
           <td class="r">
            The University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#345756" title="Click to go to the Author Index">
             Go, Sarah
            </a>
           </td>
           <td class="r">
            University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#347101" title="Click to go to the Author Index">
             Bonyun, Jeff
            </a>
           </td>
           <td class="r">
            University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#310690" title="Click to go to the Author Index">
             Amadio, Jordan P.
            </a>
           </td>
           <td class="r">
            University of Texas Dell Medical School
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#415510" title="Click to go to the Author Index">
             Tilton, Maryam
            </a>
           </td>
           <td class="r">
            University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#179020" title="Click to go to the Author Index">
             Khadem, Mohsen
            </a>
           </td>
           <td class="r">
            University of Edinburgh
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#190490" title="Click to go to the Author Index">
             Alambeigi, Farshid
            </a>
           </td>
           <td class="r">
            University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2926" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#surgical_robotics__steerable_catheters_needles" title="Click to go to the Keyword Index">
               Surgical Robotics: Steerable Catheters/Needles
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Spinal fixation procedures are currently limited by the rigidity of the existing instruments and pedicle screws leading to fixation failures and rigid pedicle screw pull out. Leveraging our recently developed Concentric Tube Steerable Drilling Robot (CT-SDR) in integration with a robotic manipulator, to address the aforementioned issue, here we introduce the transformative concept of Spatial Spinal Fixation (SSF) using a unique Flexible Pedicle Screw (FPS). The proposed SSF procedure enables planar and out-of-plane placement of the FPS throughout the full volume of the vertebral body. In other words, not only does our fixation system provide the option of drilling in-plane and out-of-plane trajectories, it also enables implanting the FPS inside linear (represented by an I-shape) and/or non-linear (represented by J-shape) trajectories. To thoroughly evaluate the functionality of our proposed robotic system and the SSF procedure, we have performed various experiments by drilling different I-J and J-J drilling trajectory pairs into our custom-designed L3 vertebral phantoms and analyzed the accuracy of the procedure using various metrics.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt8">
             <b>
              ThDT8
             </b>
            </a>
           </td>
           <td class="r">
            Room 8
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thdt8" title="Click to go to the Program at a Glance">
             <b>
              Localization VII
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#103628" title="Click to go to the Author Index">
             Matteucci, Matteo
            </a>
           </td>
           <td class="r">
            Politecnico Di Milano
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt8_01">
             17:30-17:45, Paper ThDT8.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2421'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Advancements in Radar Odometry
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#295789" title="Click to go to the Author Index">
             Frosi, Matteo
            </a>
           </td>
           <td class="r">
            Politecnico Di Milano
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#375950" title="Click to go to the Author Index">
             Usuelli, Mirko
            </a>
           </td>
           <td class="r">
            Politecnico Di Milano
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#103628" title="Click to go to the Author Index">
             Matteucci, Matteo
            </a>
           </td>
           <td class="r">
            Politecnico Di Milano
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2421" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#range_sensing" title="Click to go to the Keyword Index">
               Range Sensing
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Radar odometry estimation has emerged as a critical technique in the field of autonomous navigation, providing robust and reliable motion estimation under various environmental conditions. Despite its potential, the complex nature of radar signals and the inherent challenges associated with processing these signals have limited the widespread adoption of this technology. This paper aims to address these challenges and simultaneously present an understanding about the current advancements in radar odometry estimation. First, we propose novel improvements to an existing state-of-the-art method, which are designed to enhance accuracy and reliability in diverse scenarios. Our pipeline consists of filtering, motion compensation, oriented surface points computation, smoothing, one-to-many radar scan registration, and pose refinement. In particular, we enforce local understanding of a scene by including additional information through smoothing (Gaussian kernels) and alignment (ICP), introduced by us in the existing pipeline. Then, we present an in-depth investigation of the contribution of each improvement to the localization accuracy. Lastly, we benchmark our system and state-of-the-art methods on all sequences of well-known datasets for radar understanding, i.e., the Oxford Radar RobotCar, MulRan, and Boreas datasets. In particular, Boreas includes scenarios with challenging weather conditions, such as snow or overcast, and, to our knowledge, it has never been used for evaluation or benchmarking in the literature. The effectiveness of the proposed improvements is proven by an increased translation and rotation accuracy on the majority of scenarios considered.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt8_02">
             17:45-18:00, Paper ThDT8.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2593'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              BEVRender: Vision-Based Cross-View Vehicle Registration in Off-Road GNSS-Denied Environment
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#390580" title="Click to go to the Author Index">
             Lihong, Jin
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#217675" title="Click to go to the Author Index">
             Dong, Wei
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#171185" title="Click to go to the Author Index">
             Wang, Wenshan
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#104298" title="Click to go to the Author Index">
             Kaess, Michael
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2593" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#autonomous_vehicle_navigation" title="Click to go to the Keyword Index">
               Autonomous Vehicle Navigation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We introduce BEVRender, a novel learning-based approach for the localization of ground vehicles in Global Navigation Satellite System (GNSS)-denied off-road scenarios. These environments are typically challenging for conventional vision-based state estimation due to the lack of distinct visual landmarks and the instability of vehicle poses. To address this, BEVRender generates high-quality local bird's-eye-view (BEV) images of the local terrain. Subsequently, these images are aligned with a georeferenced aerial map through template matching to achieve accurate cross-view registration. Our approach overcomes the inherent limitations of visual inertial odometry systems and the substantial storage requirements of image-retrieval localization strategies, which are susceptible to drift and scalability issues, respectively. Extensive experimentation validates BEVRender's advancement over existing GNSS-denied visual localization methods, demonstrating notable enhancements in both localization accuracy and update frequency.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt8_03">
             18:00-18:15, Paper ThDT8.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2679'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              3D-BLUE: Backscatter Localization for Underwater Robotics
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#398475" title="Click to go to the Author Index">
             Afzal, Sayed Saad
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#382991" title="Click to go to the Author Index">
             Chen, Wei-Tung
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#291658" title="Click to go to the Author Index">
             Adib, Fadel
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2679" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#marine_robotics" title="Click to go to the Keyword Index">
               Marine Robotics
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#sensor_networks" title="Click to go to the Keyword Index">
               Sensor Networks
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present the design, implementation, and evaluation of 3D-BLUE, an ultra-low-power underwater 3D localization system that can be deployed on compact robots to accurately localize them in shallow underwater environments. 3D-BLUEs design introduces two core components. First, it adapts a recent ultra-low-power underwater acoustic communication technology (called piezo-electric backscatter) to the underwater robotics localization problem; specifically, it integrates backscatter nodes into the underwater robot and uses them for localizing it. Second, it leverages the physical properties of the backscatter technology to efficiently extract spatio-temporal-spectral features from the backscatter signal; using these features, it devises a particle-filter-based algorithm to localize the corresponding robot accurately in challenging shallow-water environments. We implemented an end-to-end prototype of 3D-BLUE on a BlueROV2 robot and custom-built backscatter localization system, and evaluated it in dozens of experimental trials in a pool. Our results demonstrate that 3D-BLUE can localize the robot with an accuracy of around 0.25m at close range and an accuracy of around 1.4m at a range of 10m. This high localization accuracy opens important commercial, naval, and environmental applications in challenging shallow-water environments such as shores, rivers, pools, and narrow waterways.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt8_04">
             18:15-18:30, Paper ThDT8.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2854'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              BEV-CV: Birds-Eye-View Transform for Cross-View Geo-Localisation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#347207" title="Click to go to the Author Index">
             Shore, Tavis
            </a>
           </td>
           <td class="r">
            University of Surrey
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#196563" title="Click to go to the Author Index">
             Hadfield, Simon
            </a>
           </td>
           <td class="r">
            University of Surrey
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#192270" title="Click to go to the Author Index">
             Mendez, Oscar
            </a>
           </td>
           <td class="r">
            University of Surrey
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2854" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#data_sets_for_robotic_vision" title="Click to go to the Keyword Index">
               Data Sets for Robotic Vision
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Cross-view image matching for geo-localisation is a challenging problem due to the significant visual difference between aerial and ground-level viewpoints. The method provides localisation capabilities from geo-referenced images, eliminating the need for external devices or costly equipment. This enhances the capacity of agents to autonomously determine their position, navigate, and operate effectively in GNSS-denied environments. Current research employs a variety of techniques to reduce the domain gap such as applying polar transforms to aerial images or synthesising between perspectives. However, these approaches generally rely on having a 360 field of view, limiting real-world feasibility. We propose BEV-CV, an approach introducing two key novelties with a focus on improving the real-world viability of cross-view geo-localisation. Firstly bringing ground-level images into a semantic Birds-Eye-View before matching embeddings, allowing for direct comparison with aerial image representations. Secondly, we adapt datasets into application realistic format - limited Field-of-View images aligned to vehicle direction. BEV-CV achieves state-of-the-art recall accuracies, improving Top-1 rates of 70 crops of CVUSA and CVACT by 23% and 24% respectively. Also decreasing computational requirements by reducing floating point operations to below previous works, and decreasing embedding dimensionality by 33% - together allowing for faster localisation capabilities.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt9">
             <b>
              ThDT9
             </b>
            </a>
           </td>
           <td class="r">
            Room 9
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thdt9" title="Click to go to the Program at a Glance">
             <b>
              Motion and Path Planning VII
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#102420" title="Click to go to the Author Index">
             Bekris, Kostas E.
            </a>
           </td>
           <td class="r">
            Rutgers, the State University of New Jersey
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#133617" title="Click to go to the Author Index">
             Dantam, Neil
            </a>
           </td>
           <td class="r">
            Colorado School of Mines
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt9_01">
             17:30-17:45, Paper ThDT9.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2721'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              BOMP: Bin-Optimized Motion Planning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#306767" title="Click to go to the Author Index">
             Tam, Zachary
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#326226" title="Click to go to the Author Index">
             Dharmarajan, Karthik
            </a>
           </td>
           <td class="r">
            UC Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#340609" title="Click to go to the Author Index">
             Qiu, Tianshuang
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#268771" title="Click to go to the Author Index">
             Avigal, Yahav
            </a>
           </td>
           <td class="r">
            UC Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#148019" title="Click to go to the Author Index">
             Ichnowski, Jeffrey
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#107102" title="Click to go to the Author Index">
             Goldberg, Ken
            </a>
           </td>
           <td class="r">
            UC Berkeley
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2721" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#manipulation_planning" title="Click to go to the Keyword Index">
               Manipulation Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In logistics, the ability to quickly compute and execute pick-and-place motions from bins is critical to increasing productivity. We present Bin-Optimized Motion Planning (BOMP), a motion planning framework that plans arm motions for a six-axis industrial robot with a long-nosed suction tool to remove boxes from deep bins. BOMP considers robot arm kinematics, actuation limits, the dimensions of a grasped box, and a varying height map of a bin environment to rapidly generate time-optimized, jerk-limited, and collision-free trajectories. The optimization is warm-started using a deep neural network trained offline in simulation with 25,000 scenes and corresponding trajectories. Experiments with 96 simulated and 15 physical environments suggest that BOMP generates collision-free trajectories that are up to 58% faster than baseline sampling-based planners and up to 36% faster than an industry-standard Up-Over-Down algorithm, which has an extremely low 15% success rate in this context. BOMP also generates jerk-limited trajectories while baselines do not. Website: https://sites.google.com/berkeley.edu/bomp.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt9_02">
             17:45-18:00, Paper ThDT9.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3023'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Roadmaps with Gaps Over Controllers: Achieving Efficiency in Planning under Dynamics
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#239130" title="Click to go to the Author Index">
             Sivaramakrishnan, Aravind
            </a>
           </td>
           <td class="r">
            Rutgers University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#375694" title="Click to go to the Author Index">
             Tangirala, Sumanth
            </a>
           </td>
           <td class="r">
            Rutgers University, New Brunswick
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#345299" title="Click to go to the Author Index">
             Granados, Edgar
            </a>
           </td>
           <td class="r">
            Rutgers University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#358879" title="Click to go to the Author Index">
             Carver, Noah
            </a>
           </td>
           <td class="r">
            Rutgers University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#102420" title="Click to go to the Author Index">
             Bekris, Kostas E.
            </a>
           </td>
           <td class="r">
            Rutgers, the State University of New Jersey
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3023" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#nonholonomic_motion_planning" title="Click to go to the Keyword Index">
               Nonholonomic Motion Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper aims to improve the computational efficiency of motion planning for mobile robots with non-trivial dynamics through the use of learned controllers. Offline, a system-specific controller is first trained in an empty environment. Then, for the target environment, the approach constructs a data structure, a "Roadmap with Gaps,'' to approximately learn how to solve planning queries using the learned controller. The roadmap nodes correspond to local regions. Edges correspond to applications of the learned controller that approximately connect these regions. Gaps arise as the controller does not perfectly connect pairs of individual states along edges. Online, given a query, a tree sampling-based motion planner uses the roadmap so that the tree's expansion is informed towards the goal region. The tree expansion selects local subgoals given a wavefront on the roadmap that guides towards the goal. When the controller cannot reach a subgoal region, the planner resorts to random exploration to maintain probabilistic completeness and asymptotic optimality. The accompanying experimental evaluation shows that the approach significantly improves the computational efficiency of motion planning on various benchmarks, including physics-based vehicular models on uneven and varying friction terrains as well as a quadrotor under air pressure effects. Website: https://prx-kinodynamic.github.io/projects/rogue
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt9_03">
             18:00-18:15, Paper ThDT9.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3375'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Sampling Ensemble for Asymptotically Complete Motion Planning with Volume-Reducing Workspace Constraints
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#226935" title="Click to go to the Author Index">
             Li, Sihui
            </a>
           </td>
           <td class="r">
            Colorado School of Mines
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#257214" title="Click to go to the Author Index">
             Schack, Matthew
            </a>
           </td>
           <td class="r">
            Colorado School of Mines
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#208749" title="Click to go to the Author Index">
             Upadhyay, Aakriti
            </a>
           </td>
           <td class="r">
            Colorado School of Mines
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#133617" title="Click to go to the Author Index">
             Dantam, Neil
            </a>
           </td>
           <td class="r">
            Colorado School of Mines
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3375" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Many robot tasks impose constraints on the workspace. For example, a robot may need to move a container without spilling its contents or open a door following the doorknob's arc. Such constraints may induce narrow volumes in the configuration space, traditionally a challenge for sampling-based methods, and further cause infeasibility. We extend sample-driven connectivity learning (SDCL), a robust approach for planning with narrow passages, to develop a sampling ensemble for workspace constraints. In particular, the ensemble combines SDCL, projection via dual quaternion optimization, and random sampling. These complementary sampling approaches support efficient and robust planning under workspace constraints. Further, this framework offers the ability to determine infeasibility under workspace constraints, which is unaddressed by previous constrained planning methods.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt9_04">
             18:15-18:30, Paper ThDT9.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3443'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Deep Geometric Potential Functions for Tracking on Manifolds
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#369980" title="Click to go to the Author Index">
             Potu Surya Prakash, Nikhil
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#275399" title="Click to go to the Author Index">
             Seo, Joohwan
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#138299" title="Click to go to the Author Index">
             Sreenath, Koushil
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#129064" title="Click to go to the Author Index">
             Choi, Jongeun
            </a>
           </td>
           <td class="r">
            Yonsei University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#125316" title="Click to go to the Author Index">
             Horowitz, Roberto
            </a>
           </td>
           <td class="r">
            Berkeley
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3443" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#machine_learning_for_robot_control" title="Click to go to the Keyword Index">
               Machine Learning for Robot Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#force_control" title="Click to go to the Keyword Index">
               Force Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper, we introduce a novel approach for designing invariant control laws through potential functions for fully actuated dynamical systems evolving on manifolds by leveraging the power of neural networks. The geometry and non-linearity inherent to manifold-based dynamical systems pose challenges for traditional control law design, necessitating techniques with the interplay of differential geometry and dynamical systems for ensuring stability. Apart from stability, performance and optimality are other challenging areas to address for dynamical systems evolving on manifolds. On top of these, the concept of invariance helps us improve learning transferability skills from one scene to another scene. We propose invariant potential functions on manifolds defined by neural networks that can be used to generate elastic forces for asymptotic tracking of trajectories. The weights of the potential function can be tuned to shape the potential functions according to the performance requirements through minimizing a loss function.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt10">
             <b>
              ThDT10
             </b>
            </a>
           </td>
           <td class="r">
            Room 10
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thdt10" title="Click to go to the Program at a Glance">
             <b>
              Data Sets for Robotic Vision III
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#123399" title="Click to go to the Author Index">
             Pomerleau, Francois
            </a>
           </td>
           <td class="r">
            Universit Laval
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt10_01">
             17:30-17:45, Paper ThDT10.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1585'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              NeuralLabeling: A Versatile Toolset for Labeling Vision Datasets Using Neural Radiance Fields
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#197869" title="Click to go to the Author Index">
             Erich, Floris Marc Arden
            </a>
           </td>
           <td class="r">
            National Institute of Advanced Industrial Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#217536" title="Click to go to the Author Index">
             Chiba, Naoya
            </a>
           </td>
           <td class="r">
            Tohoku University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#396867" title="Click to go to the Author Index">
             Mustafa, Abdullah
            </a>
           </td>
           <td class="r">
            National Institute of Advanced Industrial Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#225225" title="Click to go to the Author Index">
             Yoshiyasu, Yusuke
            </a>
           </td>
           <td class="r">
            CNRS-AIST JRL
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#100205" title="Click to go to the Author Index">
             Ando, Noriaki
            </a>
           </td>
           <td class="r">
            National Institute of Advanced Industrial Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#111344" title="Click to go to the Author Index">
             Hanai, Ryo
            </a>
           </td>
           <td class="r">
            National Institute of Industrial Science and Technology(AIST)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#109212" title="Click to go to the Author Index">
             Domae, Yukiyasu
            </a>
           </td>
           <td class="r">
            The National Institute of Advanced Industrial Science and Techno
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1585" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#data_sets_for_robotic_vision" title="Click to go to the Keyword Index">
               Data Sets for Robotic Vision
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present NeuralLabeling, a labeling approach and toolset for annotating 3D scenes using either bounding boxes or meshes and generating segmentation masks, affordance maps, 2D bounding boxes, 3D bounding boxes, 6DOF object poses, depth maps, and object meshes. NeuralLabeling uses Neural Radiance Fields (NeRF) as a renderer, allowing labeling to be performed using 3D spatial tools while incorporating geometric clues such as occlusions, relying only on images captured from multiple viewpoints as input. To demonstrate the applicability of NeuralLabeling to a practical problem in robotics, we added ground truth depth maps to 30000 frames of transparent object RGB and noisy depth maps of glasses placed in a dishwasher captured using an RGBD sensor, yielding the Dishwasher30k dataset. We show that training a simple deep neural network with supervision using the annotated depth maps yields a higher reconstruction performance than training with the previously applied weakly supervised approach. We also show how instance segmentation and depth completion datasets generated using NeuralLabeling can be incorporated into a robot application for grasping transparent objects placed in a dishwasher with an accuracy of 83.3%, compared to 16.3% without depth completion. Supplementary URI: https://florise.github.io/neural_labeling_web/
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt10_02">
             17:45-18:00, Paper ThDT10.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2005'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              DaDiff: Domain-Aware Diffusion Model for Nighttime UAV Tracking
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#324459" title="Click to go to the Author Index">
             Zuo, Haobo
            </a>
           </td>
           <td class="r">
            University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#160488" title="Click to go to the Author Index">
             Fu, Changhong
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#287784" title="Click to go to the Author Index">
             Zheng, Guangze
            </a>
           </td>
           <td class="r">
            The University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#339398" title="Click to go to the Author Index">
             Yao, Liangliang
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#326955" title="Click to go to the Author Index">
             Lu, Kunhan
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#131357" title="Click to go to the Author Index">
             Pan, Jia
            </a>
           </td>
           <td class="r">
            University of Hong Kong
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2005" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#data_sets_for_robotic_vision" title="Click to go to the Keyword Index">
               Data Sets for Robotic Vision
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Domain adaptation is an inspiring solution to the misalignment issue of day/night image features for nighttime UAV tracking. However, the one-step adaptation paradigm is inadequate in addressing the prevalent difficulties posed by low-resolution (LR) objects when viewed from the UAVs at night, owing to the blurry edge contour and limited detail information. Moreover, these approaches struggle to perceive LR objects disturbed by nighttime noise. To address these challenges, this work proposes a novel progressive alignment paradigm, named domain-aware diffusion model (DaDiff), aligning nighttime LR object features to the daytime by virtue of progressive and stable generations. The proposed DaDiff includes an alignment encoder to enhance the detail information of nighttime LR objects, a tracking-oriented layer designed to achieve close collaboration with tracking tasks, and a successive distribution discriminator presented to distinguish different feature distributions at each diffusion timestep successively. Furthermore, an elaborate nighttime UAV tracking benchmark is constructed for LR objects, namely NUT-LR, consisting of 100 annotated sequences. Exhaustive experiments have demonstrated the robustness and feature alignment ability of the proposed DaDiff. The source code and video demo are available at https://github.com/vision4robotics/DaDiff.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt10_03">
             18:00-18:15, Paper ThDT10.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2713'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              LiDAR-Based 4D Occupancy Completion and Forecasting
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#333297" title="Click to go to the Author Index">
             Liu, Xinhao
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#381035" title="Click to go to the Author Index">
             Gong, Moonjun
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#381037" title="Click to go to the Author Index">
             Fang, Qi
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#382086" title="Click to go to the Author Index">
             Xie, Haoyu
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#245328" title="Click to go to the Author Index">
             Li, Yiming
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#207381" title="Click to go to the Author Index">
             Zhao, Hang
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#160443" title="Click to go to the Author Index">
             Feng, Chen
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2713" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#data_sets_for_robotic_vision" title="Click to go to the Keyword Index">
               Data Sets for Robotic Vision
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#performance_evaluation_and_benchmarking" title="Click to go to the Keyword Index">
               Performance Evaluation and Benchmarking
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Scene completion and forecasting are two popular perception problems in research for mobile agents like autonomous vehicles. Existing approaches treat the two problems in isolation, resulting in a separate perception of the two aspects. In this paper, we introduce a novel LiDAR perception task of Occupancy Completion and Forecasting (OCF) in the context of autonomous driving to unify these aspects into a cohesive framework. This task requires new algorithms to address three challenges altogether: (1) sparse-to-dense reconstruction, (2) partial-to-complete hallucination, and (3) 3D-to-4D prediction. To enable supervision and evaluation, we curate a large-scale dataset termed OCFBench from public autonomous driving datasets. We analyze the performance of closely related existing baselines and variants on our dataset. We envision that this research will inspire and call for further investigation in this evolving and crucial area of 4D perception. Our code for data curation and baseline implementation is available at https://github.com/ai4ce/Occ4cast.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt10_04">
             18:15-18:30, Paper ThDT10.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2857'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Exposing the Unseen: Exposure Time Emulation for Offline Benchmarking of Vision Algorithms
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#325305" title="Click to go to the Author Index">
             Gamache, Olivier
            </a>
           </td>
           <td class="r">
            Universit Laval
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#325295" title="Click to go to the Author Index">
             Fortin, Jean-Michel
            </a>
           </td>
           <td class="r">
            Universit Laval
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#378517" title="Click to go to the Author Index">
             Boxan, Matej
            </a>
           </td>
           <td class="r">
            Norlab, Universit Laval
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#314953" title="Click to go to the Author Index">
             Vaidis, Maxime
            </a>
           </td>
           <td class="r">
            Universit Laval
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#123399" title="Click to go to the Author Index">
             Pomerleau, Francois
            </a>
           </td>
           <td class="r">
            Universit Laval
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#110648" title="Click to go to the Author Index">
             Gigure, Philippe
            </a>
           </td>
           <td class="r">
            Universit Laval
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2857" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#data_sets_for_robotic_vision" title="Click to go to the Keyword Index">
               Data Sets for Robotic Vision
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#robotics_and_automation_in_agriculture_and_forestry" title="Click to go to the Keyword Index">
               Robotics and Automation in Agriculture and Forestry
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#data_sets_for_slam" title="Click to go to the Keyword Index">
               Data Sets for SLAM
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Visual Odometry (VO) is one of the fundamental tasks in computer vision for robotics. However, its performance is deeply affected by High Dynamic Range (HDR) scenes, omnipresent outdoor. While new Automatic-Exposure (AE) approaches to mitigate this have appeared, their comparison in a reproducible manner is problematic. This stems from the fact that the behavior of AE depends on the environment, and it affects the image acquisition process. Consequently, AE has traditionally only been benchmarked in an online manner, making the experiments non-reproducible. To solve this, we propose a new methodology based on an emulator that can generate images at any exposure time. It leverages BorealHDR, a unique multi-exposure stereo dataset collected over 10 km, on 55 trajectories with challenging illumination conditions. Moreover, it includes lidar-inertial-based global maps with pose estimation for each image frame as well as Global Navigation Satellite System (GNSS) data, for comparison. We show that using these images acquired at different exposure times, we can emulate realistic images, keeping a Root-Mean-Square Error (RMSE) below 1.78 % compared to ground truth images. To demonstrate the practicality of our approach for offline benchmarking, we compared three state-of-the-art AE algorithms on key elements of Visual Simultaneous Localization And Mapping (VSLAM) pipeline, against four baselines. Consequently, reproducible evaluation of AE is now possible, speeding up the development of future approaches. Our code and dataset are available online at this link: https://github.com/norlab-ulaval/BorealHDR
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt11">
             <b>
              ThDT11
             </b>
            </a>
           </td>
           <td class="r">
            Room 11
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thdt11" title="Click to go to the Program at a Glance">
             <b>
              Quadruped Locomotion
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#153776" title="Click to go to the Author Index">
             Barasuol, Victor
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#100179" title="Click to go to the Author Index">
             Caldwell, Darwin G.
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt11_01">
             17:30-17:45, Paper ThDT11.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('255'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Stable Wheel Gait Generation for Planar X-Shaped Walker with Telescopic Legs Based on Asymmetric Impact Posture
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#103082" title="Click to go to the Author Index">
             Asano, Fumihiko
            </a>
           </td>
           <td class="r">
            Japan Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#391339" title="Click to go to the Author Index">
             Komori, Mikito
            </a>
           </td>
           <td class="r">
            Japan Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#374282" title="Click to go to the Author Index">
             Sedoguchi, Taiki
            </a>
           </td>
           <td class="r">
            Japan Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#194561" title="Click to go to the Author Index">
             Zheng, Yanqiu
            </a>
           </td>
           <td class="r">
            Ritsumeikan University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab255" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#motion_control" title="Click to go to the Keyword Index">
               Motion Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#underactuated_robots" title="Click to go to the Keyword Index">
               Underactuated Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper introduces a novel X-shaped walker with telescopic legs and investigates its control method with the aim of generating a stable wheel gait on a horizontal plane without including zero dynamics which is essentially unstable and difficult to stabilize. First, we outline a planar 6-DOF robot model with three control inputs, and describe the equations of motion and inelastic collision. Second, we design an output-following control system that smoothly controls the extension/contraction lengths of the legs and relative hip-joint angle to their target terminal values, and creates an asymmetric impact posture in the anteroposterior direction so that the robot can easily overcome the next potential barrier. The coefficients of the desired-time trajectory for each control output are updated with the position and velocity values immediately after each impact as the target initial values, so the generated leg motion and control inputs exhibit smooth time variation. The validity of the proposed gait generation method and the change trend of fundamental motion characteristics with respect to control parameters are investigated through numerical simulations.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt11_02">
             17:45-18:00, Paper ThDT11.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('887'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Whole-Body Compliance Control for Quadruped Manipulator with Actuation Saturation of Joint Torque and Ground Friction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#275916" title="Click to go to the Author Index">
             Zhang, Tianlin
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#370870" title="Click to go to the Author Index">
             Peng, Xuanbin
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology, Shenzhen
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#388669" title="Click to go to the Author Index">
             Lin, Fenghao
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology, Shenzhen
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#249197" title="Click to go to the Author Index">
             Xiong, Xiaogang
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology, Shenzhen
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#100484" title="Click to go to the Author Index">
             Lou, Yunjiang
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology, Shenzhen
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab887" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#whole_body_motion_planning_and_control" title="Click to go to the Keyword Index">
               Whole-Body Motion Planning and Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#compliance_and_impedance_control" title="Click to go to the Keyword Index">
               Compliance and Impedance Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In normal operations, when quadruped manipulators with impedance control experience external disturbances, they may become unstable and lose balance due to actuation saturation, affecting their stability, safety, and compliance with the environment. To address this issue, we propose a whole-body compliance controller to prevent unstable behaviors like slip, oscillation, and overshoot, which arise from actuation saturation. The controller includes an admittance scheme with a set-valued operator as the internal feedback, to constrain joint torques within actuators' limits and ground reaction forces within friction cones to ensure stability against disturbances. Then, it formulates a hierarchical optimization problem using the Hierarchical Quadratic Programming (HQP) to impose the output of the admittance scheme while ensuring physical consistency to maintain compliance behaviors. Unlike traditional compliance control with one-dimensional torque limitations, our approach considers both joints torque limits of manipulator joints and friction cones of quadruped ground reaction as actuation saturation. This ensures overall compliance and stability for the quadruped manipulators, even under significant external forces, regardless of where they are exerted on the robot. We demonstrate through experiments involving variable stiffness environments and external forces during normal operations how effective our approach is in enhancing the safety of quadruped manipulators.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt11_03">
             18:00-18:15, Paper ThDT11.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1077'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Exploring Constrained Reinforcement Learning Algorithms for Quadrupedal Locomotion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#217937" title="Click to go to the Author Index">
             Lee, Joonho
            </a>
           </td>
           <td class="r">
            Neuromeka
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#375461" title="Click to go to the Author Index">
             Schroth, Lukas
            </a>
           </td>
           <td class="r">
            ETH Zrich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#236286" title="Click to go to the Author Index">
             Klemm, Victor
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#195453" title="Click to go to the Author Index">
             Bjelonic, Marko
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#287842" title="Click to go to the Author Index">
             Reske, Alexander
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#114045" title="Click to go to the Author Index">
             Hutter, Marco
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1077" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#field_robots" title="Click to go to the Keyword Index">
               Field Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Shifting from traditional control strategies to Deep Reinforcement Learning (RL) for legged robots poses inherent challenges, especially when addressing real-world physical constraints during training. While high-fidelity simulations provide significant benefits, they often bypass these essential physical limitations. In this paper, we experiment with the Constrained Markov Decision Process (CMDP) framework instead of the conventional unconstrained RL for robotic applications. We evaluated five constrained policy optimization algorithms for quadrupedal locomotion using three different robot models. Our aim is to evaluate their applicability in real-world scenarios. Our robot experiments demonstrate the critical role of incorporating physical constraints, yielding successful sim-to-real transfers, and reducing operational errors on physical systems. The CMDP formulation streamlines the training process by separately handling constraints from rewards. Our findings underscore the potential of constrained RL for the effective development and deployment of learned controllers in robotics.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt11_04">
             18:15-18:30, Paper ThDT11.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2304'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              PACC: A Passive-Arm Approach for High-Payload Collaborative Carrying with Quadruped Robots Using Model Predictive Control
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#277908" title="Click to go to the Author Index">
             Turrisi, Giulio
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#254113" title="Click to go to the Author Index">
             Schulze, Lucas
            </a>
           </td>
           <td class="r">
            Technische Universitt Darmstadt
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#272207" title="Click to go to the Author Index">
             Suzano Medeiros, Vivian
            </a>
           </td>
           <td class="r">
            University of So Paulo
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#108792" title="Click to go to the Author Index">
             Semini, Claudio
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#153776" title="Click to go to the Author Index">
             Barasuol, Victor
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2304" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#compliant_joints_and_mechanisms" title="Click to go to the Keyword Index">
               Compliant Joints and Mechanisms
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#cooperating_robots" title="Click to go to the Keyword Index">
               Cooperating Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper, we introduce the concept of using passive arm structures with intrinsic impedance for robot-robot and human-robot collaborative carrying with quadruped robots. The concept is meant for a leader-follower task and takes a minimalist approach that focuses on exploiting the robots' payload capabilities and reducing energy consumption, without compromising the robot locomotion capabilities. We introduce a preliminary arm mechanical design and describe how to use its joint displacements to guide the robot's motion. To control the robot's locomotion, we propose a decentralized Model Predictive Controller that incorporates an approximation of the arm dynamics and the estimation of the external forces from the collaborative carrying. We validate the overall system experimentally by performing both robot-robot and human-robot collaborative carrying on a stair-like obstacle and on rough terrain.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt12">
             <b>
              ThDT12
             </b>
            </a>
           </td>
           <td class="r">
            Room 12
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thdt12" title="Click to go to the Program at a Glance">
             <b>
              Robot Learning
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#285401" title="Click to go to the Author Index">
             Puranic, Aniruddh Gopinath
            </a>
           </td>
           <td class="r">
            University of Southern California
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt12_01">
             17:30-17:45, Paper ThDT12.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('595'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Signal Temporal Logic-Guided Apprenticeship Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#285401" title="Click to go to the Author Index">
             Puranic, Aniruddh Gopinath
            </a>
           </td>
           <td class="r">
            University of Southern California
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#241241" title="Click to go to the Author Index">
             Deshmukh, Jyotirmoy
            </a>
           </td>
           <td class="r">
            University of Southern California
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#117265" title="Click to go to the Author Index">
             Nikolaidis, Stefanos
            </a>
           </td>
           <td class="r">
            University of Southern California
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab595" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#formal_methods_in_robotics_and_automation" title="Click to go to the Keyword Index">
               Formal Methods in Robotics and Automation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Apprenticeship learning crucially depends on effectively learning rewards, and hence control policies from user demonstrations. Of particular difficulty is the setting where the desired task consists of a number of sub-goals with temporal dependencies. The quality of inferred rewards and hence policies are typically limited by the quality of demonstrations, and poor inference of these can lead to undesirable outcomes. In this paper, we show how temporal logic specifications that describe high level task objectives, are encoded in a graph to define a temporal-based metric that reasons about behaviors of demonstrators and the learner agent to improve the quality of inferred rewards and policies. Through experiments on a diverse set of robot manipulator simulations, we show how our framework overcomes the drawbacks of prior literature by drastically improving the number of demonstrations required to learn a control policy.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt12_02">
             17:45-18:00, Paper ThDT12.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1646'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Working Backwards: Learning to Place by Picking
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#220207" title="Click to go to the Author Index">
             Limoyo, Oliver
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#270880" title="Click to go to the Author Index">
             Konar, Abhisek
            </a>
           </td>
           <td class="r">
            McGill University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#220206" title="Click to go to the Author Index">
             Ablett, Trevor
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#106824" title="Click to go to the Author Index">
             Kelly, Jonathan
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#198199" title="Click to go to the Author Index">
             Hogan, Francois
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#102604" title="Click to go to the Author Index">
             Dudek, Gregory
            </a>
           </td>
           <td class="r">
            McGill University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1646" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#compliance_and_impedance_control" title="Click to go to the Keyword Index">
               Compliance and Impedance Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present placing via picking (PvP), a method to autonomously collect real-world demonstrations for a family of placing tasks in which objects must be manipulated to specific, contact-constrained locations. With PvP, we approach the collection of robotic object placement demonstrations by reversing the grasping process and exploiting the inherent symmetry of the pick and place problems. Specifically, we obtain placing demonstrations from a set of grasp sequences of objects initially located at their target placement locations. Our system can collect hundreds of demonstrations in contact-constrained environments without human intervention using two modules: compliant control for grasping and tactile regrasping. We train a policy directly from visual observations through behavioural cloning, using the autonomously-collected demonstrations. By doing so, the policy can generalize to object placement scenarios outside of the training environment without privileged information (e.g., placing a plate picked up from a table). We validate our approach in home robot scenarios that include dishwasher loading and table setting. Our approach yields robotic placing policies that outperform policies trained with kinesthetic teaching, both in terms of success rate and data efficiency, while requiring no human supervision.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt12_03">
             18:00-18:15, Paper ThDT12.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2487'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Learning Deep Dynamical Systems Using Stable Neural ODEs
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#354768" title="Click to go to the Author Index">
             Sochopoulos, Andreas
            </a>
           </td>
           <td class="r">
            The University of Edinburgh
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#105270" title="Click to go to the Author Index">
             Gienger, Michael
            </a>
           </td>
           <td class="r">
            Honda Research Institute Europe
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#103221" title="Click to go to the Author Index">
             Vijayakumar, Sethu
            </a>
           </td>
           <td class="r">
            University of Edinburgh
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2487" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#dynamics" title="Click to go to the Keyword Index">
               Dynamics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Learning complex trajectories from demonstrations in robotic tasks has been effectively addressed through the utilization of Dynamical Systems (DS). State-of-the-art DS learning methods ensure stability of the generated trajectories; however, they have three shortcomings: a) the DS is assumed to have a single attractor, which limits the diversity of tasks it can achieve, b) state derivative information is assumed to be available in the learning process and c) the state of the DS is assumed to be measurable at inference time. We propose a class of provably stable latent DS with possibly multiple attractors, that inherit the training methods of Neural Ordinary Differential Equations, thus, dropping the dependency on state derivative information. A diffeomorphic mapping for the output and a loss that captures time-invariant trajectory similarity are proposed. We validate the efficacy of our approach through experiments conducted on a public dataset of handwritten shapes and within a simulated object manipulation task.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt12_04">
             18:15-18:30, Paper ThDT12.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('614'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Iterative Reference Learning for Cartesian Impedance Control of Robot Manipulators
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#288666" title="Click to go to the Author Index">
             Salt Ducaju, Julian Mauricio
            </a>
           </td>
           <td class="r">
            LTH, Lund University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#146479" title="Click to go to the Author Index">
             Olofsson, Bjorn
            </a>
           </td>
           <td class="r">
            Lund University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#10013" title="Click to go to the Author Index">
             Johansson, Rolf
            </a>
           </td>
           <td class="r">
            Lund University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab614" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#learning_from_experience" title="Click to go to the Keyword Index">
               Learning from Experience
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#compliance_and_impedance_control" title="Click to go to the Keyword Index">
               Compliance and Impedance Control
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper, an iterative learning strategy was developed to improve trajectory tracking for an impedance-controlled robot manipulator. In this learning strategy, an update law was proposed to modify the Cartesian reference of an impedance controller. Also, the conditions that ensure its convergence considering the dynamics of the robot were derived. Finally, an experimental evaluation was performed using a Franka Emika Panda robot in two different robot tasks, and its results showed that robot task completion was achieved in a lower number of iterations, while maintaining a smooth physical interaction between the robot and its surroundings.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt13">
             <b>
              ThDT13
             </b>
            </a>
           </td>
           <td class="r">
            Room 13
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thdt13" title="Click to go to the Program at a Glance">
             <b>
              Computer Vision for Automation II
             </b>
            </a>
           </td>
           <td class="r">
            Regular session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#105608" title="Click to go to the Author Index">
             Knoll, Alois
            </a>
           </td>
           <td class="r">
            Tech. Univ. Muenchen TUM
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#102317" title="Click to go to the Author Index">
             Triebel, Rudolph
            </a>
           </td>
           <td class="r">
            German Aerospace Center (DLR)
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt13_01">
             17:30-17:45, Paper ThDT13.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('505'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SDGE: Stereo Guided Depth Estimation for 360Camera Sets
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#391636" title="Click to go to the Author Index">
             Xu, Jialei
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#280806" title="Click to go to the Author Index">
             Yin, Wei
            </a>
           </td>
           <td class="r">
            University of Adelaide
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#392483" title="Click to go to the Author Index">
             Gong, Dong
            </a>
           </td>
           <td class="r">
            UNSW
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#217355" title="Click to go to the Author Index">
             Jiang, Junjun
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#365568" title="Click to go to the Author Index">
             Liu, Xianming
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab505" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Depth estimation is a critical technology in autonomous driving, and multi-camera systems are often used to achieve a 360 perception. These 360 camera sets often have limited or low-quality overlap regions, making multi-view stereo methods infeasible for the entire image. Alternatively, monocular methods may not produce consistent cross-view predictions. To address these issues, we propose the Stereo Guided Depth Estimation (SGDE) method, which enhances depth estimation of the full image by explicitly utilizing multi-view stereo results on the overlap. We suggest building virtual pinhole cameras to resolve the distortion problem of fisheye cameras and unify the processing for the two types of 360 cameras. For handling the varying noise on camera poses caused by unstable movement, the approach employs a self-calibration method to obtain highly accurate relative poses of the adjacent cameras with minor overlap. These enable the use of robust stereo methods to obtain high-quality depth prior in the overlap region. This prior serves not only as an additional input but also as pseudo-labels that enhance the accuracy of depth estimation methods and improve cross-view prediction consistency. The effectiveness of SGDE is evaluated on one fisheye camera dataset, Synthetic Urban, and two pinhole camera datasets, DDAD and nuScenes. Our experiments demonstrate that SGDE is effective for both supervised and self-supervised depth estimation, and highlight the potential of our method for advancing autonomous driving technology. Our project page is at https://github.com/JialeiXu/SGDE.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt13_02">
             17:45-18:00, Paper ThDT13.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('523'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              PCDepth: Pattern-Based Complementary Learning for Monocular Depth Estimation by Best of Both Worlds
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#294097" title="Click to go to the Author Index">
             Liu, Haotian
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#322981" title="Click to go to the Author Index">
             Qu, Sanqing
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#273028" title="Click to go to the Author Index">
             Lu, Fan
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#386683" title="Click to go to the Author Index">
             Bu, Zongtao
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#118404" title="Click to go to the Author Index">
             Roehrbein, Florian
            </a>
           </td>
           <td class="r">
            Chemnitz University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#105608" title="Click to go to the Author Index">
             Knoll, Alois
            </a>
           </td>
           <td class="r">
            Tech. Univ. Muenchen TUM
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#157845" title="Click to go to the Author Index">
             Chen, Guang
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab523" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Event cameras can record scene dynamics with high temporal resolution, providing rich scene details for monocular depth estimation (MDE) even at low-level illumination. Therefore, existing complementary learning approaches for MDE fuse intensity information from images and scene details from event data for better scene understanding. However, most methods directly fuse two modalities at pixel level, ignoring that the attractive complementarity mainly impacts high-level patterns that only occupy a few pixels. For example, event data is likely to complement contours of scene objects. In this paper, we discretize the scene into a set of high-level patterns to explore the complementarity and propose a Pattern-based Complementary learning architecture for monocular Depth estimation (PCDepth). Concretely, PCDepth comprises two primary components: a complementary visual representation learning module for discretizing the scene into high-level patterns and integrating complementary patterns across modalities and a refined depth estimator aimed at scene reconstruction and depth prediction while maintaining an efficiency-accuracy balance. Through pattern-based complementary learning, PCDepth fully exploits two modalities and achieves more accurate predictions than existing methods, especially in challenging nighttime scenarios. Extensive experiments on MVSEC and DSEC datasets verify the effectiveness and superiority of our PCDepth. Remarkably, compared with state-of-the-art, PCDepth achieves a 37.9% improvement in accuracy in MVSEC nighttime scenarios.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt13_03">
             18:00-18:15, Paper ThDT13.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('629'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Making the Flow Glow -- Robot Perception under Severe Lighting Conditions Using Normalizing Flow Gradients
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#346027" title="Click to go to the Author Index">
             Kristoffersson Lind, Simon
            </a>
           </td>
           <td class="r">
            Lund University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#115506" title="Click to go to the Author Index">
             Krueger, Volker
            </a>
           </td>
           <td class="r">
            Lund University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#102317" title="Click to go to the Author Index">
             Triebel, Rudolph
            </a>
           </td>
           <td class="r">
            German Aerospace Center (DLR)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab629" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#robot_safety" title="Click to go to the Keyword Index">
               Robot Safety
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#failure_detection_and_recovery" title="Click to go to the Keyword Index">
               Failure Detection and Recovery
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Modern robotic perception is highly dependent on neural networks. It is well known that neural network-based perception can be unreliable in real-world deployment, especially in difficult imaging conditions. Out-of-distribution detection is commonly proposed as a solution for ensuring reliability in real-world deployment. Previous work has shown that normalizing flow models can be used for out-of-distribution detection to improve reliability of robotic perception tasks. Specifically, camera parameters can be optimized with respect to the likelihood output from a normalizing flow, which allows a perception system to adapt to difficult vision scenarios. With this work we propose to use the absolute gradient values from a normalizing flow, which allows the perception system to optimize local regions rather than the whole image. By setting up a table top picking experiment with exceptionally difficult lighting conditions, we show that our method achieves a 60% higher success rate for an object detection task compared to previous methods.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt13_04">
             18:15-18:30, Paper ThDT13.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('829'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              GS-Planner: A Gaussian-Splatting-Based Planning Framework for Active High-Fidelity Reconstruction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#302454" title="Click to go to the Author Index">
             Jin, Rui
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#298347" title="Click to go to the Author Index">
             Gao, Yuman
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#288151" title="Click to go to the Author Index">
             Wang, Yingjian
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#310263" title="Click to go to the Author Index">
             Wu, Yuze
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#184494" title="Click to go to the Author Index">
             Lu, Haojian
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#213352" title="Click to go to the Author Index">
             Xu, Chao
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#200893" title="Click to go to the Author Index">
             Gao, Fei
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab829" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="IROS24_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#aerial_systems__perception_and_autonomy" title="Click to go to the Keyword Index">
               Aerial Systems: Perception and Autonomy
              </a>
              ,
              <a href="IROS24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Active reconstruction technique enables robots to autonomously collect scene data for full coverage, relieving users from tedious and time-consuming data capturing process. However, designed based on unsuitable scene representations, existing methods show unrealistic reconstruction results or the inability of online quality evaluation. Due to the recent advancements in explicit radiance field technology, online active high-fidelity reconstruction has become achievable. In this paper, we propose GS-Planner, a planning framework for active high-fidelity reconstruction using 3D Gaussian Splatting. With improvement on 3DGS to recognize unobserved regions, we evaluate the reconstruction quality and completeness of 3DGS map online to guide the robot. Then we design a sampling-based active reconstruction strategy to explore the unobserved areas and improve the reconstruction geometric and textural quality. To establish a complete robot active reconstruction system, we choose quadrotor as the robotic platform for its high agility. Then we devise a safety constraint with 3DGS to generate executable trajectories for quadrotor navigation in the 3DGS map. To validate the effectiveness of our method, we conduct extensive experiments and ablation studies in highly realistic simulation scenes.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thf8o">
             <b>
              ThF8O
             </b>
            </a>
           </td>
           <td class="r">
            Auditorium
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thf8o" title="Click to go to the Program at a Glance">
             <b>
              Forum 8 - Sustainable Medical and Surgical Robotics
             </b>
            </a>
           </td>
           <td class="r">
            Forum
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#101725" title="Click to go to the Author Index">
             Fiorini, Paolo
            </a>
           </td>
           <td class="r">
            University of Verona
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#101892" title="Click to go to the Author Index">
             Valdastri, Pietro
            </a>
           </td>
           <td class="r">
            University of Leeds
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thf8o_01">
             15:30-18:30, Paper ThF8O.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             Sustainable Medical and Surgical Robotics
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#101725" title="Click to go to the Author Index">
             Fiorini, Paolo
            </a>
           </td>
           <td class="r">
            University of Verona
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#101892" title="Click to go to the Author Index">
             Valdastri, Pietro
            </a>
           </td>
           <td class="r">
            University of Leeds
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#106795" title="Click to go to the Author Index">
             Ren, Hongliang
            </a>
           </td>
           <td class="r">
            Chinese Univ Hong Kong (CUHK) &amp; National Univ Singapore(NUS)
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#108277" title="Click to go to the Author Index">
             Vander Poorten, Emmanuel B
            </a>
           </td>
           <td class="r">
            KU Leuven
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#220900" title="Click to go to the Author Index">
             Horeman, Tim
            </a>
           </td>
           <td class="r">
            Delft University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#108147" title="Click to go to the Author Index">
             Rodriguez y Baena, Ferdinando
            </a>
           </td>
           <td class="r">
            Imperial College, London, UK
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#223931" title="Click to go to the Author Index">
             Chandler, James Henry
            </a>
           </td>
           <td class="r">
            University of Leeds
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#141354" title="Click to go to the Author Index">
             Mathis-Ullrich, Franziska
            </a>
           </td>
           <td class="r">
            Friedrich-Alexander-University Erlangen-Nurnberg (FAU)
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thi5n">
             <b>
              ThI5N
             </b>
            </a>
           </td>
           <td class="r">
            Poster Area
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thi5n" title="Click to go to the Program at a Glance">
             <b>
              Interactive Session 5
             </b>
            </a>
           </td>
           <td class="r">
            Interactive Poster session
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thf9o">
             <b>
              ThF9O
             </b>
            </a>
           </td>
           <td class="r">
            Room 17/18
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="IROS24_ProgramAtAGlanceWeb.html#thf9o" title="Click to go to the Program at a Glance">
             <b>
              Forum 9 - Moonshot R&amp;D Program Goal 3: Envisioning a Future of Human-Robot
              <br/>
              Co-Living: Potential for Robotics to Transform Human Lives
             </b>
            </a>
           </td>
           <td class="r">
            Forum
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="IROS24_AuthorIndexWeb.html#100151" title="Click to go to the Author Index">
             Sugano, Shigeki
            </a>
           </td>
           <td class="r">
            Waseda University
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="IROS24_AuthorIndexWeb.html#321011" title="Click to go to the Author Index">
             Fukuda, Toshio
            </a>
           </td>
           <td class="r">
            Nagoya University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thf9o_01">
             15:30-18:30, Paper ThF9O.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             Moonshot R&amp;D Program Goal 3 Forum - Envisioning a Future of Human-Robot Co-Living: Potential for Robotics to Transform Human Lives
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#107293" title="Click to go to the Author Index">
             Shimoda, Shingo
            </a>
           </td>
           <td class="r">
            Nagoya University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#100909" title="Click to go to the Author Index">
             Hirata, Yasuhisa
            </a>
           </td>
           <td class="r">
            Tohoku University
           </td>
          </tr>
          <tr>
           <td>
            <a href="IROS24_AuthorIndexWeb.html#100151" title="Click to go to the Author Index">
             Sugano, Shigeki
            </a>
           </td>
           <td class="r">
            Waseda University
           </td>
          </tr>
         </table>
        </div>
        <p>
         <br/>
        </p>
        <p>
         <br/>
        </p>
        <p>
         <p>
         </p>
        </p>
       </td>
       <td height="100%" style="background-color:#9F7F59;" width="5">
       </td>
      </tr>
      <tr>
       <td alt="" border="0" colspan="4" height="16" style="background-color:#9F7F59;" valign="center" width="100%">
        <p align="center">
         <span style="font-size:8pt;line-height:10pt;color:#ffffff;">
          Technical Content  IEEE Robotics &amp; Automation Society
         </span>
        </p>
       </td>
      </tr>
      <tr>
       <td colspan="4" width="100%">
        <p align="right">
         <span style="text-decoration:none;">
          <img align="right" border="0" src="/images/pc_logo_small.jpg" style="margin-left: 10px; margin-right: 10px"/>
          This site is protected
by copyright and trademark laws under US and International law.
          <br/>
          All rights
reserved.  2002-2024 PaperCept, Inc.
          <br/>
          Page generated 2024-10-0501:22:01 PST
          <a href="" onclick="window.open('/conferences/scripts/about.pl','tc','width=1000,scrollbars=yes'); return false">
           Terms of use
          </a>
         </span>
        </p>
       </td>
      </tr>
     </table>
    </body>
   </div>
  </form>
 </body>
</html>
