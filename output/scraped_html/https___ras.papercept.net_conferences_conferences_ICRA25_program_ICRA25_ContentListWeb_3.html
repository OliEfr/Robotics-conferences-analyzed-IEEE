<!DOCTYPE HTML>
<html>
 <head>
  <meta content="en-us" http-equiv="Content-Language"/>
  <meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
  <meta content="width=device-width" name="viewport"/>
  <script src="https://ras.papercept.net/conferences/scripts/dom-drag.js" type="text/javascript">
  </script>
  <script src="jquery-1.11.1.min.js">
  </script>
  <title>
   ICRA 2025 Program | Thursday May 22, 2025
  </title>
  <style type="text/css">
   body, table, td, th{
	Font-Family : sans-serif;
	Font-Size : 10pt;
}
.r {text-align: right}
.blue {color: #0000FF;}
td {vertical-align: top; text-align: left}
.c {text-align: center}
table.s {
	border-collapse:collapse;
	border-width: 1px;
}
table.s td{
	border-width: 1px;
	padding: 4px;
	border-style: solid;
	border-color: gray;
}
table.t {
	border-collapse: collapse;
	border-width: 0px;
}
table.t td{
	border-width: 0px;
	padding: 4px;
	border-style: solid;
	border-color: gray;
}
.dots {
    background:url('./images/dot.gif') repeat-x center;
}
.field {
    background-color: #FFFFFF;
}
#pTitle { /* Page title */
   font-size: 14pt;
   line-height: 1.5em;
}
#pSubTitle { /* Page subtitle */
   color: #909090;
   font-size: 10pt; 
   line-height: 1.5em;
}
#container {
	position: absolute;
	width: 100%;
	margin-top: 2px;
/*	overflow: hidden; */
}

.sHdr {   /* Session header Content list */
   background-color: #F0E68C
}
      
.sSHdr {   /* Subsession header Content list */
   background-color: #f8f3c6 
}
      
table.trk { /* Track table Content list */
   border-collapse: collapse;
   border-width: 0px;
   margin: auto;
/**   width: 640px; **/
   width: 720px;
}
table.trk td{
   border-width: 0px;
   padding: 4px;
   border-style: solid;
   border-color: gray;
 }
      
.pHdr {  /* Paper header Content list */
   background-color: #E6E6FA;
   color: black;
}
hr.thin { /* Horizontal rule content list */
   border: 0px; 
   height: .8px; 
   background-color: #8888FF;
}
      
.pTtl {  /* Paper title Content list */
   font-size: 11pt;
   font-style: italic;
}
      
.ssHdr {  /* Subsession header container session Content list */
   background-color: #DDDDDD;
   color: black;
}
      
.ssTtl {  /* Subsession title container session Content list */
   font-size: 10pt;
   font-style: normal;
   font-weight: bold;
}
  </style>
  <script language="JavaScript">
   function initXMLHttp(){
   var oRequest = false;
   try {
      oRequest = new XMLHttpRequest();
   }  catch (trymicrosoft) {
      try {
         oRequest = new ActiveXObject("Msxml2.XMLHTTP");
      }  catch (othermicrosoft) {
         try {
            oRequest = new ActiveXObject("Microsoft.XMLHTTP");
         }  catch (failed) {
            oRequest = false;
         }
      }
   }
   if (!oRequest){
      alert("Error initializing XMLHttpRequest! Your browser does not support AJAX");
   }
   return oRequest;
}
function modify(number,kk){
   var check = document.getElementById('mod' + kk).checked;
   if (check){
      action = 'Add';
   }
   else{
      action = 'Delete';
   }
   
//   url = 'https://ras.papercept.net/conferences/scripts/myprogram.pl?ConfID=479&' + action + number;
//   window.open(url,'myprogrampage');

   modifyItem("https://ras.papercept.net/conferences/scripts/myprogram_aja.pl","479",action,number)

}


function modifyItem(url,ConfID,action,number){
   var oRequest = initXMLHttp();
   if (!oRequest){return;}
   
   // Send the request

   oRequest.open("post",url, true);
   var sParams = 'ConfID=' + ConfID + '&Action=' + action + '&Number=' + number;
   document.body.style.cursor = 'wait';
   oRequest.send(sParams);

   // Process the response
   
   oRequest.onreadystatechange = function(){ 
      if (oRequest && oRequest.readyState && oRequest.readyState == 4){
         document.body.style.cursor = 'auto';
         var responseText = oRequest.responseText;
         if (responseText.substring(0,5) == 'Error'){
            alert(responseText);
         }
         else{
         
//            alert(responseText);

         }
      }
   } 
}

var iIntervalId;  // Global variable
function modsession(id,kk){
   var check = document.getElementById('mod' + kk).checked;
   if (check){
      action = 'AddSession';
   }
   else{
      action = 'DelSession';
   }

//   url = 'https://ras.papercept.net/conferences/scripts/myprogram.pl?ConfID=479&' + action + id;
//   window.open(url,'myprogrampage');

   modifySession("https://ras.papercept.net/conferences/scripts/myprogram_aja.pl","479",action,id)

}

function modifySession(url,ConfID,action,id){
   var oRequest = initXMLHttp();
   if (!oRequest){return;}
   
   // Send the request

   oRequest.open("post",url, true);
   var sParams = 'ConfID=' + ConfID + '&Action=' + action + '&ID=' + id;
   document.body.style.cursor = 'wait';
   oRequest.send(sParams);

   // Process the response
   
   oRequest.onreadystatechange = function(){ 
      if (oRequest && oRequest.readyState && oRequest.readyState == 4){
         document.body.style.cursor = 'auto';
         var responseText = oRequest.responseText;
         if (responseText.substring(0,5) == 'Error'){
            alert(responseText);
         }
         else{
         
//            alert(responseText);

         }
      }
   } 
}

function getCookie(sName){
   var sRE = "(?:; )?" + sName + "=([^;]*);?";   
   var oRE = new RegExp(sRE);
   if (oRE.test(document.cookie)){
      return decodeURIComponent(RegExp["$1"]);}
   else{
      return null;
   }
}
function loadprogram(){
   var list = getCookie("ICRA25");
   if (list){
      var List = list.split(",");
      for (var i=0; i<List.length; i++){
         var names = document.getElementsByName('modify' + List[i]);
         if (names.length){
            for (var j=0; j<names.length; j++){
               names[j].checked = true;
            }
         }
      }
   }
}
function reset(){

   // Uncheck all modify and addsession checkboxes

   var ins = document.getElementsByTagName('input');
   for (var i=0; i<ins.length; i++){
      if (ins[i].type == 'checkbox' && ins[i].id && ins[i].id.substring(0,3) == 'mod'){
         ins[i].checked = false;
      }
   }
   
   // Reload the program
   
   loadprogram();
}
function startreset(){
   iIntervalId = setInterval(reset,2000);
}
function viewAbstract(number){
   var box = document.getElementById('Ab' + number);
   if (box.style.display == 'block'){
      box.style.display = 'none';
   }
   else if (box && box.style.display == 'none'){
      box.style.display = 'block';
   }
}
function openAllAbstracts(){
   var d = document.getElementsByTagName('div');
   var count = d.length;
   if (count == 0){return;}
   for (var i=0; i<d.length; i++){
      if (d[i].id && d[i].id.substring(0,2) == 'Ab' && d[i].style.display == 'none'){
         d[i].style.display = 'block';
      }
   }
}
function closeAllAbstracts(){
   var d = document.getElementsByTagName('div');
   for (var i=0; i<d.length; i++){
      if (d[i].id && d[i].id.substring(0,2) == 'Ab'){
         d[i].style.display = 'none';
      }
   }
}
var uhash;
var pColor;
$(function() { 

   // Check for URL hash

   uhash = location.hash.substring(1);
   if (uhash.slice(-1) == '_'){
      uhash = null;
   }
   if (uhash){
   
      // Mark the session
   
      pColor = $('#' + uhash).parent().css('backgroundColor');
      $('#' + uhash).parent().css('backgroundColor','#FF8888');
   }
});
  </script>
 </head>
 <body>
  <form action="https://ras.papercept.net/conferences/scripts/myprogram.pl" name="myprogram">
   <div id="container">
    <body leftmargin="0" marginheight="0" marginwidth="0" topmargin="0">
     <table border="0" cellpadding="0" cellspacing="0" width="100%">
      <tr>
       <td height="140" style="background-color:#2C1A77;" width="100%">
        <img alt="" border="0" height="100" src="/images/icra/icra25_logo.webp" style="position:absolute;left:20px;top:20px;z-index:1;"/>
        <img alt="" border="0" height="140" src="/images/icra/icra25.webp" style="position:absolute;right:0px;top:0px;"/>
        <span style="font-size: 32px; font-family: Arial, sans serif; color: #E86950; text-align: left; position: absolute; left: 150px; top: 10px">
        </span>
        <span style="font-size: 16px; font-family: Arial, sans serif; color: #000; text-align: left; position: absolute; left: 150px; top: 80px">
        </span>
        <span style="font-size: 16px; font-family: Arial, sans serif; color: #E86950; text-align: left; position: absolute; left: 150px; top: 115px">
        </span>
       </td>
      </tr>
     </table>
     <table border="0" cellpadding="0" cellspacing="0" height="80%" width="100%">
      <tr>
       <td height="100%" style="background-color:#2C1A77;" width="5">
       </td>
       <td width="5">
       </td>
       <td height="100%" valign="top" width="100%">
        <br/>
        <div class="c" id="TheTop">
         <span id="pTitle">
          <a href="http://2025.ieee-icra.org" target="_blank">
           <b>
            2025 IEEE International Conference on Robotics and Automation (ICRA)
           </b>
          </a>
          <br/>
         </span>
         <span id="pSubTitle">
          <b>
           May 19-23, 2025, Atlanta, USA
          </b>
         </span>
         <br/>
         <br/>
        </div>
        <div class="c" style="position: relative">
         <a href="ICRA25_ProgramAtAGlanceWeb.html">
          Program at a Glance
         </a>
         <a href="ICRA25_ContentListWeb_1.html">
          Tuesday
         </a>
         <a href="ICRA25_ContentListWeb_2.html">
          Wednesday
         </a>
         <a href="ICRA25_ContentListWeb_3.html">
          Thursday
         </a>
         <a href="ICRA25_AuthorIndexWeb.html">
          Author Index
         </a>
         <a href="ICRA25_KeywordIndexWeb.html">
          Keyword Index
         </a>
        </div>
        <div class="c">
         <p style="color: gray">
          Last updated on April 22, 2025. This conference program is tentative and subject to change
         </p>
        </div>
        <div class="c">
         <h3>
          Technical Program for Thursday May 22, 2025
         </h3>
        </div>
        <p class="c">
        </p>
        <div class="c">
         <span style="color:gray ">
          To show or hide the keywords and abstract (text summary) of a paper (if available), click on the paper title
         </span>
         <br/>
         <a href="javascript:openAllAbstracts()" title="Click to open all abstracts">
          Open all abstracts
         </a>
         <a href="javascript:closeAllAbstracts()" title="Click to close all abstracts">
          Close all abstracts
         </a>
        </div>
        <div class="c">
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that1">
             <b>
              ThAT1
             </b>
            </a>
           </td>
           <td class="r">
            302
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#that1" title="Click to go to the Program at a Glance">
             <b>
              Planning and Large Language Models
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#101287" title="Click to go to the Author Index">
             Ikeuchi, Katsushi
            </a>
           </td>
           <td class="r">
            Microsoft
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that1_01">
             08:30-08:35, Paper ThAT1.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('853'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              DELTA: Decomposed Efficient Long-Term Robot Task Planning Using Large Language Models
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#346181" title="Click to go to the Author Index">
             Liu, Yuchen
            </a>
           </td>
           <td class="r">
            Robert Bosch GmbH
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#155650" title="Click to go to the Author Index">
             Palmieri, Luigi
            </a>
           </td>
           <td class="r">
            Robert Bosch GmbH
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#323226" title="Click to go to the Author Index">
             Koch, Sebastian
            </a>
           </td>
           <td class="r">
            Ulm University, Robert Bosch GmbH
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#193240" title="Click to go to the Author Index">
             Georgievski, IIche
            </a>
           </td>
           <td class="r">
            University of Stuttgart
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#346184" title="Click to go to the Author Index">
             Aiello, Marco
            </a>
           </td>
           <td class="r">
            University of Stuttgart
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab853" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#task_planning" title="Click to go to the Keyword Index">
               Task Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#ai_based_methods" title="Click to go to the Keyword Index">
               AI-Based Methods
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#planning__scheduling_and_coordination" title="Click to go to the Keyword Index">
               Planning, Scheduling and Coordination
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Recent advancements in Large Language Models (LLMs) have sparked a revolution across many research fields. In robotics, the integration of common-sense knowledge from LLMs into task and motion planning has drastically advanced the field by unlocking unprecedented levels of context awareness. Despite their vast collection of knowledge, large language models may generate infeasible plans due to hallucinations or missing domain information. To address these challenges and improve plan feasibility and computational efficiency, we introduce DELTA, a novel LLM-informed task planning approach. By using scene graphs as environment representations within LLMs, DELTA achieves rapid generation of precise planning problem descriptions. To enhance planning performance, DELTA decomposes long-term task goals with LLMs into an autoregressive sequence of sub-goals, enabling automated task planners to efficiently solve complex problems. In our extensive evaluation, we show that DELTA enables an efficient and fully automatic task planning pipeline, achieving higher planning success rates and significantly shorter planning times compared to the state of the art.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that1_02">
             08:35-08:40, Paper ThAT1.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('995'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Hey Robot! Personalizing Robot Navigation through Model Predictive Control with a Large Language Model
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#324271" title="Click to go to the Author Index">
             Martinez-Baselga, Diego
            </a>
           </td>
           <td class="r">
            University of Zaragoza
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#285577" title="Click to go to the Author Index">
             de Groot, Oscar
            </a>
           </td>
           <td class="r">
            Delft University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#310517" title="Click to go to the Author Index">
             Knoedler, Luzia
            </a>
           </td>
           <td class="r">
            Delft University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#142433" title="Click to go to the Author Index">
             Alonso-Mora, Javier
            </a>
           </td>
           <td class="r">
            Delft University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#117934" title="Click to go to the Author Index">
             Riazuelo, Luis
            </a>
           </td>
           <td class="r">
            Instituto De Investigación En IngenieríadeAragón, University of Z
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#105222" title="Click to go to the Author Index">
             Montano, Luis
            </a>
           </td>
           <td class="r">
            Universidad De Zaragoza
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab995" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#human_centered_robotics" title="Click to go to the Keyword Index">
               Human-Centered Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#human_aware_motion_planning" title="Click to go to the Keyword Index">
               Human-Aware Motion Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robot navigation methods allow mobile robots to operate in applications such as warehouses or hospitals. While the environment in which the robot operates imposes requirements on its navigation behavior, most existing methods do not allow the end-user to configure the robot's behavior and priorities, possibly leading to undesirable behavior (e.g., fast driving in a hospital). We propose a novel approach to adapt robot motion behavior based on natural language instructions provided by the end-user. Our zero-shot method uses an existing Visual Language Model to interpret a user text query or an image of the environment. This information is used to generate the cost function and reconfigure the parameters of a Model Predictive Controller, translating the user's instruction to the robot's motion behavior. This allows our method to safely and effectively navigate in dynamic and challenging environments. We extensively evaluate our method's individual components and demonstrate the effectiveness of our method on a ground robot in simulation and real-world experiments, and across a variety of environments and user specifications.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that1_03">
             08:40-08:45, Paper ThAT1.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1912'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Large Language Model Based Autonomous Task Planning for Abstract Commands
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#414726" title="Click to go to the Author Index">
             Kwon, Seokjoon
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#316944" title="Click to go to the Author Index">
             Park, Jae-Hyeon
            </a>
           </td>
           <td class="r">
            Samsung Display
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#374193" title="Click to go to the Author Index">
             Jang, Hee-Deok
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#450784" title="Click to go to the Author Index">
             Roh, CheolLae
            </a>
           </td>
           <td class="r">
            Samsung Display Co
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104290" title="Click to go to the Author Index">
             Chang, Dong Eui
            </a>
           </td>
           <td class="r">
            KAIST
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1912" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#task_planning" title="Click to go to the Keyword Index">
               Task Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#robotics_and_automation_in_life_sciences" title="Click to go to the Keyword Index">
               Robotics and Automation in Life Sciences
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Recent advances in large language models (LLMs) have demonstrated exceptional reasoning capabilities in natural language processing, sparking interest in applying LLMs to task planning problems in robotics. Most studies focused on task planning for clear natural language commands that specify target objects and their locations. However, for more user-friendly task execution, it is crucial for robots to autonomously plan and carry out tasks based on abstract natural language commands that may not explicitly mention target objects or locations, such as ‘Put the food ingredients in the same place.’ In this study, we propose an LLM-based autonomous task planning framework that generates task plans for abstract natural language commands. This framework consists of two phases: an environment recognition phase and a task planning phase. In the environment recognition phase, a large vision-language model generates a hierarchical scene graph that captures the relationships between objects and spaces in the environment surrounding a robot agent. During the task planning phase, an LLM uses the scene graph and the abstract user command to formulate a plan for the given task. We validate the effectiveness of the proposed framework in the AI2THOR simulation environment, demonstrating its superior performance in task execution when handling abstract commands.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that1_04">
             08:45-08:50, Paper ThAT1.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2458'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Self-Corrective Task Planning by Inverse Prompting with Large Language Models
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#320640" title="Click to go to the Author Index">
             Lee, Jiho
            </a>
           </td>
           <td class="r">
            Chung-Ang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419050" title="Click to go to the Author Index">
             Lee, Hayun
            </a>
           </td>
           <td class="r">
            Chung-Ang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419033" title="Click to go to the Author Index">
             Kim, Jonghyeon
            </a>
           </td>
           <td class="r">
            Chung-Ang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#179481" title="Click to go to the Author Index">
             Lee, Kyungjae
            </a>
           </td>
           <td class="r">
            Korea University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#163619" title="Click to go to the Author Index">
             Kim, Eunwoo
            </a>
           </td>
           <td class="r">
            Chung-Ang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2458" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#task_planning" title="Click to go to the Keyword Index">
               Task Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In robot task planning, large language models (LLMs) have shown significant promise in generating complex and long-horizon action sequences. However, it is observed that LLMs often produce responses that sound plausible but are not accurate. To address these problems, existing methods typically employ predefined error sets or external knowledge sources, requiring human efforts and computation resources. Recently, self-correction approaches have emerged, where LLM generates and refines plans, identifying errors by itself. Despite their effectiveness, they are more prone to failures in correction due to insufficient reasoning. In this paper, we propose a novel self-corrective planning of tasks with inverse prompting, named InversePrompt, which contains reasoning steps to provide interpretable groundings for feedback. It generates the inverse actions corresponding to generated actions and verifies if these inverse actions can restore the system to its original state, thereby explicitly validating the logical flow of the generated plans. The results on benchmark datasets show an average 16.3% higher success rate over existing LLM-based task planning methods. Our approach offers clearer justifications for feedback in real-world environments, resulting in more successful task completion than existing self-correction approaches across various scenarios.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that1_05">
             08:50-08:55, Paper ThAT1.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4372'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Traffic Regulation-Aware Path Planning with Regulation Databases and Vision-Language Models
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#311159" title="Click to go to the Author Index">
             Han, Xu
            </a>
           </td>
           <td class="r">
            University of California Los Angeles
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425711" title="Click to go to the Author Index">
             Wu, Zhiwen
            </a>
           </td>
           <td class="r">
            University of California, Los Angeles
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#283337" title="Click to go to the Author Index">
             Xia, Xin
            </a>
           </td>
           <td class="r">
            University of California, Los Angeles
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#311261" title="Click to go to the Author Index">
             Ma, Jiaqi
            </a>
           </td>
           <td class="r">
            University of California, Los Angeles
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4372" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#integrated_planning_and_control" title="Click to go to the Keyword Index">
               Integrated Planning and Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#planning_under_uncertainty" title="Click to go to the Keyword Index">
               Planning under Uncertainty
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper introduces and tests a framework that integrates traffic regulation compliance into automated driving systems (ADS). The framework enables ADS to follow traffic laws and make informed decisions based on the driving environment. Using RGB camera inputs and a vision-language model (VLM), the system generates descriptive text to support a regulation-aware decision-making process, ensuring legal and safe driving practices. This information is combined with a machine-readable ADS regulation database to guide future driving plans within legal constraints. Key features include: 1) a regulation database supporting ADS decision-making, 2) an automated process using sensor input for regulation-aware path planning, and 3) validation in both simulated and real-world environments. Particularly, the real-world vehicle tests not only assess the framework's performance but also evaluate the potential and challenges of VLMs to solve complex driving problems by integrating detection, reasoning, and planning. This work enhances the legality, safety, and public trust in ADS, representing a significant step forward in the field.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that1_06">
             08:55-09:00, Paper ThAT1.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('5043'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              DrPlanner: Diagnosis and Repair of Motion Planners for Automated Vehicles Using Large Language Models
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#334045" title="Click to go to the Author Index">
             Lin, Yuanfei
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#309158" title="Click to go to the Author Index">
             Li, Chenran
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#266445" title="Click to go to the Author Index">
             Ding, Mingyu
            </a>
           </td>
           <td class="r">
            UC Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#100071" title="Click to go to the Author Index">
             Tomizuka, Masayoshi
            </a>
           </td>
           <td class="r">
            University of California
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#170266" title="Click to go to the Author Index">
             Zhan, Wei
            </a>
           </td>
           <td class="r">
            Univeristy of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#114506" title="Click to go to the Author Index">
             Althoff, Matthias
            </a>
           </td>
           <td class="r">
            Technische Universität München
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab5043" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#integrated_planning_and_learning" title="Click to go to the Keyword Index">
               Integrated Planning and Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#intelligent_transportation_systems" title="Click to go to the Keyword Index">
               Intelligent Transportation Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Motion planners are essential for the safe operation of automated vehicles across various scenarios. However, no motion planning algorithm has achieved perfection in the literature, and improving its performance is often time-consuming and labor-intensive. To tackle the aforementioned issues, we present DrPlanner, the first framework designed to automatically diagnose and repair motion planners using large language models. Initially, we generate a structured description of the planner and its planned trajectories from both natural and programming languages. Leveraging the profound capabilities of large language models in addressing reasoning challenges, our framework returns repaired planners with detailed diagnostic descriptions. Furthermore, the framework advances iteratively with continuous feedback from the evaluation of the repaired outcomes. Our approach is validated using both search- and sampling-based motion planners for automated vehicles; experimental results highlight the need for demonstrations in the prompt and show the ability of our framework to effectively identify and rectify elusive issues.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that2">
             <b>
              ThAT2
             </b>
            </a>
           </td>
           <td class="r">
            301
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#that2" title="Click to go to the Program at a Glance">
             <b>
              SLAM 5
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#102280" title="Click to go to the Author Index">
             Zelek, John S.
            </a>
           </td>
           <td class="r">
            University of Waterloo
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that2_01">
             08:30-08:35, Paper ThAT2.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('478'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              MGS-SLAM: Monocular Sparse Tracking and Gaussian Mapping with Depth Smooth Regularization
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#390189" title="Click to go to the Author Index">
             Zhu, Pengcheng
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#347140" title="Click to go to the Author Index">
             Zhuang, Yaoming
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#406408" title="Click to go to the Author Index">
             Chen, Baoquan
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#349174" title="Click to go to the Author Index">
             Li, Li
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#288164" title="Click to go to the Author Index">
             Wu, Chengdong
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#349209" title="Click to go to the Author Index">
             Liu, Zhanlin
            </a>
           </td>
           <td class="r">
            University of Washington
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab478" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This letter introduces a novel framework for dense Visual Simultaneous Localization and Mapping (VSLAM) based on Gaussian Splatting. Recently, SLAM based on Gaussian Splatting has shown promising results. However, in monocular scenarios, the Gaussian maps reconstructed lack geometric accuracy and exhibit weaker tracking capability. To address these limitations, we jointly optimize sparse visual odometry tracking and 3D Gaussian Splatting scene representation for the first time. Estimating depth maps on visual odometry keyframes window using a fast Multi-View Stereo (MVS) network for the geometric supervision of Gaussian maps. Furthermore, we propose a depth smooth loss and Sparse-Dense Adjustment Ring (SDAR) to reduce the negative effect of estimated depth maps and preserve the consistency in scale between the visual odometry and Gaussian maps. We have evaluated our system across various synthetic and real-world datasets. The accuracy of our poses estimation surpasses existing methods and achieves state-of-the-art. Additionally, it outperforms previous monocular methods in terms of novel view synthesis and geometric reconstruction fidelities.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that2_02">
             08:35-08:40, Paper ThAT2.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1306'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              GARAD-SLAM: 3D GAussian Splatting for Real-Time Anti Dynamic SLAM
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#346050" title="Click to go to the Author Index">
             Li, Mingrui
            </a>
           </td>
           <td class="r">
            Dalian University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416683" title="Click to go to the Author Index">
             Chen, Weijian
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417545" title="Click to go to the Author Index">
             Cheng, Na
            </a>
           </td>
           <td class="r">
            Dalian University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416965" title="Click to go to the Author Index">
             Xu, Jingyuan
            </a>
           </td>
           <td class="r">
            Dalian University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#395306" title="Click to go to the Author Index">
             Li, Dong
            </a>
           </td>
           <td class="r">
            University of Macau
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#360071" title="Click to go to the Author Index">
             Wang, Hongyu
            </a>
           </td>
           <td class="r">
            Dalian University of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1306" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The 3D Gaussian Splatting (3DGS)-based SLAM system has garnered widespread attention due to its excellent performance in real-time high-fidelity rendering. However, in real-world environments filled with dynamic objects, existing 3DGS-based SLAM systems often face mapping errors and tracking drift issues. To address this, we propose GARAD-SLAM, a real-time 3DGS-based SLAM system tailored for dynamic scenes. In terms of tracking, unlike traditional methods, we directly perform dynamic segmentation on Gaussians and map them back to the front end to obtain dynamic point labels through a Gaussian pyramid network, achieving precise dynamic removal and robust tracking. For mapping, we impose rendering penalties on dynamically labeled Gaussians updated through the network to avoid irreversible erroneous removal caused by simple pruning. Our results on real-world datasets demonstrate that our method is competitive in tracking compared to baseline methods, generating fewer artifacts and higher-quality reconstructions in rendering.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that2_03">
             08:40-08:45, Paper ThAT2.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1519'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Optimizing NeRF-Based SLAM with Trajectory Smoothness Constraints
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#412579" title="Click to go to the Author Index">
             He, Yicheng
            </a>
           </td>
           <td class="r">
            Southern University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#320158" title="Click to go to the Author Index">
             Chen, Guangcheng
            </a>
           </td>
           <td class="r">
            Southern University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#100155" title="Click to go to the Author Index">
             Zhang, Hong
            </a>
           </td>
           <td class="r">
            SUSTech
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1519" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The joint optimization of Neural Radiance Fields (NeRF) and camera trajectories has been widely applied in SLAM tasks due to its superior dense mapping quality and consistency. NeRF-based SLAM learns camera poses using constraints by implicit map representation. A widely observed phenomenon that results from the constraints of this form is jerky and physically unrealistic estimated camera motion, which in turn affects the map quality. To address this deficiency of current NeRF-based SLAM, we propose in this paper TS-SLAM (TS for Trajectory Smoothness). It introduces smoothness constraints on camera trajectories by representing them with uniform cubic B-splines with continuous acceleration that guarantees smooth camera motion. Benefiting from the differentiability and local control properties of B-splines, TS-SLAM can incrementally learn the control points end-to-end using a sliding window paradigm. Additionally, we regularize camera trajectories by exploiting the dynamics prior to further smooth trajectories. Experimental results demonstrate that TS-SLAM achieves superior trajectory accuracy and improves mapping quality versus NeRF-based SLAM that does not employ the above smoothness constraints.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that2_04">
             08:45-08:50, Paper ThAT2.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1525'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              MGSO: Monocular Real-Time Photometric SLAM with Efficient 3D Gaussian Splatting
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#362380" title="Click to go to the Author Index">
             Hu, Kevin
            </a>
           </td>
           <td class="r">
            University of Waterloo
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#356547" title="Click to go to the Author Index">
             Abboud, Nicolas
            </a>
           </td>
           <td class="r">
            American University of Beirut
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421751" title="Click to go to the Author Index">
             Ali, Muhammad Q.
            </a>
           </td>
           <td class="r">
            University of Waterloo
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#357650" title="Click to go to the Author Index">
             Yang, Adam Srebrnjak
            </a>
           </td>
           <td class="r">
            University of Waterloo
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#100044" title="Click to go to the Author Index">
             Elhajj, Imad
            </a>
           </td>
           <td class="r">
            American University of Beirut
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101924" title="Click to go to the Author Index">
             Asmar, Daniel
            </a>
           </td>
           <td class="r">
            American University of Beirut
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#327962" title="Click to go to the Author Index">
             Chen, Yuhao
            </a>
           </td>
           <td class="r">
            University of Waterloo
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#102280" title="Click to go to the Author Index">
             Zelek, John S.
            </a>
           </td>
           <td class="r">
            University of Waterloo
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1525" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Real-time SLAM with dense 3D mapping is computationally challenging, especially on resource-limited devices. The recent development of 3D Gaussian Splatting (3DGS) offers a promising approach for real-time dense 3D reconstruction. However, existing 3DGS-based SLAM systems struggle to balance hardware simplicity, speed, and map quality. Most systems excel in one or two of the aforementioned aspects but rarely achieve all. A key issue is the difficulty of initializing 3D Gaussians while concurrently conducting SLAM. To address these challenges, we present Monocular GSO (MGSO), a novel real-time SLAM system that integrates photometric SLAM with 3DGS. Photometric SLAM provides dense structured point clouds for 3DGS initialization, accelerating optimization and producing more efficient maps with fewer Gaussians. As a result, experiments show that our system generates reconstructions with a balance of quality, memory efficiency, and speed that outperforms the state-of-the-art. Furthermore, our system achieves all results using RGB inputs. We evaluate the Replica, TUM-RGBD, and EuRoC datasets against current live dense reconstruction systems. Not only do we surpass contemporary systems, but experiments also show that we maintain our performance on laptop hardware, making it a practical solution for robotics, A/R, and other real-time applications.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that2_05">
             08:50-08:55, Paper ThAT2.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2360'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              RGB-Only Gaussian Splatting SLAM for Unbounded Outdoor Scenes
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420043" title="Click to go to the Author Index">
             Yu, Sicheng
            </a>
           </td>
           <td class="r">
            HKUST(gz)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#372764" title="Click to go to the Author Index">
             Cheng, Chong
            </a>
           </td>
           <td class="r">
            HKUST(GZ)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419987" title="Click to go to the Author Index">
             Zhou, Yifan
            </a>
           </td>
           <td class="r">
            The Hong Kong University of Science and Technology (Guangzhou)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423182" title="Click to go to the Author Index">
             Yang, Xiaojun
            </a>
           </td>
           <td class="r">
            The Hong Kong University of Science and Technology (Guangzhou)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423920" title="Click to go to the Author Index">
             Wang, Hao
            </a>
           </td>
           <td class="r">
            HKUST(GZ)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2360" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             3D Gaussian Splatting (3DGS) has become a popular solution in SLAM, as it can produce high-fidelity novel views. However, previous GS-based methods primarily target indoor scenes and rely on RGB-D sensors or pre-trained depth estimation models, hence underperforming in outdoor scenarios. To address this issue, we propose a RGB-only gaussian splatting SLAM method for unbounded outdoor scenes—OpenGS-SLAM. Technically, we first employ a pointmap regression network to generate consistent pointmaps between frames for pose estimation. Compared to commonly used depth maps, pointmaps include spatial relationships and scene geometry across multiple views, enabling robust camera pose estimation. Then, we propose integrating the estimated camera poses with 3DGS rendering as an end-to-end differentiable pipeline. Our method achieves simultaneous optimization of camera poses and 3DGS scene parameters, significantly enhancing system tracking accuracy. Specifically, we also design an adaptive scale mapper for the pointmap regression network, which provides more accurate pointmap mapping to the 3DGS map representation. Our experiments on the Waymo dataset demonstrate that OpenGS-SLAM reduces tracking error to 9.8% of previous 3DGS methods, and achieves state-of-the-art results in novel view synthesis. Project page: https://opengsslam.github.io/.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that2_06">
             08:55-09:00, Paper ThAT2.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3326'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              FGO-SLAM: Enhancing Gaussian SLAM with Globally Consistent Opacity Radiance Field
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416440" title="Click to go to the Author Index">
             Zhu, Fan
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423201" title="Click to go to the Author Index">
             Zhao, Yifan
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#211831" title="Click to go to the Author Index">
             Chen, Ziyu
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#359790" title="Click to go to the Author Index">
             Yu, Biao
            </a>
           </td>
           <td class="r">
            Hefei Institutes of Physical Science, Chinese Academy of Science
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#359793" title="Click to go to the Author Index">
             Zhu, Hui
            </a>
           </td>
           <td class="r">
            Hefei Institutes of Physical Science, Chinese Academy of Science
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3326" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#embodied_cognitive_science" title="Click to go to the Keyword Index">
               Embodied Cognitive Science
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Visual SLAM has regained attention due to its ability to provide perception capabilities and simulation test data for Embodied AI. However, traditional SLAM systems struggle to meet the demands of high-quality scene reconstruction, and Gaussian SLAM systems, despite their rapid rendering and high-quality mapping capabilities, lack effective pose optimization methods and face challenges in geometric reconstruction. To address these issues, we introduce FGO-SLAM, a Gaussian SLAM system that employs an opacity radiance field as the scene representation to enhance geometric mapping performance. After initial pose estimation, we apply global adjustment to optimize camera poses and sparse point cloud, ensuring robust tracking of our system. Additionally, we maintain a globally consistent opacity radiance field based on 3D Gaussians and introduce depth distortion and normal consistency terms to refine the scene representation. Furthermore, after constructing tetrahedral grids, we identify level sets to directly extract surfaces from 3D Gaussians. Results across various real-world and large-scale synthetic datasets demonstrate that our method achieves state-of-the-art tracking accuracy and mapping performance.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that3">
             <b>
              ThAT3
             </b>
            </a>
           </td>
           <td class="r">
            303
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#that3" title="Click to go to the Program at a Glance">
             <b>
              Point Cloud Registration
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#227869" title="Click to go to the Author Index">
             Lim, Hyungtae
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that3_01">
             08:30-08:35, Paper ThAT3.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('280'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Multi-View Registration of Partially Overlapping Point Clouds for Robotic Manipulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#393004" title="Click to go to the Author Index">
             Xie, Yuzhen
            </a>
           </td>
           <td class="r">
            Southeast University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#112790" title="Click to go to the Author Index">
             Song, Aiguo
            </a>
           </td>
           <td class="r">
            Southeast University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab280" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#data_sets_for_robotic_vision" title="Click to go to the Keyword Index">
               Data Sets for Robotic Vision
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Point cloud registration is a fundamental task in intelligent robots, aiming to achieve globally consistent geometric structures and providing data support for robotic manipulation. Due to the limited view of measurement devices, it is necessary to collect point clouds from multiple views to construct a complete model. Previous multi-view registration methods rely on sufficient overlap and registering all pairs of point clouds, resulting in slow convergence and high cumulative errors. To solve these challenges, we present a multi-view registration method based on the point-to-plane model and pose graph. We introduce a robust kernel into the objective function to diminish registration errors caused by mismatched points. Additionally, an enhanced Euclidean clustering method is proposed for extracting object point clouds. Subsequently, by establishing pose constraints on non-adjacent frames of point clouds, the cumulative error is reduced, achieving global optimization based on the pose graph. Experimental results demonstrate the robustness of our method with respect to overlap ratios, successfully registering point clouds with overlap ratio exceeding 30%. In comparison to other techniques, our method can reduce the E(R) of multi-view registration by 13.54% and E(t) by 18.72%, effectively reducing the cumulative error.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that3_02">
             08:35-08:40, Paper ThAT3.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1090'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Kinematic-ICP: Enhancing LiDAR Odometry with Kinematic Constraints for Wheeled Mobile Robots Moving on Planar Surfaces
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#246518" title="Click to go to the Author Index">
             Guadagnino, Tiziano
            </a>
           </td>
           <td class="r">
            University of Bonn
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#288232" title="Click to go to the Author Index">
             Mersch, Benedikt
            </a>
           </td>
           <td class="r">
            University of Bonn
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#244059" title="Click to go to the Author Index">
             Vizzo, Ignacio
            </a>
           </td>
           <td class="r">
            Dexory
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#373782" title="Click to go to the Author Index">
             Gupta, Saurabh
            </a>
           </td>
           <td class="r">
            University of Bonn
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#354333" title="Click to go to the Author Index">
             Malladi, Meher Venkata Ramakrishna
            </a>
           </td>
           <td class="r">
            University of Bonn
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#354300" title="Click to go to the Author Index">
             Lobefaro, Luca
            </a>
           </td>
           <td class="r">
            University of Bonn
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#158954" title="Click to go to the Author Index">
             Doisy, Guillaume
            </a>
           </td>
           <td class="r">
            Dexory
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101642" title="Click to go to the Author Index">
             Stachniss, Cyrill
            </a>
           </td>
           <td class="r">
            University of Bonn
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1090" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             LiDAR odometry is essential for many robotics applications, including 3D mapping, navigation, and simultaneous localization and mapping. LiDAR odometry systems are usually based on some form of point cloud registration to compute the ego-motion of a mobile robot. Yet, few of today's LiDAR odometry systems consider domain-specific knowledge or the kinematic model of the mobile platform during the point cloud alignment. In this paper, we present Kinematic-ICP, a LiDAR odometry system that focuses on wheeled mobile robots equipped with a 3D LiDAR and moving on a planar surface, which is a common assumption for warehouses, offices, hospitals, etc. Our approach introduces kinematic constraints within the optimization of a traditional point-to-point iterative closest point scheme. In this way, the resulting motion follows the kinematic constraints of the platform, effectively exploiting the robot's wheel odometry and the 3D LiDAR observations. We dynamically adjust the influence of LiDAR measurements and wheel odometry in our optimization scheme, allowing the system to handle degenerate scenarios such as feature-poor corridors. We evaluate our approach on robots operating in large-scale warehouse environments, but also outdoors. The experiments show that our approach achieves top performances and is more accurate than wheel odometry and common LiDAR odometry systems. Kinematic-ICP has been recently deployed in the Dexory fleet of robots operating in warehouses worldwide at their customers' sites, showing that our method can run in the real world alongside a complete navigation stack.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that3_03">
             08:40-08:45, Paper ThAT3.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1440'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              GERA: Geometric Embedding for Efficient Point Registration Analysis
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#391846" title="Click to go to the Author Index">
             Li, Geng
            </a>
           </td>
           <td class="r">
            Shandong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#336709" title="Click to go to the Author Index">
             Cao, Haozhi
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#391757" title="Click to go to the Author Index">
             Liu, Mingyang
            </a>
           </td>
           <td class="r">
            Shandong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#185927" title="Click to go to the Author Index">
             Yuan, Shenghai
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#212797" title="Click to go to the Author Index">
             Yang, Jianfei
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1440" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_medical_robotics" title="Click to go to the Keyword Index">
               Computer Vision for Medical Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#representation_learning" title="Click to go to the Keyword Index">
               Representation Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Point cloud registration aims to provide estimated transformations to align 3D point clouds, which plays a crucial role in pose estimation of various navigation systems, such as surgical guidance systems and autonomous vehicles. Despite the impressive performance of recent models on benchmark datasets, many rely on complex modules like KPConv and Transformers, which impose significant computational and memory demands. These requirements hinder their practical application, particularly in resource-constrained environments such as mobile robotics. In this paper, we propose a novel point cloud registration network that leverages a pure MLP architecture, constructing geometric information offline. This approach eliminates the computational and memory burdens associated with traditional complex feature extractors and significantly reduces training time and resource consumption. Our method is the first to replace 3D coordinate inputs with offline-constructed geometric encoding, improving generalization and stability, as demonstrated by Maximum Mean Discrepancy (MMD) comparisons. This efficient and accurate geometric representation marks a significant advancement in point cloud analysis, particularly for applications requiring fast and reliable processing.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that3_04">
             08:45-08:50, Paper ThAT3.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2907'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              KISS-Matcher: Fast and Robust Point Cloud Registration Revisited
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#227869" title="Click to go to the Author Index">
             Lim, Hyungtae
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#350836" title="Click to go to the Author Index">
             Kim, Daebeom
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#412155" title="Click to go to the Author Index">
             Shin, Gunhee
            </a>
           </td>
           <td class="r">
            KAIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#225329" title="Click to go to the Author Index">
             Shi, Jingnan
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#244059" title="Click to go to the Author Index">
             Vizzo, Ignacio
            </a>
           </td>
           <td class="r">
            Dexory
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104490" title="Click to go to the Author Index">
             Myung, Hyun
            </a>
           </td>
           <td class="r">
            KAIST (Korea Advanced Institute of Science and Technology)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#148100" title="Click to go to the Author Index">
             Park, Jaesik
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#119426" title="Click to go to the Author Index">
             Carlone, Luca
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2907" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             While global point cloud registration systems have advanced significantly in all aspects, many studies have focused on specific components, such as feature extraction, graph-theoretic pruning, or pose solvers. In this paper, we take a holistic view on the registration problem and develop an open-source and versatile C++ library for point cloud registration, called textit{KISS-Matcher}. textit{KISS-Matcher} combines a novel feature detector, textit{Faster-PFH}, that improves over the classical fast point feature histogram (FPFH). Moreover, it adopts a k-core-based graph-theoretic pruning to reduce the time complexity of rejecting outlier correspondences. Finally, it combines these modules in a complete, user-friendly, and ready-to-use pipeline. As verified by extensive experiments, KISS-Matcher has superior scalability and broad applicability, achieving a substantial speed-up compared to state-of-the-art outlier-robust registration pipelines while preserving accuracy. Our code will be available at href{https://github.com/MIT-SPARK/KISS-Matcher}{texttt{ht tps://github.com/MIT-SPARK/KISS-Matcher}}.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that3_05">
             08:50-08:55, Paper ThAT3.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3444'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SANDRO: A Robust Solver with a Splitting Strategy for Point Cloud Registration
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#418328" title="Click to go to the Author Index">
             Adlerstein, Michael
            </a>
           </td>
           <td class="r">
            Italian Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#253766" title="Click to go to the Author Index">
             Soares, João Carlos Virgolino
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#266110" title="Click to go to the Author Index">
             Bratta, Angelo
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#108792" title="Click to go to the Author Index">
             Semini, Claudio
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3444" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Point cloud registration is a critical problem in computer vision and robotics, especially in the field of navigation. Current methods often fail when faced with high outlier rates or take a long time to converge to a suitable solution. In this work, we introduce a novel algorithm for point cloud registration called SANDRO (Splitting strategy for point cloud Alignment using Non-convex anD Robust Optimization), which combines an Iteratively Reweighted Least Squares (IRLS) framework with a robust loss function with graduated non-convexity. This approach is further enhanced by a splitting strategy designed to handle high outlier rates and skewed distributions of outliers. SANDRO is capable of addressing important limitations of existing methods, as in challenging scenarios where the presence of high outlier rates and point cloud symmetries significantly hinder convergence. SANDRO achieves superior performance in terms of success rate when compared to the state-of-the-art methods, demonstrating a 20% improvement from the current state of the art when tested on the Redwood real dataset and 60% improvement when tested on synthetic data.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that3_06">
             08:55-09:00, Paper ThAT3.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3516'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Bridging In-Situ and Satellite Data: Enhancing Gas Concentration Estimation through Integration of Data-Driven and Physics-Based Modeling
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#235923" title="Click to go to the Author Index">
             Lu, Guoyu
            </a>
           </td>
           <td class="r">
            University of Georgia
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3516" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#visual_tracking" title="Click to go to the Keyword Index">
               Visual Tracking
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Gas concentration estimation is crucial for understanding and mitigating climate change. While most research and monitoring efforts focus on major greenhouse gases such as CO2, significantly less attention has been given to trace gases like NO2, which play a critical role in atmospheric chemistry and air quality. This paper aims to enhance trace gas concentration estimation by integrating physics-based models into data-driven neural network frameworks. Furthermore, to improve large-scale estimation accuracy, we incorporate in-situ measurements to refine neural network models trained on satellite observations. The resulting model can provide reliable large-scale gas concentration estimates, particularly for locations lacking precise in-situ measurements. This approach offers a novel pathway to enhance the accuracy and applicability of gas monitoring for climate and environmental research. While NO2 serves as the target trace gas in this study, the proposed framework is potentially applicable to the prediction of other atmospheric gas concentrations.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that4">
             <b>
              ThAT4
             </b>
            </a>
           </td>
           <td class="r">
            304
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#that4" title="Click to go to the Program at a Glance">
             <b>
              Image and 3D Segmentation 1
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that4_01">
             08:30-08:35, Paper ThAT4.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1233'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Novel Decomposed Feature-Oriented Framework for Open-Set Semantic Segmentation on LiDAR Data
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#351800" title="Click to go to the Author Index">
             Deng, Wenbang
            </a>
           </td>
           <td class="r">
            National University of Defense Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#215437" title="Click to go to the Author Index">
             Chen, Xieyuanli
            </a>
           </td>
           <td class="r">
            National University of Defense Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#232757" title="Click to go to the Author Index">
             Yu, Qinghua
            </a>
           </td>
           <td class="r">
            National University of Defense Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420768" title="Click to go to the Author Index">
             He, Yunze
            </a>
           </td>
           <td class="r">
            Hunan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#134317" title="Click to go to the Author Index">
             Xiao, Junhao
            </a>
           </td>
           <td class="r">
            National University of Defense Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#122468" title="Click to go to the Author Index">
             Lu, Huimin
            </a>
           </td>
           <td class="r">
            National University of Defense Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1233" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_transportation" title="Click to go to the Keyword Index">
               Computer Vision for Transportation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Semantic segmentation is a key technique that enables mobile robots to understand and navigate surrounding environments autonomously. However, most existing works focus on segmenting known objects, overlooking the identification of unknown classes, which is common in real-world applications. In this paper, we propose a feature-oriented framework for open-set semantic segmentation on LiDAR data, capable of identifying unknown objects while retaining the ability to classify known ones. We design a decomposed dual-decoder network to simultaneously perform closed-set semantic segmentation and generate distinctive features for unknown objects. The network is trained with multi-objective loss functions to capture the characteristics of known and unknown objects. Using the extracted features, we introduce an anomaly detection mechanism to identify unknown objects. By integrating the results of close-set semantic segmentation and anomaly detection, we achieve effective feature-driven LiDAR open-set semantic segmentation. Evaluations on both SemanticKITTI and nuScenes datasets demonstrate that our proposed framework significantly outperforms state-of-the-art methods. The source code will be made publicly available at https://github.com/nubot-nudt/DOSS.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that4_02">
             08:35-08:40, Paper ThAT4.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1557'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SAM-Guided Pseudo Label Enhancement for Multi-Modal 3D Semantic Segmentation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#381516" title="Click to go to the Author Index">
             Yang, Mingyu
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421063" title="Click to go to the Author Index">
             Lu, Jitong
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#292282" title="Click to go to the Author Index">
             Kim, Hun-Seok
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1557" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#sensor_fusion" title="Click to go to the Keyword Index">
               Sensor Fusion
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Multi-modal 3D semantic segmentation is vital for applications such as autonomous driving and virtual reality (VR). To effectively deploy these models in real-world scenarios, it is essential to employ cross-domain adaptation techniques that bridge the gap between training data and real-world data. Recently, self-training with pseudo-labels has emerged as a predominant method for cross-domain adaptation in multi-modal 3D semantic segmentation. However, generating reliable pseudo-labels necessitates stringent constraints, which often result in sparse pseudo-labels after pruning. This sparsity can potentially hinder performance improvement during the adaptation process. We propose an image-guided pseudo-label enhancement approach that leverages the complementary 2D prior knowledge from the Segment Anything Model (SAM) to introduce more reliable pseudo-labels, thereby boosting domain adaptation performance. Specifically, given a 3D point cloud and the SAM masks from its paired image data, we collect all 3D points covered by each SAM mask that potentially belong to the same object. Then our method refines the pseudo-labels within each SAM mask in two steps. First, we determine the class label for each mask using majority voting and employ various constraints to filter out unreliable mask labels. Next, we introduce Geometry-Aware Progressive Propagation (GAPP) which propagates the mask label to all 3D points within the SAM mask while avoiding outliers caused by 2D-3D misalignment. Experiments conducted across multiple datasets and domain adaptation scenarios demonstrate that our proposed method significantly increases the quantity of high-quality pseudo-labels and enhances the adaptation performance over baseline methods.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that4_03">
             08:40-08:45, Paper ThAT4.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1995'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Robot Manipulation in Salient Vision through Referring Image Segmentation and Geometric Constraints
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#236054" title="Click to go to the Author Index">
             Jiang, Chen
            </a>
           </td>
           <td class="r">
            University of Alberta
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#329360" title="Click to go to the Author Index">
             Wang, Allie
            </a>
           </td>
           <td class="r">
            University of Alberta
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#106699" title="Click to go to the Author Index">
             Jagersand, Martin
            </a>
           </td>
           <td class="r">
            University of Alberta
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1995" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#learning_categories_and_concepts" title="Click to go to the Keyword Index">
               Learning Categories and Concepts
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#visual_servoing" title="Click to go to the Keyword Index">
               Visual Servoing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper, we perform robot manipulation activities in real-world environments with language contexts by integrating a compact referring image segmentation model into the robot's perception module. First, we propose CLIPU^2Net, a lightweight referring image segmentation model designed for fine-grain boundary and structure segmentation from language expressions. Then, we deploy the model in an eye-in-hand visual servoing system to enact robot control in the real world. The key to our system is the representation of salient visual information as geometric constraints, linking the robot’s visual perception to actionable commands. Experimental results on 46 real-world robot manipulation tasks demonstrate that our method outperforms traditional visual servoing methods relying on labor-intensive feature annotations, excels in fine-grain referring image segmentation with a compact decoder size of 6.6 MB, and supports robot control across diverse contexts.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that4_04">
             08:45-08:50, Paper ThAT4.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2156'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Boosting Cross-Spectral Unsupervised Domain Adaptation for Thermal Semantic Segmentation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#345626" title="Click to go to the Author Index">
             Kwon, SeokJun
            </a>
           </td>
           <td class="r">
            Sejong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#302967" title="Click to go to the Author Index">
             Shin, Jeongmin
            </a>
           </td>
           <td class="r">
            Sejong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#192830" title="Click to go to the Author Index">
             Kim, Namil
            </a>
           </td>
           <td class="r">
            NAVER LABS
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#192842" title="Click to go to the Author Index">
             Hwang, Soonmin
            </a>
           </td>
           <td class="r">
            Hanyang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#111307" title="Click to go to the Author Index">
             Choi, Yukyung
            </a>
           </td>
           <td class="r">
            Sejong University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2156" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#recognition" title="Click to go to the Keyword Index">
               Recognition
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In autonomous driving, thermal image semantic segmentation has emerged as a critical research area, owing to its ability to provide robust scene understanding under adverse visual conditions. In particular, unsupervised domain adaptation (UDA) for thermal image segmentation can be an efficient solution to address the lack of labeled thermal datasets. Nevertheless, since these methods do not effectively utilize the complementary information between RGB and thermal images, they significantly decrease performance during domain adaptation. In this paper, we present a comprehensive study on cross-spectral UDA for thermal image semantic segmentation. We first propose a novel masked mutual learning strategy that promotes complementary information exchange by selectively transferring results between each spectral model while masking out uncertain regions. Additionally, we introduce a novel prototypical self-supervised loss designed to enhance the performance of the thermal segmentation model in nighttime scenarios. This approach addresses the limitations of RGB pre-trained networks, which cannot effectively transfer knowledge under low illumination due to the inherent constraints of RGB sensors. In experiments, our method achieves higher performance over previous UDA methods and comparable performance to state-of-the-art supervised methods.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that4_05">
             08:50-08:55, Paper ThAT4.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2712'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              VideoSAM: Open-World Video Segmentation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417383" title="Click to go to the Author Index">
             Guo, Pinxue
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#283806" title="Click to go to the Author Index">
             Zhao, Zixu
            </a>
           </td>
           <td class="r">
            Amazon Web Services
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#377646" title="Click to go to the Author Index">
             Gao, Jianxiong
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420170" title="Click to go to the Author Index">
             Wu, Chongruo
            </a>
           </td>
           <td class="r">
            UC Davis
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423691" title="Click to go to the Author Index">
             He, Tong
            </a>
           </td>
           <td class="r">
            Amazon.com
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423915" title="Click to go to the Author Index">
             Zhang, Zheng
            </a>
           </td>
           <td class="r">
            AWS
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423106" title="Click to go to the Author Index">
             Xiao, Tianjun
            </a>
           </td>
           <td class="r">
            AWS
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#110529" title="Click to go to the Author Index">
             Zhang, Wenqiang
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2712" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#recognition" title="Click to go to the Keyword Index">
               Recognition
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Video segmentation is essential for advancing robotics and autonomous driving, particularly in open-world settings where continuous perception and object association across video frames are critical. While the Segment Anything Model (SAM) has excelled in static image segmentation, extending its capabilities to video segmentation poses significant challenges. We tackle two major hurdles: a) SAM’s embedding limitations in associating objects across frames, and b) granularity inconsistencies in object segmentation. To this end, we introduce VideoSAM, an end-to-end framework designed to address these challenges by improving object tracking and segmentation consistency in dynamic environments. VideoSAM integrates an agglomerated backbone, RADIO, enabling object association through similarity metrics and introduces Cycle-ack-Pairs Propagation with a memory mechanism for stable object tracking. Additionally, we incorporate an autoregressive object-token mechanism within the SAM decoder to maintain consistent granularity across frames. Our experiments on the UVO and BURST benchmark, and also robotic videos, demonstrate VideoSAM’s effectiveness and robustness in real-world scenarios. All codes will be available.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that4_06">
             08:55-09:00, Paper ThAT4.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4194'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Monocular Depth Estimation and Segmentation for Transparent Object with Iterative Semantic and Geometric Fusion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#381417" title="Click to go to the Author Index">
             Liu, Jiangyuan
            </a>
           </td>
           <td class="r">
            University of Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#225858" title="Click to go to the Author Index">
             Ma, Hongxuan
            </a>
           </td>
           <td class="r">
            Institute of Automation, Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#379002" title="Click to go to the Author Index">
             Guo, Yuxin
            </a>
           </td>
           <td class="r">
            University of Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#377786" title="Click to go to the Author Index">
             Zhao, Yuhao
            </a>
           </td>
           <td class="r">
            Institute of Automation, Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#255315" title="Click to go to the Author Index">
             Zhang, Chi
            </a>
           </td>
           <td class="r">
            Shijiazhuang Tiedao University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#287085" title="Click to go to the Author Index">
             Sui, Wei
            </a>
           </td>
           <td class="r">
            Soochow University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#100340" title="Click to go to the Author Index">
             Zou, Wei
            </a>
           </td>
           <td class="r">
            Chinese Academy of Sciences, University of Chinese Academy of Sci
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4194" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Transparent object perception is indispensable for numerous robotic tasks. However, accurately segmenting and estimating the depth of transparent objects remain challenging due to complex optical properties. Existing methods primarily delve into only one task using extra inputs or specialized sensors, neglecting the valuable interactions among tasks and the subsequent refinement process, leading to suboptimal and blurry predictions. To address these issues, we propose a monocular framework, which is the first to excel in both segmentation and depth estimation of transparent objects, with only a single-image input. Specifically, we devise a novel semantic and geometric fusion module, effectively integrating the multi-scale information between tasks. In addition, drawing inspiration from human perception of objects, we further incorporate an iterative strategy, which progressively refines initial features for clearer results. Experiments on two challenging synthetic and real-world datasets demonstrate that our model surpasses state-of-the-art monocular, stereo, and multi-view methods by a large margin of about 38.8%-46.2% with only a single RGB input. Codes and models are publicly available at https://github.com/L-J-Yuan/MODEST.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that5">
             <b>
              ThAT5
             </b>
            </a>
           </td>
           <td class="r">
            305
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#that5" title="Click to go to the Program at a Glance">
             <b>
              Planinng and Control for Legged Robots 3
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#194949" title="Click to go to the Author Index">
             Qian, Feifei
            </a>
           </td>
           <td class="r">
            University of Southern California
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that5_01">
             08:30-08:35, Paper ThAT5.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('136'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Obstacle-Aided Trajectory Control of a Quadrupedal Robot through Sequential Gait Composition
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#320183" title="Click to go to the Author Index">
             Hu, Haodi
            </a>
           </td>
           <td class="r">
            University of Southern California
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#194949" title="Click to go to the Author Index">
             Qian, Feifei
            </a>
           </td>
           <td class="r">
            University of Southern California
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab136" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#biologically_inspired_robots" title="Click to go to the Keyword Index">
               Biologically-Inspired Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#dynamics" title="Click to go to the Keyword Index">
               Dynamics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#rough_terrain_locomotion" title="Click to go to the Keyword Index">
               Rough Terrain Locomotion
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Modeling and controlling legged robot locomotion on terrains with densely distributed large rocks and boulders are fundamentally challenging. Unlike traditional methods which often consider these rocks and boulders as obstacles and attempt to find a clear path to circumvent them, in this study we aim to develop methods for robots to actively utilize interaction forces with these "obstacles" for locomotion and navigation. To do so, we studied the locomotion of a quadrupedal robot as it traversed a simplified obstacle field, and discovered that with different gaits, the robot could passively converge to distinct orientations. A compositional return map explained this observed passive convergence, and enabled theoretical prediction of the steady-state orientation angles for any given quadrupedal gait. We experimentally demonstrated that with these predictions, a legged robot could effectively generate desired shape of trajectories amongst large, slippery obstacles, simply by switching between different gaits. Our study offered a novel method for robots to exploit traditionally-considered "obstacles" to achieve agile movements on challenging terrains.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that5_02">
             08:35-08:40, Paper ThAT5.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2258'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Enhancing Navigation Efficiency of Quadruped Robots Via Leveraging Personal Transportation Platforms
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#313473" title="Click to go to the Author Index">
             Yoon, Minsung
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology (KAIST)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#120231" title="Click to go to the Author Index">
             Yoon, Sung-eui
            </a>
           </td>
           <td class="r">
            KAIST
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2258" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Quadruped robots face limitations in long-range navigation efficiency due to their reliance on legs. To ameliorate the limitations, we introduce a Reinforcement Learning-based Active Transporter Riding method (RL-ATR), inspired by humans' utilization of personal transporters, including Segways. The RL-ATR features a transporter riding policy and two state estimators. The policy devises adequate maneuvering strategies according to transporter-specific control dynamics, while the estimators resolve sensor ambiguities in non-inertial frames by inferring unobservable robot and transporter states. Comprehensive evaluations in simulation validate proficient command tracking abilities across various transporter-robot models and reduced energy consumption compared to legged locomotion. Moreover, we conduct ablation studies to quantify individual component contributions within the RL-ATR. This riding ability could broaden the locomotion modalities of quadruped robots, potentially expanding the operational range and efficiency.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that5_03">
             08:40-08:45, Paper ThAT5.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2591'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Continuous Control of Diverse Skills in Quadruped Robots without Complete Expert Datasets
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#377163" title="Click to go to the Author Index">
             Tu, Jiaxin
            </a>
           </td>
           <td class="r">
            FuDan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#373615" title="Click to go to the Author Index">
             Wei, Xiaoyi
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#392313" title="Click to go to the Author Index">
             Zhang, Yueqi
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#335367" title="Click to go to the Author Index">
             Hou, Taixian
            </a>
           </td>
           <td class="r">
            FuDan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#377190" title="Click to go to the Author Index">
             Gao, Xiaofei
            </a>
           </td>
           <td class="r">
            Beijing Zhitong Robot Technology Co., Ltd
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#267858" title="Click to go to the Author Index">
             Dong, Zhiyan
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#267891" title="Click to go to the Author Index">
             Zhai, Peng
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#267890" title="Click to go to the Author Index">
             ZHang, Lihua
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2591" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Learning diverse skills for quadruped robots presents significant challenges, such as mastering complex transitions between different skills and handling tasks of varying difficulty. Existing imitation learning methods, while successful, rely on expensive datasets to reproduce expert behaviors. Inspired by introspective learning, we propose Progressive Adversarial Self-Imitation Skill Transition (PASIST), a novel method that eliminates the need for complete expert datasets. PASIST autonomously explores and selects high-quality trajectories based on predefined target poses instead of demonstrations, leveraging the Generative Adversarial Self-Imitation Learning (GASIL) framework. To further enhance learning, We develop a skill selection module to mitigate mode collapse by balancing the weights of skills with varying levels of difficulty. Through these methods, PASIST is able to reproduce skills corresponding to the target pose while achieving smooth and natural transitions between them. Evaluations on both simulation platforms and the Solo 8 robot confirm the effectiveness of PASIST, offering an efficient alternative to expert-driven learning.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that5_04">
             08:45-08:50, Paper ThAT5.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2650'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              PIP-Loco: A Proprioceptive Infinite Horizon Planning Framework for Quadrupedal Robot Locomotion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#319090" title="Click to go to the Author Index">
             Shirwatkar, Aditya
            </a>
           </td>
           <td class="r">
            Indian Institute of Science Bengaluru
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#366981" title="Click to go to the Author Index">
             Saxena, Naman
            </a>
           </td>
           <td class="r">
            Indian Institute of Science, Bengaluru
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419431" title="Click to go to the Author Index">
             Chandra, Kishore P
            </a>
           </td>
           <td class="r">
            Visvesvaraya National Institute of Technology, Nagpur
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#148828" title="Click to go to the Author Index">
             Kolathaya, Shishir
            </a>
           </td>
           <td class="r">
            Indian Institute of Science
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2650" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#machine_learning_for_robot_control" title="Click to go to the Keyword Index">
               Machine Learning for Robot Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             A core strength of Model Predictive Control (MPC) for quadrupedal locomotion has been its ability to enforce constraints and provide interpretability of the sequence of commands over the horizon. However, despite being able to plan, MPC struggles to scale with task complexity, often failing to achieve robust behavior on rapidly changing surfaces. On the other hand, model-free Reinforcement Learning (RL) methods have outperformed MPC on multiple terrains, showing emergent motions but inherently lack any ability to handle constraints or perform planning. To address these limitations, we propose a framework that integrates proprioceptive planning with RL, allowing for agile and safe locomotion behaviors through the horizon. Inspired by MPC, we incorporate an internal model that includes a velocity estimator and a Dreamer module. During training, the framework learns an expert policy and an internal model that are co-dependent, facilitating exploration for improved locomotion behaviors. During deployment, the Dreamer module solves an infinite-horizon MPC problem, adapting actions and velocity commands to respect the constraints. We validate the robustness of our training framework through ablation studies on internal model components and demonstrate improved robustness to training noise. Finally, we evaluate our approach across multi-terrain scenarios in both simulation and hardware.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that5_05">
             08:50-08:55, Paper ThAT5.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2726'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Whole-Body End-Effector Pose Tracking
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#373236" title="Click to go to the Author Index">
             Portela, Tifanny
            </a>
           </td>
           <td class="r">
            ETH
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#231863" title="Click to go to the Author Index">
             Cramariuc, Andrei
            </a>
           </td>
           <td class="r">
            ETHZ
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#251790" title="Click to go to the Author Index">
             Mittal, Mayank
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#114045" title="Click to go to the Author Index">
             Hutter, Marco
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2726" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#whole_body_motion_planning_and_control" title="Click to go to the Keyword Index">
               Whole-Body Motion Planning and Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Combining manipulation with the mobility of legged robots is essential for a wide range of robotic applications. However, integrating an arm with a mobile base significantly increases the system’s complexity, making precise end-effector control challenging. Existing model-based approaches are often constrained by their modeling assumptions, leading to limited robustness. Meanwhile, recent Reinforcement Learning (RL) implementations restrict the arm’s workspace to be in front of the robot or track only the position to obtain decent tracking accuracy. In this work, we address these limitations by introducing a whole-body RL formulation for end-effector pose tracking in a large workspace on rough, unstructured terrains. Our proposed method involves a terrain-aware sampling strategy for the robot’s initial configuration and end-effector pose commands, as well as a game-based curriculum to extend the robot’s operating range. We validate our approach on the ANYmal quadrupedal robot with a six DoF robotic arm. Through our experiments, we show that the learned controller achieves precise command tracking over a large workspace and adapts across varying terrains such as stairs and slopes. On deployment, it achieves a pose-tracking error of 2.64 cm and 3.64◦, outperforming existing competitive baselines.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that5_06">
             08:55-09:00, Paper ThAT5.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4582'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              MoRE : Unlocking Scalability in Reinforcement Learning for Quadruped Vision-Language-Action Models
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#334849" title="Click to go to the Author Index">
             Zhao, Han
            </a>
           </td>
           <td class="r">
            Westlake University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#389395" title="Click to go to the Author Index">
             Song, Wenxuan
            </a>
           </td>
           <td class="r">
            Westlake University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#267934" title="Click to go to the Author Index">
             Wang, Donglin
            </a>
           </td>
           <td class="r">
            Westlake University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425565" title="Click to go to the Author Index">
             Tong, Xinyang
            </a>
           </td>
           <td class="r">
            Westlake University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#391285" title="Click to go to the Author Index">
             Ding, Pengxiang
            </a>
           </td>
           <td class="r">
            Westlake University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#302119" title="Click to go to the Author Index">
             Cheng, Xuelian
            </a>
           </td>
           <td class="r">
            Monash University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#188635" title="Click to go to the Author Index">
             Ge, Zongyuan
            </a>
           </td>
           <td class="r">
            Monash University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4582" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#perception_action_coupling" title="Click to go to the Keyword Index">
               Perception-Action Coupling
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Developing versatile quadruped robots that can smoothly perform various actions and tasks in real-world environments remains a significant challenge. This paper introduces a novel vision-language-action (VLA) model, mixture of robotic experts (MoRE), for quadruped robots that aim to introduce reinforcement learning (RL) for fine-tuning large-scale VLA models with a large amount of mixed-quality data. method~integrates multiple low-rank adaptation modules as distinct experts within a dense multi-modal large language model (MLLM), forming a sparse-activated mixture of experts model. This design enables the model to effectively adapt to a wide array of downstream tasks. Moreover, we employ a reinforcement learning-based training objective to train our model as a Q-function after deeply exploring the structural properties of our tasks. Effective learning from automatically collected mixed-quality data enhances data efficiency and model performance. Extensive experiments demonstrate that method~outperforms all baselines across six different skills and exhibits superior generalization capabilities in out-of-distribution scenarios. We further validate our method in real-world scenarios, confirming the practicality of our approach and laying a solid foundation for future research on multi-task learning in quadruped robots.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that6">
             <b>
              ThAT6
             </b>
            </a>
           </td>
           <td class="r">
            307
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#that6" title="Click to go to the Program at a Glance">
             <b>
              Perception for Human-Robot Interaction
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#103296" title="Click to go to the Author Index">
             Alami, Rachid
            </a>
           </td>
           <td class="r">
            CNRS
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#225137" title="Click to go to the Author Index">
             Fu, Di
            </a>
           </td>
           <td class="r">
            University of Surrey
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that6_01">
             08:30-08:35, Paper ThAT6.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('77'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              From Seeing to Recognising -- an Extended Self-Organizing Map for Human Postures Identification
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#309939" title="Click to go to the Author Index">
             He, Xin
            </a>
           </td>
           <td class="r">
            Graduate School of Information, Production and System, Waseda Un
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104519" title="Click to go to the Author Index">
             Zielinska, Teresa
            </a>
           </td>
           <td class="r">
            Warsaw University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#190993" title="Click to go to the Author Index">
             Dutta, Vibekananda
            </a>
           </td>
           <td class="r">
            Warsaw University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#103713" title="Click to go to the Author Index">
             Matsumaru, Takafumi
            </a>
           </td>
           <td class="r">
            Waseda University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#395070" title="Click to go to the Author Index">
             Sitnik, Robert
            </a>
           </td>
           <td class="r">
            Warsaw University of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab77" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#human_centered_robotics" title="Click to go to the Keyword Index">
               Human-Centered Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#human_aware_motion_planning" title="Click to go to the Keyword Index">
               Human-Aware Motion Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#human_and_humanoid_motion_analysis_and_synthesis" title="Click to go to the Keyword Index">
               Human and Humanoid Motion Analysis and Synthesis
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The article presents a dedicated method for recognizing human postures using classification and clustering options. The ultimate goal of the research is to recognise human actions based on posture sequences. Such a task imposes expectations on the developed method. For this purpose, a Sparse Autoencoder combined with a Self-Organized Map (SOM) is proposed. SOM is equipped with an additional layer of post-labeling or clustering. This entire structure is called the extended SOM. Two task-oriented modifications are applied to improve SOM performance -- a dedicated angular distance measure and a neighbourhood function for updating the SOM weights. The research contribution is the concept of extended SOM, which is trained using unlabeled data and classifies or clusters the human postures. The Sparse Autoencoder preserves the characteristics of the data while reducing its dimensionality. Better classification efficiency of the developed method is demonstrated compared to other representative methods. Ablation studies illustrate how the introduced modifications improve classification results. The developed method is characterised by good resolution in distinguishing postures. A discussion of the concept's usefulness is provided at the end of the article.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that6_02">
             08:35-08:40, Paper ThAT6.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2062'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              MmDEAR: MmWave Point Cloud Density Enhancement for Accurate Human Body Reconstruction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416792" title="Click to go to the Author Index">
             Yang, Jiarui
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#272191" title="Click to go to the Author Index">
             Xia, Songpengcheng
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421434" title="Click to go to the Author Index">
             Lai, Zengyuan
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421558" title="Click to go to the Author Index">
             Sun, Lan
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#338263" title="Click to go to the Author Index">
             Wu, Qi
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#216334" title="Click to go to the Author Index">
             Yu, Wenxian
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#204900" title="Click to go to the Author Index">
             Pei, Ling
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2062" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#human_detection_and_tracking" title="Click to go to the Keyword Index">
               Human Detection and Tracking
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#human_and_humanoid_motion_analysis_and_synthesis" title="Click to go to the Keyword Index">
               Human and Humanoid Motion Analysis and Synthesis
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Millimeter-wave (mmWave) radar offers robust sensing capabilities in diverse environments, making it a highly promising solution for human body reconstruction due to its privacy-friendly and non-intrusive nature. However, the significant sparsity of mmWave point clouds limits the estimation accuracy. To overcome this challenge, we propose a two-stage deep learning framework that enhances mmWave point clouds and improves human body reconstruction accuracy. Our method includes a mmWave point cloud enhancement module that densifies the raw data by leveraging temporal features and a multi-stage completion network, followed by a 2D-3D fusion module that extracts both 2D and 3D motion features to refine SMPL parameters. The mmWave point cloud enhancement module learns the detailed shape and posture information from 2D human masks in single-view images. However, image-based supervision is involved only during the training phase, and the inference relies solely on sparse point clouds to maintain privacy. Experiments on multiple datasets demonstrate that our approach outperforms state-of-the-art methods, with the enhanced point clouds further improving performance when integrated into existing models.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that6_03">
             08:40-08:45, Paper ThAT6.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2066'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Human Activity Recognition by Using Enhanced Radar Point Cloud 2D Histograms and Doppler Feature Fusion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421803" title="Click to go to the Author Index">
             Liao, Guanghang
            </a>
           </td>
           <td class="r">
            Great Bay University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421917" title="Click to go to the Author Index">
             Ma, Jieming
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology, Shenzhen
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420771" title="Click to go to the Author Index">
             Luo, Fei
            </a>
           </td>
           <td class="r">
            Great Bay University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2066" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#human_centered_robotics" title="Click to go to the Keyword Index">
               Human-Centered Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#gesture__posture_and_facial_expressions" title="Click to go to the Keyword Index">
               Gesture, Posture and Facial Expressions
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multi_modal_perception_for_hri" title="Click to go to the Keyword Index">
               Multi-Modal Perception for HRI
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Human activity recognition (HAR) based on millimeter wave (mmWave) radar has recently attracted significant interest due to its diverse applications in intelligent robots and human-computer interaction (HCI), including the healthcare monitoring robot. 2-dimensional (2D) histogram features of radar point clouds have demonstrated high accuracy in HAR. But further expansion and refinement of this technique is needed. This paper presents a new precise non-invasive HAR framework based on radar point cloud 2D histograms. Our method enhances conventional 2D histograms by integrating fixed radar sensing boundaries into the histograms, which shows the relative spatial position changes of the target points detected by radar. Additionally, we have concatenated Doppler features (i.e., range-Doppler and angle-Doppler histograms) with the point cloud histograms, resulting in a more comprehensive feature representation than conventional point cloud histograms. We investigated the overfitting issue in stacked hybrid networks and established a multi-layer hybrid network with an optimal number of stacked layers for HAR. In the evaluation, our approach achieves state-of-the-art accuracy, with 99.72% on mmWaveRadarWalking dataset and 98.67% on CI4R-Human-Activity-Recognition dataset, respectively. The proposed method can be applied in the fields of robotics and HCI.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that6_04">
             08:45-08:50, Paper ThAT6.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2450'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Estimating User Engagement in Human Robot Interaction Using a Dynamic Bayesian Network
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#290413" title="Click to go to the Author Index">
             Hei, Xiaoxuan
            </a>
           </td>
           <td class="r">
            ENSTA Paris, Institut Polytechnique De Paris
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#271540" title="Click to go to the Author Index">
             Zhang, Heng
            </a>
           </td>
           <td class="r">
            ENSTA Paris, Institut Polytechnique De Paris
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#106757" title="Click to go to the Author Index">
             Tapus, Adriana
            </a>
           </td>
           <td class="r">
            ENSTA Paris, Institut Polytechnique De Paris
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2450" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#multi_modal_perception_for_hri" title="Click to go to the Keyword Index">
               Multi-Modal Perception for HRI
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#robot_companions" title="Click to go to the Keyword Index">
               Robot Companions
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#social_hri" title="Click to go to the Keyword Index">
               Social HRI
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Engagement is a key concept in Human-Robot Interaction (HRI), as high engagement often leads to improved user experience and task performance. However, accurately estimating engagement during interactions is challenging. In this study, we propose a Dynamic Bayesian Network (DBN) to infer user engagement from various modalities, including head rotation, eye movements, facial expressions captured through visual sensors, as well as facial temperature variations measured by a thermal camera. Data was gathered from a human-robot interaction (HRI) experiment, where a robot guided participants and encouraged them to share their thoughts and insights on environmental issues. Our approach successfully combines these diverse features to offer a thorough assessment of user engagement. The network was tested on its capacity to classify participants as either engaged or not engaged, achieving an accuracy of 0.83 and an Area Under the Curve (AUC) of 0.82. These findings underscore the strength of our DBN in detecting user engagement during interactions.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that6_05">
             08:50-08:55, Paper ThAT6.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3559'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              HRI-Free: Cognitive Robotic Simulation for Evaluating Embodied Social Attention Models
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#232926" title="Click to go to the Author Index">
             Abawi, Fares
            </a>
           </td>
           <td class="r">
            Universität Hamburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#225137" title="Click to go to the Author Index">
             Fu, Di
            </a>
           </td>
           <td class="r">
            University of Surrey
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3559" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#cognitive_modeling" title="Click to go to the Keyword Index">
               Cognitive Modeling
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#embodied_cognitive_science" title="Click to go to the Keyword Index">
               Embodied Cognitive Science
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#social_hri" title="Click to go to the Keyword Index">
               Social HRI
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Scaling social robot studies is constrained due to the need for human interaction, making large participant recruitment impractical. Robotics simulators help mitigate this limitation but generally lack the realism to accurately simulate social cues. We introduce a cognitive robotic simulation scheme to evaluate social attention models in physical environments. By projecting ground-truth priority maps to a simulated environment, we can directly compare predicted maps using common saliency metrics. Using the iCub robot, we assess a dynamic scanpath model that predicts attention targets, simulating human scanpaths. Evaluations with the FindWho and MVVA datasets show strong correlations between robot-captured metrics and direct-streamed video metrics. Our results indicate robustness of the social attention model to noise and real-world conditions, suggesting its practical usability for predicting personalized scanpaths in real settings. This approach reduces the need for extensive human-robot interaction studies in the early stages of study design, enabling the scalability and reproducibility of social robot evaluations.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that6_06">
             08:55-09:00, Paper ThAT6.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3664'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              An EEG Conformer Model for Error Feedback During Human-Robot Interaction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#341235" title="Click to go to the Author Index">
             Han, Jinpei
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425532" title="Click to go to the Author Index">
             Li, Yinxuan
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#243054" title="Click to go to the Author Index">
             Gu, Xiao
            </a>
           </td>
           <td class="r">
            University of Oxford
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#167021" title="Click to go to the Author Index">
             Faisal, Aldo
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3664" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#brain_machine_interfaces" title="Click to go to the Keyword Index">
               Brain-Machine Interfaces
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#human_factors_and_human_in_the_loop" title="Click to go to the Keyword Index">
               Human Factors and Human-in-the-Loop
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#intention_recognition" title="Click to go to the Keyword Index">
               Intention Recognition
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Identifying a brain signal that enables the detection of incorrect execution in human-robot interaction (HRI) is considered a holy grail for real-time systems. A major challenge in achieving this is the inherent imbalance caused by the sparsity of error-related potential (ErrP) events in streaming electroencephalogram (EEG) data, which often leads models to learn irrelevant features and perform poorly. Thus, while Deep learning-based ErrP detection has seen considerable advancements, the variability in individual user reaction times introduces labelling errors, complicating model adaptation to new subjects. Moreover, most deep learning methods are developed and validated on discrete, offline experiments using pre-defined windows, which fail to translate effectively to continuous, real-time HRI. Addressing these challenges is crucial to improving the robustness and adaptability of real-time ErrP detection in practical HRI applications. Here, we develop a causal EEG conformer framework, combining a Convolutional neural network (CNN) encoder and a transformer with causal attention for real-time prediction of ErrP signals during HRI. We evaluated our ErrP model in a pseudo-online environment in both inter-session and inter-subject cross-validation settings for exoskeleton assistive robotics. Our model demonstrated superior performance in decoding accuracy and efficiency, showcasing better generalization for real-world dynamic HRI applications.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that7">
             <b>
              ThAT7
             </b>
            </a>
           </td>
           <td class="r">
            309
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#that7" title="Click to go to the Program at a Glance">
             <b>
              Marine Robotics 5
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that7_01">
             08:30-08:35, Paper ThAT7.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2464'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Cross-Platform Learning-Based Fault Tolerant Surfacing Controller for Underwater Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#274756" title="Click to go to the Author Index">
             Hamamatsu, Yuya
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#233649" title="Click to go to the Author Index">
             Remmas, Walid
            </a>
           </td>
           <td class="r">
            Tallinn University of Technology / Université De Montpellier
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#336566" title="Click to go to the Author Index">
             Rebane, Jaan
            </a>
           </td>
           <td class="r">
            Tallinna Tehnikaülikool
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#102702" title="Click to go to the Author Index">
             Kruusmaa, Maarja
            </a>
           </td>
           <td class="r">
            Tallinn University of Technology (TalTech)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#147489" title="Click to go to the Author Index">
             Ristolainen, Asko
            </a>
           </td>
           <td class="r">
            Tallinn University of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2464" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#marine_robotics" title="Click to go to the Keyword Index">
               Marine Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#field_robots" title="Click to go to the Keyword Index">
               Field Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#model_learning_for_control" title="Click to go to the Keyword Index">
               Model Learning for Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper, we propose a novel cross-platform fault-tolerant surfacing controller for underwater robots, based on reinforcement learning (RL). Unlike conventional approaches, which require explicit identification of malfunctioning actuators, our method allows the robot to surface using only the remaining operational actuators without needing to pinpoint the failures. The proposed controller learns a robust policy capable of handling diverse failure scenarios across different actuator configurations. Moreover, we introduce a transfer learning mechanism that shares a part of the control policy across various underwater robots with different actuators, thus improving learning efficiency and generalization across platforms. To validate our approach, we conduct simulations on three different types of underwater robots: a hovering-type AUV, a torpedo shaped AUV, and a turtle-shaped robot (U-CAT). Additionally, real-world experiments are performed, successfully transferring the learned policy from simulation to a physical U-CAT in a controlled environment. Our RL-based controller demonstrates superior performance in terms of stability and success rate compared to a baseline controller, achieving an 85.7 percent success rate in real-world tests compared to 57.1 percent with a baseline controller. This research provides a scalable and efficient solution for fault-tolerant control for diverse underwater platforms, with potential applications in real-world aquatic missions.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that7_02">
             08:35-08:40, Paper ThAT7.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2903'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Optimizing Underwater Robot Navigation: A Study of DRL Algorithms and Multi-Modal Sensor Fusion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#418368" title="Click to go to the Author Index">
             Deowan, Md Ether
            </a>
           </td>
           <td class="r">
            University of Toulon
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422135" title="Click to go to the Author Index">
             Yousha, Md Shamin Yeasher
            </a>
           </td>
           <td class="r">
            Norwegian University of Science and Technology - NTNU
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422179" title="Click to go to the Author Index">
             Hossain, Tihan Mahmud
            </a>
           </td>
           <td class="r">
            Norwegian University of Science and Technology - NTNU
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422149" title="Click to go to the Author Index">
             Hassan, Shahriar
            </a>
           </td>
           <td class="r">
            Instituto Superior Técnico
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#310424" title="Click to go to the Author Index">
             Marxer, Ricard
            </a>
           </td>
           <td class="r">
            Université De Toulon, Aix Marseille Univ, CNRS, LIS
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2903" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#marine_robotics" title="Click to go to the Keyword Index">
               Marine Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#autonomous_agents" title="Click to go to the Keyword Index">
               Autonomous Agents
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Autonomous underwater navigation faces significant challenges due to the complexity of the environment, limited localization methods, and poor visibility. This paper investigates the performance of various reinforcement learning (RL) algorithms—Proximal Policy Optimization (PPO), Trust Region Policy Optimization (TRPO), Soft Actor-Critic (SAC), Twin Delayed DDPG (TD3), and Advantage Actor-Critic (A2C)—to improve navigation capabilities of low-cost underwater robots equipped with multi-modal sensors. Advanced depth estimation models such as MiDaS and Depth Anything, combined with domain randomization techniques, are employed to enhance the system's robustness and generalization across varying underwater conditions.
             <p>
              The proposed approach integrates real-time sensor data and historical actions to enable 3D maneuvering in simulated environments, leading to significant improvements in sensor fusion, depth perception, and obstacle avoidance. Simulation results demonstrate that the combination of RL techniques with sensor fusion considerably improves mapless autonomous underwater exploration, providing a robust solution for navigating unstructured aquatic environments.
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that7_03">
             08:40-08:45, Paper ThAT7.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3481'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              PUGS: Perceptual Uncertainty for Grasp Selection in Underwater Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#343378" title="Click to go to the Author Index">
             Bagoren, Onur
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422286" title="Click to go to the Author Index">
             Micatka, Marc
            </a>
           </td>
           <td class="r">
            University of Washington
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#180427" title="Click to go to the Author Index">
             Skinner, Katherine
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#178865" title="Click to go to the Author Index">
             Marburg, Aaron
            </a>
           </td>
           <td class="r">
            University of Washington
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3481" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#marine_robotics" title="Click to go to the Keyword Index">
               Marine Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             When navigating and interacting in challenging environments where sensory information is imperfect and incomplete, robots must make decisions that account for these shortcomings. We propose a novel method for quantifying and representing such perceptual uncertainty in 3D reconstruction through occupancy uncertainty estimation. We develop a framework to incorporate it into grasp selection for autonomous manipulation in underwater environments. Instead of treating each measurement equally when deciding which location to grasp from, we present a framework that propagates uncertainty inherent in the multi-view reconstruction process into the grasp selection. We evaluate our method with both simulated and the real world data, showing that by accounting for uncertainty, the grasp selection becomes robust against partial and noisy measurements. Code will be made available at https://onurbagoren.github.io/PUGS/
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that7_04">
             08:45-08:50, Paper ThAT7.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4045'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Learning to Swim: Reinforcement Learning for 6-DOF Control of Thruster-Driven Autonomous Underwater Vehicles
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#237740" title="Click to go to the Author Index">
             Cai, Levi
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426165" title="Click to go to the Author Index">
             Chang, Kevin
            </a>
           </td>
           <td class="r">
            Oregon State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#118013" title="Click to go to the Author Index">
             Girdhar, Yogesh
            </a>
           </td>
           <td class="r">
            Woods Hole Oceanographic Institution
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4045" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#field_robots" title="Click to go to the Keyword Index">
               Field Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#marine_robotics" title="Click to go to the Keyword Index">
               Marine Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Controlling AUVs can be challenging because of the effect of complex non-linear hydrodynamic forces acting on the robot, which are significant in water and cannot be ignored. The problem is exacerbated for small AUVs for which the dynamics can change significantly with payload changes and deployments under different hydrodynamic conditions. The common approach to AUV control is a combination of passive stabilization with added buoyancy on top and weights on the bottom, and a PID controller tuned for simple and smooth motion primitives. However, the approach comes at the cost of sluggish controls and often the need to re-tune controllers with configuration changes. In this paper, we propose a fast (trainable in minutes), reinforcement learning-based approach for full 6 degree of freedom (DOF) control of a thruster-driven AUVs, taking 6-DOF command-conditioned inputs direct to thruster outputs. We present a new, highly parallelized simulator for underwater vehicle dynamics. We demonstrate this approach through zero-shot sim-to-real (with no tuning) transfer onto a real AUV that produces comparable results to hand-tuned PID controllers. Furthermore, we show that domain randomization on the simulator produces policies that are robust to small variations in vehicle's physical parameters.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that7_05">
             08:50-08:55, Paper ThAT7.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4075'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Underwater Motions Analysis and Control of a Coupling-Tiltable Unmanned Aerial-Aquatic Vehicle
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#325648" title="Click to go to the Author Index">
             Huang, Dongyue
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#325620" title="Click to go to the Author Index">
             Dou, Minghao
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#325015" title="Click to go to the Author Index">
             Liu, Xuchen
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426168" title="Click to go to the Author Index">
             Sun, Tao
            </a>
           </td>
           <td class="r">
            Shenzhen Institute of Artificial Intelligence and Robotics for S
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#350548" title="Click to go to the Author Index">
             Zhang, Jianguo
            </a>
           </td>
           <td class="r">
            Shenzhen Institute of Artificial Intelligence and Robotics for S
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#238574" title="Click to go to the Author Index">
             Ding, Ning
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong, Shenzhen
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#354068" title="Click to go to the Author Index">
             Chen, Xinlei
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#171587" title="Click to go to the Author Index">
             Chen, Ben M.
            </a>
           </td>
           <td class="r">
            Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4075" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#marine_robotics" title="Click to go to the Keyword Index">
               Marine Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__mechanics_and_control" title="Click to go to the Keyword Index">
               Aerial Systems: Mechanics and Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_control" title="Click to go to the Keyword Index">
               Motion Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Coupling-Tiltable Unmanned Aerial-Aquatic Vehicles (UAAVs) have gained increasing importance, yet lack comprehensive analysis and suitable controllers. This paper analyzes the underwater motion characteristics of a self-designed UAAV, Mirs-Alioth, and designs a controller for it. The effectiveness of the controller is validated through experiments. The singularities of Mirs-Alioth are derived as Singular Thrust Tilt Angle (STTA), which serve as an essential tool for an analysis of its underwater motion characteristics. The analysis reveals several key factors for designing the controller. These include the need for logic switching, using a Nussbaum function to compensate control direction uncertainty in the auxiliary channel, and employing an auxiliary controller to mitigate coupling effects. Based on these key points, a control scheme is designed. It consists of a controller that regulates the thrust tilt angle to the singular value, an auxiliary controller incorporating a Saturated Nussbaum function, and a logic switch. Eventually, two sets of experiments are conducted to validate the effectiveness of the controller and demonstrate the necessity of the Nussbaum function.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that7_06">
             08:55-09:00, Paper ThAT7.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('5039'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Adaptive Integral Sliding Mode Control for Attitude Tracking of Underwater Robots with Large Range Pitch Variations in Confined Spaces
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#405420" title="Click to go to the Author Index">
             Wang, Xiaorui
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#362162" title="Click to go to the Author Index">
             Sha, Zeyu
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#148116" title="Click to go to the Author Index">
             Zhang, Feitian
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab5039" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#marine_robotics" title="Click to go to the Keyword Index">
               Marine Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_control" title="Click to go to the Keyword Index">
               Motion Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#robust_adaptive_control" title="Click to go to the Keyword Index">
               Robust/Adaptive Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             水下机器人在探索水生环境中发挥着至关重要的作用。灵活调整姿态的能力，尤其是俯仰，对于水下机器人在狭窄空间内有效完成任务至关重要。然而，由姿态变化导致的高度耦合的六自由度动力学和有限空间区域内的复杂湍流带来了重大挑战。为了解决水下机器人的姿态控制问题，本文研究了站位保持期间的大范围俯仰角跟踪以及同步滚转和偏航角控制，以实现多功能姿态调整。基于动态建模，本文提出了一种自适应积分滑模控制器 （AISMC），该控制器将积分模块集成到传统的滑模控制 （SMC） 中，并自适应地调整开关增益，以提高跟踪精度、减少颤振并增强鲁棒ö
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that8">
             <b>
              ThAT8
             </b>
            </a>
           </td>
           <td class="r">
            311
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#that8" title="Click to go to the Program at a Glance">
             <b>
              Aerial Robots: Learning 1
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#101787" title="Click to go to the Author Index">
             Yim, Mark
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#204392" title="Click to go to the Author Index">
             Jagannatha Sanket, Nitin
            </a>
           </td>
           <td class="r">
            Worcester Polytechnic Institute
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that8_01">
             08:30-08:35, Paper ThAT8.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('108'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Learning Local Urban Wind Flow Fields from Range Sensing
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#301534" title="Click to go to the Author Index">
             Folk, Spencer
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#386543" title="Click to go to the Author Index">
             Melton, John
            </a>
           </td>
           <td class="r">
            NASA Ames Research Center
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#386544" title="Click to go to the Author Index">
             Margolis, Benjamin W. L.
            </a>
           </td>
           <td class="r">
            NASA Ames Research Center
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101787" title="Click to go to the Author Index">
             Yim, Mark
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104342" title="Click to go to the Author Index">
             Kumar, Vijay
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab108" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__perception_and_autonomy" title="Click to go to the Keyword Index">
               Aerial Systems: Perception and Autonomy
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#automation_technologies_for_smart_cities" title="Click to go to the Keyword Index">
               Automation Technologies for Smart Cities
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Obtaining accurate and timely predictions of the wind through an urban environment is a challenging task, but has wide-ranging implications for the safety and efficiency of autonomous aerial vehicles in future urban airspaces. Prior work relies strongly on global information about the environment, such as a precise map of the city and in-situ wind measurements at various locations, to run expensive computational fluid dynamics solvers to predict the entire wind flow field. In contrast, this paper introduces a new method to estimate the wind flow field in a region around the robot in real time, utilizing on-board range measurements to sense nearby buildings and sparse wind measurements to infer windspeed and direction. We propose that this information sufficiently characterizes the structure of the wind flow field in the local region of interest. To that end, we introduce a deep learning-based approach to predict local flow fields from range measurements. Our results indicate that a neural network trained on numerous simulated winds through small randomized maps is capable of reconstructing local wind flows while generalizing to larger environments with over 200 buildings. This contribution empowers computationally-constrained aerial robots to reason about the structure of local wind flow fields, thereby enabling new planning, control, and estimation strategies in windy urban environments without textit{a priori} knowledge of the map.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that8_02">
             08:35-08:40, Paper ThAT8.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('450'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Whole-Body Control through Narrow Gaps from Pixels to Action
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#336017" title="Click to go to the Author Index">
             Wu, Tianyue
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#394455" title="Click to go to the Author Index">
             Chen, Yeke
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#404448" title="Click to go to the Author Index">
             Chen, Tianyang
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#379164" title="Click to go to the Author Index">
             Zhao, Guangyu
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#200893" title="Click to go to the Author Index">
             Gao, Fei
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab450" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#sensorimotor_learning" title="Click to go to the Keyword Index">
               Sensorimotor Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Flying through body-size narrow gaps in the environment is one of the most challenging moments for an underactuated multirotor. We explore a purely data-driven method to master this flight skill in simulation, where a neural network directly maps pixels and proprioception to continuous low-level control commands. This learned policy enables whole-body control through gaps with different geometries demanding sharp attitude changes (e.g., near-vertical roll angle). The policy is achieved by successive model-free reinforcement learning (RL) and online observation space distillation. The RL policy receives (virtual) point clouds of the gaps' edges for scalable simulation and is then distilled into the high-dimensional pixel space. However, this flight skill is fundamentally expensive to learn by exploring in RL due to restricted feasible solution space. We propose to reset the agent as states on the trajectories by a model-based trajectory optimizer to alleviate this problem. The presented training pipeline is compared with baseline methods, and ablation studies are conducted to identify the key ingredients of the method. The immediate next step is to scale up the variation of gap sizes and geometries in anticipation of emergent policies and demonstrate the sim-to-real transformation.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that8_03">
             08:40-08:45, Paper ThAT8.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('518'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              VisFly: An Efficient and Versatile Simulator for Training Vision-Based Flight
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#407945" title="Click to go to the Author Index">
             Li, Fanxing
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#413104" title="Click to go to the Author Index">
             Sun, Fangyu
            </a>
           </td>
           <td class="r">
            Shanghai Jiaotong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#363241" title="Click to go to the Author Index">
             Zhang, Tianbao
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#204895" title="Click to go to the Author Index">
             Zou, Danping
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Ton University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab518" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__perception_and_autonomy" title="Click to go to the Keyword Index">
               Aerial Systems: Perception and Autonomy
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#simulation_and_animation" title="Click to go to the Keyword Index">
               Simulation and Animation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present VisFly, a quadrotor simulator designed to efficiently train vision-based flight policies using reinforcement learning algorithms. VisFly offers a user-friendly framework and interfaces, leveraging Habitat-Sim's rendering engines to achieve frame rates exceeding 10,000 frames per second for rendering motion and sensor data. The simulator incorporates differentiable physics and is seamlessly wrapped with the Gym environment, facilitating the straightforward implementation of various learning algorithms. It supports the directly importing open-source scene datasets compatible with Habitat-Sim, enabling training on diverse real-world environments simultaneously. To validate our simulator, we also make three reinforcement learning examples for typical flight tasks relying on visual observations. The simulator is now available at [https://github.com/SJTU-ViSYS-team/VisFly].
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that8_04">
             08:45-08:50, Paper ThAT8.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2843'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Environment As Policy: Learning to Race in Unseen Tracks
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421187" title="Click to go to the Author Index">
             Wang, Hongze
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#311106" title="Click to go to the Author Index">
             Xing, Jiaxu
            </a>
           </td>
           <td class="r">
            University of Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#305338" title="Click to go to the Author Index">
             Messikommer, Nico
            </a>
           </td>
           <td class="r">
            University of Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#105662" title="Click to go to the Author Index">
             Scaramuzza, Davide
            </a>
           </td>
           <td class="r">
            University of Zurich
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2843" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__perception_and_autonomy" title="Click to go to the Keyword Index">
               Aerial Systems: Perception and Autonomy
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Reinforcement learning (RL) has achieved outstanding success in complex robot control tasks, such as drone racing, where the RL agents have outperformed human champions in a known racing track. However, these agents fail in unseen track configurations, always requiring complete retraining when presented with new track layouts. This work aims to develop RL agents that generalize effectively to novel track configurations without retraining. The naive solution of training directly on a diverse set of track layouts can overburden the agent, resulting in suboptimal policy learning as the increased complexity of the environment impairs the agent’s ability to learn to fly. To enhance the generalizability of the RL agent, we propose an adaptive environment-shaping framework that dynamically adjusts the training environment based on the agent’s performance. We achieve this by leveraging a secondary RL policy to design environments that strike a balance between being challenging and achievable, allowing the agent to adapt and improve progressively. Using our adaptive environment shaping, one single racing policy efficiently learns to race in diverse and challenging tracks. Experimental results validated in both simulation and the real world show that our method enables drones to successfully fly complex and unseen race tracks, outperforming existing environment-shaping techniques.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that8_05">
             08:50-08:55, Paper ThAT8.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4367'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              UAV-Assisted Self-Supervised Terrain Awareness for Off-Road Navigation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#325295" title="Click to go to the Author Index">
             Fortin, Jean-Michel
            </a>
           </td>
           <td class="r">
            Université Laval
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#325305" title="Click to go to the Author Index">
             Gamache, Olivier
            </a>
           </td>
           <td class="r">
            Université Laval
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420671" title="Click to go to the Author Index">
             Fecteau, William
            </a>
           </td>
           <td class="r">
            Université Laval
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#355181" title="Click to go to the Author Index">
             Daum, Effie
            </a>
           </td>
           <td class="r">
            Université Laval
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#375960" title="Click to go to the Author Index">
             Larrivée-Hardy, William
            </a>
           </td>
           <td class="r">
            Laval University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#123399" title="Click to go to the Author Index">
             Pomerleau, Francois
            </a>
           </td>
           <td class="r">
            Université Laval
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#110648" title="Click to go to the Author Index">
             Giguère, Philippe
            </a>
           </td>
           <td class="r">
            Université Laval
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4367" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#field_robots" title="Click to go to the Keyword Index">
               Field Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#learning_from_experience" title="Click to go to the Keyword Index">
               Learning from Experience
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Terrain awareness is an essential milestone to enable truly autonomous off-road navigation. Accurately predicting terrain characteristics allows optimizing a vehicle's path against potential hazards. Recent methods use deep neural networks to predict traversability-related terrain properties in a self-supervised manner, relying on proprioception as a training signal. However, onboard cameras are inherently limited by their point-of-view relative to the ground, suffering from occlusions and vanishing pixel density with distance. This paper introduces a novel approach for self-supervised terrain characterization using an aerial perspective from a hovering drone. We capture terrain-aligned images while sampling the environment with a ground vehicle, effectively training a simple predictor for vibrations, bumpiness, and energy consumption. Our dataset includes 2.8 km of off-road data collected in forest environment, comprising 13 484 ground-based images and 12 935 aerial images. Our findings show that drone imagery improves terrain property prediction by 21.37 % on the whole dataset and 37.35 % in high vegetation, compared to ground images. We conduct ablation studies to identify the main causes of these performance improvements. We also demonstrate the real-world applicability of our approach by scouting an unseen area with a drone, planning and executing an optimized path on the ground.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that8_06">
             08:55-09:00, Paper ThAT8.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4889'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              EdgeFlowNet: 100FPS@1W Dense Optical Flow for Tiny Mobile Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#391386" title="Click to go to the Author Index">
             Pinnama Raju, Sai Ramana Kiran
            </a>
           </td>
           <td class="r">
            Worcester Polytechnic Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#391072" title="Click to go to the Author Index">
             Singh, Rishabh
            </a>
           </td>
           <td class="r">
            Worcester Polytechnic Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#388243" title="Click to go to the Author Index">
             Velmurugan, Manoj
            </a>
           </td>
           <td class="r">
            Worcester Polytechnic Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#204392" title="Click to go to the Author Index">
             Jagannatha Sanket, Nitin
            </a>
           </td>
           <td class="r">
            Worcester Polytechnic Institute
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4889" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__perception_and_autonomy" title="Click to go to the Keyword Index">
               Aerial Systems: Perception and Autonomy
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Optical flow estimation is a critical task for tiny mobile robotics to enable safe and accurate navigation, obstacle avoidance, and other functionalities. However, optical flow estimation on tiny robots is challenging due to limited onboard sensing and computation capabilities. In this paper, we propose EdgeFlowNet, a high-speed, low-latency dense optical flow approach for tiny autonomous mobile robots by harnessing the power of edge computing. We demonstrate the efficacy of our approach by deploying EdgeFlowNet on a tiny quadrotor to perform static obstacle avoidance, flight through unknown gaps and dynamic obstacle dodging. EdgeFlowNet is about 20X faster than the previous state-of-the-art approaches while improving accuracy by over 20% and using only 1.08W of power enabling advanced autonomy on palm-sized tiny mobile robots.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that9">
             <b>
              ThAT9
             </b>
            </a>
           </td>
           <td class="r">
            312
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#that9" title="Click to go to the Program at a Glance">
             <b>
              Multi-Robot Formation Control
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#152911" title="Click to go to the Author Index">
             Parasuraman, Ramviyas
            </a>
           </td>
           <td class="r">
            University of Georgia
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that9_01">
             08:30-08:35, Paper ThAT9.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('166'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              GMF: Gravitational Mass-Force Framework for Parametric Multi-Level Coordination in Multi-Robot and Swarm Robotic Systems
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#336107" title="Click to go to the Author Index">
             Starks, Michael
            </a>
           </td>
           <td class="r">
            University of Georgia Heterogeneous Robotics Research Lab
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#152911" title="Click to go to the Author Index">
             Parasuraman, Ramviyas
            </a>
           </td>
           <td class="r">
            University of Georgia
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab166" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#swarm_robotics" title="Click to go to the Keyword Index">
               Swarm Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#cooperating_robots" title="Click to go to the Keyword Index">
               Cooperating Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Distributed multi-robot coordination is critical to achieving reliable robotic missions that exploit the collective capability of swarm robots. In particular, the consensus and formation control problems have been extensively studied, resulting in distributed controllers that enable robots to rely only on information from themselves and their immediate neighbors. However, these algorithms are usually designed for specific objectives (e.g., cooperative object transportation, environmental coverage, etc.), requiring the controllers to be re-designed for domain variations. Therefore, we propose a new parametric framework inspired by gravitational fields that allow simultaneous coordination of robots at multiple levels, enabling generalization and domain adaptation. Our approach is built on top of a connectivity-preserving formation controller, with need-based and task-based ad hoc coordination at private, local, and global layers of a swarm robot team. We demonstrate the remarkable potential of our framework through extensive simulations and real-world swarm robot experiments in three representative multi-robot tasks involving tight coordination: 1) robot-initiated rendezvous at different coordination layers, 2) coordinated boundary tracking and coverage of environmental processes, and 3) accommodating task executions and motion control while satisfying the coordination laws.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that9_02">
             08:35-08:40, Paper ThAT9.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('216'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Leader-Follower Formation Control of Perturbed Nonholonomic Agents Along Parametric Curves with Directed Communication
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#315057" title="Click to go to the Author Index">
             Zhang, Bin
            </a>
           </td>
           <td class="r">
            The Hong Kong Polytechnic University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#283315" title="Click to go to the Author Index">
             Shao, Xiaodong
            </a>
           </td>
           <td class="r">
            Beihang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#343401" title="Click to go to the Author Index">
             Zhi, Hui
            </a>
           </td>
           <td class="r">
            The Hong Kong Polytechnic University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#343909" title="Click to go to the Author Index">
             Qiu, Liuming
            </a>
           </td>
           <td class="r">
            The Hong Kong Polytechnic University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#159452" title="Click to go to the Author Index">
             Romero Velazquez, Jose Guadalupe
            </a>
           </td>
           <td class="r">
            ITAM
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#123555" title="Click to go to the Author Index">
             Navarro-Alarcon, David
            </a>
           </td>
           <td class="r">
            The Hong Kong Polytechnic University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab216" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_control" title="Click to go to the Keyword Index">
               Motion Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#nonholonomic_motion_planning" title="Click to go to the Keyword Index">
               Nonholonomic Motion Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this letter, we propose a novel formation controller for nonholonomic agents to form general parametric curves. First, we derive a unified parametric representation for both open and closed curves. Then, a leader-follower formation controller is designed to drive agents to form the desired parametric curves using the curve coefficients as feedbacks. We consider directed communications and constant input disturbances rejection in the controller design. Rigorous Lyapunov-based stability analysis proves the asymptotic stability of the proposed controller. The convergence of the orientations of agents to some constant values is also guaranteed. The method has the potential to be extended to deal with various real-world applications, such as object enclosing. Detailed numerical simulations and experimental studies are conducted to verify the performance of the proposed method.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that9_03">
             08:40-08:45, Paper ThAT9.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1304'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Versatile Distributed Maneuvering with Generalized Formations Using Guiding Vector Fields
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#294021" title="Click to go to the Author Index">
             Lu, Yang
            </a>
           </td>
           <td class="r">
            National University of Defense Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#291631" title="Click to go to the Author Index">
             Luo, Sha
            </a>
           </td>
           <td class="r">
            University of Groningen
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#296594" title="Click to go to the Author Index">
             Zhu, Pengming
            </a>
           </td>
           <td class="r">
            National University of Defense Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#189263" title="Click to go to the Author Index">
             Yao, Weijia
            </a>
           </td>
           <td class="r">
            Hunan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#169065" title="Click to go to the Author Index">
             Garcia de Marina, Hector
            </a>
           </td>
           <td class="r">
            Universidad De Granada
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#294023" title="Click to go to the Author Index">
             Zhang, Xinglong
            </a>
           </td>
           <td class="r">
            National University of Defense Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#131923" title="Click to go to the Author Index">
             Xu, Xin
            </a>
           </td>
           <td class="r">
            National University of Defense Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1304" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_control" title="Click to go to the Keyword Index">
               Motion Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#distributed_robot_systems" title="Click to go to the Keyword Index">
               Distributed Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a unified approach to realize versatile distributed maneuvering with generalized formations. Specifically, we decompose the robots' maneuvers into two independent components, i.e., interception and enclosing, which are parameterized by two independent virtual coordinates. Treating these two virtual coordinates as dimensions of an abstract manifold, we derive the corresponding singularity-free guiding vector field (GVF), which, along with a distributed coordination mechanism based on the consensus theory, guides robots to achieve various motions (i.e., versatile maneuvering), including (a) formation tracking, (b) target enclosing, and (c) circumnavigation. Additional motion parameters can generate more complex cooperative robot motions. Based on GVFs, we design a controller for a nonholonomic robot model. Besides the theoretical results, extensive simulations and experiments are performed to validate the effectiveness of the approach.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that9_04">
             08:45-08:50, Paper ThAT9.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1459'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Cooperative Distributed Model Predictive Control for Embedded Systems: Experiments with Hovercraft Formations
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#284081" title="Click to go to the Author Index">
             Stomberg, Gösta
            </a>
           </td>
           <td class="r">
            Hamburg University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#313039" title="Click to go to the Author Index">
             Schwan, Roland
            </a>
           </td>
           <td class="r">
            EPFL
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#418591" title="Click to go to the Author Index">
             Grillo, Andrea
            </a>
           </td>
           <td class="r">
            EPFL
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#290867" title="Click to go to the Author Index">
             Jones, Colin
            </a>
           </td>
           <td class="r">
            École Polytechnique Fédérale De Lausanne (EPFL)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#166423" title="Click to go to the Author Index">
             Faulwasser, Timm
            </a>
           </td>
           <td class="r">
            Hamburg University of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1459" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#cooperating_robots" title="Click to go to the Keyword Index">
               Cooperating Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents experiments for embedded cooperative distributed model predictive control applied to a team of hovercraft floating on an air hockey table. The hovercraft collectively solve a centralized optimal control problem in each sampling step via a stabilizing decentralized real-time iteration scheme using the alternating direction method of multipliers. The efficient implementation does not require a central coordinator, executes onboard the hovercraft, and facilitates sampling intervals in the millisecond range. The formation control experiments showcase the flexibility of the approach on scenarios with point-to-point transitions, trajectory tracking, collision avoidance, and moving obstacles.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that9_05">
             08:50-08:55, Paper ThAT9.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1725'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Coordinated Multi-Robot Navigation with Formation Adaptation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422123" title="Click to go to the Author Index">
             Deng, Zihao
            </a>
           </td>
           <td class="r">
            University of Massachusetts Amherst
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#227860" title="Click to go to the Author Index">
             Gao, Peng
            </a>
           </td>
           <td class="r">
            North Carolina State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#342736" title="Click to go to the Author Index">
             Jose, Williard Joshua
            </a>
           </td>
           <td class="r">
            University of Massachusetts Amherst
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#123373" title="Click to go to the Author Index">
             Reardon, Christopher M.
            </a>
           </td>
           <td class="r">
            MITRE
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#192056" title="Click to go to the Author Index">
             Wigness, Maggie
            </a>
           </td>
           <td class="r">
            U.S. Army Research Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#102697" title="Click to go to the Author Index">
             Rogers III, John G.
            </a>
           </td>
           <td class="r">
            US Army Research Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#138554" title="Click to go to the Author Index">
             Zhang, Hao
            </a>
           </td>
           <td class="r">
            University of Massachusetts Amherst
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1725" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#machine_learning_for_robot_control" title="Click to go to the Keyword Index">
               Machine Learning for Robot Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Coordinated multi-robot navigation is an essential ability for a team of robots operating in diverse environments. Robot teams often need to maintain specific formations, such as wedge formations, to enhance visibility, positioning, and efficiency during fast movement. However, complex environments such as narrow corridors challenge rigid team formations, which makes effective formation control difficult in real-world environments. To address this challenge, we introduce a novel Adaptive Formation with Oscillation Reduction (AFOR) approach to improve coordinated multi-robot navigation. We develop AFOR under the theoretical framework of hierarchical learning and integrate a spring-damper model with hierarchical learning to enable both team coordination and individual robot control. At the upper level, a graph neural network facilitates formation adaptation and information sharing among the robots. At the lower level, reinforcement learning enables each robot to navigate and avoid obstacles while maintaining the formations. We conducted extensive experiments using Gazebo in the Robot Operating System (ROS), a high-fidelity Unity3D simulator with ROS, and real robot teams. Results demonstrate that AFOR enables smooth navigation with formation adaptation in complex scenarios and outperforms previous methods.
             <p>
              More details of this work are provided on the project website: https://hcrlab.gitlab.io/project/afor.
             </p>
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that10">
             <b>
              ThAT10
             </b>
            </a>
           </td>
           <td class="r">
            313
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#that10" title="Click to go to the Program at a Glance">
             <b>
              Multi-Robot Systems 3
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#398302" title="Click to go to the Author Index">
             Kim, Woojun
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that10_01">
             08:30-08:35, Paper ThAT10.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2532'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              MARVEL: Multi-Agent Reinforcement Learning for Constrained Field-Of-View Multi-Robot Exploration in Large-Scale Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#368418" title="Click to go to the Author Index">
             Chiun, Jimmy
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419866" title="Click to go to the Author Index">
             Zhang, Shizhe
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#337834" title="Click to go to the Author Index">
             Wang, Yizhuo
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#305152" title="Click to go to the Author Index">
             Cao, Yuhong
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#180884" title="Click to go to the Author Index">
             Sartoretti, Guillaume Adrien
            </a>
           </td>
           <td class="r">
            National University of Singapore (NUS)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2532" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In multi-robot exploration, a team of mobile robot is tasked with efficiently mapping an unknown environments. While most exploration planners assume omnidirectional sensors like LiDAR, this is impractical for small robots such as drones, where lightweight, directional sensors like cameras may be the only option due to payload constraints. These sensors have a constrained field-of-view (FoV), which adds complexity to the exploration problem, requiring not only optimal robot positioning but also sensor orientation during movement. In this work, we propose MARVEL, a neural framework that leverages graph attention networks, together with novel frontiers and orientation features fusion technique, to develop a collaborative, decentralized policy using multi-agent reinforcement learning (MARL) for robots with constrained FoV. To handle the large action space of viewpoints planning, we further introduce a novel information-driven action pruning strategy. MARVEL improves multi-robot coordination and decision-making in challenging large-scale indoor environments, while adapting to various team sizes and sensor configurations (i.e., FoV and sensor range) without additional training. Our extensive evaluation shows that MARVEL’s learned policies exhibit effective coordinated behaviors, outperforming state-of-the-art exploration planners across multiple metrics. We experimentally demonstrate MARVEL’s generalizability in large-scale environments, of up to 90m by 90m, and validate its practical applicability through successful deployment on a team of real drone hardware.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that10_02">
             08:35-08:40, Paper ThAT10.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2859'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              RACE: A Fast and Lightweight Urban Exploration and Search Strategy for Multi-Robot Systems
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#250206" title="Click to go to the Author Index">
             Leong, Jabez Kit
            </a>
           </td>
           <td class="r">
            Singapore University of Technology and Design
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#103219" title="Click to go to the Author Index">
             Soh, Gim Song
            </a>
           </td>
           <td class="r">
            Singapore University of Technology and Design
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2859" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#search_and_rescue_robots" title="Click to go to the Keyword Index">
               Search and Rescue Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#swarm_robotics" title="Click to go to the Keyword Index">
               Swarm Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Multi-Robot Systems (MRS) are increasingly deployed for hazardous tasks in urban environments. Among many tasks, search and rescue remains challenging as it deals with exploration in an unknown indoor constrained environment. For example, without global knowledge of the map of a building floor, it is not advantageous to choose one path over another at a corridor junction. Also, if the assigned frontiers are far from the robot, backtracking along a corridor will cost more than moving forward. Since exploration along corridors is similar to solving a maze, this paper examines classical maze-solving algorithms that are known to be computationally fast and lightweight, such as the Right Hand Rule (RHR), Random Mouse (RM), and more. The authors have identified two gaps that need to be addressed before these algorithms can be applied to physical MRS. Firstly, these algorithms are not designed for the cooperation of multiple agents in exploration. Secondly, they are often applied to only a low-fidelity simulation environment, which requires some work to make these algorithms transferable to work in the commonly used occupancy grid map environment. In this paper, the authors introduced RACE, a fast and lightweight collective urban exploration and search algorithm based on a modified and condensed version of the Ant Colony Optimization (ACO) algorithm. The proposed solution is successfully verified in a low-fidelity simulation, evaluated against other exploration and search algorithms like RHR and RM. An innovative approach of RACE Simulation to Physical implementation is presented and a physical system evaluation is performed to evaluate RACE against a Rapidly-Exploring Random Tree algorithm. Finally, the proposed solution is further verified with a physical experiment, which a quadrupedal robot is assigned to explore part of a floor of SUTD, spanning approximately (55m x 40m). RACE also showed potential in handling challenging close-loop and dead-end environments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that10_03">
             08:40-08:45, Paper ThAT10.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3006'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Reinforcement Learning Driven Multi-Robot Exploration Via Explicit Communication and Density-Based Frontier Search
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#396308" title="Click to go to the Author Index">
             Calzolari, Gabriele
            </a>
           </td>
           <td class="r">
            Luleå Tekniska Universitet
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#243487" title="Click to go to the Author Index">
             Sumathy, Vidya
            </a>
           </td>
           <td class="r">
            Luleå University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#199367" title="Click to go to the Author Index">
             Kanellakis, Christoforos
            </a>
           </td>
           <td class="r">
            LTU
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#105882" title="Click to go to the Author Index">
             Nikolakopoulos, George
            </a>
           </td>
           <td class="r">
            Luleå University of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3006" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#cooperating_robots" title="Click to go to the Keyword Index">
               Cooperating Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Collaborative multi-agent exploration of unknown environments is crucial for search and rescue operations. Effective real-world deployment must address challenges such as limited inter-agent communication and static and dynamic obstacles. This paper introduces a novel decentralized collaborative framework based on Reinforcement Learning to enhance multi-agent exploration in unknown environments. Our approach enables agents to decide their next action using an agent-centered field-of-view occupancy grid, and features extracted from A* algorithm-based trajectories to frontiers in the reconstructed global map. Furthermore, we propose a constrained communication scheme that enables agents to share their environmental knowledge efficiently, minimizing exploration redundancy. The decentralized nature of our framework ensures that each agent operates autonomously, while contributing to a collective exploration mission. Extensive simulations in Gymnasium and real-world experiments demonstrate the robustness and effectiveness of our system, while all the results highlight the benefits of combining autonomous exploration with inter-agent map sharing, advancing the development of scalable and resilient robotic exploration systems.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that10_04">
             08:45-08:50, Paper ThAT10.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4575'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Integrating Multi-Robot Adaptive Sampling and Informative Path Planning for Spatiotemporal Natural Environment Prediction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#332340" title="Click to go to the Author Index">
             Kailas, Siva
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#371001" title="Click to go to the Author Index">
             Deolasee, Srujan
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#185040" title="Click to go to the Author Index">
             Luo, Wenhao
            </a>
           </td>
           <td class="r">
            University of Illinois Chicago
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#398302" title="Click to go to the Author Index">
             Kim, Woojun
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#117857" title="Click to go to the Author Index">
             Sycara, Katia
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4575" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#path_planning_for_multiple_mobile_robots_or_agents" title="Click to go to the Keyword Index">
               Path Planning for Multiple Mobile Robots or Agents
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Learning to predict spatiotemporal (ST) environmental processes from a sparse set of samples collected autonomously is a difficult task from both a sampling perspective (collecting the best sparse samples) and from a learning perspective (predicting the next timestep). In this work, we focus on investigating the sample collection process via multi-robot informative path planning. We present an approach for incorporating multi-robot informative path planning into a spatiotemporal adaptive sampling framework while considering path length constraints for sampling location selection. We also incorporate informative path planning to determine the best path to collect samples along while en route to collecting the desired sample. We achieve this in a decentralized manner by decoupling the process into two stages: the first stage uses our spatiotemporal mixture of Gaussian Processes (STMGP) model to determine the most informative sampling location via a mutual information lower bound heuristic and the second stage plans an informative path to collect the desired sample and other additional informative samples via submodular function optimization. Moreover, we effectively leverage peer-to-peer communication to enable coordination. Simulation results on real-world spatiotemporal data are provided to validate the effectiveness of our proposed approach.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that10_05">
             08:50-08:55, Paper ThAT10.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4594'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              D-PBS: Dueling Priority-Based Search for Multiple Nonholonomic Robots Motion Planning in Congested Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#301522" title="Click to go to the Author Index">
             Zhang, Xiaotong
            </a>
           </td>
           <td class="r">
            Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#191286" title="Click to go to the Author Index">
             Xiong, Gang
            </a>
           </td>
           <td class="r">
            Institute of Automation, Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#384538" title="Click to go to the Author Index">
             Wang, Yuanjing
            </a>
           </td>
           <td class="r">
            Durham University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#313821" title="Click to go to the Author Index">
             Teng, Siyu
            </a>
           </td>
           <td class="r">
            HKBU; UIC
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#167297" title="Click to go to the Author Index">
             Chen, Long
            </a>
           </td>
           <td class="r">
            Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4594" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#path_planning_for_multiple_mobile_robots_or_agents" title="Click to go to the Keyword Index">
               Path Planning for Multiple Mobile Robots or Agents
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#nonholonomic_motion_planning" title="Click to go to the Keyword Index">
               Nonholonomic Motion Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This letter focuses on the multiple nonholonomic robots motion planning (MRMP) problem in congested and complex environments, where the complexity escalates dramatically with the increase in the number of robots, frequently leading to deadlocks. We present the Dueling Priority-Based Search (D-PBS), an efficient and scalable priority-based motion planner for multiple nonholonomic car-like robots, capable of enabling robots to move safely to destinations in spatially-constrained settings. We achieve this by adopting the alternate dueling collision resolution approach, coupled with the exploration of comprehensive priority relationships, effectively addressing the deadlock situations. We also introduce a novel priority-binding algorithm to enhance the scalability of our planner in restricted spaces densely populated with robots. Experimental evaluations in various scenarios demonstrate that D-PBS outperforms standard approaches to MRMP, offering superior path quality and scalability for larger robot swarms.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that11">
             <b>
              ThAT11
             </b>
            </a>
           </td>
           <td class="r">
            314
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#that11" title="Click to go to the Program at a Glance">
             <b>
              Haptics 1
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#114542" title="Click to go to the Author Index">
             Moore, Carl A.
            </a>
           </td>
           <td class="r">
            FAMU-FSU College of Engineering
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#269294" title="Click to go to the Author Index">
             Chen, Cheng-Wei
            </a>
           </td>
           <td class="r">
            National Taiwan University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that11_01">
             08:30-08:35, Paper ThAT11.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('142'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Vision-Based Haptic Rendering with Self-Occlusion Resilience Using Shadow Correspondence
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#350902" title="Click to go to the Author Index">
             Mao, Mu-Ting
            </a>
           </td>
           <td class="r">
            National Taiwan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#269294" title="Click to go to the Author Index">
             Chen, Cheng-Wei
            </a>
           </td>
           <td class="r">
            National Taiwan University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab142" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#haptics_and_haptic_interfaces" title="Click to go to the Keyword Index">
               Haptics and Haptic Interfaces
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#telerobotics_and_teleoperation" title="Click to go to the Keyword Index">
               Telerobotics and Teleoperation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Vision-based haptic feedback provides cost-effective preemptive protection and real-time guidance, enhancing teleoperation with reduced system complexity. However, challenges arise as the instrument approaches target object, leading to occlusion of the point cloud behind the remote instrument, known as the self-occlusion issue. Prior solutions relying on historical point clouds or multiple viewpoints to refill the occluded region encounter adaptability issues for prolonged occlusion and limited space, thus hindering practical implementation. This paper introduces a novel non-refilling-based method for haptic force rendering, leveraging the correspondence between the tool-tip position and the tip position of the shadow-like occluded region. Experimental results demonstrate the proposed method's resilience across self-occlusion and dynamic environments, highlighting its practical applicability in robotic teleoperation.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that11_02">
             08:35-08:40, Paper ThAT11.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('731'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A New Expression for the Passivity Bound for a Class of Sampled-Data Systems
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101815" title="Click to go to the Author Index">
             Roberts, Rodney
            </a>
           </td>
           <td class="r">
            Florida State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#114542" title="Click to go to the Author Index">
             Moore, Carl A.
            </a>
           </td>
           <td class="r">
            FAMU-FSU College of Engineering
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#100147" title="Click to go to the Author Index">
             Colgate, Edward
            </a>
           </td>
           <td class="r">
            Northwestern University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab731" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#haptics_and_haptic_interfaces" title="Click to go to the Keyword Index">
               Haptics and Haptic Interfaces
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#telerobotics_and_teleoperation" title="Click to go to the Keyword Index">
               Telerobotics and Teleoperation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#force_control" title="Click to go to the Keyword Index">
               Force Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#passivity" title="Click to go to the Keyword Index">
               Passivity
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this article, we characterize the passivity of a class of haptic systems modeled as a simple sampled-data system. Passivity is guaranteed by ensuring that there is enough damping in the haptic interface. A necessary and sufficient bound was determined in earlier work, but the corresponding mathematical expressions were complicated, and the derivations were not completely rigorous. In this article, a more tractable expression is derived. Based on the improved expression, passivity conditions are obtained for several classes of transfer functions representing virtual environments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that11_03">
             08:40-08:45, Paper ThAT11.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1442'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Haptic Feedback Device Actuated by Electromagnetic Torque
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416482" title="Click to go to the Author Index">
             Luo, Xionghuan
            </a>
           </td>
           <td class="r">
            Hong Kong Institute of Science &amp; Innovation, Chinese Academy Of
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#317510" title="Click to go to the Author Index">
             Huang, Yuanrui
            </a>
           </td>
           <td class="r">
            Xi'an Jiaotong-Liverpool University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419850" title="Click to go to the Author Index">
             Zhao, Wenda
            </a>
           </td>
           <td class="r">
            Institute of Automation，Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104575" title="Click to go to the Author Index">
             Liu, Hongbin
            </a>
           </td>
           <td class="r">
            Hong Kong Institute of Science &amp; Innovation, Chinese Academy Of
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1442" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#haptics_and_haptic_interfaces" title="Click to go to the Keyword Index">
               Haptics and Haptic Interfaces
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#wearable_robotics" title="Click to go to the Keyword Index">
               Wearable Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#virtual_reality_and_interfaces" title="Click to go to the Keyword Index">
               Virtual Reality and Interfaces
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Haptic feedback enhances user interaction with systems by adding the sense of touch, thereby improving immersion and realism in applications like virtual reality (VR), augmented reality (AR), video games, education, and robotic surgery. To address the challenges in mechanically actuated haptic feedback devices such as limited mobility, mechanical wear, and complex mechanical structures, several research sought to develop electromagnetic haptic feedback systems. However, they also suffer from the rapid decay of magnetic force with distance, thus restricting their workspace size and application potential. In this paper, we propose a novel electromagnetic haptic feedback device that is actuated by magnetic torque instead of magnetic force. By controlling the magnetic torque, which decays with distance only at a third-order rate, our device achieves a large workspace—a 200-mm-diameter hemisphere—while still delivering perceptible real-time haptic feedback within the hemisphere. While using the device, the user wears a lightweight haptic thimble housing a permanent magnet on their finger, which enables 2 degree-of-freedom (DoF) haptic feedback. A 13-coil electromagnet array serves as the source of the magnetic field. A mathematical model is proposed to determine the currents in the electromagnet array to generate the desired amount of haptic feedback torque. We conducted two experiments to prove the viability of the device. A haptic feedback accuracy experiment was conducted and validated the device's ability to generate sufficient torque within a large workspace. A user evaluation experiment showed that the device achieved an overall accuracy of 77.86% in a virtual enclosure exploration task, indicating its effectiveness and usability in haptic feedback applications.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that11_04">
             08:45-08:50, Paper ThAT11.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3492'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Vibrotactile Haptics with Soft Magnetoresponsive Surface Interface
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425014" title="Click to go to the Author Index">
             Rimer, Evan
            </a>
           </td>
           <td class="r">
            Queen's University, Ingenuity Labs Research Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#105575" title="Click to go to the Author Index">
             Hashtrudi-Zaad, Keyvan
            </a>
           </td>
           <td class="r">
            Queen's University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#201565" title="Click to go to the Author Index">
             Robertson, Matthew
            </a>
           </td>
           <td class="r">
            Queen's University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3492" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#haptics_and_haptic_interfaces" title="Click to go to the Keyword Index">
               Haptics and Haptic Interfaces
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#soft_robot_materials_and_design" title="Click to go to the Keyword Index">
               Soft Robot Materials and Design
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#wearable_robotics" title="Click to go to the Keyword Index">
               Wearable Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper explores the feasibility of using magnetoresponsive silicone as the primary mechanism for generating vibrotactile feedback in haptic interfaces. The distinctive feature of this research lies in the integration of magnetoresponsive silicone, a flexible material that responds to electromagnetic fields to produce localized vibrations. Preliminary experiments evaluate the performance of these actuators, focusing on their ability to produce controlled vibrations across a range of frequencies and amplitudes relevant to human tactile perception. Building on this foundation, we introduce the VibroFlex Pad, a haptic interface featuring a magnetoresponsive silicone sheet and an array of electromagnets. The VibroFlex Pad demonstrates its versatility in generating varied tactile effects and simulating dynamic wave-like movements across its surface. To assess the VibroFlex Pad's effectiveness, a user study was conducted, separately evaluating tactile accuracy, overall performance, and user comfort. The findings suggest that the VibroFlex Pad offers reliable and precise vibrotactile feedback, highlighting its potential to enhance wearable haptic technologies and improve the user experience in a variety of applications.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that11_05">
             08:50-08:55, Paper ThAT11.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4284'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Haptic Shoulder for Rendering Biomechanically Accurate Joint Limits for Human-Robot Physical Interactions
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#215008" title="Click to go to the Author Index">
             Peiros, Lizzie
            </a>
           </td>
           <td class="r">
            University of California, San Diego
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#342167" title="Click to go to the Author Index">
             Joyce, Calvin
            </a>
           </td>
           <td class="r">
            University of California, San Diego
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425816" title="Click to go to the Author Index">
             Murugesan, Tarun
            </a>
           </td>
           <td class="r">
            University of California, San Diego
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#378824" title="Click to go to the Author Index">
             Nguyen, Roger
            </a>
           </td>
           <td class="r">
            University of California, San Diego
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425832" title="Click to go to the Author Index">
             Fiorini, Isabella
            </a>
           </td>
           <td class="r">
            University of California, San Diego
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425837" title="Click to go to the Author Index">
             Galibut, Rizzi
            </a>
           </td>
           <td class="r">
            University of California, San Diego
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#137547" title="Click to go to the Author Index">
             Yip, Michael C.
            </a>
           </td>
           <td class="r">
            University of California, San Diego
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4284" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#physical_human_robot_interaction" title="Click to go to the Keyword Index">
               Physical Human-Robot Interaction
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#safety_in_hri" title="Click to go to the Keyword Index">
               Safety in HRI
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#biologically_inspired_robots" title="Click to go to the Keyword Index">
               Biologically-Inspired Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Human-robot physical interaction (pHRI) is a rapidly evolving research field with significant implications for physical therapy, search and rescue, and telemedicine. However, a major challenge lies in accurately understanding human constraints and safety in human-robot physical experiments without an IRB and physical human experiments. Concerns regarding human studies include safety concerns, repeatability, and scalability of the number and diversity of participants. This paper examines whether a physical approximation can serve as a stand-in for human subjects to enhance robot autonomy for physical assistance. This paper introduces the SHULDRD (Shoulder Haptic Universal Limb Dynamic Repositioning Device), an economical and anatomically similar device designed for real-time testing and deployment of pHRI planning tasks onto robots in the real world. SHULDRD replicates human shoulder motion, providing crucial force feedback and safety data. The device's open-source CAD and software facilitate easy construction and use, ensuring broad accessibility for researchers. By providing a flexible platform able to emulate infinite human subjects, ensure repeatable trials, and provide quantitative metrics to assess the effectiveness of the robotic intervention, SHULDRD aims to improve the safety and efficacy of human-robot physical interactions.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that11_06">
             08:55-09:00, Paper ThAT11.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4896'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Experimental Evaluation of Haptic Shared Control for Multiple Electromagnetic Untethered Microrobots (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#202190" title="Click to go to the Author Index">
             Ferro, Marco
            </a>
           </td>
           <td class="r">
            CNRS
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#304697" title="Click to go to the Author Index">
             Pinan Basualdo, Franco Nicolas
            </a>
           </td>
           <td class="r">
            Katholieke Universiteit Leuven
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#103169" title="Click to go to the Author Index">
             Robuffo Giordano, Paolo
            </a>
           </td>
           <td class="r">
            Irisa Cnrs Umr6074
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#120094" title="Click to go to the Author Index">
             Misra, Sarthak
            </a>
           </td>
           <td class="r">
            University of Twente
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#138065" title="Click to go to the Author Index">
             Pacchierotti, Claudio
            </a>
           </td>
           <td class="r">
            Centre National De La Recherche Scientifique (CNRS)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4896" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#haptics_and_haptic_interfaces" title="Click to go to the Keyword Index">
               Haptics and Haptic Interfaces
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#telerobotics_and_teleoperation" title="Click to go to the Keyword Index">
               Telerobotics and Teleoperation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#micro_nano_robots" title="Click to go to the Keyword Index">
               Micro/Nano Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The precise manipulation of microrobots presents challenges arising from their small size and susceptibility to external disturbances. To address these challenges, we present the experimental evaluation of a haptic shared control teleoperation framework for the locomotion of multiple microrobots, relying on a kinesthetic haptic interface and a custom electromagnetic system. Six combinations of haptic and shared control strategies are evaluated during a safe 3D navigation scenario in a cluttered environment. 18 participants are asked to steer two spherical magnetic microrobots among obstacles to reach a predefined goal, under different conditions. For each condition, participants are provided with different obstacle avoidance and navigation guidance cues. Results show that providing assistance in avoiding obstacles guarantees safer performance, regardless if the assistance is autonomous or delivered through a haptic repulsive force. Moreover, autonomous obstacle avoidance also reduces the completion time by 30% compared to haptic obstacle avoidance and no obstacle avoidance cases, although haptic feedback is preferred by the users. Finally, providing haptic guidance towards the target improves by the 65% the positioning accuracy of the microrobots with respect to not providing this guidance.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that12">
             <b>
              ThAT12
             </b>
            </a>
           </td>
           <td class="r">
            315
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#that12" title="Click to go to the Program at a Glance">
             <b>
              Assembly
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#171352" title="Click to go to the Author Index">
             Liu, Changliu
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#246096" title="Click to go to the Author Index">
             Bahar, Iris
            </a>
           </td>
           <td class="r">
            Colorado School of Mines
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that12_01">
             08:30-08:35, Paper ThAT12.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('30'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              StableLego: Stability Analysis of Block Stacking Assembly
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#292677" title="Click to go to the Author Index">
             Liu, Ruixuan
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#361271" title="Click to go to the Author Index">
             Deng, Kangle
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#229538" title="Click to go to the Author Index">
             Wang, Ziwei
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#171352" title="Click to go to the Author Index">
             Liu, Changliu
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab30" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#assembly" title="Click to go to the Keyword Index">
               Assembly
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#performance_evaluation_and_benchmarking" title="Click to go to the Keyword Index">
               Performance Evaluation and Benchmarking
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#robotics_and_automation_in_construction" title="Click to go to the Keyword Index">
               Robotics and Automation in Construction
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Structural stability is a necessary condition for successful construction of an assembly. However, designing a stable assembly requires a non-trivial effort since a slight variation in the design could significantly affect the structural stability. To address the challenge, this paper studies the stability of assembly structures, in particular, block stacking assembly. The paper proposes a new optimization formulation, which optimizes over force balancing equations, for inferring the structural stability of 3D block stacking structures. The proposed stability analysis is verified on hand-crafted Lego examples. The experiment results demonstrate that the proposed method can correctly predict whether the structure is stable. In addition, it outperforms the existing methods since it can accurately locate the weakest parts in the design, and more importantly, solve any given assembly structures. To further validate the proposed method, we provide StableLego: a comprehensive dataset including 50k+ 3D objects with their Lego layouts. We test the proposed stability analysis and include the stability inference for each corresponding object in StableLego. Our code and the dataset are available at https://github.com/intelligent-control-lab/StableLego.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that12_02">
             08:35-08:40, Paper ThAT12.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('169'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Component Selection for Craft Assembly Tasks
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#400904" title="Click to go to the Author Index">
             Isume, Vitor Hideyo
            </a>
           </td>
           <td class="r">
            Osaka University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#232912" title="Click to go to the Author Index">
             Kiyokawa, Takuya
            </a>
           </td>
           <td class="r">
            Osaka University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#111816" title="Click to go to the Author Index">
             Yamanobe, Natsuki
            </a>
           </td>
           <td class="r">
            Advanced Industrial Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#109212" title="Click to go to the Author Index">
             Domae, Yukiyasu
            </a>
           </td>
           <td class="r">
            The National Institute of Advanced Industrial Science and Techno
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#109858" title="Click to go to the Author Index">
             Wan, Weiwei
            </a>
           </td>
           <td class="r">
            Osaka University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#102257" title="Click to go to the Author Index">
             Harada, Kensuke
            </a>
           </td>
           <td class="r">
            Osaka University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab169" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#assembly" title="Click to go to the Keyword Index">
               Assembly
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Inspired by traditional handmade crafts, where a person improvises assemblies based on the available objects, we formally introduce the Craft Assembly Task. It is a robotic assembly task that involves building an accurate representation of a given target object using the available objects, which do not directly correspond to its parts. In this work, we focus on selecting the subset of available objects for the final craft, when the given input is an RGB image of the target in the wild. We use a mask segmentation neural network to identify visible parts, followed by retrieving labeled template meshes. These meshes undergo pose optimization to determine the most suitable template. Then, we propose to simplify the parts of the transformed template mesh to primitive shapes like cuboids or cylinders. Finally, we design a search algorithm to find correspondences in the scene based on local and global proportions. We develop baselines for comparison that consider all possible combinations, and choose the highest scoring combination for common metrics used in foreground maps and mask accuracy. Our approach achieves comparable results to the baselines for two different scenes, and we show qualitative results for an implementation in a real-world scenario.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that12_03">
             08:40-08:45, Paper ThAT12.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('545'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Assembly Order Planning for Modular Structures by Autonomous Multi-Robot Systems
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417693" title="Click to go to the Author Index">
             Peters, Tom
            </a>
           </td>
           <td class="r">
            TU Eindhoven
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#131537" title="Click to go to the Author Index">
             Cheung, Kenneth C.
            </a>
           </td>
           <td class="r">
            National Aeronautics and Space Administration (NASA)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#354677" title="Click to go to the Author Index">
             Kostitsyna, Irina
            </a>
           </td>
           <td class="r">
            KBR at NASA Ames Research Center
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab545" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#assembly" title="Click to go to the Keyword Index">
               Assembly
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#path_planning_for_multiple_mobile_robots_or_agents" title="Click to go to the Keyword Index">
               Path Planning for Multiple Mobile Robots or Agents
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#parallel_robots" title="Click to go to the Keyword Index">
               Parallel Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Coordinated multi-agent robotic construction provides a means to build infrastructure in extreme environments and improve efficiency in high performance applications. Planning methods are key to understanding and achieving the scope of such applications, and are typically tailored to specific models of construction material and a consideration of passivity or activity thereof. Here, we focus on the NASA Automated Reconfigurable Mission Adaptive Digital Assembly Systems (ARMADAS) model, which includes passive lightweight structural modules and small robots that traverse the structure. We present an algorithm for calculating a build plan for robots under the constraints of this type of system. We then evaluate the quality of this plan experimentally. Many of the techniques we use can be applied to any robotic assembly system whose robots perform locomotion over the structure that they are building.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that12_04">
             08:45-08:50, Paper ThAT12.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1087'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Master Rules from Chaos: Learning to Reason, Plan, and Interact from Chaos for Tangram Assembly
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#236041" title="Click to go to the Author Index">
             Zhao, Chao
            </a>
           </td>
           <td class="r">
            Hong Kong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#245310" title="Click to go to the Author Index">
             Jiang, Chunli
            </a>
           </td>
           <td class="r">
            The Hong Kong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#342039" title="Click to go to the Author Index">
             Luo, Lifan
            </a>
           </td>
           <td class="r">
            The Hong Kong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#281097" title="Click to go to the Author Index">
             Zhang, Guanlan
            </a>
           </td>
           <td class="r">
            The Hong Kong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#294459" title="Click to go to the Author Index">
             Yu, Hongyu
            </a>
           </td>
           <td class="r">
            The Hong Kong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#100161" title="Click to go to the Author Index">
             Wang, Michael Yu
            </a>
           </td>
           <td class="r">
            Mywang@gbu.edu.cn
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#206397" title="Click to go to the Author Index">
             Chen, Qifeng
            </a>
           </td>
           <td class="r">
            HKUST
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1087" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#grasping" title="Click to go to the Keyword Index">
               Grasping
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#assembly" title="Click to go to the Keyword Index">
               Assembly
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Tangram assembly, the art of human intelligence and manipulation dexterity, is a new challenge for robotics and reveals the limitations of state-of-the-arts. Here, we describe our initial exploration and highlight key problems in reasoning, planning, and manipulation for robotic tangram assembly. We present MRChaos (Master Rules from Chaos), a robust and general solution for learning assembly policies that can generalize to novel objects. In contrast to conventional methods based on prior geometric and kinematic models, MRChaos learns to assemble randomly generated objects through self-exploration in simulation without prior experience in assembling target objects. The reward signal is obtained from the visual observation change without manually designed models or annotations. MRChaos retains its robustness in assembling various novel tangram objects that have never been encountered during training, with only silhouette prompts. We show the potential of MRChaos in wider applications such as cutlery combinations. The presented work indicates that radical generalization in robotic assembly can be achieved by learning in much simpler domains.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that12_05">
             08:50-08:55, Paper ThAT12.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2932'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Robot Planning under Uncertainty for Object Assembly and Troubleshooting Using Human Causal Models
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#313323" title="Click to go to the Author Index">
             Basu, Semanti
            </a>
           </td>
           <td class="r">
            Brown University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#360036" title="Click to go to the Author Index">
             Tatlidil, Semir
            </a>
           </td>
           <td class="r">
            Brown University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#394687" title="Click to go to the Author Index">
             Kim, Moon Hwan
            </a>
           </td>
           <td class="r">
            Brown University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419625" title="Click to go to the Author Index">
             Tran, Tiffany
            </a>
           </td>
           <td class="r">
            Brown University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419634" title="Click to go to the Author Index">
             Saxena, Serena
            </a>
           </td>
           <td class="r">
            Brown University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#174132" title="Click to go to the Author Index">
             Williams, Tom
            </a>
           </td>
           <td class="r">
            Colorado School of Mines
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#360045" title="Click to go to the Author Index">
             Sloman, Steven
            </a>
           </td>
           <td class="r">
            Brown University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#246096" title="Click to go to the Author Index">
             Bahar, Iris
            </a>
           </td>
           <td class="r">
            Colorado School of Mines
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2932" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#human_centered_robotics" title="Click to go to the Keyword Index">
               Human-Centered Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#embodied_cognitive_science" title="Click to go to the Keyword Index">
               Embodied Cognitive Science
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#planning_under_uncertainty" title="Click to go to the Keyword Index">
               Planning under Uncertainty
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper we explore if human mental models of objects, even when flawed, can be integrated with a collaborative robot's decision making framework to allow it to make smarter choices under partial observability for different object-related tasks such as assembly and troubleshooting. We demonstrate how (1) these informative causal models can be extracted from humans through crowdsourcing, (2) object assembly and troubleshooting can be formulated as Partially Observable Markov Decision Processes (POMDPs) and (3) our extracted causal models can be incorporated into those models in the form of approximate priors. Finally, (4) we use systematic experimentation in simulation to demonstrate the success of this approach, with 2X average improvement in reward observed for object assembly tasks, and 1.4X average improvement in reward observed for troubleshooting tasks.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that12_06">
             08:55-09:00, Paper ThAT12.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3314'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Robotic Dry-Stacking of Clocháin with Irregular Stones
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#205492" title="Click to go to the Author Index">
             Liu, Yifang
            </a>
           </td>
           <td class="r">
            Oak Ridge National Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#105917" title="Click to go to the Author Index">
             Napp, Nils
            </a>
           </td>
           <td class="r">
            Cornell University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3314" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#robotics_and_automation_in_construction" title="Click to go to the Keyword Index">
               Robotics and Automation in Construction
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#assembly" title="Click to go to the Keyword Index">
               Assembly
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper explores automated robotic construction of clocháin, a type of corbelled rock shelter, traditionally crafted by skilled workers. While robots have been employed for simple dry-stacking tasks in the past, such as construction of stone walls or vertical stone towers, the question of whether robots possess the capacity to construct more functional structures remains unanswered. This study presents a significant step forward in robotic dry-stacking of functional structures: the assembly of natural stones into freestanding clocháin structures. We also present a set of stackability measures to aid stone selection, which significantly improves the stability of the planned structures. Our sequential filtering approach, originally designed for planning stone walls, plays a foundational role in achieving stable clochán construction. Experimental results validate the effectiveness of the stackability measures and demonstrate the physical execution of dry-stacking clocháin. The progress demonstrated in this paper opens the door to robotic construction of a wide range of utility structures in unstructured environments.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that13">
             <b>
              ThAT13
             </b>
            </a>
           </td>
           <td class="r">
            316
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#that13" title="Click to go to the Program at a Glance">
             <b>
              Reinforcement Learning Applications
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that13_01">
             08:30-08:35, Paper ThAT13.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('681'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Synthesizing Depowdering Trajectories for Robot Arms Using Deep Reinforcement Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417807" title="Click to go to the Author Index">
             Maurer, Maximilian
            </a>
           </td>
           <td class="r">
            Festo SE &amp; Co. KG
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#418185" title="Click to go to the Author Index">
             Seefeldt, Simon
            </a>
           </td>
           <td class="r">
            University of Tübingen
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#314717" title="Click to go to the Author Index">
             Seyler, Jan Reinke
            </a>
           </td>
           <td class="r">
            Festo SE &amp; Co. KG
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#314715" title="Click to go to the Author Index">
             Eivazi, Shahram
            </a>
           </td>
           <td class="r">
            University of Tübingen
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab681" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#task_and_motion_planning" title="Click to go to the Keyword Index">
               Task and Motion Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#representation_learning" title="Click to go to the Keyword Index">
               Representation Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Research into robotics applications of deep reinforcement learning (DRL) has increasingly been focussed on learning precise object manipulation and trajectory planning. Extending these tasks to continuous robot-object interactions with the surface of complex geometries remains an open problem. In this paper we investigate end-to-end DRL solutions for depowdering tasks that work by directing a pressurized air stream onto the object's surfaces using a blast nozzle head mounted on a robotic arm. We develop a GPU accelerated vectorized cleaning effect for integration into RL training and consider ways to expose vision-less trajectory synthesis for surface treatment applications to the RL agent based on UV mapping. Our experimental evaluation demonstrates that DRL has the potential to be used for generating object-specific agents for depowdering tasks on a variety of 3D objects without requiring intermediate path planners even in a full 3D motion setup. Finally, we show that DRL-generated trajectories can be transferred to a real-world setup. Our task formulation lends itself to approximate a wide range of surface treatment applications (e.g., cleaning and spray painting) with various effects.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that13_02">
             08:35-08:40, Paper ThAT13.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1012'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              World Model-Based Perception for Visual Legged Locomotion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#338350" title="Click to go to the Author Index">
             Lai, Hang
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#418268" title="Click to go to the Author Index">
             Cao, Jiahang
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#230162" title="Click to go to the Author Index">
             Xu, Jiafeng
            </a>
           </td>
           <td class="r">
            ByteDance
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#248783" title="Click to go to the Author Index">
             Wu, Hongtao
            </a>
           </td>
           <td class="r">
            Bytedance
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#418153" title="Click to go to the Author Index">
             Lin, Yunfeng
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#192738" title="Click to go to the Author Index">
             Kong, Tao
            </a>
           </td>
           <td class="r">
            ByteDance
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#313565" title="Click to go to the Author Index">
             Yu, Yong
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#312696" title="Click to go to the Author Index">
             Zhang, Weinan
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1012" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Legged locomotion over various terrains is challenging and requires precise perception of the robot and its surroundings from both proprioception and vision. However, learning directly from high-dimensional visual input is often data-inefficient and intricate. To address this issue, traditional methods attempt to learn a teacher policy with access to privileged information first and then learn a student policy to imitate the teacher's behavior with visual input. Despite some progress, this imitation framework prevents the student policy from achieving optimal performance due to the information gap between inputs. Furthermore, the learning process is unnatural since animals intuitively learn to traverse different terrains based on their understanding of the world without privileged knowledge. Inspired by this natural ability, we propose a simple yet effective method, World Model-based Perception (WMP), which builds a world model of the environment and learns a policy based on the world model. We illustrate that though completely trained in simulation, the world model can make accurate predictions of real-world trajectories, thus providing informative signals for the policy controller. Extensive simulated and real-world experiments demonstrate that WMP outperforms state-of-the-art baselines in traversability and robustness. Videos and Code are available at: https://wmp-loco.github.io/.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that13_03">
             08:40-08:45, Paper ThAT13.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1363'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              V-Pilot: A Velocity Vector Control Agent for Fixed-Wing UAVs from Imperfect Demonstrations
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#414610" title="Click to go to the Author Index">
             Gong, Xudong
            </a>
           </td>
           <td class="r">
            National University of Defense Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420881" title="Click to go to the Author Index">
             Dawei, Feng
            </a>
           </td>
           <td class="r">
            National University of Defense Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#237618" title="Click to go to the Author Index">
             Xu, Kele
            </a>
           </td>
           <td class="r">
            National University of Defense Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#294024" title="Click to go to the Author Index">
             Zhou, Xing
            </a>
           </td>
           <td class="r">
            National University of Defense Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420890" title="Click to go to the Author Index">
             Zheng, Si
            </a>
           </td>
           <td class="r">
            Qiyuan Lab
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#206056" title="Click to go to the Author Index">
             Ding, Bo
            </a>
           </td>
           <td class="r">
            National University of Defense Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#206062" title="Click to go to the Author Index">
             Wang, Huaimin
            </a>
           </td>
           <td class="r">
            National University of Defense Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1363" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper addresses the challenge of Velocity Vector Control (VVC) for fixed-wing UAVs using Reinforcement Learning (RL) in the presence of imperfect demonstrations. The multi-objective and long-horizon nature of VVC introduces significant spatial and temporal complexities, complicating RL's exploration. While demonstration-based RL methods can help mitigate exploration challenges, their effectiveness is often limited by the quality of the provided demonstrations. To tackle this, we propose V-Pilot, a novel approach that integrates: (1) a controller equipped with a control law model to reduce action oscillation, thus alleviating temporal exploration issues, and (2) a VVC-specific training workflow for iterative policy refinement and demonstration quality improvement. This framework is designed to enhance the performance of demonstration-based RL under imperfect demonstrations. We evaluate V-Pilot on the fixed-wing UAV RL environment, FlyCraft. Experimental results demonstrate that V-Pilot outperforms PID and Behavioral Cloning across multiple performance metrics.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that13_04">
             08:45-08:50, Paper ThAT13.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4119'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Efficiently Generating Expressive Quadruped Behaviors Via Language-Guided Preference Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#324203" title="Click to go to the Author Index">
             Clark, Jaden
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#322619" title="Click to go to the Author Index">
             Hejna, Donald
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#195462" title="Click to go to the Author Index">
             Sadigh, Dorsa
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4119" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#social_hri" title="Click to go to the Keyword Index">
               Social HRI
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#emotional_robotics" title="Click to go to the Keyword Index">
               Emotional Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Expressive robotic behavior is essential for the widespread acceptance of robots in social environments. Recent advancements in learned legged locomotion controllers have enabled more dynamic and versatile robot behaviors. However, determining the optimal behavior for interactions with different users across varied scenarios remains a challenge. Current methods either rely on natural language input, which is efficient but low-resolution, or learn from human preferences, which, although high-resolution, is sample inefficient. This paper introduces a novel approach that leverages priors generated by pre-trained LLMs alongside the precision of preference learning. Our method, termed Language-Guided Preference Learning (LGPL), uses LLMs to generate initial behavior samples, which are then refined through preference-based feedback to learn behaviors that closely align with human expectations. Our core insight is that LLMs can guide the sampling process for preference learning, leading to a substantial improvement in sample efficiency. We demonstrate that LGPL can quickly learn accurate and expressive behaviors with as few as four queries, outperforming both purely language-parameterized models and traditional preference learning approaches. Website with videos: lgpl-gaits.github.io/
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that13_05">
             08:50-08:55, Paper ThAT13.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('5017'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Learning Multi-Agent Coordination for Replenishment at Sea
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#360457" title="Click to go to the Author Index">
             Han, Byeolyi
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#430283" title="Click to go to the Author Index">
             Cho, Minwoo
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#282818" title="Click to go to the Author Index">
             Chen, Letian
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#249983" title="Click to go to the Author Index">
             Paleja, Rohan
            </a>
           </td>
           <td class="r">
            MIT Lincoln Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#288397" title="Click to go to the Author Index">
             Wu, Zixuan
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#251252" title="Click to go to the Author Index">
             Ye, Sean
            </a>
           </td>
           <td class="r">
            Zoox
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#255783" title="Click to go to the Author Index">
             Seraj, Esmaeil
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#178319" title="Click to go to the Author Index">
             Sidoti, David
            </a>
           </td>
           <td class="r">
            US Naval Research Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#160727" title="Click to go to the Author Index">
             Gombolay, Matthew
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab5017" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#planning__scheduling_and_coordination" title="Click to go to the Keyword Index">
               Planning, Scheduling and Coordination
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Optimizing large-scale logistics is computationally challenging due to its scale and requirement to be robust to stochastic and time-varying weather disturbances. However, prior research in multi-agent reinforcement learning (MARL) does not address scenarios that capture complexity of logistics operations influenced by dynamic weather patterns. To address this gap, we suggest a new MARL environment, textsc{Marine} that has two types of agents equipped with limited resources and integrates real wave data to model the influences of weather on the replenishment at sea (RAS) operation. To this end, we propose SchedHGNN, a novel MARL algorithm that incorporates a heterogeneous graph neural network and an intrinsic reward scheme to enhance agent coordination and mitigate challenges induced by environment non-stationarity. Our results show that the combination of effective RAS scheduling and improved communication enables our model to outperform competitive baselines by up to 37.8%. This achievement marks a significant advancement in applying MARL to complex, real-world logistics scenarios.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that14">
             <b>
              ThAT14
             </b>
            </a>
           </td>
           <td class="r">
            402
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#that14" title="Click to go to the Program at a Glance">
             <b>
              Exoskeletons
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#133414" title="Click to go to the Author Index">
             Sharma, Nitin
            </a>
           </td>
           <td class="r">
            North Carolina State University
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#131647" title="Click to go to the Author Index">
             Zarrouk, David
            </a>
           </td>
           <td class="r">
            Ben Gurion University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that14_01">
             08:30-08:35, Paper ThAT14.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('124'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Real-Time Ultrasound Imaging of a Human Muscle to Optimize Shared Control in a Hybrid Exoskeleton
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#274225" title="Click to go to the Author Index">
             Iyer, Ashwin
            </a>
           </td>
           <td class="r">
            North Carolina State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#279814" title="Click to go to the Author Index">
             Sun, Ziyue
            </a>
           </td>
           <td class="r">
            NCSU
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#315150" title="Click to go to the Author Index">
             Lambeth, Krysten
            </a>
           </td>
           <td class="r">
            North Carolina State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#311696" title="Click to go to the Author Index">
             Singh, Mayank
            </a>
           </td>
           <td class="r">
            North Carolina State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#364024" title="Click to go to the Author Index">
             Cleveland, Christine
            </a>
           </td>
           <td class="r">
            University of North Carolina-Chapel Hill
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#133414" title="Click to go to the Author Index">
             Sharma, Nitin
            </a>
           </td>
           <td class="r">
            North Carolina State University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab124" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#prosthetics_and_exoskeletons" title="Click to go to the Keyword Index">
               Prosthetics and Exoskeletons
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#rehabilitation_robotics" title="Click to go to the Keyword Index">
               Rehabilitation Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#ultrasound_imaging" title="Click to go to the Keyword Index">
               Ultrasound Imaging
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             A hybrid exoskeleton is a class of wearable robotic technology that simultaneously uses a powered exoskeleton and functional electrical stimulation (FES) to generate assistive joint torques for people with impaired mobility due to neurological disorders such as spinal cord injury (SCI). The hybrid assistive technology benefits from FES that actively elicits force from paralyzed muscles via their neural excitation, leading to muscle strengthening. The main technical barrier to realizing the hybrid technology is to attain stable coordination between FES and the exoskeleton despite the quick onset of FES-induced muscle fatigue, which causes a rapid decline in the muscle force. Current methods to measure the induced fatigue lack direct muscle state measurements and may be ineffective at capturing the muscle force decay due to FES. Instead, ultrasound (US) imaging accurately quantifies FES-related muscle contractility and fatigue due to the direct visualization of muscle fibers. In this paper, we use real-time US imaging-derived muscle strain changes as biomarkers of FES-induced fatigue in an optimal controller that modulates exoskeleton assistance and FES dosage. To demonstrate that real-time US imaging is a promising muscle-machine interface technology that can optimize shared control in a hybrid exoskeleton, we perform experiments involving continuous seated knee extension and over-ground walking tasks on two participants with SCI and four participants without disabilities. Furthermore, this work helps design a novel and unprecedented robotic gait technology with the capability to impart FES-associated therapeutic benefits while assisting the gait of neurologically impaired individuals, including those with SCI, stroke, multiple sclerosis, etc.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that14_02">
             08:35-08:40, Paper ThAT14.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('474'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Design and Control of a Novel Semi-Passive Knee Exoskeleton
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#406186" title="Click to go to the Author Index">
             Sade, Alon
            </a>
           </td>
           <td class="r">
            Ben Gurion University of the Negev
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#406185" title="Click to go to the Author Index">
             Coifman, Itay
            </a>
           </td>
           <td class="r">
            Ben Gurion University of the Negev
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#123249" title="Click to go to the Author Index">
             Riemer, Raziel
            </a>
           </td>
           <td class="r">
            Ben-Gurion University of the Negev
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#131647" title="Click to go to the Author Index">
             Zarrouk, David
            </a>
           </td>
           <td class="r">
            Ben Gurion University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab474" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#mechanism_design" title="Click to go to the Keyword Index">
               Mechanism Design
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#prosthetics_and_exoskeletons" title="Click to go to the Keyword Index">
               Prosthetics and Exoskeletons
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#wearable_robotics" title="Click to go to the Keyword Index">
               Wearable Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a novel semi-passive knee exoskeleton designed to provide running assistance. It incorporates an energy-efficient clutch mechanism activated by a mini servomotor which engages and disengages the spring that supports the leg during running. The exoskeleton extracts energy during the running phase when the muscles are acting as brakes (negative power), stores it in the spring, and then returns this energy during the positive power phase (when the muscles are acting as motors). The exoskeleton controller implements an inertial measurement unit (IMU) sensor to estimate the shank orientation that determines when to engage and disengage the spring. Two experiments designed to probe the functionality of the exoskeleton were conducted to evaluate its control performance and actuation, and the exoskeleton's biomechanical impact on three subjects. The findings showed that the control mechanism could be engaged and disengaged in real time. The maximum moment created on the knee muscles was 17 Nm, although the device could supply 28 Nm. The ratio of the consumed servo energy consumption to the subjects’ saved energy was 1:160 (0.1W input to 16W saved). This study thus paves the way for the development of lightweight, inexpensive exoskeletons that can contribute to their greater availability for a broader range of individuals.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that14_03">
             08:40-08:45, Paper ThAT14.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('712'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Model-Based Control Strategies Comparison of One Bionic Ankle Tensegrity Exoskeleton: BATE
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#217280" title="Click to go to the Author Index">
             Wei, Dunwen
            </a>
           </td>
           <td class="r">
            University of Electronic Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#401030" title="Click to go to the Author Index">
             Mao, Shiyu
            </a>
           </td>
           <td class="r">
            University of Electronic Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#317098" title="Click to go to the Author Index">
             Zhang, Zhichao
            </a>
           </td>
           <td class="r">
            University of Electronic Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#317097" title="Click to go to the Author Index">
             Wei, Ximing
            </a>
           </td>
           <td class="r">
            University of Electronic Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#256707" title="Click to go to the Author Index">
             Gao, Tao
            </a>
           </td>
           <td class="r">
            University of Electronic Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#137686" title="Click to go to the Author Index">
             Ficuciello, Fanny
            </a>
           </td>
           <td class="r">
            Università Di Napoli Federico II
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab712" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#rehabilitation_robotics" title="Click to go to the Keyword Index">
               Rehabilitation Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#wearable_robotics" title="Click to go to the Keyword Index">
               Wearable Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#biologically_inspired_robots" title="Click to go to the Keyword Index">
               Biologically-Inspired Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a comparative analysis of model-based control strategies for a Bionic Ankle Tensegrity Exoskeleton (BATE). The BATE is designed to mimic the self-stress equilibrium and self-supporting characteristics of the human ankle biotensegrity structure. Model-based control strategies are conventional methods that can help discover the principles of complex tensegrity systems. The high dimensions and non-linearity of the BATE pose challenges for physical modelling and require unique model-based control strategies. In this study, we propose a modelling method that considers interaction force and explore the trajectory tracking performance and robustness of the ankle exoskeleton under three power-assisted control methods: position control, force control, and hybrid force-position contorl. The experimental results suggest that the PC method offers superior tracking performance and robustness compared to the other two methods. This method can be used for early rehabilitation training to improve flexibility. The control concept emphasizes its advantages over current wearable exoskeletons and introduces new ideas for high-performance exoskeletons.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that14_04">
             08:45-08:50, Paper ThAT14.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4177'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Human-Like Walking Motion Generation for Self-Balancing Lower Limb Rehabilitation Exoskeletons
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#369878" title="Click to go to the Author Index">
             Yang, Ming
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#369856" title="Click to go to the Author Index">
             Chen, Ziqiang
            </a>
           </td>
           <td class="r">
            Shenzhen Institute of Advanced Technology, Chinese Academy of Sc
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#362329" title="Click to go to the Author Index">
             Li, Wentao
            </a>
           </td>
           <td class="r">
            Shenzhen Institute of Advanced Technology, Chinese Academy of Sc
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#310628" title="Click to go to the Author Index">
             Li, Feng
            </a>
           </td>
           <td class="r">
            Shenzhen Institute of Advanced Technology Chinere Academy of Sci
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#139703" title="Click to go to the Author Index">
             Shang, Weiwei
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#362318" title="Click to go to the Author Index">
             Tian, Dingkui
            </a>
           </td>
           <td class="r">
            Shenzhen Advanced Technology Research Institute, Chinese Academy
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#100984" title="Click to go to the Author Index">
             Wu, Xinyu
            </a>
           </td>
           <td class="r">
            CAS
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4177" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#prosthetics_and_exoskeletons" title="Click to go to the Keyword Index">
               Prosthetics and Exoskeletons
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#rehabilitation_robotics" title="Click to go to the Keyword Index">
               Rehabilitation Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#wearable_robotics" title="Click to go to the Keyword Index">
               Wearable Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Self-balancing lower limb rehabilitation exoskeletons (SLLREs) allow individuals with lower limb dysfunction to walk without the use of crutches. Stable and human-like walking motions are crucial for SLLREs because achieving a close imitation of healthy human walking is a key goal in rehabilitation therapy. Existing SLLREs can realize stable walking but lack human-like features. This paper designs a walking motion generator based on hierarchical optimization to generate a human-like walking motion with variable hip height, heel-strike, toe-off, and knee-stretched features. This hierarchically optimized human-like walking motion generator consists of a knee-stretched optimizer and an optimization-based stabilizing filter. Specifically, the knee-stretched optimizer realizes the stretched knee feature by optimizing the hip trajectory with varying heights. And the stabilizing filter realizes stable walking by optimizing the hip trajectory in the sagittal plane direction.To validate the effectiveness of the proposed human-like walking motion generator, walking experiments were conducted on SLLRE AutoLEE-G3 both in a simulation environment and the real world. The experimental results show that the human-like walking motions look more natural and reduce the required torque for the knee joint compared with knee-bent walking.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that14_05">
             08:50-08:55, Paper ThAT14.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4950'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Kinematic Benefits of a Cable-Driven Exosuit for Head-Neck Mobility
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#367399" title="Click to go to the Author Index">
             Bales, Ian
            </a>
           </td>
           <td class="r">
            University of Utah
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#203843" title="Click to go to the Author Index">
             Zhang, Haohan
            </a>
           </td>
           <td class="r">
            University of Utah
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4950" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#prosthetics_and_exoskeletons" title="Click to go to the Keyword Index">
               Prosthetics and Exoskeletons
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#wearable_robotics" title="Click to go to the Keyword Index">
               Wearable Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#tendon_wire_mechanism" title="Click to go to the Keyword Index">
               Tendon/Wire Mechanism
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This letter presents a novel cable-driven exosuit intended for head-neck support and movement assistance. Mobility limitations in the head-neck, such as dropped head syndrome, can result from various neurological disorders. Current solutions, ranging from static neck collars to rigid-link robotic neck exoskeletons, are unsatisfactory. Neck collars are the most used clinically but fail to restore head-neck motion. Rigid-link neck exoskeletons can enable head movement but are bulky and restrictive. In this letter, we present the design of this exosuit, an analysis of its ability to balance the gravitational moment of the head in simulation, and the results of a user study comparing its kinematic performance to a state-of-the-art rigid-link neck exoskeleton. The exosuit is able to support the head across its full range of motion according to simulation results. It fits users of different sizes and participants exhibited more natural head-neck movement wearing the exosuit as compared to wearing the rigid-link exoskeleton. The exosuit allowed more head rotations than the rigid-link neck exoskeleton and required less compensatory torso movement for three daily tasks (looking for traffic, drinking from a bottle, and picking up an object from the floor). Its absolute range of motion was also much larger than the one allowed by the rigid-link neck exoskeleton. These results demonstrate the kinematic benefits of a cable-driven neck exosuit and provide justification for studying the use of such an exosuit for head-neck movement assistance in patient groups.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that15">
             <b>
              ThAT15
             </b>
            </a>
           </td>
           <td class="r">
            403
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#that15" title="Click to go to the Program at a Glance">
             <b>
              Continuum Robots 1
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#187439" title="Click to go to the Author Index">
             Morimoto, Tania K.
            </a>
           </td>
           <td class="r">
            University of California San Diego
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#423104" title="Click to go to the Author Index">
             Yuan, Sichen
            </a>
           </td>
           <td class="r">
            The University of Alabama
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that15_01">
             08:30-08:35, Paper ThAT15.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('33'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              PH-Gauss-Lobatto Reduced-Order-Model for Shape Control of Soft-Continuum Manipulators
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#271706" title="Click to go to the Author Index">
             Mbakop, Steeve
            </a>
           </td>
           <td class="r">
            Junia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#193125" title="Click to go to the Author Index">
             Tagne, Gilles
            </a>
           </td>
           <td class="r">
            Yncréa Hauts De France / ISEN Lille
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#331004" title="Click to go to the Author Index">
             Chevillon, Tanguy
            </a>
           </td>
           <td class="r">
            Junia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#106147" title="Click to go to the Author Index">
             Drakunov, Sergey
            </a>
           </td>
           <td class="r">
            IHMC
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#117837" title="Click to go to the Author Index">
             Merzouki, Rochdi
            </a>
           </td>
           <td class="r">
            CRIStAL, CNRS UMR 9189, University of Lille1
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab33" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#modeling__control__and_learning_for_soft_robots" title="Click to go to the Keyword Index">
               Modeling, Control, and Learning for Soft Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#kinematics" title="Click to go to the Keyword Index">
               Kinematics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#soft_robot_applications" title="Click to go to the Keyword Index">
               Soft Robot Applications
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Soft and hyper-elastic materials possess properties of resilience and flexibility, characterizing a class of Soft-Continuum Manipulators (SCM). The latter describes a robot structure with an infinite number of degrees of freedom (DoFs), useful for mobility and manipulation. However, these geometric characteristics are source of modeling and control problems. In this paper, a Pythagorean Hodograph (PH) curve based Reduced-Order-Model (ROM) relying on the Gauss-Lobatto quadrature is investigated for the modeling and the control of SCM. This allows, first, reducing the dimension of the SCM kinematics based on the PH parametric curves with a predefined length and second, developing the shape kinematics control from its control polygon. The use of the Gauss-Lobatto quadrature allows to move independently the PH curve control points, while preserving PH features of length and minimum curve energy. These features are important to control in real-time the shape of the SCM. The proposed approach has been validated numerically and experimentally, carried out on a bio-inspired Soft continuum Elephant Trunk Robot.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that15_02">
             08:35-08:40, Paper ThAT15.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('90'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Towards Contact-Aided Motion Planning for Tendon-Driven Continuum Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#285996" title="Click to go to the Author Index">
             Rao, Priyanka
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#159588" title="Click to go to the Author Index">
             Salzman, Oren
            </a>
           </td>
           <td class="r">
            Technion
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#113447" title="Click to go to the Author Index">
             Burgner-Kahrs, Jessica
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab90" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#modeling__control__and_learning_for_soft_robots" title="Click to go to the Keyword Index">
               Modeling, Control, and Learning for Soft Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#soft_robot_applications" title="Click to go to the Keyword Index">
               Soft Robot Applications
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Tendon-driven continuum robots (TDCRs), with their flexible backbones, offer the advantage of being used for navigating complex, cluttered environments. However, to do so, they typically require multiple segments, often leading to complex actuation and control challenges. To this end, we propose a novel approach to navigate cluttered spaces effectively for a single-segment long TDCR which is the simplest topology from a mechanical point of view. Our key insight is that by leveraging contact with the environment we can achieve multiple curvatures without mechanical alterations to the robot. Specifically, we propose a search-based motion planner for a single-segment TDCR. This planner, guided by a specially designed heuristic, discretizes the configuration space and employs a best-first search. The heuristic, crucial for efficient navigation, provides an effective cost-to-go estimation while respecting the kinematic constraints of the TDCR and environmental interactions. We empirically demonstrate the efficiency of our planner-testing over 525 queries in environments with both convex and non-convex obstacles, our planner is demonstrated to have a success rate of about 80% while baselines were not able to obtain a success rate higher than 30%. The difference is attributed to our novel heuristic which is shown to significantly reduce the required search space.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that15_02">
             08:35-08:40, Paper ThAT15.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('632'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Simple Dynamics Model for Cable Driven Continuum Robots with Actuator Coupling
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#256468" title="Click to go to the Author Index">
             Watson, Connor
            </a>
           </td>
           <td class="r">
            Morimoto Lab, UCSD
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#187439" title="Click to go to the Author Index">
             Morimoto, Tania K.
            </a>
           </td>
           <td class="r">
            University of California San Diego
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab632" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#modeling__control__and_learning_for_soft_robots" title="Click to go to the Keyword Index">
               Modeling, Control, and Learning for Soft Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#surgical_robotics__steerable_catheters_needles" title="Click to go to the Keyword Index">
               Surgical Robotics: Steerable Catheters/Needles
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#tendon_wire_mechanism" title="Click to go to the Keyword Index">
               Tendon/Wire Mechanism
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The flexibility and dexterity of cable-driven continuum robots (CDCRs) make them well-suited for intricate tasks such as minimally invasive surgery. However, the complexity of accurately modeling their dynamics has limited their broader adoption and effective control. Current models either oversimplify the dynamics by assuming quasi-static conditions or overcomplicate them, making real-time application challenging. Additionally, many existing models neglect the critical coupling between the robot's body and actuator dynamics, a factor essential for accurate control. In this paper, we propose a new, minimal dynamics model for CDCRs that strikes a balance between simplicity and accuracy. Our model captures the essential dynamics of both the robot and its actuators, providing a practical tool for control design. We also establish connections between our model and those used for other robotic systems, enabling the transfer of well-established control strategies to CDCRs. The model is validated through hardware experiments, demonstrating its capability to effectively address complex control challenges in CDCR applications.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that15_03">
             08:40-08:45, Paper ThAT15.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('587'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Novel Tendon-Driven Articulated Continuum Robot with Stabilized Self-Locking Joints
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416679" title="Click to go to the Author Index">
             Ren, Jiankun
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#293890" title="Click to go to the Author Index">
             Qi, Lizhe
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#418014" title="Click to go to the Author Index">
             Jia, Yu
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#372479" title="Click to go to the Author Index">
             Wang, Hecheng
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#404742" title="Click to go to the Author Index">
             Wang, Ziheng
            </a>
           </td>
           <td class="r">
            Academy for Engineering &amp; Technology, Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#353960" title="Click to go to the Author Index">
             Sun, Yunquan
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab587" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#mechanism_design" title="Click to go to the Keyword Index">
               Mechanism Design
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#tendon_wire_mechanism" title="Click to go to the Keyword Index">
               Tendon/Wire Mechanism
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#actuation_and_joint_mechanisms" title="Click to go to the Keyword Index">
               Actuation and Joint Mechanisms
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Articulated continuum robots (ACRs) are characterized by flexibility, controllability, and adaptability and perform excellently in complex and constrained environments. However, the large number of motor drives limit the ACRs' portability and make them cumbersome to control. This paper presents a novel tendon-driven ACR composed of stabilized self-locking joints (SLJs) connected in series. After triggering the mechanical constraints with shape memory alloy coils, each joint can be maintained in either a self-locking or release state with zero power consumption. Consequently, even with a single set of drive units, the ACR can operate in multiple modes, enabling variable motion performance and workspace adaptability, effectively reducing the number of motors. The ACR's stiffness also varies with the locking state of its SLJs, and no motor drive is required to maintain its shape when all SLJs are self-locking. The performance and reliability of the SLJ prototype were validated. The workspace of the ACR prototype model was analyzed, and its partial motion performance, motion error, and variable stiffness were verified.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that15_04">
             08:45-08:50, Paper ThAT15.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2022'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Tensiworm: A Novel Tensegrity Robot with Enhanced Peristaltic Locomotion Efficiency
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423131" title="Click to go to the Author Index">
             Kazoleas, Christian
            </a>
           </td>
           <td class="r">
            The University of Alabama
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423136" title="Click to go to the Author Index">
             Zhang, Jiajun
            </a>
           </td>
           <td class="r">
            The University of Alabama
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423104" title="Click to go to the Author Index">
             Yuan, Sichen
            </a>
           </td>
           <td class="r">
            The University of Alabama
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2022" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#actuation_and_joint_mechanisms" title="Click to go to the Keyword Index">
               Actuation and Joint Mechanisms
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#biomimetics" title="Click to go to the Keyword Index">
               Biomimetics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#soft_robot_materials_and_design" title="Click to go to the Keyword Index">
               Soft Robot Materials and Design
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Tensegrity structures have been widely explored for their lightweight, high-stiffness, and foldable properties. These unique characteristics have enabled their application in various fields including robotics. Tensegrity robots have demonstrated diverse locomotion modes offering versatile solutions for navigation in complex environments. Recent efforts in bio-inspired robotics have led to designs mimicking the movement of natural organisms, such as earthworms. However, existing designs, particularly those utilizing motor-pulley mechanisms for robot body contraction, face significant challenges due to their bulky actuation systems that reduce locomotion efficiency. This paper introduces a novel tensegrity robot, "Tensiworm," inspired by the peristaltic locomotion of an earthworm. Composed of three icosahedron tensegrity unit cells connected in series, the Tensiworm robot employs a sequential contraction and relaxation mechanism driven by active cable members made of shape memory actuators. This innovative design achieves a 59.13% folding ratio and weighs only 46.9 grams. The robot can travel a distance equal to its body length in approximately ten cycles with an average speed of 10.01 mm per minute. Furthermore, the use of thinner, flexible structural members broadens possibilities for development of millimeter-scale tensegrity robots, which hold significant potential for biomedical applications, including in-vivo testing and targeted drug delivery.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that15_05">
             08:50-08:55, Paper ThAT15.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2411'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Accelerated Quasi-Static FEM for Real-Time Modeling of Continuum Robots with Multiple Contacts and Large Deformation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#361812" title="Click to go to the Author Index">
             Chen, Hao
            </a>
           </td>
           <td class="r">
            University of Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#317513" title="Click to go to the Author Index">
             Chen, Jian
            </a>
           </td>
           <td class="r">
            Hong Kong Institute of Science and Innovation, Chinese Academy O
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#368283" title="Click to go to the Author Index">
             Liu, Xinran
            </a>
           </td>
           <td class="r">
            University of Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#336380" title="Click to go to the Author Index">
             Zhang, Zihui
            </a>
           </td>
           <td class="r">
            Institute of Automation, Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#317510" title="Click to go to the Author Index">
             Huang, Yuanrui
            </a>
           </td>
           <td class="r">
            Xi'an Jiaotong-Liverpool University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#195998" title="Click to go to the Author Index">
             Zhang, Zhongkai
            </a>
           </td>
           <td class="r">
            University of Montpellier, CNRS
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#336379" title="Click to go to the Author Index">
             Liu, Hongbin
            </a>
           </td>
           <td class="r">
            Institute of Automation，Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2411" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#simulation_and_animation" title="Click to go to the Keyword Index">
               Simulation and Animation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#contact_modeling" title="Click to go to the Keyword Index">
               Contact Modeling
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Continuum 机器人提供高度的灵活性和多个自由度，使其成为导航窄流腔的理想选择。然而，准确模拟它们在大变形和频繁环境接触下的行为仍然具有挑战性。当前求解这些机器人变形的方法，例如模型降阶法和高斯-塞德尔 （GS） 方法，都存在明显的缺点。随着接触点数量的增加，他们的计算速度会降低，并且难以在速度和模型精度之间取得平衡。为了克服这些限制，我们引入了一种名为 Acc-FEM 的新型有限元方法 （FEM）。Acc-FEM 采用大变形准静态有限元模型，并集成了加速求解器方案，以高效处理多触点仿真。此外，它还利用图形处理单元 （GPU） 的并行计算来实时更新有限元模型和
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that16">
             <b>
              ThAT16
             </b>
            </a>
           </td>
           <td class="r">
            404
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#that16" title="Click to go to the Program at a Glance">
             <b>
              Grasping 3
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#105834" title="Click to go to the Author Index">
             Sun, Yu
            </a>
           </td>
           <td class="r">
            University of South Florida
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#111056" title="Click to go to the Author Index">
             Natale, Lorenzo
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that16_01">
             08:30-08:35, Paper ThAT16.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('34'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Multi-Object Grasping -- Experience Forest for Robotic Finger Movement Strategies
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#246956" title="Click to go to the Author Index">
             Chen, Tianze
            </a>
           </td>
           <td class="r">
            University of South Florida
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#105834" title="Click to go to the Author Index">
             Sun, Yu
            </a>
           </td>
           <td class="r">
            University of South Florida
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab34" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#logistics" title="Click to go to the Keyword Index">
               Logistics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#grasping" title="Click to go to the Keyword Index">
               Grasping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper introduces a novel Experience Forest algorithm designed for multi-object grasping (MOG). Different from single-object grasping, for MOG, the hand poses a few steps before the end of grasping play important roles in the success of MOG. But similar to single-object grasping, the hand poses that are far away from the end grasping pose are not as relevant. Therefore, the proposed approach invented the Experience Forest structure to organize the finger movement sequences collected in naive MOG approaches with a set of trees instead of a single tree. The algorithm propagates success or failure results in the trials from end-pose nodes only to the nodes representing several preceding hand poses. When using the trees to generate a grasping sequence, the algorithm generates a finger-movement policy that follows a MOG synergy at the beginning and then transits to a tree in the Experience Forest and then employs a breadth-first search to achieve a more reliable solution. Tested on various objects using a UR5e robotic arm and Barrett hand in both simulated and real environments, the strategy significantly boosts efficiency in object transfer tasks by up to 60%, marking a 10% improvement over our previous methods.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that16_02">
             08:35-08:40, Paper ThAT16.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2641'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              VMF-Contact: Uncertainty-Aware Evidential Learning for Probabilistic Contact-Grasp in Noisy Clutter
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#366994" title="Click to go to the Author Index">
             Shi, Yitian
            </a>
           </td>
           <td class="r">
            Karlsruhe Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#396767" title="Click to go to the Author Index">
             Welte, Edgar
            </a>
           </td>
           <td class="r">
            Karlsruhe Institute of Technology (KIT)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#285619" title="Click to go to the Author Index">
             Gilles, Maximilian
            </a>
           </td>
           <td class="r">
            Karlsruhe Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#217449" title="Click to go to the Author Index">
             Rayyes, Rania
            </a>
           </td>
           <td class="r">
            Karlsruhe Institute for Technology (KIT)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2641" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#grasping" title="Click to go to the Keyword Index">
               Grasping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Grasp learning in noisy environments, such as occlusions, sensor noise, and out-of-distribution (OOD) objects, poses significant challenges. Recent learning-based approaches focus primarily on capturing aleatoric uncertainty from inherent data noise. The epistemic uncertainty, which represents the OOD recognition, is often addressed by ensembles with multiple forward paths, limiting real-time application. In this paper, we propose an uncertainty-aware approach for 6-DoF grasp detection using evidential learning to comprehensively capture both uncertainties in real-world robotic grasping. As a key contribution, we introduce vMF-Contact, a novel architecture for learning hierarchical contact grasp representations with probabilistic modeling of directional uncertainty as von Mises–Fisher (vMF) distribution. To achieve this, we analyze the theoretical formulation of the second-order objective on the posterior parametrization, providing formal guarantees for the model's ability to quantify uncertainty and improve grasp prediction performance. Moreover, we enhance feature expressiveness by applying partial point reconstructions as an auxiliary task, improving the comprehension of uncertainty quantification as well as the generalization to unseen objects. In the real-world experiments, our method demonstrates a significant improvement by 39% in the overall clearance rate compared to the baselines. The code is available under: https://github.com/YitianShi/vMF-Contact/tree/main
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that16_03">
             08:40-08:45, Paper ThAT16.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2822'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              QuadWBG: Generalizable Quadrupedal Whole-Body Grasping
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#289271" title="Click to go to the Author Index">
             Wang, Jilong
            </a>
           </td>
           <td class="r">
            Galaxy General Robot Co., Ltd
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424027" title="Click to go to the Author Index">
             Rajabov, Javokhirbek
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#377082" title="Click to go to the Author Index">
             Xu, Chaoyi
            </a>
           </td>
           <td class="r">
            Beijing University of Posts and Telecommunications
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423927" title="Click to go to the Author Index">
             Zheng, Yiming
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#155911" title="Click to go to the Author Index">
             Wang, He
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2822" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#mobile_manipulation" title="Click to go to the Keyword Index">
               Mobile Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#whole_body_motion_planning_and_control" title="Click to go to the Keyword Index">
               Whole-Body Motion Planning and Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Legged robots with advanced manipulation capabilities have the potential to significantly improve household duties and urban maintenance. Despite considerable progress in developing robust locomotion and precise manipulation methods, seamlessly integrating these into cohesive whole-body control for real-world applications remains challenging. In this paper, we present a modular framework for robust and generalizable whole-body loco-manipulation controller based on a single arm-mounted camera. By using reinforcement learning (RL), we enable a robust low-level policy for command execution over 5 dimensions (5D) and a grasp-aware high-level policy guided by a novel metric, Generalized Oriented Reachability Map (GORM). The proposed system achieves state-of-the-art one-time grasping accuracy of 89% in real world, including challenging tasks such as grasping transparent objects. Through extensive simulations and real-world experiments, we demonstrate that our system can effectively manage a large workspace, from floor level to above body height, and perform diverse whole-body loco-manipulation tasks.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that16_04">
             08:45-08:50, Paper ThAT16.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3243'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Composing Dextrous Grasping and In-Hand Manipulation Via Scoring with a Reinforcement Learning Critic
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#323464" title="Click to go to the Author Index">
             Röstel, Lennart
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#286526" title="Click to go to the Author Index">
             Winkelbauer, Dominik
            </a>
           </td>
           <td class="r">
            DLR
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#310399" title="Click to go to the Author Index">
             Pitz, Johannes
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#312100" title="Click to go to the Author Index">
             Sievers, Leon
            </a>
           </td>
           <td class="r">
            German Aerospace Center
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#105517" title="Click to go to the Author Index">
             Bäuml, Berthold
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3243" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#in_hand_manipulation" title="Click to go to the Keyword Index">
               In-Hand Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#dexterous_manipulation" title="Click to go to the Keyword Index">
               Dexterous Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In-hand manipulation and grasping are fundamental yet often separately addressed tasks in robotics. For deriving in-hand manipulation policies, reinforcement learning has recently shown great success. However, the derived controllers are not yet useful in real-world scenarios because they often require a human operator to place the objects in suitable initial (grasping) states. Finding stable grasps that also promote the desired in-hand manipulation goal is an open problem. In this work, we propose a method for bridging this gap by leveraging the critic network of a reinforcement learning agent trained for in-hand manipulation to score and select initial grasps. Our experiments show that this method significantly increases the success rate of in-hand manipulation without requiring additional training. We also present an implementation of a full grasp manipulation pipeline on a real-world system, enabling autonomous grasping and reorientation even of unwieldy objects.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that16_05">
             08:50-08:55, Paper ThAT16.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3604'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Bring Your Own Grasp Generator: Leveraging Robot Grasp Generation for Prosthetic Grasping
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425619" title="Click to go to the Author Index">
             Stracquadanio, Giuseppe
            </a>
           </td>
           <td class="r">
            Italian Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#325269" title="Click to go to the Author Index">
             Vasile, Federico
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#216562" title="Click to go to the Author Index">
             Maiettini, Elisa
            </a>
           </td>
           <td class="r">
            Humanoid Sensing and Perception, Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#232991" title="Click to go to the Author Index">
             Boccardo, Nicolò
            </a>
           </td>
           <td class="r">
            IIT - Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#111056" title="Click to go to the Author Index">
             Natale, Lorenzo
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3604" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#sensor_fusion" title="Click to go to the Keyword Index">
               Sensor Fusion
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#prosthetics_and_exoskeletons" title="Click to go to the Keyword Index">
               Prosthetics and Exoskeletons
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             One of the most important research challenges in upper-limb prosthetics is enhancing the user-prosthesis communication to closely resemble the experience of a natural limb. As prosthetic devices become more complex, users often struggle to control the additional degrees of freedom. In this context, leveraging
             <i>
              shared-autonomy
             </i>
             principles can significantly improve the usability of these systems. In this paper, we present a novel
             <i>
              eye-in-hand
             </i>
             prosthetic grasping system that follows these principles. Our system initiates the approach-to-grasp action based on user's command and automatically configures the DoFs of a prosthetic hand. First, it reconstructs the 3D geometry of the target object without the need of a depth camera. Then, it tracks the hand motion during the approach-to-grasp action and finally selects a candidate grasp configuration according to user's intentions. We deploy our system on the Hannes prosthetic hand and test it on able-bodied subjects and amputees to validate its effectiveness. We compare it with a multi-DoF prosthetic control baseline and find that our method enables faster grasps, while simplifying the user experience. Code and demo videos are available online at
             <a href='"https://hsp-iit.github.io/byogg/"'>
              this https URL
             </a>
             .
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that17">
             <b>
              ThAT17
             </b>
            </a>
           </td>
           <td class="r">
            405
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#that17" title="Click to go to the Program at a Glance">
             <b>
              Localization 5
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#235923" title="Click to go to the Author Index">
             Lu, Guoyu
            </a>
           </td>
           <td class="r">
            University of Georgia
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#212673" title="Click to go to the Author Index">
             Jiao, Jianhao
            </a>
           </td>
           <td class="r">
            University College London
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that17_01">
             08:30-08:35, Paper ThAT17.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('487'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              AIR-HLoc: Adaptive Retrieved Images Selection for Efficient Visual Localisation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#372341" title="Click to go to the Author Index">
             Liu, Changkun
            </a>
           </td>
           <td class="r">
            The Hong Kong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#212673" title="Click to go to the Author Index">
             Jiao, Jianhao
            </a>
           </td>
           <td class="r">
            University College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#274358" title="Click to go to the Author Index">
             Huang, Huajian
            </a>
           </td>
           <td class="r">
            The Hong Kong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#390652" title="Click to go to the Author Index">
             Ma, Zhengyang
            </a>
           </td>
           <td class="r">
            The Hong Kong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#147896" title="Click to go to the Author Index">
             Kanoulas, Dimitrios
            </a>
           </td>
           <td class="r">
            University College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#372650" title="Click to go to the Author Index">
             Braud, Tristan
            </a>
           </td>
           <td class="r">
            HKUST
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab487" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             State-of-the-art hierarchical localisation pipelines (HLoc) employ image retrieval (IR) to establish 2D-3D correspondences by selecting the top-k most similar images from a reference database. While increasing k improves localisation robustness, it also linearly increases computational cost and runtime, creating a significant bottleneck. This paper investigates the relationship between global and local descriptors, showing that greater similarity between the global descriptors of query and database images increases the proportion of feature matches. Low similarity queries significantly benefit from increasing k, while high similarity queries rapidly experience diminishing returns. Building on these observations, we propose an adaptive strategy that adjusts k based on the similarity between the query's global descriptor and those in the database, effectively mitigating the feature-matching bottleneck. Our approach reduces computational costs and processing time without sacrificing accuracy. Experiments on three indoor and outdoor datasets show that AIR-HLoc reduces feature matching time by up to 30% while preserving state-of-the-art accuracy. The results demonstrate that AIR-HLoc facilitates a latency-sensitive localisation system.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that17_02">
             08:35-08:40, Paper ThAT17.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('585'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              NeuraLoc: Visual Localization in Neural Implicit Map with Dual Complementary Features
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#381966" title="Click to go to the Author Index">
             Zhai, Hongjia
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#401917" title="Click to go to the Author Index">
             Boming, Zhao
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#343557" title="Click to go to the Author Index">
             Li, Hai
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#412842" title="Click to go to the Author Index">
             Pan, Xiaokun
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#201400" title="Click to go to the Author Index">
             He, Yijia
            </a>
           </td>
           <td class="r">
            TCL RayNeo
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#234116" title="Click to go to the Author Index">
             Cui, Zhaopeng
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#204923" title="Click to go to the Author Index">
             Bao, Hujun
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#204095" title="Click to go to the Author Index">
             Zhang, Guofeng
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab585" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Recently, neural radiance fields (NeRF) have gained significant attention in the field of visual localization. However, existing NeRF-based approaches either lack geometric constraints or require extensive storage for feature matching, limiting their practical applications. To address these challenges, we propose an efficient and novel visual localization approach based on the neural implicit map with complementary features. Specifically, to enforce geometric constraints and reduce storage requirements, we implicitly learn a 3D keypoint descriptor field, avoiding the need to explicitly store point-wise features. To further address the semantic ambiguity of descriptors, we introduce additional semantic contextual feature fields, which enhance the quality and reliability of 2D-3D correspondences. Besides, we propose descriptor similarity distribution alignment to minimize the domain gap between 2D and 3D feature spaces during matching. Finally, we construct the matching graph using both complementary descriptors and contextual features to establish accurate 2D-3D correspondences for 6-DoF pose estimation. Compared with the recent NeRF-based approach, our method achieves a 3x faster training speed and a 45x reduction in model storage. Extensive experiments on two widely used datasets demonstrate that our approach outperforms or is highly competitive with other state-of-the-art NeRF-based visual localization methods.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that17_03">
             08:40-08:45, Paper ThAT17.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1419'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              LiftFeat: 3D Geometry-Aware Local Feature Matching
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#414621" title="Click to go to the Author Index">
             Liu, Yepeng
            </a>
           </td>
           <td class="r">
            Wuhan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420366" title="Click to go to the Author Index">
             Lai, Wenpeng
            </a>
           </td>
           <td class="r">
            SFMAP Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#204091" title="Click to go to the Author Index">
             Zhao, Zhou
            </a>
           </td>
           <td class="r">
            Central China Normal University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421122" title="Click to go to the Author Index">
             Xiong, Yuxuan
            </a>
           </td>
           <td class="r">
            Wuhan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420951" title="Click to go to the Author Index">
             Zhu, Jinchi
            </a>
           </td>
           <td class="r">
            Wuhan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#418979" title="Click to go to the Author Index">
             Cheng, Jun
            </a>
           </td>
           <td class="r">
            Institute for Infocomm Research, A*STAR
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420074" title="Click to go to the Author Index">
             Xu, Yongchao
            </a>
           </td>
           <td class="r">
            Wuhan University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1419" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robust and efficient local feature matching plays a crucial role in applications such as SLAM and visual localization for robotics. Despite great progress, it is still very challenging to extract robust and discriminative visual features in scenarios with drastic lighting changes, low texture areas, or repetitive patterns. In this paper, we propose a new lightweight network called LiftFeat, which lifts the robustness of raw descriptor by aggregating 3D geometric feature. Specifically, we first adopt a pre-trained monocular depth estimation model to generate pseudo surface normal label, supervising the extraction of 3D geometric feature in terms of predicted surface normal. We then design a 3D geometry-aware feature lifting module to fuse surface normal feature with raw 2D descriptor feature. Integrating such 3D geometric feature enhances the discriminative ability of 2D feature description in extreme conditions. Extensive experimental results on relative pose estimation, homography estimation, and visual localization tasks, demonstrate that our LiftFeat outperforms some lightweight state-of-the-art methods. Code will be released at : https://github.com/lyp-deeplearning/LiftFeat.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that17_04">
             08:45-08:50, Paper ThAT17.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2690'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              DVS-Aware Visual Perception for Mobile Robots with Neuromorphic Hardware
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#297476" title="Click to go to the Author Index">
             Zhong, Hanzhong
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#209556" title="Click to go to the Author Index">
             Jin, YingJie
            </a>
           </td>
           <td class="r">
            Lenovo Research
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424116" title="Click to go to the Author Index">
             Li, Guangbin
            </a>
           </td>
           <td class="r">
            Lenovo Research
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#132634" title="Click to go to the Author Index">
             Li, Xiang
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#342587" title="Click to go to the Author Index">
             Wang, Zhepeng
            </a>
           </td>
           <td class="r">
            Lenovo Research
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2690" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#neurorobotics" title="Click to go to the Keyword Index">
               Neurorobotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#sensor_based_control" title="Click to go to the Keyword Index">
               Sensor-based Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The Dynamic Vision Sensor (DVS) is a distinctive visual sensor that exclusively responds to alterations in pixel brightness, enabling the real-time capture of swift and subtle movements with reduced power consumption and data bandwidth requirements. This paper proposes a DVS-aware visual perception method and presents its application for pose estimation of mobile robots. Specifically, a new marker is designed to provide pose reference data that leverages the inherent advantages of DVS more effectively. Moreover, we formulate a pose recognition system incorporating DVS, an algorithm based on Spiking Convolutional Neural Networks (SCNN) and a neuromorphic computing accelerator (Lynxi HS110). Such a formulation can well explore the DVS's advantages, as its event-triggered feature matches the nature of SCNN while the neuromorphic hardware enables efficient, low-power execution, making the system highly suitable for real-time embedded applications. Comparative analysis with traditional ARcode-based pose recognition methods reveals that our innovative approach demonstrates significant advantages in recognition speed and energy efficiency. The whole system is deployed on mobile robots and evaluated in real-world scenarios.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that17_05">
             08:50-08:55, Paper ThAT17.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2700'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Feedback RoI Features Improve Aerial Object Detection
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421008" title="Click to go to the Author Index">
             Ren, Botao
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#372280" title="Click to go to the Author Index">
             Xu, Botian
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#335718" title="Click to go to the Author Index">
             Wang, Jingyi
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417001" title="Click to go to the Author Index">
             Gao, Hanwei
            </a>
           </td>
           <td class="r">
            SAIC AILab
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#214581" title="Click to go to the Author Index">
             Yu, Qiankun
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#140686" title="Click to go to the Author Index">
             Deng, Zhidong
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2700" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__perception_and_autonomy" title="Click to go to the Keyword Index">
               Aerial Systems: Perception and Autonomy
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Research in visual perception has shown that the human visual system utilizes high-level feedback information to guide lower-level processing, enabling adaptation to signals of varying characteristics. Inspired by this, we propose the Feedback multi-Level feature EXtractor (Flex) to dynamically adjust feature selection in object detection based on image-wise and instance-level feedback information. This is particularly beneficial for applications such as aerial object detection, UAV-based target recognition and autonomous vehicle navigation, where global image quality issues like sensor degradation, foggy, or rainy conditions can impact detection performance. Flex adapts to variations in image quality, refining the feature extraction process to improve robustness against these challenges. Experimental results demonstrate that Flex consistently enhances a range of state-of-the-art methods on challenging aerial object detection datasets, including DOTA-v1.0, DOTA-v1.5, and HRSC2016. Furthermore, additional experiments on MS COCO confirm the module's effectiveness in general object detection tasks. Our quantitative and qualitative analyses reveal that the improvements are strongly correlated with image quality, aligning with our original motivation to address global image quality issues in real-world scenarios.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that17_06">
             08:55-09:00, Paper ThAT17.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4277'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Keypoint Detection and Description for Raw Bayer Images
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#395575" title="Click to go to the Author Index">
             Lin, Jiakai
            </a>
           </td>
           <td class="r">
            University of Georgia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#379578" title="Click to go to the Author Index">
             Zhang, Jinchang
            </a>
           </td>
           <td class="r">
            University of Georgia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#235923" title="Click to go to the Author Index">
             Lu, Guoyu
            </a>
           </td>
           <td class="r">
            University of Georgia
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4277" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#visual_tracking" title="Click to go to the Keyword Index">
               Visual Tracking
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Keypoint detection and local feature description are fundamental tasks in robotic perception, critical for applications such as SLAM, robot localization, feature matching, pose estimation, and 3D mapping. While existing methods predominantly operate on RGB images, we propose a novel network that directly processes raw images, bypassing the need for the Image Signal Processor (ISP). This approach significantly reduces hardware requirements and memory consumption, which is crucial for robotic vision systems. Our method introduces two custom-designed convolutional kernels capable of performing convolutions directly on raw images, preserving inter-channel information without converting to RGB. Experimental results show that our network outperforms existing algorithms on raw images, achieving higher accuracy and stability under large rotations and scale variations. This work represents the first attempt to develop a keypoint detection and feature description network specifically for raw images, offering a more efficient solution for resource-constrained environments.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that18">
             <b>
              ThAT18
             </b>
            </a>
           </td>
           <td class="r">
            406
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#that18" title="Click to go to the Program at a Glance">
             <b>
              Planning under Uncertainty 1
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#398222" title="Click to go to the Author Index">
             Tariq, Faizan M.
            </a>
           </td>
           <td class="r">
            Honda Research Institute USA, Inc
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#191308" title="Click to go to the Author Index">
             Kennedy, Monroe
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that18_01">
             08:30-08:35, Paper ThAT18.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('885'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Delayed-Decision Motion Planning in the Presence of Multiple Predictions
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#165476" title="Click to go to the Author Index">
             Isele, David
            </a>
           </td>
           <td class="r">
            University of Pennsylvania, Honda Research Institute USA
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#374646" title="Click to go to the Author Index">
             Anon, Alexandre Miranda
            </a>
           </td>
           <td class="r">
            Honda Research Institute USA
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#398222" title="Click to go to the Author Index">
             Tariq, Faizan M.
            </a>
           </td>
           <td class="r">
            Honda Research Institute USA, Inc
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#411045" title="Click to go to the Author Index">
             Yeh, Zheng-Hang
            </a>
           </td>
           <td class="r">
            Honda Research Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#210722" title="Click to go to the Author Index">
             Singh, Avinash
            </a>
           </td>
           <td class="r">
            Honda Research Institute, USA
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#268381" title="Click to go to the Author Index">
             Bae, Sangjae
            </a>
           </td>
           <td class="r">
            Honda Research Institute, USA
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab885" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#planning_under_uncertainty" title="Click to go to the Keyword Index">
               Planning under Uncertainty
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#autonomous_agents" title="Click to go to the Keyword Index">
               Autonomous Agents
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Reliable automated driving technology is challenged by various sources of uncertainties, in particular, behavioral uncertainties of traffic agents. It is not uncommon for traffic agents to contain multiple intentions followed by distinguishable maneuvers, and the automated driving car must reflect the uncertainty. This paper formalizes a behavior planning scheme in the presence of multiple possible futures with corresponding probabilities. In essence, we present a maximum entropy formulation and show how, under certain assumptions, this allows delayed decision-making to improve safety. The general formulation is then turned into a model predictive control formulation, which is solved as a quadratic program or a set of quadratic programs. We discuss implementation details for improving computation and present validation results in simulation and on a mobile robot.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that18_02">
             08:35-08:40, Paper ThAT18.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('910'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Stochastic Trajectory Prediction under Unstructured Constraints
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419407" title="Click to go to the Author Index">
             Ma, Hao
            </a>
           </td>
           <td class="r">
            Institute of Automation, Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#201246" title="Click to go to the Author Index">
             Pu, Zhiqiang
            </a>
           </td>
           <td class="r">
            University of Chinese Academy of Sciences; Institute of Automati
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#353284" title="Click to go to the Author Index">
             Wang, Shijie
            </a>
           </td>
           <td class="r">
            Institute of Automation, Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#355345" title="Click to go to the Author Index">
             Liu, Boyin
            </a>
           </td>
           <td class="r">
            University of Chinese Academy of Sciences School of Artificial I
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#248497" title="Click to go to the Author Index">
             Wang, Huimu
            </a>
           </td>
           <td class="r">
            University of Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#393166" title="Click to go to the Author Index">
             Liang, Yanyan
            </a>
           </td>
           <td class="r">
            Macau University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#103045" title="Click to go to the Author Index">
             Yi, Jianqiang
            </a>
           </td>
           <td class="r">
            Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab910" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#constrained_motion_planning" title="Click to go to the Keyword Index">
               Constrained Motion Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#task_and_motion_planning" title="Click to go to the Keyword Index">
               Task and Motion Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Trajectory prediction facilitates effective planning and decision-making, while constrained trajectory prediction integrates regulation into prediction. Recent advances in constrained trajectory prediction focus on structured constraints by constructing optimization objectives. However, handling unstructured constraints is challenging due to the lack of differentiable formal definitions. To address this, we propose a novel method for constrained trajectory prediction using a conditional generative paradigm, named Controllable Trajectory Diffusion (CTD). The key idea is that any trajectory corresponds to a degree of conformity to a constraint. By quantifying this degree and treating it as a condition, a model can implicitly learn to predict trajectories under unstructured constraints. CTD employs a pre-trained scoring model to predict the degree of conformity (i.e., a score), and uses this score as a condition for a conditional diffusion model to generate trajectories. Experimental results demonstrate that CTD achieves high accuracy on the ETH/UCY and SDD benchmarks. Qualitative analysis confirms that CTD ensures adherence to unstructured constraints and can predict trajectories that satisfy combinatorial constraints.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that18_03">
             08:40-08:45, Paper ThAT18.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2878'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Control Barrier Function for Safe Navigation with Online Gaussian Splatting Maps
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#310580" title="Click to go to the Author Index">
             Chen, Timothy
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#310782" title="Click to go to the Author Index">
             Swann, Aiden
            </a>
           </td>
           <td class="r">
            Stanford
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#219764" title="Click to go to the Author Index">
             Yu, Javier
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#251686" title="Click to go to the Author Index">
             Shorinwa, Ola
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#266777" title="Click to go to the Author Index">
             Murai, Riku
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#191308" title="Click to go to the Author Index">
             Kennedy, Monroe
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#106233" title="Click to go to the Author Index">
             Schwager, Mac
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2878" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#collision_avoidance" title="Click to go to the Keyword Index">
               Collision Avoidance
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#robot_safety" title="Click to go to the Keyword Index">
               Robot Safety
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             SAFER-Splat (Simultaneous Action Filtering and Environment Reconstruction) is a real-time, scalable, and minimally invasive action filter, based on control barrier functions, for safe robotic navigation in a detailed map constructed at runtime using Gaussian Splatting (GSplat). We propose a novel Control Barrier Function (CBF) that not only induces safety with respect to all Gaussian primitives in the scene, but when synthesized into a controller, is capable of processing hundreds of thousands of Gaussians while maintaining a minimal memory footprint and operating at 15 Hz during online Splat training. Of the total compute time, a small fraction of it consumes GPU resources, enabling uninterrupted training. The safety layer is minimally invasive, correcting robot actions only when they are unsafe. To showcase the safety filter, we also introduce SplatBridge, an open-source software package built with ROS for real-time GSplat mapping for robots. We demonstrate the safety and robustness of our pipeline first in simulation, where our method is 20-50x faster, safer, and less conservative than competing methods based on neural radiance fields. Further, we demonstrate simultaneous GSplat mapping and safety filtering on a drone hardware platform using only on-board perception. We verify that under teleoperation a human pilot cannot invoke a collision. Our videos and codebase can be found at https://chengine.github.io/safer-splat.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that18_04">
             08:45-08:50, Paper ThAT18.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3360'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Skeleton-Based Topological Planner for Exploration in Complex Unknown Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#331596" title="Click to go to the Author Index">
             Niu, Haochen
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#287970" title="Click to go to the Author Index">
             Ji, Xingwu
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#384416" title="Click to go to the Author Index">
             Zhang, Lantao
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#232758" title="Click to go to the Author Index">
             Wen, Fei
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#233037" title="Click to go to the Author Index">
             Ying, Rendong
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#204907" title="Click to go to the Author Index">
             Liu, Peilin
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3360" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reactive_and_sensor_based_planning" title="Click to go to the Keyword Index">
               Reactive and Sensor-Based Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The capability of autonomous exploration in complex, unknown environments is important in many robotic applications. While recent research on autonomous exploration have achieved much progress, there are still limitations, e.g., existing methods relying on greedy heuristics or optimal path planning are often hindered by repetitive paths and high computational demands.To address such limitations, we propose a novel exploration framework that utilizes the global topology information of observed environment to improve exploration efficiency while reducing computational overhead.Specifically, global information is utilized based on a skeletal topological graph representation of the environment geometry. We first propose an incremental skeleton extraction method based on wavefront propagation, based on which we then design an approach to generate a lightweight topological graph that can effectively capture the environment's structural characteristics. Building upon this, we introduce a finite state machine that leverages the topological structure to efficiently plan coverage paths, which can substantially mitigate the back-and-forth maneuvers (BFMs) problem. Experimental results demonstrate the superiority of our method in comparison with state-of-the-art methods. The source code will be made publicly available at: url{https://github.com/Haochen-Niu/STGPlanner}.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that18_05">
             08:50-08:55, Paper ThAT18.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4195'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Safety-Critical Online Quadrotor Trajectory Planner for Agile Flights in Unknown Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425980" title="Click to go to the Author Index">
             Yuan, Jiazhe
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425981" title="Click to go to the Author Index">
             Cao, Dongcheng
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#388363" title="Click to go to the Author Index">
             Mei, Jiahao
            </a>
           </td>
           <td class="r">
            Zhejiang University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#163844" title="Click to go to the Author Index">
             Chen, Jiming
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#230382" title="Click to go to the Author Index">
             Li, Shuo
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4195" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#collision_avoidance" title="Click to go to the Keyword Index">
               Collision Avoidance
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__mechanics_and_control" title="Click to go to the Keyword Index">
               Aerial Systems: Mechanics and Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Autonomous high-speed flight in unknown, cluttered environments is essential for a variety of quadrotor applications, such as inspection, search, and rescue. In this study, we propose a novel trajectory planner designed to achieve efficient, high-speed, collision-free flights in such environments. The proposed approach begins by generating a safe flight corridor based on the path found by Lazy Theta*, representing the safe regions with polytopic sets. These sets are then used to define discrete-time control barrier function (DCBF), ensuring the quadrotor stays within safe bounds during flight. By selecting one single waypoint ahead of the quadrotor on the path as the next waypoint, the trajectory is optimized by considering both the total flight time and safety constraints. Extensive simulations and real-world experiments have confirmed our method's feasibility, demonstrating its capability for high-speed performance and reliable obstacle avoidance.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that18_06">
             08:55-09:00, Paper ThAT18.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4906'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Anytime Replanning of Robot Coverage Paths for Partially Unknown Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#216151" title="Click to go to the Author Index">
             Ramesh, Megnath
            </a>
           </td>
           <td class="r">
            University of Waterloo
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#168930" title="Click to go to the Author Index">
             Imeson, Frank
            </a>
           </td>
           <td class="r">
            Avidbots
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#120506" title="Click to go to the Author Index">
             Fidan, Baris
            </a>
           </td>
           <td class="r">
            University of Waterloo
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#116598" title="Click to go to the Author Index">
             Smith, Stephen L.
            </a>
           </td>
           <td class="r">
            University of Waterloo
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4906" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#coverage_path_planning" title="Click to go to the Keyword Index">
               Coverage Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reactive_and_sensor_based_planning" title="Click to go to the Keyword Index">
               Reactive and Sensor-Based Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#service_robots" title="Click to go to the Keyword Index">
               Service Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper, we propose a method to replan coverage paths for a robot operating in an environment with initially unknown static obstacles. Existing coverage approaches reduce coverage time by covering along the minimum number of coverage lines (straight-line paths). However, recomputing such paths online can be computationally expensive resulting in robot stoppages that increase coverage time. A naive alternative is greedy detour replanning, i.e., replanning with minimum deviation from the initial path, which is efficient to compute but may result in unnecessary detours. In this work, we propose an anytime coverage replanning approach named OARP-Replan that performs near-optimal replans to an interrupted coverage path within a given time budget. We do this by solving linear relaxations of integer linear programs (ILPs) to identify sections of the interrupted path that can be optimally replanned within the time budget. We validate OARP-Replan in simulation and perform comparisons against a greedy detour replanner and other state-of-the-art coverage planners. We also demonstrate OARP-Replan in experiments using an industrial-level autonomous robot.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that19">
             <b>
              ThAT19
             </b>
            </a>
           </td>
           <td class="r">
            407
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#that19" title="Click to go to the Program at a Glance">
             <b>
              Tactile Sensing 3
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#184771" title="Click to go to the Author Index">
             She, Yu
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that19_01">
             08:30-08:35, Paper ThAT19.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('557'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              LeTac-MPC: Learning Model Predictive Control for Tactile-Reactive Grasping
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#357663" title="Click to go to the Author Index">
             Xu, Zhengtong
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#184771" title="Click to go to the Author Index">
             She, Yu
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab557" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#force_and_tactile_sensing" title="Click to go to the Keyword Index">
               Force and Tactile Sensing
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#grasping" title="Click to go to the Keyword Index">
               Grasping
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#sensor_based_control" title="Click to go to the Keyword Index">
               Sensor-based Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Grasping is a crucial task in robotics, necessitating tactile feedback and reactive grasping adjustments for robust grasping of objects under various conditions and with differing physical properties. In this article, we introduce LeTac-MPC, a learning-based model predictive control (MPC) for tactile-reactive grasping. Our approach enables the gripper to grasp objects with different physical properties on dynamic and force-interactive tasks. We utilize a vision-based tactile sensor, GelSight (Yuan et al. 2017), which is capable of perceiving high-resolution tactile feedback that contains information on the physical properties and states of the grasped object. LeTac-MPC incorporates a differentiable MPC layer designed to model the embeddings extracted by a neural network from tactile feedback. This design facilitates convergent and robust grasping control at a frequency of 25 Hz. We propose a fully automated data collection pipeline and collect a dataset only using standardized blocks with different physical properties. However, our trained controller can generalize to daily objects with different sizes, shapes, materials, and textures. The experimental results demonstrate the effectiveness and robustness of the proposed approach. We compare LeTac-MPC with two purely model-based tactile-reactive controllers (MPC and PD) and open-loop grasping. Our results show that LeTac-MPC has optimal performance in dynamic and force-interactive tasks and optimal generalizability.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that19_02">
             08:35-08:40, Paper ThAT19.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1102'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              The Role of Tactile Sensing for Learning Reach and Grasp
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420484" title="Click to go to the Author Index">
             Zhang, Boya
            </a>
           </td>
           <td class="r">
            University of Tübingen
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#305641" title="Click to go to the Author Index">
             Andrussow, Iris
            </a>
           </td>
           <td class="r">
            Max-Planck-Institute for Intelligent Systems
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#105216" title="Click to go to the Author Index">
             Zell, Andreas
            </a>
           </td>
           <td class="r">
            University of Tübingen
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#118177" title="Click to go to the Author Index">
             Martius, Georg
            </a>
           </td>
           <td class="r">
            Max Planck Institute for Intelligent Systems
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1102" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#force_and_tactile_sensing" title="Click to go to the Keyword Index">
               Force and Tactile Sensing
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Stable and robust robotic grasping is essential for current and future robot applications. In recent works, the use of large datasets and supervised learning has enhanced speed and precision in antipodal grasping. However, these methods struggle with perception and calibration errors due to large planning horizons. To obtain more robust and reactive grasping motions, leveraging reinforcement learning combined with tactile sensing is a promising direction. Yet, there is no systematic evaluation of how the complexity of force-based tactile sensing affects the learning behavior for grasping tasks. This paper compares various tactile and environmental setups using two model-free reinforcement learning approaches for antipodal grasping. Our findings suggest that under imperfect visual perception, various tactile features improve learning outcomes, while complex tactile inputs complicate training.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that19_03">
             08:40-08:45, Paper ThAT19.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1762'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              PolyTouch: A Robust Multi-Modal Tactile Sensor for Contact-Rich Manipulation Using Tactile-Diffusion Policies
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#277205" title="Click to go to the Author Index">
             Zhao, Jialiang
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#176232" title="Click to go to the Author Index">
             Kuppuswamy, Naveen
            </a>
           </td>
           <td class="r">
            Toyota Research Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#132354" title="Click to go to the Author Index">
             Feng, Siyuan
            </a>
           </td>
           <td class="r">
            Toyota Research Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#186298" title="Click to go to the Author Index">
             Burchfiel, Benjamin
            </a>
           </td>
           <td class="r">
            Toyota Research Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#162234" title="Click to go to the Author Index">
             Adelson, Edward
            </a>
           </td>
           <td class="r">
            MIT
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1762" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#force_and_tactile_sensing" title="Click to go to the Keyword Index">
               Force and Tactile Sensing
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#sensorimotor_learning" title="Click to go to the Keyword Index">
               Sensorimotor Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Achieving robust dexterous manipulation in unstructured domestic environments remains a significant challenge in robotics. Even with state-of-the-art robot learning methods, haptic-oblivious control strategies (i.e. those relying only on external vision and/or proprioception) often fall short due to occlusions, visual complexities, and the need for precise contact interaction control. To address these limitations, we introduce PolyTouch, a novel robot finger that integrates camera-based tactile sensing, acoustic sensing, and peripheral visual sensing into a single design that is compact and durable. PolyTouch provides high-resolution tactile feedback across multiple temporal scales, which is essential for efficiently learning complex manipulation tasks. Experiments demonstrate an at least 20-fold increase in lifespan over commercial tactile sensors, with a design that is both easy to manufacture and scalable. We then use this multi-modal tactile feedback along with visuo-proprioceptive observations to synthesize a tactile-diffusion policy from human demonstrations; the resulting contact-aware control policy significantly outperforms haptic-oblivious policies in multiple contact-aware manipulation policies. This paper highlights how effectively integrating multi-modal contact sensing can hasten the development of effective contact-aware manipulation policies, paving the way for more reliable and versatile domestic robots. More information can be found at https://polytouch.alanz.info.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that19_04">
             08:45-08:50, Paper ThAT19.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3416'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Task-Specific Embodied Tactile Sensing for Dexterous Hand
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#367758" title="Click to go to the Author Index">
             Wei, Qi
            </a>
           </td>
           <td class="r">
            Nanchang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#163285" title="Click to go to the Author Index">
             Xiong, Pengwen
            </a>
           </td>
           <td class="r">
            Nanchang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#112790" title="Click to go to the Author Index">
             Song, Aiguo
            </a>
           </td>
           <td class="r">
            Southeast University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#149603" title="Click to go to the Author Index">
             Li, Qiang
            </a>
           </td>
           <td class="r">
            Shenzhen Technology University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3416" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#haptics_and_haptic_interfaces" title="Click to go to the Keyword Index">
               Haptics and Haptic Interfaces
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#embodied_cognitive_science" title="Click to go to the Keyword Index">
               Embodied Cognitive Science
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#behavior_based_systems" title="Click to go to the Keyword Index">
               Behavior-Based Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In order to obtain a good tactile sensing, traditional dexterous hands always enable all the sensing units installed on them all the time, even if just a few sensor units are actually used, which make the tactile sensing system resource-wasting and energy consuming. In order to reduce their complexities by placing the tactile sensing units only at critical locations, this work proposes an embodied tactile dexterous hand (ET-Hand) and a novel multimodal sensor placement framework that learns multiple tasks to generate optimal placement proposal. Furthermore, our ET-Hand can dynamically adjust the perceived tactile sensor positions, types and numbers during robotic manipulation, providing novel tools and methods for investigating the tactile channels and placement scale required for robot exploration. In the object recognition and slip detection tasks, the results show that our proposed method performs close to or even better than traditional sensing way with large-scale placement.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that19_05">
             08:50-08:55, Paper ThAT19.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3597'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              TacDiffusion: Force-Domain Diffusion Policy for Precise Tactile Manipulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#299533" title="Click to go to the Author Index">
             Wu, Yansong
            </a>
           </td>
           <td class="r">
            Technische Universität München
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423841" title="Click to go to the Author Index">
             Chen, Zongxie
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#212176" title="Click to go to the Author Index">
             Wu, Fan
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#296908" title="Click to go to the Author Index">
             Chen, Lingyun
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#370306" title="Click to go to the Author Index">
             Zhang, Liding
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#195330" title="Click to go to the Author Index">
             Bing, Zhenshan
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#292159" title="Click to go to the Author Index">
             Swikir, Abdalla
            </a>
           </td>
           <td class="r">
            Mohamed Bin Zayed University of Artificial Intelligence
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#108317" title="Click to go to the Author Index">
             Haddadin, Sami
            </a>
           </td>
           <td class="r">
            Mohamed Bin Zayed University of Artificial Intelligence
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#105608" title="Click to go to the Author Index">
             Knoll, Alois
            </a>
           </td>
           <td class="r">
            Tech. Univ. Muenchen TUM
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3597" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#dexterous_manipulation" title="Click to go to the Keyword Index">
               Dexterous Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#assembly" title="Click to go to the Keyword Index">
               Assembly
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Assembly is a crucial skill for robots in both modern manufacturing and service robotics. However, mastering transferable insertion skills that can handle a variety of high-precision assembly tasks remains a significant challenge. This paper presents a novel framework that utilizes diffusion models to generate 6D wrench for high-precision tactile robotic insertion tasks. It learns from demonstrations performed on a single task and achieves a zero-shot transfer success rate of 95.7% across various novel high-precision tasks. Our method effectively inherits the self-adaptability demonstrated by our previous work. In this framework, we address the frequency misalignment between the diffusion policy and the real-time control loop with a dynamic system-based filter, significantly improving the task success rate by 9.15%. Furthermore, we provide a practical guideline regarding the trade-off between diffusion models' inference ability and speed.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that19_06">
             08:55-09:00, Paper ThAT19.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3618'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              UpViTaL: Unpaired Visual-Tactile Self-Supervised Representation Learning for Dexterous Robotic Manipulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#371454" title="Click to go to the Author Index">
             Han, Guwen
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#351354" title="Click to go to the Author Index">
             Liu, Qingtao
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#355010" title="Click to go to the Author Index">
             Cui, Yu
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#336308" title="Click to go to the Author Index">
             Chen, Anjun
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#163844" title="Click to go to the Author Index">
             Chen, Jiming
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#195776" title="Click to go to the Author Index">
             Ye, Qi
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3618" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#dexterous_manipulation" title="Click to go to the Keyword Index">
               Dexterous Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#representation_learning" title="Click to go to the Keyword Index">
               Representation Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Visual and tactile pretraining have been extensively studied in dexterous robot manipulation tasks. However, existing methods typically require the simultaneous acquisition of visual and tactile data, making it difficult to utilize low-cost, unpaired visual-tactile datasets. Moreover, these methods often rely on tactile sensors to provide input data for reinforcement learning (RL) during the physical deployment of robotic dexterous hands, which highly increases deployment costs. To address these challenges, we propose UpViTaL, an unpaired visual- tactile self-supervised representation learning method for RL- based robot dexterous manipulation. Specifically, we collect low-cost unpaired visual and tactile datasets for manipulation skill learning using a camera and tactile gloves on three robot manipulation tasks. The temporal tactile self-supervised representation learning module of UpViTaL is used to explore efficient tactile representations from time-series tactile data. In parallel, the visual pretraining module of UpViTaL helps to extract efficient visual representations from visual data. In addition, we fuse unpaired visual-tactile representations through an RL reward mechanism, which does not require robotic dexterous hands tactile sensors for practical deployment. We validate our approach on three dexterous robot manipulation tasks. Experimental results demonstrate that UpViTaL can efficiently learn robot manipulation skills. Compared to existing approaches for visual pretraining, our method significantly improves the success rate by more than 30%.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that20">
             <b>
              ThAT20
             </b>
            </a>
           </td>
           <td class="r">
            408
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#that20" title="Click to go to the Program at a Glance">
             <b>
              Acceptability and Trust
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#157323" title="Click to go to the Author Index">
             de Graaf, Maartje
            </a>
           </td>
           <td class="r">
            Utrecht University
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#147846" title="Click to go to the Author Index">
             Doshi, Prashant
            </a>
           </td>
           <td class="r">
            University of Georgia
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that20_01">
             08:30-08:35, Paper ThAT20.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('120'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Trust-Preserved Human-Robot Shared Autonomy Enabled by Bayesian Relational Event Modeling
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#237610" title="Click to go to the Author Index">
             Li, Yingke
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#107391" title="Click to go to the Author Index">
             Zhang, Fumin
            </a>
           </td>
           <td class="r">
            Hong Kong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab120" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#acceptability_and_trust" title="Click to go to the Keyword Index">
               Acceptability and Trust
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#human_robot_teaming" title="Click to go to the Keyword Index">
               Human-Robot Teaming
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#probability_and_statistical_methods" title="Click to go to the Keyword Index">
               Probability and Statistical Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Shared autonomy functions as a flexible framework that empowers robots to operate across a spectrum of autonomy levels, allowing for efficient task execution with minimal human oversight. However, humans might be intimidated by the autonomous decision-making capabilities of robots due to perceived risks and a lack of trust. This paper proposed a trust-preserved shared autonomy strategy that allows robots to seamlessly adjust their autonomy level, striving to optimize team performance and enhance their acceptance among human collaborators. By enhancing the relational event modeling framework with Bayesian learning techniques, this paper enables dynamic inference of human trust based solely on time-stamped relational events communicated within human-robot teams. Adopting a longitudinal perspective on trust development and calibration in human-robot teams, the proposed trust-preserved shared autonomy strategy warrants robots to actively establish, maintain, and repair human trust, rather than merely passively adapting to it. We validate the effectiveness of the proposed approach through a user study on a human-robot collaborative search and rescue scenario. The objective and subjective evaluations demonstrate its merits on both task execution and user acceptability over the baseline approach that does not consider the preservation of trust.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that20_02">
             08:35-08:40, Paper ThAT20.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('348'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Fostering Trust through Gesture and Voice-Controlled Robot Trajectories in Industrial Human-Robot Collaboration
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#310229" title="Click to go to the Author Index">
             Campagna, Giulio
            </a>
           </td>
           <td class="r">
            Aalborg University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#223101" title="Click to go to the Author Index">
             Frommel, Christoph
            </a>
           </td>
           <td class="r">
            German Aerospace Center
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#277227" title="Click to go to the Author Index">
             Haase, Tobias
            </a>
           </td>
           <td class="r">
            German Aerospace Center (DLR)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#339171" title="Click to go to the Author Index">
             Gottardi, Alberto
            </a>
           </td>
           <td class="r">
            University of Padova
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#152837" title="Click to go to the Author Index">
             Villagrossi, Enrico
            </a>
           </td>
           <td class="r">
            Italian National Research Council
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#108136" title="Click to go to the Author Index">
             Chrysostomou, Dimitrios
            </a>
           </td>
           <td class="r">
            Aalborg University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#158245" title="Click to go to the Author Index">
             Rehm, Matthias
            </a>
           </td>
           <td class="r">
            Aalborg University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab348" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#human_factors_and_human_in_the_loop" title="Click to go to the Keyword Index">
               Human Factors and Human-in-the-Loop
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#acceptability_and_trust" title="Click to go to the Keyword Index">
               Acceptability and Trust
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In the Industry 5.0 era, the focus shifts from basic automation to fostering collaboration between humans and robots. Trust is crucial in this new paradigm, enabling smooth interaction, especially for users with limited robotics knowledge. This study presents a novel framework that uses human hand gestures and voice commands to control robot movements, aiming to enhance trust, reduce cognitive workload, and minimize task execution time—key for efficient manufacturing. In automated systems, swift completion of micromanagement tasks is essential to prevent process disruption. To evaluate this framework, we devised a testbed scenario within an automated carbon fiber transportation and draping process, focusing on a maintenance task as the micromanagement challenge. Participants inspected the gripper, guided the robot along a defined path, and performed maintenance, such as attaching cables. Two conditions were tested: gestures and voice commands versus a smartPAD. The results showed that gestures and voice commands increased trust, lowered cognitive load, and shortened execution times, improving overall manufacturing efficiency.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that20_03">
             08:40-08:45, Paper ThAT20.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2266'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Would You Trust Me Now? a Study on Trust Repair Strategies in Human-Robot Collaboration
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421067" title="Click to go to the Author Index">
             Mélot-Chesnel, Joséphine
            </a>
           </td>
           <td class="r">
            Utrecht University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#157323" title="Click to go to the Author Index">
             de Graaf, Maartje
            </a>
           </td>
           <td class="r">
            Utrecht University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2266" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#acceptability_and_trust" title="Click to go to the Keyword Index">
               Acceptability and Trust
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#design_and_human_factors" title="Click to go to the Keyword Index">
               Design and Human Factors
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             As robots are prone to make errors that undermine trust, effective trust repair strategies are essential in effective human-robot collaboration. Our lab study evaluates three trust repair strategies --apology, denial, and compensation-- following two types of trust violations: competence-based and integrity-based. Consistent with prior research, integrity-based violations reduced moral trust more, while competence-based violations impacted performance trust. Denial caused greater discomfort than apology or compensation across both violation types. Dispositional trust influenced repair strategies effectiveness, particularly in willingness to engage and re-engage. Notably, individuals with high dispositional trust were more receptive to apologies. These findings underscore the need to consider individual trust differences, suggesting robots should assess human trust disposition to effectively foster continued collaboration.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that20_04">
             08:45-08:50, Paper ThAT20.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3643'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Using Physiological Measures, Gaze, and Facial Expressions to Model Human Trust in a Robot Partner
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#331802" title="Click to go to the Author Index">
             Green, Haley N.
            </a>
           </td>
           <td class="r">
            University of Virginia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#172618" title="Click to go to the Author Index">
             Iqbal, Tariq
            </a>
           </td>
           <td class="r">
            University of Virginia
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3643" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#acceptability_and_trust" title="Click to go to the Keyword Index">
               Acceptability and Trust
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             With robots becoming increasingly prevalent in various domains, it has become crucial to equip them with tools to achieve greater fluency in interactions with humans. One of the promising areas for further exploration lies in human trust. A real-time, objective model of human trust could be used to maximize productivity, preserve safety, and mitigate failure. In this work, we attempt to use physiological measures, gaze, and facial expressions to model human trust in a robot partner. We are the first to design an in-person, human-robot supervisory interaction study to create a dedicated trust dataset. Using this dataset, we train machine learning algorithms to identify the objective measures that are most indicative of trust in a robot partner, advancing trust prediction in human-robot interactions. Our findings indicate that a combination of sensor modalities (blood volume pulse, electrodermal activity, skin temperature, and gaze) can enhance the accuracy of detecting human trust in a robot partner. Furthermore, the Extra Trees, Random Forest, and Decision Trees classifiers exhibit consistently better performance in measuring the person's trust in the robot partner. These results lay the groundwork for constructing a real-time trust model for human-robot interaction, which could foster more efficient interactions between humans and robots.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that20_05">
             08:50-08:55, Paper ThAT20.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3762'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Novel Computational Framework of Robot Trust for Human-Robot Teams
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#378240" title="Click to go to the Author Index">
             Nare, Bhavana
            </a>
           </td>
           <td class="r">
            University of Georgia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#378236" title="Click to go to the Author Index">
             Frericks, John Bradley
            </a>
           </td>
           <td class="r">
            University of Georgia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#393592" title="Click to go to the Author Index">
             Challa, Anusha
            </a>
           </td>
           <td class="r">
            University of Georgia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#147846" title="Click to go to the Author Index">
             Doshi, Prashant
            </a>
           </td>
           <td class="r">
            University of Georgia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425009" title="Click to go to the Author Index">
             Johnsen, Kyle
            </a>
           </td>
           <td class="r">
            University of Georgia
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3762" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#acceptability_and_trust" title="Click to go to the Keyword Index">
               Acceptability and Trust
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#human_robot_teaming" title="Click to go to the Keyword Index">
               Human-Robot Teaming
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             When humans collaborate, they form positive or negative experiences with each other. These experiences depend on various factors such as the individual's skills, abilities, and agency. In this paper, we consider human-robot collaborations and present a novel model of an autonomous robot's trust in humans based on the probability of the robot having a positive experience with the human. The model defines a dynamic trust-building process that translates into a computationally-accessible implementation. We hypothesize predictors of a positive experience with human teammates and derive trust in individual humans. As the interactions continue, team members develop an affinity toward each other. The robot's affinity towards humans can be viewed as kinship, and we also investigate how kinship affects trust and distrust. We present an algorithm for how the robot may use kinship-mediated trust in its decision-making, and demonstrate its use in simulated missions truly requiring human-robot collaboration.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that20_06">
             08:55-09:00, Paper ThAT20.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4152'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Modeling Trust Dynamics in Robot-Assisted Delivery: Impact of Trust Repair Strategies
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#400312" title="Click to go to the Author Index">
             Mangalindan, Dong Hae
            </a>
           </td>
           <td class="r">
            Michigan State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421683" title="Click to go to the Author Index">
             Kandikonda, Karthik
            </a>
           </td>
           <td class="r">
            Michigan State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426279" title="Click to go to the Author Index">
             Rovira, Ericka
            </a>
           </td>
           <td class="r">
            United States Military Academy, West Point, NY
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#166469" title="Click to go to the Author Index">
             Srivastava, Vaibhav
            </a>
           </td>
           <td class="r">
            Michigan State University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4152" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#acceptability_and_trust" title="Click to go to the Keyword Index">
               Acceptability and Trust
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#design_and_human_factors" title="Click to go to the Keyword Index">
               Design and Human Factors
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             With increasing efficiency and reliability, autonomous systems are becoming valuable assistants to humans in various tasks. In the context of robot-assisted delivery, we investigate how robot performance and trust repair strategies impact human trust. In this task, humans can choose to either send the robot to deliver autonomously or manually control it while handling a secondary task. The trust repair strategies examined include short and long explanations, apology and promise, and denial. Using data from human participants, we model human behavior using an Input-Output Hidden Markov Model (IOHMM) to capture the dynamics of trust and human action probabilities. Our findings indicate that humans are more likely to deploy the robot autonomously when their trust is high. Furthermore, state transition estimates show that long explanations are the most effective at repairing trust following a failure, while denial is most effective at preventing trust loss. We also demonstrate that the trust estimates generated by our model are isomorphic to self-reported trust values, making them interpretable. This model lays the groundwork for developing optimal policies that facilitate real-time adjustment of human trust in autonomous systems.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that21">
             <b>
              ThAT21
             </b>
            </a>
           </td>
           <td class="r">
            410
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#that21" title="Click to go to the Program at a Glance">
             <b>
              Manipulation Planning and Control 1
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#103228" title="Click to go to the Author Index">
             Kim, Keehoon
            </a>
           </td>
           <td class="r">
            POSTECH, Pohang University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#224814" title="Click to go to the Author Index">
             Pang, Tao
            </a>
           </td>
           <td class="r">
            Boston Dynamics AI Institute
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that21_01">
             08:30-08:35, Paper ThAT21.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('282'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Planning for Tabletop Object Rearrangement
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#289572" title="Click to go to the Author Index">
             Hu, Jiaming
            </a>
           </td>
           <td class="r">
            UC San Diego
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#413580" title="Click to go to the Author Index">
             Szczekulski, Jan
            </a>
           </td>
           <td class="r">
            University of California San Diego
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#415269" title="Click to go to the Author Index">
             Peddabomma, Sudhansh
            </a>
           </td>
           <td class="r">
            University of California San Diego
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#102379" title="Click to go to the Author Index">
             Christensen, Henrik Iskov
            </a>
           </td>
           <td class="r">
            UC San Diego
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab282" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#manipulation_planning" title="Click to go to the Keyword Index">
               Manipulation Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mobile_manipulation" title="Click to go to the Keyword Index">
               Mobile Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Finding an high-quality solution for the tabletop object rearrangement planning is a challenging problem. Compared to determining a goal arrangement, rearrangement planning is challenging due to the dependencies between objects and the buffer capacity available to hold objects. Although ORLA* has proposed an A* based searching strategy with lazy evaluation for the optimal solution, it is not scalable, with the success rate decreasing as the number of objects increases. Additionally, for noisy state representations, ORLA* provides only suboptimal solutions. To overcome these limitations, we propose an enhanced A*-based algorithm that improves state representation and employs incremental goal attempts with lazy evaluation at each iteration. This approach aims to enhance scalability while maintaining solution quality. Our evaluation demonstrates that our algorithm can provide superior solutions compared to ORLA*, in a shorter time, for both stationary and mobile robots.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that21_02">
             08:35-08:40, Paper ThAT21.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2887'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              DA-VIL: Adaptive Dual-Arm Manipulation with Reinforcement Learning and Variable Impedance Control
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#391554" title="Click to go to the Author Index">
             Karim, Md Faizal
            </a>
           </td>
           <td class="r">
            IIIT Hyderabad
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420379" title="Click to go to the Author Index">
             Bollimuntha, Shreya
            </a>
           </td>
           <td class="r">
            International Institute of Information Technology Hyderabad
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420563" title="Click to go to the Author Index">
             Hashmi, Mohammed Saad
            </a>
           </td>
           <td class="r">
            International Institute of Information Technology Hyderabad
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420570" title="Click to go to the Author Index">
             Das, Autrio
            </a>
           </td>
           <td class="r">
            International Institute of Information Technology Hyderabad
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#341638" title="Click to go to the Author Index">
             Singh, Gaurav
            </a>
           </td>
           <td class="r">
            IIIT Hyderabad
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#173768" title="Click to go to the Author Index">
             Sridhar, Srinath
            </a>
           </td>
           <td class="r">
            Brown University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#123110" title="Click to go to the Author Index">
             Singh, Arun Kumar
            </a>
           </td>
           <td class="r">
            University of Tartu
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#216911" title="Click to go to the Author Index">
             Govindan, Nagamanikandan
            </a>
           </td>
           <td class="r">
            IIITDM Kancheepuram
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#102906" title="Click to go to the Author Index">
             Krishna, Madhava
            </a>
           </td>
           <td class="r">
            IIIT Hyderabad
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2887" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#dual_arm_manipulation" title="Click to go to the Keyword Index">
               Dual Arm Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#compliance_and_impedance_control" title="Click to go to the Keyword Index">
               Compliance and Impedance Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Dual-arm manipulation is an area of growing interest in the robotics community. Enabling robots to perform tasks that require the coordinated use of two arms, is essential for complex manipulation tasks such as handling complex large objects, assembling components, and performing human-like interactions. However, achieving effective dual-arm manipulation is challenging due to the need for precise coordination, dynamic adaptability, and the ability to manage interaction forces between the arms and the objects being manipulated. We propose a novel pipeline that combines advantages of policy learning based on environment feedback and gradient based optimization to learn controller gains as well as the control outputs. This allows the robotic system to dynamically modulate its impedance in response to task demands, ensuring stability and dexterity in dual-arm operations. We evaluate our pipeline on a trajectory-tracking task involving a variety of large, complex objects with different masses and geometries. The performance is then compared to three other established methods for controlling dual-arm robots, demonstrating superior results.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that21_03">
             08:40-08:45, Paper ThAT21.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3002'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Goal-Driven Robotic Pushing Manipulation under Uncertain Object Properties
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#379801" title="Click to go to the Author Index">
             Lee, Yongseok
            </a>
           </td>
           <td class="r">
            Pohang University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#103228" title="Click to go to the Author Index">
             Kim, Keehoon
            </a>
           </td>
           <td class="r">
            POSTECH, Pohang University of Science and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3002" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#dexterous_manipulation" title="Click to go to the Keyword Index">
               Dexterous Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#manipulation_planning" title="Click to go to the Keyword Index">
               Manipulation Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#model_learning_for_control" title="Click to go to the Keyword Index">
               Model Learning for Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robotic pushing is one of the intuitive non-prehensile manipulation skills that can handle ungraspable objects without any complex task-specific tools. In this paper, we proposed a goal-driven accurate robotic pushing framework to achieve the robotic pushing tasks in practice that can operate under uncertain object properties. We employed a model predictive path integral (MPPI) as a goal-driven pushing controller building upon our prior work to operate pushing tasks under uncertain object properties. Unlike our prior work, the proposed framework can push the object toward the goal pose without predefined trajectories. The results of the numerical experiments demonstrated that the proposed framework can accomplish the pushing task with a significantly shorter total length, smaller total step, and a higher success rate even though the model parameters are unknown. Moreover, we demonstrated the proposed framework also works well in the real world through real-robot demonstrations.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that21_04">
             08:45-08:50, Paper ThAT21.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3202'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Synthesizing Grasps and Regrasps for Complex Manipulation Tasks
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#269340" title="Click to go to the Author Index">
             Patankar, Aditya
            </a>
           </td>
           <td class="r">
            Stony Brook University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#312636" title="Click to go to the Author Index">
             Mahalingam, Dasharadhan
            </a>
           </td>
           <td class="r">
            Stony Brook University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#107599" title="Click to go to the Author Index">
             Chakraborty, Nilanjan
            </a>
           </td>
           <td class="r">
            Stony Brook University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3202" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#grasping" title="Click to go to the Keyword Index">
               Grasping
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#manipulation_planning" title="Click to go to the Keyword Index">
               Manipulation Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In complex manipulation tasks, e.g., manipulation by pivoting, the motion of the object being manipulated has to satisfy path constraints that can change during the motion. Therefore, a single grasp may not be sufficient for the entire path, and the object may need to be regrasped. Additionally, geometric data for objects from a sensor are usually available in the form of point clouds. The problem of computing grasps and regrasps from point-cloud representation of objects for complex manipulation tasks is a key problem in endowing robots with manipulation capabilities beyond pick-and-place. In this paper, we formalize the problem of grasping/regrasping for complex manipulation tasks with objects represented by (partial) point clouds and present an algorithm to solve it. We represent a complex manipulation task as a sequence of constant screw motions. Using a manipulation plan skeleton as a sequence of constant screw motions, we use a grasp metric to find graspable regions on the object for every constant screw segment. The overlap of the graspable regions for contiguous screws are then used to determine when and how many times the object needs to be regrasped. We present experimental results on point cloud data collected from RGB-D sensors to illustrate our approach.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that21_05">
             08:50-08:55, Paper ThAT21.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3332'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Helping (Human) Hand in Kinematic Structure Estimation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419172" title="Click to go to the Author Index">
             Pfisterer, Adrian
            </a>
           </td>
           <td class="r">
            Technische Universitaet Berlin
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#314099" title="Click to go to the Author Index">
             Li, Xing
            </a>
           </td>
           <td class="r">
            TU Berlin
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#296362" title="Click to go to the Author Index">
             Mengers, Vito
            </a>
           </td>
           <td class="r">
            Technische Universität Berlin
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101767" title="Click to go to the Author Index">
             Brock, Oliver
            </a>
           </td>
           <td class="r">
            Technische Universität Berlin
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3332" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#probability_and_statistical_methods" title="Click to go to the Keyword Index">
               Probability and Statistical Methods
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Visual uncertainties such as occlusions, lack of texture, and noise present significant challenges in obtaining accurate kinematic models for safe robotic manipulation. We introduce a probabilistic real-time approach that leverages the human hand as a prior to mitigate these uncertainties. By tracking the constrained motion of the human hand during manipulation and explicitly modeling uncertainties in visual observations, our method reliably estimates an object’s kinematic model online. We validate our approach on a novel dataset featuring challenging objects that are occluded during manipulation and offer limited articulations for perception. The results demonstrate that by incorporating an appropriate prior and explicitly accounting for uncertainties, our method produces accurate estimates, outperforming two recent baselines by 195% and 140%, respectively. Furthermore, we demonstrate that our approach's estimates are precise enough to allow a robot to manipulate even small objects safely.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that21_06">
             08:55-09:00, Paper ThAT21.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3485'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Is Linear Feedback on Smoothed Dynamics Sufficient for Stabilizing Contact-Rich Plans?
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#217652" title="Click to go to the Author Index">
             Shirai, Yuki
            </a>
           </td>
           <td class="r">
            Mitsubishi Electric Research Laboratories
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#354224" title="Click to go to the Author Index">
             Zhao, Tong
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#266676" title="Click to go to the Author Index">
             Suh, Hyung Ju Terry
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#213769" title="Click to go to the Author Index">
             Zhu, Huaijiang
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#332032" title="Click to go to the Author Index">
             Ni, Xinpei
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#120582" title="Click to go to the Author Index">
             Wang, Jiuguang
            </a>
           </td>
           <td class="r">
            Boston Dynamics AI Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#233896" title="Click to go to the Author Index">
             Simchowitz, Max
            </a>
           </td>
           <td class="r">
            MIT
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#224814" title="Click to go to the Author Index">
             Pang, Tao
            </a>
           </td>
           <td class="r">
            Boston Dynamics AI Institute
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3485" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#dexterous_manipulation" title="Click to go to the Keyword Index">
               Dexterous Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multi_contact_whole_body_motion_planning_and_control" title="Click to go to the Keyword Index">
               Multi-Contact Whole-Body Motion Planning and Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Designing planners and controllers for contact-rich manipulation is extremely challenging as contact violates the smoothness conditions that many gradient-based controller synthesis tools assume. Contact smoothing approximates a non-smooth system with a smooth one, allowing one to use these synthesis tools more effectively. However, applying classical control synthesis methods to smoothed contact dynamics remains relatively under-explored. This paper analyzes the efficacy of linear controller synthesis using differential simulators based on contact smoothing. We introduce natural baselines for leveraging contact smoothing to compute (a) open-loop plans robust to uncertain conditions and/or dynamics, and (b) feedback gains to stabilize around open-loop plans. Using robotic bimanual whole-body manipulation as a testbed, we perform extensive empirical experiments on over 300 trajectories and analyze why LQR seems insufficient for stabilizing contact-rich plans.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that22">
             <b>
              ThAT22
             </b>
            </a>
           </td>
           <td class="r">
            411
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#that22" title="Click to go to the Program at a Glance">
             <b>
              Learning for Manipulation and Navigation
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#180041" title="Click to go to the Author Index">
             Kingston, Zachary
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that22_01">
             08:30-08:35, Paper ThAT22.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1814'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Interaction-Driven Updates: 3D Scene Graph Maintenance During Robot Task Execution
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#348726" title="Click to go to the Author Index">
             Li, Qingfeng
            </a>
           </td>
           <td class="r">
            Beihang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#414340" title="Click to go to the Author Index">
             Zhang, Xinlei
            </a>
           </td>
           <td class="r">
            BUAA
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419081" title="Click to go to the Author Index">
             Chen, Chen
            </a>
           </td>
           <td class="r">
            Hangzhou Innovation Institute of Beihanga University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#289058" title="Click to go to the Author Index">
             Niu, Jianwei
            </a>
           </td>
           <td class="r">
            Beihang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421716" title="Click to go to the Author Index">
             Zhao, Haochen
            </a>
           </td>
           <td class="r">
            BUAA
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1814" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#cognitive_control_architectures" title="Click to go to the Keyword Index">
               Cognitive Control Architectures
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#embodied_cognitive_science" title="Click to go to the Keyword Index">
               Embodied Cognitive Science
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robots powered by large language model (LLM) demonstrate significant research and application potential by effectively interpreting scene information to respond to human commands. However, when robots rely on static scene information during task execution, they face difficulties in adapting to changes in the environment, posing a major challenge for dynamic scene perception. To address the above issues, we propose an innovative interaction-driven approach to enhance robots' ability to perceive dynamic scene information. This approach consists of two contributions, the observation point selection module and the dynamic scene maintenance module. Specifically, first, the robot uses the 3D scene graph (3DSG) containing assets and objects to perceive static scene information through the LLM planner. Next, the best observation point for each asset is obtained through the observation point selection module. Then, with the help of the best observation point, the dynamic scene maintenance module interacts with the asset-related objects to dynamically update all the object node information related to the asset node. This approach enables robots to maintain dynamic scene information, enhancing their adaptability in unpredictable environments and improving task reliability.We evaluated our method using the iTHOR and RoboTHOR datasets within the AI2-THOR simulator and in real-world scenarios. Experimental results demonstrate that our method effectively and accurately maintains robots' perception of dynamic scene information.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that22_02">
             08:35-08:40, Paper ThAT22.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2057'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              ME-PATS: Mutually Enhancing Search-Based Planner and Learning-Based Agent for Tractor-Trailer Systems
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421867" title="Click to go to the Author Index">
             Fan, Ke
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422812" title="Click to go to the Author Index">
             Ren, Zhizhou
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422815" title="Click to go to the Author Index">
             Guo, Ruihan
            </a>
           </td>
           <td class="r">
            Helixon
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423258" title="Click to go to the Author Index">
             Zhang, Jinpeng
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422805" title="Click to go to the Author Index">
             Huang, Zhuo
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423114" title="Click to go to the Author Index">
             Zhou, Yuan
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423271" title="Click to go to the Author Index">
             Zhang, Zufeng
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2057" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#ai_based_methods" title="Click to go to the Keyword Index">
               AI-Based Methods
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#integrated_planning_and_learning" title="Click to go to the Keyword Index">
               Integrated Planning and Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Planning a kinodynamically feasible path for a tractor-trailer vehicle is challenging for both search-based and learning-based methods due to the vehicle’s unique kinematics and complex obstacles. These factors increase the likelihood of infeasible paths and exacerbate long-horizon issues. We introduce ME-PATS: a framework that mutually enhances the search-based planner and the learning-based agent for tractor-trailer systems. The search-based planner provides successful trajectories to help the learning-based agent update its policy, while the agent improves the planner’s efficiency through direct path simulation. Additionally, we propose two approaches to apply our framework to more challenging tasks: designing obstacle-aware networks to enhance the learning-based agent’s capabilities, and combining the planner’s paths with the trained agent’s simulated paths through multi-segment integration. Full details and results are available on our project website at href{https://github.com/FrankSinatral/TTsystems}{https://g ithub.com/FrankSinatral/TTsystems}.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that22_03">
             08:40-08:45, Paper ThAT22.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2536'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Jailbreaking LLM-Controlled Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#314700" title="Click to go to the Author Index">
             Robey, Alexander
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#286020" title="Click to go to the Author Index">
             Ravichandran, Zachary
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104342" title="Click to go to the Author Index">
             Kumar, Vijay
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#246324" title="Click to go to the Author Index">
             Hassani, Hamed
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#102357" title="Click to go to the Author Index">
             Pappas, George J.
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2536" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#robot_safety" title="Click to go to the Keyword Index">
               Robot Safety
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#machine_learning_for_robot_control" title="Click to go to the Keyword Index">
               Machine Learning for Robot Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The recent introduction of large language models (LLMs) has revolutionized the field of robotics by enabling contextual reasoning and intuitive human-robot interaction in domains as varied as manipulation, locomotion, and self-driving vehicles. When viewed as a stand-alone technology, LLMs are known to be vulnerable to jailbreaking attacks, wherein malicious prompters elicit harmful text by bypassing LLM safety guardrails. To assess the risks of deploying LLMs in robotics, in this paper, we introduce RoboPAIR, the first algorithm designed to jailbreak LLM-controlled robots. Unlike existing, textual attacks on LLM chatbots, RoboPAIR elicits harmful physical actions from LLM-controlled robots, a phenomenon we experimentally demonstrate in three scenarios: (i) a white-box setting, wherein the attacker has full access to the NVIDIA Dolphins self-driving LLM, (ii) a gray-box setting, wherein the attacker has partial access to a Clearpath Robotics Jackal UGV robot equipped with a GPT-4o planner, and (iii) a black-box setting, wherein the attacker has only query access to the GPT-3.5-integrated Unitree Robotics Go2 robot dog. In each scenario and across three new datasets of harmful robotic actions, we demonstrate that RoboPAIR, as well as several static baselines, finds jailbreaks quickly and effectively, often achieving 100% attack success rates. Our results reveal, for the first time, that the risks of jailbroken LLMs extend far beyond text generation, given the distinct possibility that jailbroken robots could cause physical damage in the real world. Indeed, our results on the Unitree Go2 represent the first successful jailbreak of a deployed commercial robotic system. Addressing this emerging vulnerability is critical for ensuring the safe deployment of LLMs in robotics. Additional media is available at: https://robopair.org.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that22_04">
             08:45-08:50, Paper ThAT22.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3252'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              CaStL: Constraints As Specifications through LLM Translation for Long-Horizon Task and Motion Planning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425272" title="Click to go to the Author Index">
             Guo, Weihang
            </a>
           </td>
           <td class="r">
            Rice University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#180041" title="Click to go to the Author Index">
             Kingston, Zachary
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#102678" title="Click to go to the Author Index">
             Kavraki, Lydia
            </a>
           </td>
           <td class="r">
            Rice University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3252" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#task_and_motion_planning" title="Click to go to the Keyword Index">
               Task and Motion Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Large Language Models (LLMs) have demonstrated remarkable ability in long-horizon Task and Motion Planning (TAMP) by translating clear and straightforward natural language problems into formal specifications such as the Planning Domain Definition Language (PDDL). However, real-world problems are often ambiguous and involve many complex constraints. In this paper, we introduce Constraints as Specifications through LLMs (CaStL), a framework that identifies constraints such as goal conditions, action ordering, and action blocking from natural language in multiple stages. CaStL translates these constraints into PDDL and Python scripts, which are solved using an custom PDDL solver. Tested across three PDDL domains, CaStL significantly improves constraint handling and planning success rates from natural language specification in complex scenarios.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that22_05">
             08:50-08:55, Paper ThAT22.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3963'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Skills Made to Order: Efficient Acquisition of Robot Cooking Skills Guided by Multiple Forms of Internet Data
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#218173" title="Click to go to the Author Index">
             Verghese, Mrinal
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101843" title="Click to go to the Author Index">
             Atkeson, Christopher
            </a>
           </td>
           <td class="r">
            CMU
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3963" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#big_data_in_robotics_and_automation" title="Click to go to the Keyword Index">
               Big Data in Robotics and Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This study explores the utility of various internet data sources to select among a set of template robot behaviors to perform skills. Learning contact-rich skills involving tool use from internet data sources has typically been challenging due to the lack of physical information such as contact existence, location, areas, and force in this data. Prior works have generally used internet data and foundation models trained on this data to generate low-level robot behavior. We hypothesize that these data and models may be better suited to selecting among a set of basic robot behaviors to perform these contact-rich skills. We explore three methods of template selection: querying large language models, comparing video of robot execution to retrieved human video using features from a pretrained video encoder common in prior work, and performing the same comparison using features from an optic flow encoder trained on internet data. Our results show that LLMs are surprisingly capable template selectors despite their lack of visual information, optical flow encoding significantly outperforms video encoders trained with an order of magnitude more data, and important synergies exist between various forms of internet data for template selection. By exploiting these synergies, we create a template selector using multiple forms of internet data that achieves a 79% success rate on a set of 16 different cooking skills involving tool-use.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that22_06">
             08:55-09:00, Paper ThAT22.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3982'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              LEMMo-Plan: LLM-Enhanced Learning from Mutli-Modal Demonstration for Planning Sequential Contact-Rich Manipulation Tasks
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#338585" title="Click to go to the Author Index">
             Chen, Kejia
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#377737" title="Click to go to the Author Index">
             Shen, Zheng
            </a>
           </td>
           <td class="r">
            TU Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426166" title="Click to go to the Author Index">
             Zhang, Yue
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#296908" title="Click to go to the Author Index">
             Chen, Lingyun
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#212176" title="Click to go to the Author Index">
             Wu, Fan
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#195330" title="Click to go to the Author Index">
             Bing, Zhenshan
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#108317" title="Click to go to the Author Index">
             Haddadin, Sami
            </a>
           </td>
           <td class="r">
            Mohamed Bin Zayed University of Artificial Intelligence
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#105608" title="Click to go to the Author Index">
             Knoll, Alois
            </a>
           </td>
           <td class="r">
            Tech. Univ. Muenchen TUM
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3982" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#dexterous_manipulation" title="Click to go to the Keyword Index">
               Dexterous Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#compliant_assembly" title="Click to go to the Keyword Index">
               Compliant Assembly
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Large Language Models (LLMs) have gained popularity in task planning for long-horizon manipulation tasks. To enhance the validity of LLM-generated plans, visual demonstrations and online videos have been widely employed to guide the planning process. However, for manipulation tasks involving subtle movements but rich contact interactions, visual perception alone may be insufficient for the LLM to fully interpret the demonstration. Additionally, visual data provides limited information on force-related parameters and conditions, which are crucial for effective execution on real robots.
             <p>
              In this paper, we introduce an in-context learning framework that incorporates tactile and force-torque information from human demonstrations to enhance the LLM's ability to generate plans for new task scenarios. We propose a bootstrapped reasoning pipeline that sequentially integrates each modality into a comprehensive task plan. This task plan is then used as a reference for planning in new task configurations. Real-world experiments on two different sequential manipulation tasks demonstrate the effectiveness of our framework in improving LLMs' understanding of multi-modal demonstrations and enhancing the overall planning performance.
             </p>
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that23">
             <b>
              ThAT23
             </b>
            </a>
           </td>
           <td class="r">
            412
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#that23" title="Click to go to the Program at a Glance">
             <b>
              Diffusion Models
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#188794" title="Click to go to the Author Index">
             Romeres, Diego
            </a>
           </td>
           <td class="r">
            Mitsubishi Electric Research Laboratories
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#160727" title="Click to go to the Author Index">
             Gombolay, Matthew
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that23_01">
             08:30-08:35, Paper ThAT23.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('170'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              LTLDoG: Satisfying Temporally-Extended Symbolic Constraints for Safe Diffusion-Based Planning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#316717" title="Click to go to the Author Index">
             Feng, Zeyu
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#313610" title="Click to go to the Author Index">
             Luan, Hao
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#335075" title="Click to go to the Author Index">
             Goyal, Pranav
            </a>
           </td>
           <td class="r">
            University of Michigan - Ann Arbor
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#137443" title="Click to go to the Author Index">
             Soh, Harold
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab170" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#machine_learning_for_robot_control" title="Click to go to the Keyword Index">
               Machine Learning for Robot Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#safety_in_hri" title="Click to go to the Keyword Index">
               Safety in HRI
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Operating effectively in complex environments while complying with specified constraints is crucial for the safe and successful deployment of robots that interact with and operate around people. In this work, we focus on generating long-horizon trajectories that adhere to novel static and temporally-extended constraints/instructions at test time. We propose a data-driven diffusion-based framework, LTLDoG, that modifies the inference steps of the reverse process given an instruction specified using finite linear temporal logic (LTLf). LTLDoG leverages a satisfaction value function on LTLf and guides the sampling steps using its gradient field. This value function can also be trained to generalize to new instructions not observed during training, enabling flexible test-time adaptability. Experiments in robot navigation and manipulation illustrate that the method is able to generate trajectories that satisfy formulae that specify obstacle avoidance and visitation sequences.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that23_02">
             08:35-08:40, Paper ThAT23.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2702'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              DARE: Diffusion Policy for Autonomous Robot Exploration
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#305152" title="Click to go to the Author Index">
             Cao, Yuhong
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423387" title="Click to go to the Author Index">
             Lew, Jeric Jieyi
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#398991" title="Click to go to the Author Index">
             Liang, Jingsong
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#324989" title="Click to go to the Author Index">
             Cheng, Jin
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#180884" title="Click to go to the Author Index">
             Sartoretti, Guillaume Adrien
            </a>
           </td>
           <td class="r">
            National University of Singapore (NUS)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2702" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#view_planning_for_slam" title="Click to go to the Keyword Index">
               View Planning for SLAM
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Autonomous robot exploration requires a robot to efficiently explore and map unknown environments. Compared to conventional methods that can only optimize paths based on the current robot belief, learning-based methods show the potential to achieve improved performance by drawing on past experiences to reason about unknown areas. In this paper, we propose DARE, a novel generative approach that leverages diffusion models trained on expert demonstrations, which can explicitly generate an exploration path through one-time inference. We build DARE upon an attention-based encoder and a diffusion model, and introduce ground truth optimal demonstrations for training to learn better patterns for exploration. The trained planner can reason about the partial belief to recognize the potential structure in unknown areas and consider these areas during path planning. Our experiments demonstrate that DARE achieves on-par performance with both conventional and learning-based state-of-the-art exploration planners, as well as good generalizability in both simulations and real-life scenarios.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that23_03">
             08:40-08:45, Paper ThAT23.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2727'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              NaviDiffusor: Cost-Guided Diffusion Model for Visual Navigation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#376991" title="Click to go to the Author Index">
             Zeng, Yiming
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#362248" title="Click to go to the Author Index">
             Ren, Hao
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425875" title="Click to go to the Author Index">
             Wang, Shuhang
            </a>
           </td>
           <td class="r">
            Sun Yet-Sen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#286873" title="Click to go to the Author Index">
             Huang, Junlong
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#169114" title="Click to go to the Author Index">
             Cheng, Hui
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2727" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#integrated_planning_and_learning" title="Click to go to the Keyword Index">
               Integrated Planning and Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Visual navigation, a fundamental challenge in mobile robotics, demands versatile policies to handle diverse environments. Classical methods leverage geometric solutions to minimize specific costs, offering adaptability to new scenarios but are prone to system errors due to their multi-modular design and reliance on hand-crafted rules. Learning-based methods, while achieving high planning success rates, face difficulties in generalizing to unseen environments beyond the training data and often require extensive training. To address these limitations, we propose a hybrid approach that combines the strengths of learning-based methods and classical approaches for RGB-only visual navigation. Our method first trains a conditional diffusion model on diverse path-RGB observation pairs. During inference, it integrates the gradients of differentiable scene-specific and task-level costs, guiding the diffusion model to generate valid paths that meet the constraints. This approach alleviates the need for retraining, offering a plug-and-play solution. Extensive experiments in both indoor and outdoor settings, across simulated and real-world scenarios, demonstrate zero-shot transfer capability of our approach, achieving higher success rates and fewer collisions compared to baseline methods. Code will be released at url{https://github.com/SYSU-RoboticsLab/NaviD}.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that23_04">
             08:45-08:50, Paper ThAT23.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3111'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              NavigateDiff: Visual Predictors Are Zero-Shot Navigation Assistants
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#371597" title="Click to go to the Author Index">
             Qin, Yiran
            </a>
           </td>
           <td class="r">
            CUHKsz
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420374" title="Click to go to the Author Index">
             Sun, Ao
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong, Shenzhen
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420769" title="Click to go to the Author Index">
             Hong, Yuze
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong，Shenzhen
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420817" title="Click to go to the Author Index">
             Wang, Benyou
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong, Shenzhen
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#371684" title="Click to go to the Author Index">
             Zhang, Ruimao
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong (Shenzhen)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3111" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Navigating unfamiliar environments presents significant challenges for household robots, requiring the ability to recognize and reason about novel decoration and layout. Existing reinforcement learning methods cannot be directly transferred to new environments, as they typically rely on extensive mapping and exploration, leading to time-consuming and inefficient. To address these challenges, we try to transfer the logical knowledge and the generalization ability of pre-trained foundation models to zero-shot navigation. By integrating a large vision-language model with a diffusion network, our approach named NavigateDiff constructs a visual predictor that continuously predicts the agent's potential observations in the next step which can assist robots generate robust actions. Furthermore, to adapt the temporal property of navigation, we introduce temporal historical information to ensure that the predicted image is aligned with the navigation scene. We then carefully designed an information fusion framework that embeds the predicted future frames as guidance into goal-reaching policy to solve downstream image navigation tasks. This approach enhances navigation control and generalization across both simulated and real-world environments. Through extensive experimentation, we demonstrate the robustness and versatility of our method, showcasing its potential to improve the efficiency and effectiveness of robotic navigation in diverse settings. Project Page: https://21styouth.github.io/NavigateDiff/.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that23_05">
             08:50-08:55, Paper ThAT23.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3203'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              FDPP: Fine-Tune Diffusion Policy with Human Preference
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#266418" title="Click to go to the Author Index">
             Chen, Yuxin
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#192415" title="Click to go to the Author Index">
             Jha, Devesh
            </a>
           </td>
           <td class="r">
            Mitsubishi Electric Research Laboratories
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#100071" title="Click to go to the Author Index">
             Tomizuka, Masayoshi
            </a>
           </td>
           <td class="r">
            University of California
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#188794" title="Click to go to the Author Index">
             Romeres, Diego
            </a>
           </td>
           <td class="r">
            Mitsubishi Electric Research Laboratories
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3203" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#sensorimotor_learning" title="Click to go to the Keyword Index">
               Sensorimotor Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Imitation learning from human demonstrations enables robots to perform complex manipulation tasks and has recently witnessed huge success. However, these techniques often struggle to adapt behavior to new preferences or changes in the environment. To address these limitations, we propose Fine-tuning Diffusion Policy with Human Preference (FDPP). FDPP learns a reward function through preference-based learning. This reward is then used to fine-tune the pre-trained policy with reinforcement learning (RL), resulting in alignment of pre-trained policy with new human preferences while still solving the original task. Our experiments across various robotic tasks and preferences demonstrate that FDPP effectively customizes policy behavior without compromising performance. Additionally, we show that incorporating Kullback–Leibler (KL) regularization during fine-tuning prevents over-fitting and helps maintain the competencies of the initial policy.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that23_06">
             08:55-09:00, Paper ThAT23.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3675'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Learning Diverse Robot Striking Motions with Diffusion Models and Kinematically Constrained Gradient Guidance
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#343213" title="Click to go to the Author Index">
             Lee, Kin Man
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#251252" title="Click to go to the Author Index">
             Ye, Sean
            </a>
           </td>
           <td class="r">
            Zoox
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#271076" title="Click to go to the Author Index">
             Xiao, Qingyu
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#288397" title="Click to go to the Author Index">
             Wu, Zixuan
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#269908" title="Click to go to the Author Index">
             Zaidi, Zulfiqar
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#145247" title="Click to go to the Author Index">
             D'Ambrosio, David
            </a>
           </td>
           <td class="r">
            Google
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#278053" title="Click to go to the Author Index">
             Sanketi, Pannag
            </a>
           </td>
           <td class="r">
            Google
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#160727" title="Click to go to the Author Index">
             Gombolay, Matthew
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3675" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#constrained_motion_planning" title="Click to go to the Keyword Index">
               Constrained Motion Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Advances in robot learning have enabled robots to generate skills for a variety of tasks. Yet, robot learning is typically sample inefficient, struggles to learn from data sources exhibiting varied behaviors, and does not naturally incorporate constraints. These properties are critical for fast, agile tasks such as playing table tennis. Modern techniques for learning from demonstration improve sample efficiency and scale to diverse data, but are rarely evaluated on agile tasks. In the case of reinforcement learning, achieving good performance requires training on high-fidelity simulators. To overcome these limitations, we develop a novel diffusion modeling approach that is offline, constraint-guided, and expressive of diverse agile behaviors. The key to our approach is a kinematic constraint gradient guidance (KCGG) technique that computes gradients through both the forward kinematics of the robot arm and the diffusion model to direct the sampling process. KCGG minimizes the cost of violating constraints while simultaneously keeping the sampled trajectory in-distribution of the training data. We demonstrate the effectiveness of our approach for time-critical robotic tasks by evaluating KCGG in two challenging domains: simulated air hockey and real table tennis. In simulated air hockey, we achieved a 25.4% increase in block rate, while in table tennis, we achieved a 17.3% increase in success rate compared to imitation learning baselines.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt1">
             <b>
              ThBT1
             </b>
            </a>
           </td>
           <td class="r">
            302
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thbt1" title="Click to go to the Program at a Glance">
             <b>
              Planning and Simulation
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt1_01">
             09:55-10:00, Paper ThBT1.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('204'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Guarantees on Robot System Performance Using Stochastic Simulation Rollouts
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#291157" title="Click to go to the Author Index">
             Vincent, Joseph
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#371496" title="Click to go to the Author Index">
             Feldman, Aaron
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#106233" title="Click to go to the Author Index">
             Schwager, Mac
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab204" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#probability_and_statistical_methods" title="Click to go to the Keyword Index">
               Probability and Statistical Methods
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#risk_sensitive_control" title="Click to go to the Keyword Index">
               Risk-Sensitive Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We provide finite-sample performance guarantees for control policies executed on stochastic robotic systems. Given an open- or closed-loop policy and a finite set of trajectory rollouts under the policy, we bound the expected value, value-at-risk, and conditional-value-at-risk of the trajectory cost, and the probability of failure in a sparse cost setting. The bounds hold, with user-specified probability, for any policy synthesis technique and can be seen as a post-design safety certification. Generating the bounds only requires sampling simulation rollouts, without assumptions on the distribution or complexity of the underlying stochastic system. We adapt these bounds to also give a constraint satisfaction test to verify safety of the robot system. We provide a thorough analysis of the bound sensitivity to sim-to-real distribution shifts and provide results for constructing robust bounds that can tolerate some specified amount of distribution shift. Furthermore, we extend our method to apply when selecting the best policy from a set of candidates, requiring a multi-hypothesis correction. We show the statistical validity of our bounds in the Ant, Half-cheetah, and Swimmer MuJoCo environments and demonstrate our constraint satisfaction test with the Ant. Finally, using the 20 degree-of-freedom MuJoCo Shadow Hand, we show the necessity of the multi-hypothesis correction.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt1_02">
             10:00-10:05, Paper ThBT1.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('775'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              In-Pipe Navigation Development Environment and a Smooth Path Planning Method on Pipeline Surface
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416635" title="Click to go to the Author Index">
             Liu, Hao
            </a>
           </td>
           <td class="r">
            Independent
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#418430" title="Click to go to the Author Index">
             Li, Xiang
            </a>
           </td>
           <td class="r">
            The Lab for High Technology, Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#418467" title="Click to go to the Author Index">
             Zhang, Xiang
            </a>
           </td>
           <td class="r">
            Qylab
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417585" title="Click to go to the Author Index">
             Liu, Gang
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#310234" title="Click to go to the Author Index">
             Lu, Mingquan
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab775" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#climbing_robots" title="Click to go to the Keyword Index">
               Climbing Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Autonomous in-pipe inspection robots can automatically navigate through complex pipeline networks and detect potential risks from corrosion and defects, demonstrating great potential for replacing costly manual inspections. However, there is no publicly available simulation environment where researchers can validate their in-pipe navigation algorithms as far as we know, and the navigation algorithms on constrained 3D pipe surface which is the critical software component are less discussed. Firstly, this paper proposes an open-source In-Pipe Navigation Development Environment. It contains various pipeline models, a magnetic wheel climbing robot model realized by the adhesion plugin, and baseline algorithms for navigation tasks. Secondly, a novel effective path planning method is introduced. Instead of planning based on surface structures, the proposed method plans based on pipeline axis and maps it into local path using the Frenet-Serret formula, thereby generating smooth, feasible, and efficient paths. Finally, we conduct both qualitative and quantitative experiments in the proposed simulation and real-world environments. The results show the usability of the development environment, also robustness and efficiency of the proposed planning method.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt1_03">
             10:05-10:10, Paper ThBT1.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1038'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Extended Friction Models for the Physics Simulation of Servo Actuators
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#409742" title="Click to go to the Author Index">
             Duclusaud, Marc
            </a>
           </td>
           <td class="r">
            LaBRI - University of Bordeaux
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#163740" title="Click to go to the Author Index">
             Passault, Grégoire
            </a>
           </td>
           <td class="r">
            LaBRI
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#123360" title="Click to go to the Author Index">
             Padois, Vincent
            </a>
           </td>
           <td class="r">
            Inria Bordeaux
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#137536" title="Click to go to the Author Index">
             Ly, Olivier
            </a>
           </td>
           <td class="r">
            LaBRI - Bordeaux University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1038" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#simulation_and_animation" title="Click to go to the Keyword Index">
               Simulation and Animation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#calibration_and_identification" title="Click to go to the Keyword Index">
               Calibration and Identification
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Accurate physical simulation is crucial for the development and validation of control algorithms in robotic systems. Recent works in Reinforcement Learning (RL) take notably advantage of extensive simulations to produce efficient robot control. State-of-the-art servo actuator models generally fail at capturing the complex friction dynamics of these systems. This limits the transferability of simulated behaviors to real-world applications. In this work, we present extended friction models that allow to more accurately simulate servo actuator dynamics. We propose a comprehensive analysis of various friction models, present a method for identifying model parameters using recorded trajectories from a pendulum test bench, and demonstrate how these models can be integrated into physics engines. The proposed friction models are validated on four distinct servo actuators and tested on 2R manipulators, showing significant improvements in accuracy over the standard Coulomb-Viscous model. Our results highlight the importance of considering advanced friction effects in the simulation of servo actuators to enhance the realism and reliability of robotic simulations.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt1_04">
             10:10-10:15, Paper ThBT1.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1757'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Hierarchically Accelerated Coverage Path Planning for Redundant Manipulators
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#214972" title="Click to go to the Author Index">
             Wang, Yeping
            </a>
           </td>
           <td class="r">
            University of Wisconsin-Madison
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#178409" title="Click to go to the Author Index">
             Gleicher, Michael
            </a>
           </td>
           <td class="r">
            University of Wisconsin - Madison
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1757" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#industrial_robots" title="Click to go to the Keyword Index">
               Industrial Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Many robotic applications, such as sanding, polishing, wiping and sensor scanning, require a manipulator to dexterously cover a surface using its end-effector. In this paper, we provide an efficient and effective coverage path planning approach that leverages a manipulator's redundancy and task tolerances to minimize costs in joint space. We formulate the problem as a Generalized Traveling Salesman Problem and hierarchically streamline the graph size. Our strategy is to identify guide paths that roughly cover the surface and accelerate the computation by solving a sequence of smaller problems. We demonstrate the effectiveness of our method through a simulation experiment and an illustrative demonstration using a physical robot.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt1_05">
             10:15-10:20, Paper ThBT1.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1783'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Decentralized Safe and Scalable Multi-Agent Control under Limited Actuation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#232910" title="Click to go to the Author Index">
             Zinage, Vrushabh
            </a>
           </td>
           <td class="r">
            University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421854" title="Click to go to the Author Index">
             Jha, Abhishek
            </a>
           </td>
           <td class="r">
            Delhi Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#238992" title="Click to go to the Author Index">
             Chandra, Rohan
            </a>
           </td>
           <td class="r">
            University of Virginia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#162425" title="Click to go to the Author Index">
             Bakolas, Efstathios
            </a>
           </td>
           <td class="r">
            The University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1783" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#integrated_planning_and_control" title="Click to go to the Keyword Index">
               Integrated Planning and Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             To deploy safe and agile robots in cluttered environments, there is a need to develop fully decentralized controllers that guarantee safety, respect actuation limits, prevent deadlocks, and scale to thousands of agents. Current approaches fall short of meeting all these goals: optimization-based methods ensure safety but lack scalability, while learning-based methods scale but do not guarantee safety. We propose a novel algorithm to achieve safe and scalable control for multiple agents under limited actuation. Specifically, our approach includes: (i) learning a decentralized neural Integral Control Barrier function (neural ICBF) for scalable, input-constrained control, (ii) embedding a lightweight decentralized Model Predictive Control-based Integral Control Barrier Function (MPC-ICBF) into the neural network policy to ensure safety while maintaining scalability, and (iii) introducing a novel method to minimize deadlocks based on gradient-based optimization techniques from machine learning to address local minima in deadlocks. Our numerical simulations show that this approach outperforms state-of-the-art multi-agent control algorithms in terms of safety, input constraint satisfaction, and minimizing deadlocks. Additionally, we demonstrate strong generalization across scenarios with varying agent counts, scaling up to 1000 agents.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt1_06">
             10:20-10:25, Paper ThBT1.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4312'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Multi-Agent Collective Construction of General Modular Structures
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#354677" title="Click to go to the Author Index">
             Kostitsyna, Irina
            </a>
           </td>
           <td class="r">
            KBR at NASA Ames Research Center
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#131537" title="Click to go to the Author Index">
             Cheung, Kenneth C.
            </a>
           </td>
           <td class="r">
            National Aeronautics and Space Administration (NASA)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426600" title="Click to go to the Author Index">
             Gloyd, James
            </a>
           </td>
           <td class="r">
            KBR Inc
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4312" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#parallel_robots" title="Click to go to the Keyword Index">
               Parallel Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#robotics_and_automation_in_construction" title="Click to go to the Keyword Index">
               Robotics and Automation in Construction
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present an algorithmic framework for a multi-robot modular assembly system. Motivated by the prospects of in-space assembly, we focus on the NASA Automated Reconfigurable Mission Adaptive Digital Assembly Systems (ARMADAS) framework, in which multiple types of robots work together in a team to build large structures. Unlike with other multi-robot construction systems, the geometry of structures that ARMADAS robots can build is not limited to the class of histogram shapes. To address the intractability of path planning for a robot system with the exponentially growing number of dimensions, we present a decoupled planning approach, where the assembly and path planning is performed iteratively by one robot team at a time. We present a number of data structures which help us avoid collisions and deadlocks in the resulting robot schedule.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt1_07">
             10:25-10:30, Paper ThBT1.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4965'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              BPMP-Tracker: A Versatile Aerial Target Tracker Using Bernstein Polynomial Motion Primitives
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#274412" title="Click to go to the Author Index">
             Lee, Yunwoo
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#245313" title="Click to go to the Author Index">
             Park, Jungwon
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#238330" title="Click to go to the Author Index">
             Jeon, Boseong
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#337567" title="Click to go to the Author Index">
             Jung, Seungwoo
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#103663" title="Click to go to the Author Index">
             Kim, H. Jin
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4965" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#visual_servoing" title="Click to go to the Keyword Index">
               Visual Servoing
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reactive_and_sensor_based_planning" title="Click to go to the Keyword Index">
               Reactive and Sensor-Based Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This letter presents a versatile trajectory planning pipeline for aerial tracking. The proposed tracker is capable of handling various chasing settings such as complex unstructured environments, crowded dynamic obstacles and multiple-target following. Among the entire pipeline, we focus on developing a predictor for future target motion and a chasing trajectory planner. For rapid computation, we employ the sample-check-select strategy: modules sample a set of candidate movements, check multiple constraints, and then select the best trajectory. Also, we leverage the properties of Bernstein polynomials for quick calculations. The prediction module predicts the trajectories of the targets, which do not overlap with static and dynamic obstacles. Then the trajectory planner outputs a trajectory, ensuring various conditions such as occlusion and collision avoidance, the visibility of all targets within a camera image and dynamical limits. We fully test the proposed tracker in simulations and hardware experiments under challenging scenarios, including dual-target following, environments with dozens of dynamic obstacles and complex indoor and outdoor spaces.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt2">
             <b>
              ThBT2
             </b>
            </a>
           </td>
           <td class="r">
            301
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thbt2" title="Click to go to the Program at a Glance">
             <b>
              SLAM 6
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#107631" title="Click to go to the Author Index">
             Leonard, John
            </a>
           </td>
           <td class="r">
            MIT
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt2_01">
             09:55-10:00, Paper ThBT2.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('55'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              PIN-SLAM: LiDAR SLAM Using a Point-Based Implicit Neural Representation for Achieving Global Map Consistency
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#284469" title="Click to go to the Author Index">
             Pan, Yue
            </a>
           </td>
           <td class="r">
            University of Bonn
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#286743" title="Click to go to the Author Index">
             Zhong, Xingguang
            </a>
           </td>
           <td class="r">
            University of Bonn
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#286641" title="Click to go to the Author Index">
             Wiesmann, Louis
            </a>
           </td>
           <td class="r">
            University of Bonn
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#289132" title="Click to go to the Author Index">
             Posewsky, Thorbjörn
            </a>
           </td>
           <td class="r">
            University of Bonn
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#137262" title="Click to go to the Author Index">
             Behley, Jens
            </a>
           </td>
           <td class="r">
            University of Bonn
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101642" title="Click to go to the Author Index">
             Stachniss, Cyrill
            </a>
           </td>
           <td class="r">
            University of Bonn
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab55" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning" title="Click to go to the Keyword Index">
               Deep Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Accurate and robust localization and mapping are essential components for most autonomous robots. In this paper, we propose a SLAM system for building globally consistent maps, called PIN-SLAM, that is based on an elastic and compact point-based implicit neural map representation. Taking range measurements as input, our approach alternates between incremental learning of the local implicit signed distance field and the pose estimation given the current local map using a correspondence-free, point-to-implicit model registration. Our implicit map is based on sparse optimizable neural points, which are inherently elastic and deformable with the global pose adjustment when closing a loop. Loops are also detected using the neural point features. Extensive experiments validate that PIN-SLAM is robust to various environments and versatile to different range sensors such as LiDAR and RGB-D cameras. PIN-SLAM achieves pose estimation accuracy better or on par with the state-of-the-art LiDAR odometry or SLAM systems and outperforms the recent neural implicit SLAM approaches while maintaining a more consistent, and highly compact implicit map that can be reconstructed as accurate and complete meshes. Finally, thanks to the voxel hashing for efficient neural points indexing and the fast implicit map-based registration without closest point association, PIN-SLAM can run at the sensor frame rate on a moderate GPU.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt2_02">
             10:00-10:05, Paper ThBT2.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('162'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Data-Driven Batch Localization and SLAM Using Koopman Linearization
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#309551" title="Click to go to the Author Index">
             Guo, Zi Cong
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#256420" title="Click to go to the Author Index">
             Dümbgen, Frederike
            </a>
           </td>
           <td class="r">
            ENS, PSL University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#140525" title="Click to go to the Author Index">
             Forbes, James Richard
            </a>
           </td>
           <td class="r">
            McGill University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#115139" title="Click to go to the Author Index">
             Barfoot, Timothy
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab162" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#koopman" title="Click to go to the Keyword Index">
               Koopman
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#model_learning_for_control" title="Click to go to the Keyword Index">
               Model Learning for Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present a framework for model-free batch localization and SLAM. We use lifting functions to map a control-affine system into a high-dimensional space, where both the process model and the measurement model are rendered bilinear. During training, we solve a least-squares problem using groundtruth data to compute the high-dimensional model matrices associated with the lifted system purely from data. At inference time, we solve for the unknown robot trajectory and landmarks through an optimization problem, where constraints are introduced to keep the solution on the manifold of the lifting functions. The problem is efficiently solved using a sequential
             <p>
              quadratic program (SQP), where the complexity of an SQP iteration scales linearly with the number of timesteps. Our algorithms, called Reduced Constrained Koopman Linearization Localization (RCKL-Loc) and Reduced Constrained Koopman Linearization SLAM (RCKL-SLAM), are validated experimentally in simulation and on two datasets: one with an
              <p>
               indoor mobile robot equipped with a laser rangefinder that measures range to cylindrical landmarks, and one on a golf cart equipped with RFID range sensors. We compare RCKL-
              </p>
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt2_03">
             10:05-10:10, Paper ThBT2.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('215'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Certifiably Correct Range-Aided SLAM
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#303569" title="Click to go to the Author Index">
             Papalia, Alan
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#239054" title="Click to go to the Author Index">
             Fishberg, Andrew
            </a>
           </td>
           <td class="r">
            MIT
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#256926" title="Click to go to the Author Index">
             O'Neill, Brendan
            </a>
           </td>
           <td class="r">
            WHOI/MIT
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104610" title="Click to go to the Author Index">
             How, Jonathan
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#150161" title="Click to go to the Author Index">
             Rosen, David
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#107631" title="Click to go to the Author Index">
             Leonard, John
            </a>
           </td>
           <td class="r">
            MIT
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab215" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#range_sensing" title="Click to go to the Keyword Index">
               Range Sensing
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#certifiable_perception" title="Click to go to the Keyword Index">
               Certifiable Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present the first algorithm to efficiently compute certifiably optimal solutions to range-aided simultaneous localization and mapping (RA-SLAM) problems. Robotic navigation systems increasingly incorporate point-to-point ranging sensors, leading to state estimation problems in the form of RA-SLAM. However, the RA-SLAM problem is significantly more difficult to solve than traditional pose-graph SLAM: ranging sensor models introduce non-convexity and single range measurements do not uniquely determine the transform between the involved sensors. As a result, RA-SLAM inference is sensitive to initial estimates yet lacks reliable initialization techniques. Our approach, certifiably correct RA-SLAM (CORA), leverages a novel quadratically constrained quadratic programming (QCQP) formulation of RA-SLAM to relax the RA-SLAM problem to a semidefinite program (SDP). CORA solves the SDP efficiently using the Riemannian Staircase methodology; the SDP solution provides both (i) a lower bound on the RA-SLAM problem's optimal value, and (ii) an approximate solution of the RA-SLAM problem, which can be subsequently refined using local optimization. CORA applies to problems with arbitrary pose-pose, pose-landmark, and ranging measurements and, due to using convex relaxation, is insensitive to initialization. We evaluate CORA on several real-world problems. In contrast to state-of-the-art approaches, CORA is able to obtain high-quality solutions on all problems despite being initialized with random values. Additionally, we study the tightness of the SDP relaxation with respect to important problem parameters: the number of (i) robots, (ii) landmarks, and (iii) range measurements. These experiments demonstrate that the SDP relaxation is often tight and reveal relationships between graph connectivity and the tightness of the SDP relaxation.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt2_04">
             10:10-10:15, Paper ThBT2.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4125'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              DiTer++: Diverse Terrain and Multi-Modal Dataset for Multi-Robot SLAM in Multi-Session Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#345836" title="Click to go to the Author Index">
             Kim, Juwon
            </a>
           </td>
           <td class="r">
            Dept. Electr. and Comput. Eng., Inha University, South Korea
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#334472" title="Click to go to the Author Index">
             Kim, Hogyun
            </a>
           </td>
           <td class="r">
            Inha University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#334481" title="Click to go to the Author Index">
             Jeong, Seokhwan
            </a>
           </td>
           <td class="r">
            Inha University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#219090" title="Click to go to the Author Index">
             Shin, Young-Sik
            </a>
           </td>
           <td class="r">
            KIMM
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#203908" title="Click to go to the Author Index">
             Cho, Younggun
            </a>
           </td>
           <td class="r">
            Inha University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4125" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#data_sets_for_slam" title="Click to go to the Keyword Index">
               Data Sets for SLAM
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We encounter large-scale environments where both structured and unstructured spaces coexist, such as on campuses. In this environment, lighting conditions and dynamic objects change constantly. To tackle the challenges of largescale mapping under such conditions, we introduce DiTer++, a diverse terrain and multi-modal dataset designed for multirobot SLAM in multi-session environments. According to our datasets’ scenarios, Agent-A and Agent-B scan the area designated for efficient large-scale mapping day and night, respectively. Also, we utilize legged robots for terrain-agnostic traversing. To generate the ground-truth of each robot, we first build the survey-grade prior map. Then, we remove the dynamic objects and outliers from the prior map and extract the trajectory through scan-to-map matching. Our dataset and supplement materials are available at https://github.com/sparolab/DiTer-plusplus/.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt2_05">
             10:15-10:20, Paper ThBT2.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4486'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              CELLmap: Enhancing LiDAR SLAM through Elastic and Lightweight Spherical Map Representation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#231281" title="Click to go to the Author Index">
             Duan, Yifan
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#376912" title="Click to go to the Author Index">
             Zhang, Xinran
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#300074" title="Click to go to the Author Index">
             Li, Yao
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#295316" title="Click to go to the Author Index">
             You, Guoliang
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#297333" title="Click to go to the Author Index">
             Chu, Xiaomeng
            </a>
           </td>
           <td class="r">
            University of Scieonce and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#148006" title="Click to go to the Author Index">
             Ji, Jianmin
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#291452" title="Click to go to the Author Index">
             Zhang, Yanyong
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4486" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             SLAM is a fundamental capability of unmanned systems, with LiDAR-based SLAM gaining widespread adop- tion due to its high precision. Current SLAM systems can achieve centimeter-level accuracy within a short period. How- ever, there are still several challenges when dealing with large- scale mapping tasks including significant storage requirements and difficulty of reusing the constructed maps. To address this, we first design an elastic and lightweight map representation called CELLmap, composed of several CELLs, each representing the local map at the corresponding location. Then, we design a general backend including CELL-based bidirectional regis- tration module and loop closure detection module to improve global map consistency. Our experiments have demonstrated that CELLmap can represent the precise geometric structure of large-scale maps of KITTI dataset using only about 60 MB. Additionally, our general backend achieves up to a 26.88% improvement over various LiDAR odometry methods.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt2_06">
             10:20-10:25, Paper ThBT2.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4955'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Benchmark Dataset for Collaborative SLAM in Service Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#407599" title="Click to go to the Author Index">
             Park, Harin
            </a>
           </td>
           <td class="r">
            UNIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#368460" title="Click to go to the Author Index">
             Lee, Inha
            </a>
           </td>
           <td class="r">
            Ulsan National Institute of Science &amp; Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#368650" title="Click to go to the Author Index">
             Kim, Minje
            </a>
           </td>
           <td class="r">
            Ulsan National Institute of Science &amp; Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#407600" title="Click to go to the Author Index">
             Park, Hyungyu
            </a>
           </td>
           <td class="r">
            Ulsan National Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#188483" title="Click to go to the Author Index">
             Joo, Kyungdon
            </a>
           </td>
           <td class="r">
            UNIST
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4955" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#data_sets_for_slam" title="Click to go to the Keyword Index">
               Data Sets for SLAM
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_slam" title="Click to go to the Keyword Index">
               Multi-Robot SLAM
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#data_sets_for_robotic_vision" title="Click to go to the Keyword Index">
               Data Sets for Robotic Vision
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We introduce a new multi-modal collaborative SLAM (C-SLAM) dataset for multiple service robots in various indoor service environments, called C-SLAM dataset in Service Environments (CSE). We use the NVIDIA Isaac Sim to generate data in various indoor service environments with the challenges that may occur in real-world service environments. By using the simulator, we can provide accurate and precisely time-synchronized sensor data, such as stereo RGB, stereo depth, IMU, and ground truth poses. We configure three common indoor service environments (Hospital, Office, and Warehouse), each of which includes various dynamic objects that perform motions suitable to each environment. In addition, we drive the three robots to mimic the actions of real service robots. Through these factors, we generate a realistic C-SLAM dataset for multiple service robots. We demonstrate our CSE dataset by evaluating diverse state-of-the-art single-robot SLAM and multi-robot SLAM methods. Our dataset will be available at https://github.com/vision3d-lab/CSE_Dataset.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt2_07">
             10:25-10:30, Paper ThBT2.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('5020'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Consistent Parallel Estimation Framework for Visual-Inertial SLAM
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#168686" title="Click to go to the Author Index">
             Huai, Zheng
            </a>
           </td>
           <td class="r">
            University of Delaware
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#114431" title="Click to go to the Author Index">
             Huang, Guoquan (Paul)
            </a>
           </td>
           <td class="r">
            University of Delaware
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab5020" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#visual_based_navigation" title="Click to go to the Keyword Index">
               Visual-Based Navigation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#sensor_fusion" title="Click to go to the Keyword Index">
               Sensor Fusion
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#estimation_consistency" title="Click to go to the Keyword Index">
               Estimation Consistency
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this article, we revisit the optimal fusion of visual and inertial information from a monocular camera and an inertial measurement unit and propose a novel parallel visual-inertial simultaneous localization and mapping (SLAM) estimation framework in favor of the multithread computation on a single CPU. We start modeling the SLAM problem with a Bayesian batch estimator, and then split it into two submodules, localization and mapping, of different scales and processing rates, however, can thus run concurrently. The estimation consistency is taken into account in decoupling the two submodules so that when loop closure occurs the localization accuracy can seamlessly benefit from the mapping result via online global optimization, which distinguishes our solution from the others. To this end, we design the corresponding front-end and back-end to consistently solve localization and mapping in parallel, especially the hybrid robocentric and world-centric formulations are used for modeling the respective problems. We also demonstrate the effectiveness of the proposed method using both the synthetic data generated for Monte-Carlo simulations and diverse real datasets acquired in highly-dynamic, long-term, and large-scale SLAM scenarios. Simulation results validate the significantly improved consistency and accuracy by applying our method. Experimental results show the better (competitive at least) performance against a state-of-the-art method, while being capable of processing a huge amount of measurements in building large-scale maps without blocking the high-accuracy real-time localization outputs.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt3">
             <b>
              ThBT3
             </b>
            </a>
           </td>
           <td class="r">
            303
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thbt3" title="Click to go to the Program at a Glance">
             <b>
              Pose Estimation
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#167220" title="Click to go to the Author Index">
             Caverly, Ryan James
            </a>
           </td>
           <td class="r">
            University of Minnesota
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt3_01">
             09:55-10:00, Paper ThBT3.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('137'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Depth-Based Efficient PnP: A Rapid and Accurate Method for Camera Pose Estimation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#403882" title="Click to go to the Author Index">
             Xie, Xinyue
            </a>
           </td>
           <td class="r">
            Dalian University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#404076" title="Click to go to the Author Index">
             Zou, Deyue
            </a>
           </td>
           <td class="r">
            Dalian University of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab137" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a novel approach, DEPnP (Depth-based Efficient PnP), addressing the Perspective-n-Point (PnP) problem crucial in vision-based navigation and SLAM (Simultaneous Localization and Mapping) in robotics and automation, which estimates the pose of a calibrated camera by observing the 2D projections of known 3D points onto the camera image plane. The method employs eight variables to control the depth of control points and orientation of camera, formulating camera pose estimation as an optimization task. By optimizing these variables utilizing mean-subtracted rotation equations, rapid and accurate camera pose estimation is achieved. Notably, the careful selection of variables and objective function simplifies the computation of the Jacobian matrix, ensuring computational efficiency. DEPnP demonstrates robustness against noise and inlier disturbances, consistently delivering accurate camera pose estimation. Experimental evaluations validate the effectiveness and accuracy of DEPnP, positioning it as a competitive solution for real-time applications requiring precise camera pose estimation in robotics and automation.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt3_02">
             10:00-10:05, Paper ThBT3.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1176'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Kalman-Filter-Based Pose Estimation of Cable-Driven Parallel Robots Using Cable-Length Measurements with Colored Noise
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#285620" title="Click to go to the Author Index">
             Nguyen, Vinh
            </a>
           </td>
           <td class="r">
            University of Minnesota
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#167220" title="Click to go to the Author Index">
             Caverly, Ryan James
            </a>
           </td>
           <td class="r">
            University of Minnesota
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1176" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#parallel_robots" title="Click to go to the Keyword Index">
               Parallel Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#kinematics" title="Click to go to the Keyword Index">
               Kinematics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#sensor_fusion" title="Click to go to the Keyword Index">
               Sensor Fusion
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper introduces a cable-length-based extended Kalman filter (L-EKF) framework to estimate the end-effector pose of a cable-driven parallel robot (CDPR). The L-EKF fuses end-effector accelerometer and rate gyroscope measurements with cable-length measurements. The main contribution compared to prior CDPR pose estimation EKF methods is that the L-EKF framework does not require an iterative forward kinematics algorithm to be solved each time step, reducing the computation time of the EKF. Moreover, the L-EKF is amenable to the inclusion of colored measurement noise, which provides a more realistic quantification of the kinematic uncertainty present in the cable-length measurements. Experimental results demonstrate that the L-EKF is computationally more efficient than previous forward-kinematics-based EKF methods, as well as the moderate improvement in pose estimation provided by the colored noise model.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt3_03">
             10:05-10:10, Paper ThBT3.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1553'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Unified End-To-End Network for Category-Level and Instance-Level Object Pose Estimation from RGB Images
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#372842" title="Click to go to the Author Index">
             Ren, Jiale
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#113062" title="Click to go to the Author Index">
             Liu, Hong
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#376578" title="Click to go to the Author Index">
             Liu, Jinfu
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#410015" title="Click to go to the Author Index">
             Jiang, Peifeng
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1553" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Accurately estimating the 6-DoF pose of objects is a fundamental challenge in computer vision and robotics. While category-level pose estimation based on RGBD data has achieved good performance in recent years, estimating poses solely from RGB images remains a significant challenge. Existing RGB-based category-level methods primarily focus on recovering object point clouds from RGB images, and pose prediction is not performed end-to-end by a network. This paper presents a Category-level and Instance-level Pose Estimation Network (CIPE), which models pose estimation as a set prediction problem and enables direct pose regression from RGB images. To further enhance the network's ability to learn object poses, first, a novel learnable rotation representation that redefines rotation learning within Euclidean space is introduced to facilitate rotation regression. Additionally, we propose a prior-query fusion strategy that utilizes a pre-trained point cloud feature extraction network to integrate categorical object features with bounding boxes, thereby improving the incorporation of category information. Experimental results demonstrate that CIPE significantly outperforms existing RGB-based methods on both category-level and instance-level datasets. The code is available at https://github.com/jialeren/CIPE.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt3_04">
             10:10-10:15, Paper ThBT3.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2275'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              MonoLDP: LED Assisted Indoor Mobile Bot Monocular Depth Prediction and Pose Estimation System
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#415542" title="Click to go to the Author Index">
             Liang, Chenxin
            </a>
           </td>
           <td class="r">
            Tsinghua Unviersity
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423888" title="Click to go to the Author Index">
             Wang, Jingyang
            </a>
           </td>
           <td class="r">
            Shenzhen International Graduate School, Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#308276" title="Click to go to the Author Index">
             Li, Shoujie
            </a>
           </td>
           <td class="r">
            Tsinghua Shenzhen International Graduate School
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#379898" title="Click to go to the Author Index">
             Sou, Kit-Wa
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423877" title="Click to go to the Author Index">
             Luo, Xinyu
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#318552" title="Click to go to the Author Index">
             Ding, Wenbo
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2275" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Multi-robot clusters are increasingly deployed in indoor environments, where effective communication and 3D perception are critical for coordinated operations. Monocular cameras, known for their lightweight design, cost-effectiveness, and versatility, present a promising solution for these tasks. However, relying solely on monocular cameras for comprehensive perception and communication presents significant challenges. To address this, we introduce MonoLDP, a novel system that leverages monocular cameras for depth estimation, mutual pose estimation, and visible light communication in indoor environments, providing an integrated framework to overcome these limitations. MonoLDP features a two-stage network: (1) a depth estimation module that infers depth from monocular images, and (2) a depth-guided 3D object recognition network for agent-relative localization and pose estimation. We created a custom dataset to validate the accuracy of MonoLDP. On our indoor dataset, MonoLDP outperforms the baseline by 43.39% in 3D detection and 42.39% in bird’s-eye view detection, with an average localization error of 0.104m and an orientation error of 1.66 degrees. Moreover, the depth estimation network demonstrates excellent performance on the NYU v2 dataset. Additionally, the system achieves a communication rate of 1.2 Kbps with a bit error rate below 10^(-2) at a distance of up to 4 meters using LED arrays. Our code will be released at https://github.com/RavenLiang1005/MonoLDP.git.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt3_05">
             10:15-10:20, Paper ThBT3.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3500'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              LCSPose: Efficient, Accurate and Scalable Markerless 6-DoF Pose Estimation of a Quay Crane Spreader Based on LiDAR and Camera
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#342006" title="Click to go to the Author Index">
             Zhou, Yichen
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#217946" title="Click to go to the Author Index">
             Zhang, Jun
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#253230" title="Click to go to the Author Index">
             Peng, Guohao
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#384047" title="Click to go to the Author Index">
             Yun, Yanpu
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#311041" title="Click to go to the Author Index">
             Liu, Yiyao
            </a>
           </td>
           <td class="r">
            NANYANG Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#240150" title="Click to go to the Author Index">
             Wang, Yuanzhe
            </a>
           </td>
           <td class="r">
            Shandong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#100565" title="Click to go to the Author Index">
             Wang, Danwei
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3500" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#field_robots" title="Click to go to the Keyword Index">
               Field Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#industrial_robots" title="Click to go to the Keyword Index">
               Industrial Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Accurate Six Degrees of Freedom (6-DoF) pose estimation of Ship-To-Shore (STS) quay crane spreaders is crucial for ensuring safe and efficient container handling in port automation. However, existing pose estimation techniques face significant challenges, as camera-based systems either rely on markers, which are prone to damage, or struggle with depth estimation inaccuracies. Additionally, 3D sensor-based approaches, particularly point cloud registration (PCR), face challenges such as initial pose errors, high-latency inference, and difficulties in object identification based purely on geometric features. To address these limitations, we propose LCSPose, a LiDAR-camera fusion-based 6-DoF pose estimation method that is marker-free, accurate, efficient, and scalable. Our approach integrates three key modules: (1) a semantic-geometric segmentation module for spreader segmentation and outlier removal, (2) a spatial consistency template sampling module based on Spatial Consistency Score (SC-Score) for reliable template selection across varying distances, and (3) a multi-view coarse-to-fine pose refinement module which incorporates multi-view PCA alignment for robust initial posture prior estimation and iterative pose refinement strategy for long-range registration. Our method demonstrates a 60% improvement in registration recall over state-of-the-art (SOTA) PCR methods, achieving up to 6 cm in translation error and 0.19 degrees in rotation error, while maintaining real-time processing at 20Hz.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt3_06">
             10:20-10:25, Paper ThBT3.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3954'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              ZeroBP: Learning Position-Aware Correspondence for Zero-Shot 6D Pose Estimation in Bin-Picking
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#411474" title="Click to go to the Author Index">
             Chen, Jianqiu
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416783" title="Click to go to the Author Index">
             Zhou, Zikun
            </a>
           </td>
           <td class="r">
            Pengcheng Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417526" title="Click to go to the Author Index">
             Li, Xin
            </a>
           </td>
           <td class="r">
            Pengcheng Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#212954" title="Click to go to the Author Index">
             Bao, Tianpeng
            </a>
           </td>
           <td class="r">
            Guangzhou Medical University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421582" title="Click to go to the Author Index">
             Zheng, Ye
            </a>
           </td>
           <td class="r">
            JD Logistics
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416914" title="Click to go to the Author Index">
             He, Zhenyu
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3954" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Bin-picking is a practical and challenging robotic manipulation task, where accurate 6D pose estimation plays a pivotal role. The workpieces in bin-picking are typically textureless and randomly stacked in a bin, which poses a significant challenge to 6D pose estimation. Existing solutions are typically learning-based methods, which require object-specific training. Their efficiency of practical deployment for novel workpieces is highly limited by data collection and model retraining. Zero-shot 6D pose estimation is a potential approach to address the issue of deployment efficiency. Nevertheless, existing zero-shot 6D pose estimation methods are designed to leverage feature matching to establish point-to-point correspondences for pose estimation, which is less effective for workpieces with textureless appearances and ambiguous local regions. In this paper, we propose ZeroBP, a zero-shot pose estimation framework designed specifically for the bin-picking task. ZeroBP learns Position-Aware Correspondence (PAC) between the scene instance and its CAD model, leveraging both local features and global positions to resolve the mismatch issue caused by ambiguous regions with similar shapes and appearances. Extensive experiments on the ROBI dataset demonstrate that ZeroBP outperforms state-of-the-art zero-shot pose estimation methods, achieving an improvement of 9.1% in average recall of correct poses.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt3_07">
             10:25-10:30, Paper ThBT3.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4766'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Virtual Frame Rotation: A Novel Two-Stage Pose Estimation Scheme of Permanent Magnet Marker for Medical Applications
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#308072" title="Click to go to the Author Index">
             Park, Jiho
            </a>
           </td>
           <td class="r">
            Gwangju Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#319889" title="Click to go to the Author Index">
             Lim, Buyong
            </a>
           </td>
           <td class="r">
            GIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#106386" title="Click to go to the Author Index">
             Yoon, Jungwon
            </a>
           </td>
           <td class="r">
            Gwangju Institutue of Science and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4766" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#micro_nano_robots" title="Click to go to the Keyword Index">
               Micro/Nano Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Permanent magnetic marker (PMM) has the potential to broaden the scope of medical robots by facilitating the localization of target points even in environments where vision-based methods cannot operate. However, conventional approaches rely on the accuracy of the modeling equations and are not adaptable to changes in the magnet's properties, which can occur due to factors like non-uniformity in the marker material or temperature fluctuations within the PMM. These constraints make it challenging to apply the PMM across diverse medical techniques. In this work, we introduce a novel two-stage PMM localization scheme, called Virtual Frame Rotation (VFR), designed to address this issue. VFR employs an approach that virtually rotates the observation frame of the hall sensors' output vector and checks the symmetry of the magnetic field in the rotated frame. This approach allows for robust pose estimation of the condition with variance in magnetic properties, as verified by comparing its localization performance with the conventional approach in the simulation and the real-world environments with temperature variance conditions. Based on these characteristics, VFR can expand the scope of medical applications that involve changes in the properties of magnetic markers, such as the in-body localization of magnetic macro particles for hyperthermia treatment.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt4">
             <b>
              ThBT4
             </b>
            </a>
           </td>
           <td class="r">
            304
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thbt4" title="Click to go to the Program at a Glance">
             <b>
              Bioinspiration and Biomimetics 1
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#150160" title="Click to go to the Author Index">
             Ramezani, Alireza
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt4_01">
             09:55-10:00, Paper ThBT4.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('67'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Back-Stepping Experience Replay with Application to Model-Free Reinforcement Learning for a Soft Snake Robot
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#271072" title="Click to go to the Author Index">
             Qi, Xinda
            </a>
           </td>
           <td class="r">
            Michigan State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#330220" title="Click to go to the Author Index">
             Chen, Dong
            </a>
           </td>
           <td class="r">
            Mississippi State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#249592" title="Click to go to the Author Index">
             Li, Zhaojian
            </a>
           </td>
           <td class="r">
            Michigan State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#100168" title="Click to go to the Author Index">
             Tan, Xiaobo
            </a>
           </td>
           <td class="r">
            Michigan State University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab67" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#modeling__control__and_learning_for_soft_robots" title="Click to go to the Keyword Index">
               Modeling, Control, and Learning for Soft Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#biologically_inspired_robots" title="Click to go to the Keyword Index">
               Biologically-Inspired Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper, we propose a novel technique, Back-stepping Experience Replay (BER), that is compatible with arbitrary off-policy reinforcement learning (RL) algorithms. BER aims to enhance learning efficiency in systems with approximate reversibility, reducing the need for complex reward shaping. The method constructs reversed trajectories using back-stepping transitions to reach random or fixed targets. Interpretable as a bi-directional approach, BER addresses inaccuracies in back-stepping transitions through a purification of the replay experience during learning. Given the intricate nature of soft robots and their complex interactions with environments, we present an application of BER in a model-free RL approach for the locomotion and navigation of a soft snake robot, which is capable of serpentine motion enabled by anisotropic friction between the body and ground. In addition, a dynamic simulator is developed to assess the effectiveness and efficiency of the BER algorithm, in which the robot demonstrates successful learning (reaching a 100% success rate) and adeptly reaches random targets, achieving an average speed 48% faster than that of the best baseline approach.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt4_02">
             10:00-10:05, Paper ThBT4.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2715'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Continuous Convolution for Automated Measurement of Sperm Flagella
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#378913" title="Click to go to the Author Index">
             Jin, Yufei
            </a>
           </td>
           <td class="r">
            The Chinese Univiersity of Hong Kong(shenzhen)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#377948" title="Click to go to the Author Index">
             Yang, Han
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong, Shenzhen
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#266836" title="Click to go to the Author Index">
             Chen, Wenyuan
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419994" title="Click to go to the Author Index">
             Wang, Xinrui
            </a>
           </td>
           <td class="r">
            The Chinese University of Hongkong (Shenzhen)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101247" title="Click to go to the Author Index">
             Sun, Yu
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#181349" title="Click to go to the Author Index">
             Zhang, Zhuoran
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong, Shenzhen
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2715" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#automation_at_micro_nano_scales" title="Click to go to the Keyword Index">
               Automation at Micro-Nano Scales
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Quantifying sperm flagellar beating behavior (e.g., beating amplitude, frequency, and wavelength) plays a crucial role in biological research, clinical diagnostics, and the design of sperm-inspired microrobots. However, existing computational methods struggle to accurately and efficiently analyze the highly dynamic, complex, and fine structures of sperm flagella, especially when portions of the flagellum become invisible due to three-dimensional out-of-focus beating. This paper proposes an automated high-throughput tool for quantitative analysis of sperm flagellar beating. The core innovation is continuous convolution (CConv), which adaptively captures the irregular, time-varying patterns of sperm flagella while ensuring continuity in segmentation outputs, even in the presence of locally invisible regions caused by out-of-focus motion. CConv can be integrated into various neural network architectures as a plug-and-play module. Extensive experiments demonstrate that integrating CConv consistently improves the accuracy and continuity of flagella segmentation across different networks. Furthermore, utilizing a curvature-based approach, we quantified key flagellar beating parameters, including length, amplitude, frequency, and wavelength. Applying the high-throughput tool on 1200 sperm revealed that sperm from fertile donors had significantly higher flagellar beating frequency than sperm from infertile patients. The proposed automated tool unlocks high-throughput, quantitative analysis of sperm flagellar beating, showing the potential for applications in reproductive biology and engineering research.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt4_03">
             10:05-10:10, Paper ThBT4.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3075'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Adaptive Concertina Locomotion of a Robotic Snake through Narrow Uncertain Channels
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#411482" title="Click to go to the Author Index">
             Koley, Jit
            </a>
           </td>
           <td class="r">
            Indian Institute of Technology Bombay
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421979" title="Click to go to the Author Index">
             Sharma, Devashish
            </a>
           </td>
           <td class="r">
            Hindustan Institute of Technology and Science, Chennai
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#181679" title="Click to go to the Author Index">
             Chakraborty, Debraj
            </a>
           </td>
           <td class="r">
            Indian Institute of Technology Bombay
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#339626" title="Click to go to the Author Index">
             K. Pillai, Harish
            </a>
           </td>
           <td class="r">
            Indian Institute of Technology Bombay
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3075" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#redundant_robots" title="Click to go to the Keyword Index">
               Redundant Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#search_and_rescue_robots" title="Click to go to the Keyword Index">
               Search and Rescue Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#actuation_and_joint_mechanisms" title="Click to go to the Keyword Index">
               Actuation and Joint Mechanisms
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The problem of mimicking the concertina locomotion mode of biological snakes through narrow channels of uncertain width, using a multi-link planar serpenoid robot, is considered. A novel algorithm for generating a reference trajectory that accurately reproduces this natural gait pattern is proposed and analysed for straight channels. A modification of this algorithm leverages feedback from the joints’ current and angular velocities to dynamically adjust the robot’s movements within channels of unknown and varying widths. Experiments through rugged artificial channels of varying width show remarkable ability of the programmed snake robot to negotiate such terrains with agility and reasonable speed.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt4_04">
             10:10-10:15, Paper ThBT4.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3394'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Bio-Inspired Distributed Neural Locomotion Controller (D-NLC) for Robust Locomotion and Emergent Behaviors
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#380962" title="Click to go to the Author Index">
             Zhang, Zhikai
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422328" title="Click to go to the Author Index">
             Guo, Siqi
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421772" title="Click to go to the Author Index">
             Kou, Henry
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422296" title="Click to go to the Author Index">
             Shikhare, Ishayu
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104011" title="Click to go to the Author Index">
             Choset, Howie
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#171204" title="Click to go to the Author Index">
             Li, Lu
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3394" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#biologically_inspired_robots" title="Click to go to the Keyword Index">
               Biologically-Inspired Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#cellular_and_modular_robots" title="Click to go to the Keyword Index">
               Cellular and Modular Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#neurorobotics" title="Click to go to the Keyword Index">
               Neurorobotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             With relatively fewer neurons than more complex life forms, insects are still capable of producing astonishing locomotive behaviors, such as traversing diverse habitats and making rapid gait adaptations after extreme injury or autotomy. Biologists attribute this to a chain of segmental neuron clusters (ganglia) within insect nervous systems, which act as distributed, self-organizing sensorimotor control units. Inspired by the neural structure of the Carausius morosus, the common stick insect, this research introduces the Distributed Neural Locomotion Controller (D-NLC), a modular control framework utilizing local proprioceptive feedback to modulate joint-level Central Pattern Generator (CPG) signals to produce emergent locomotive behaviors. We implemented this framework using a modular legged robot with distributed joint-level embedded computing units and assessed its performance and behavior under various experimental settings. Based on real-world experiments, we observe an overall 31.3% average increase in curvilinear motion performance under external (terrain) and internal (amputation) disturbances compared to a centralized predefined gait controller. This difference is statistically significant (P&lt;&lt;0.05) for larger perturbations but not for single-leg amputations. Experiments with perturbation-induced leg stance duration and leg-phase-difference analysis further validated our hypothesis regarding D-NLC's role in the robust perceptive locomotion and self-emergent gait adaptation against complex unforeseen perturbations. This proposed control framework does not require any numerical optimization or weight training processes, which are time-consuming and computationally expensive. To the best of our knowledge, this framework is the first bio-inspired neural controller deployed on a distributed embedded system.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt4_05">
             10:15-10:20, Paper ThBT4.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3438'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Reduced-Order Model-Based Gait Generation for Snake Robot Locomotion Using NMPC
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#332346" title="Click to go to the Author Index">
             Salagame, Adarsh
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#184688" title="Click to go to the Author Index">
             Sihite, Eric
            </a>
           </td>
           <td class="r">
            California Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#195559" title="Click to go to the Author Index">
             Ramezani, Milad
            </a>
           </td>
           <td class="r">
            CSIRO
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#150160" title="Click to go to the Author Index">
             Ramezani, Alireza
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3438" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#biologically_inspired_robots" title="Click to go to the Keyword Index">
               Biologically-Inspired Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_control" title="Click to go to the Keyword Index">
               Motion Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents an optimization-based motion planning methodology for snake robots operating in constrained environments. By using a reduced-order model, the proposed approach simplifies the planning process, enabling the optimizer to autonomously generate gaits while constraining the robot’s footprint within tight spaces. The method is validated through high-fidelity simulations that accurately model contact dynamics and the robot’s motion. Key locomotion strategies are identified and further demonstrated through hardware experiments, including successful navigation through narrow corridors.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt4_06">
             10:20-10:25, Paper ThBT4.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4440'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              AquaMILR+: Design of an Untethered Limbless Robot for Complex Aquatic Terrain Navigation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#379438" title="Click to go to the Author Index">
             Fernandez, Matthew
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#274915" title="Click to go to the Author Index">
             Wang, Tianyu
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426787" title="Click to go to the Author Index">
             Tunnicliffe, Galen
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426785" title="Click to go to the Author Index">
             Dortilus, Donoven
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426786" title="Click to go to the Author Index">
             Gunnarson, Peter
            </a>
           </td>
           <td class="r">
            California Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#345585" title="Click to go to the Author Index">
             Dabiri, John
            </a>
           </td>
           <td class="r">
            California Insititute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#142549" title="Click to go to the Author Index">
             Goldman, Daniel
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4440" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#biologically_inspired_robots" title="Click to go to the Keyword Index">
               Biologically-Inspired Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#redundant_robots" title="Click to go to the Keyword Index">
               Redundant Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#search_and_rescue_robots" title="Click to go to the Keyword Index">
               Search and Rescue Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents AquaMILR+, an untethered limbless robot designed for agile navigation in complex aquatic environments. The robot features a bilateral actuation mechanism that models musculoskeletal actuation in many anguilliform swimming organisms which propagates a moving wave from head to tail allowing open fluid undulatory swimming. This actuation mechanism employs mechanical intelligence, enhancing the robot's maneuverability when interacting with obstacles. AquaMILR+ also includes a compact depth control system inspired by the swim bladder and lung structures of eels and sea snakes. The mechanism, driven by a syringe and telescoping leadscrew, enables depth and pitch control -- capabilities that are difficult for most anguilliform swimming robots to achieve. Additional structures, such as fins and a tail, further improve stability and propulsion efficiency. Our tests in both open water and indoor 2D and 3D heterogeneous aquatic environments highlight AquaMILR+'s capabilities and suggest a promising system for complex underwater tasks such as search and rescue and deep-sea exploration.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt4_07">
             10:25-10:30, Paper ThBT4.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4604'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Traversing between Two Planes Using Obstacle-Aided Locomotion of a Snake Robot
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#407243" title="Click to go to the Author Index">
             Yoshida, Yuto
            </a>
           </td>
           <td class="r">
            The University of Electro-Communications
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#380515" title="Click to go to the Author Index">
             Chin, Ching Wen
            </a>
           </td>
           <td class="r">
            The University of Electro-Communications
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#103138" title="Click to go to the Author Index">
             Tanaka, Motoyasu
            </a>
           </td>
           <td class="r">
            The Univ. of Electro-Communications
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4604" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#field_robots" title="Click to go to the Keyword Index">
               Field Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#search_and_rescue_robots" title="Click to go to the Keyword Index">
               Search and Rescue Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#biologically_inspired_robots" title="Click to go to the Keyword Index">
               Biologically-Inspired Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             ペーパーでは、2種類の接続パーツを提案します。 非車輪付きヘビロボットの障害物支援移動 2つの平面。 1つの方法は、ロボットの頭を垂直に持ち上げることですが、 他の方法は、転倒を避けるために障害物の周りに巻き付くことです
             <p>
              より高い平面に移動するとき。オペレーターは高さを調整でき、 接続部分のパラメータを変更して有効にする方法。 ロボットは未知の2つの非平行平面上を移動する。を確認しました。 実験による私たちの方法の有効性。
             </p>
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt5">
             <b>
              ThBT5
             </b>
            </a>
           </td>
           <td class="r">
            305
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thbt5" title="Click to go to the Program at a Glance">
             <b>
              Model Predictive Control for Legged Robots 1
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#158317" title="Click to go to the Author Index">
             Zhao, Ye
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt5_01">
             09:55-10:00, Paper ThBT5.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1950'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Adapting Gait Frequency for Posture-Regulating Humanoid Push-Recovery Via Hierarchical Model Predictive Control
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#312874" title="Click to go to the Author Index">
             Li, Junheng
            </a>
           </td>
           <td class="r">
            University of Southern California
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419674" title="Click to go to the Author Index">
             Le, Zhanhao
            </a>
           </td>
           <td class="r">
            University of Southern California
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#312878" title="Click to go to the Author Index">
             Ma, Junchao
            </a>
           </td>
           <td class="r">
            University of Southern California
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#203386" title="Click to go to the Author Index">
             Nguyen, Quan
            </a>
           </td>
           <td class="r">
            University of Southern California
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1950" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#humanoid_and_bipedal_locomotion" title="Click to go to the Keyword Index">
               Humanoid and Bipedal Locomotion
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#whole_body_motion_planning_and_control" title="Click to go to the Keyword Index">
               Whole-Body Motion Planning and Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Current humanoid push-recovery strategies often use whole-body motion, yet they tend to overlook posture regulation. For instance, in manipulation tasks, the upper body may need to stay upright and have minimal recovery displacement. This paper introduces a novel approach to enhancing humanoid push-recovery performance under unknown disturbances and regulating body posture by tailoring the recovery stepping strategy. We propose a hierarchical-MPC-based scheme that analyzes and detects instability in the prediction window and quickly recovers through adapting gait frequency. Our approach integrates a high-level nonlinear MPC, a posture-aware gait frequency adaptation planner, and a low-level convex locomotion MPC. The planners predict the center of mass (CoM) state trajectories that can be assessed for precursors of potential instability and posture deviation. In simulation, we demonstrate improved maximum recoverable impulse by 131% on average compared with baseline approaches. In hardware experiments, a 125 ms advancement in recovery stepping timing/reflex has been observed with the proposed approach. We also demonstrate improved push-recovery performance and minimized body attitude change under 0.2 rad.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt5_02">
             10:00-10:05, Paper ThBT5.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2877'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Robots with Attitude: Singularity-Free Quaternion-Based Model-Predictive Control for Agile Legged Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#339215" title="Click to go to the Author Index">
             Zhang, Zixin
            </a>
           </td>
           <td class="r">
            Northwestern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#346357" title="Click to go to the Author Index">
             Zhang, John
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#178816" title="Click to go to the Author Index">
             Yang, Shuo
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#205543" title="Click to go to the Author Index">
             Manchester, Zachary
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2877" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#body_balancing" title="Click to go to the Keyword Index">
               Body Balancing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present a model-predictive control (MPC) framework for legged robots that avoids the singularities associated with common three-parameter attitude representations like Euler angles during large-angle rotations. Our method parameterizes the robot's attitude with singularity-free unit quaternions and makes modifications to the iterative linear-quadratic regulator (iLQR) algorithm to deal with the resulting geometry. The derivation of our algorithm requires only elementary calculus and linear algebra, deliberately avoiding the abstraction and notation of Lie groups. We demonstrate the performance and computational efficiency of quaternion MPC in several experiments on quadruped and humanoid robots.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt5_03">
             10:05-10:10, Paper ThBT5.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3032'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Online Nonlinear MPC for Multimodal Locomotion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424736" title="Click to go to the Author Index">
             Taliani, Saverio
            </a>
           </td>
           <td class="r">
            Italian Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#196567" title="Click to go to the Author Index">
             Nava, Gabriele
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#251910" title="Click to go to the Author Index">
             L'Erario, Giuseppe
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#232025" title="Click to go to the Author Index">
             Elobaid, Mohamed
            </a>
           </td>
           <td class="r">
            Fondazione Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#232284" title="Click to go to the Author Index">
             Romualdi, Giulio
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#161571" title="Click to go to the Author Index">
             Pucci, Daniele
            </a>
           </td>
           <td class="r">
            Italian Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3032" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#humanoid_robot_systems" title="Click to go to the Keyword Index">
               Humanoid Robot Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__mechanics_and_control" title="Click to go to the Keyword Index">
               Aerial Systems: Mechanics and Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#control_architectures_and_programming" title="Click to go to the Keyword Index">
               Control Architectures and Programming
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Aerial humanoid robots can enhance the efficiency and safety of rescue operations in disaster scenarios. The control of such complex machines presents many challenges, for instance, the control of the different locomotion strategies and the stabilization of the transition maneuvers. In this article, we present an online nonlinear Model Predictive Controller and the relative prediction model to stabilize walking and flying trajectories. The controller uses a reduced model to generate feasible base link references, thrust profiles, and contact forces while dealing with different locomotion strategies and transition maneuvers. The control algorithm is tested in a simulated environment using our aerial humanoid robot iRonCub under the effect of external disturbances. The proposed control strategy demonstrates to effectively stabilize the desired trajectories while keeping the problem still treatable online.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt5_04">
             10:10-10:15, Paper ThBT5.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3178'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Terrain-Aware Model Predictive Control of Heterogeneous Bipedal and Aerial Robot Coordination for Search and Rescue Tasks
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#226703" title="Click to go to the Author Index">
             Shamsah, Abdulaziz
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#396859" title="Click to go to the Author Index">
             Jiang, Jesse
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#345446" title="Click to go to the Author Index">
             Yoon, Ziwon
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#147487" title="Click to go to the Author Index">
             Coogan, Samuel
            </a>
           </td>
           <td class="r">
            Georgia Tech
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#158317" title="Click to go to the Author Index">
             Zhao, Ye
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3178" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#humanoid_and_bipedal_locomotion" title="Click to go to the Keyword Index">
               Humanoid and Bipedal Locomotion
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#search_and_rescue_robots" title="Click to go to the Keyword Index">
               Search and Rescue Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Humanoid robots offer significant advantages for search and rescue tasks, thanks to their capability to traverse rough terrains and perform transportation tasks. In this study, we present a task and motion planning framework for search and rescue operations using a heterogeneous robot team composed of humanoids and aerial robots. We propose a terrain-aware Model Predictive Controller (MPC) that incorporates terrain elevation gradients learned using Gaussian processes (GP). This terrain-aware MPC generates safe navigation paths for the bipedal robots to traverse rough terrain while minimizing terrain slopes, and it directs the quadrotors to perform aerial search and mapping tasks. The rescue subjects' locations are estimated by a target belief GP, which is updated online during the map exploration. A high-level planner for task allocation is designed by encoding the navigation tasks using syntactically cosafe Linear Temporal Logic (scLTL), and a consensus-based algorithm is designed for task assignment of individual robots. We evaluate the efficacy of our planning framework in simulation in an uncertain environment with various terrains and random rescue subject placements.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt5_05">
             10:15-10:20, Paper ThBT5.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3772'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Koopman Operator Based Linear Model Predictive Control for Quadruped Trotting
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#333352" title="Click to go to the Author Index">
             Yang, Chun-Ming
            </a>
           </td>
           <td class="r">
            University of Illinois at Chicago
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#166583" title="Click to go to the Author Index">
             Bhounsule, Pranav
            </a>
           </td>
           <td class="r">
            University of Illinois at Chicago
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3772" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#model_learning_for_control" title="Click to go to the Keyword Index">
               Model Learning for Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#force_control" title="Click to go to the Keyword Index">
               Force Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Online optimal control of quadruped robots would enable them to adapt to varying inputs and changing conditions in real time. A common way of achieving this is linear model predictive control (LMPC), where a quadratic programming (QP) problem is formulated over a finite horizon with a quadratic cost and linear constraints obtained by linearizing the equations of motion and solved on the fly. However, the model linearization may lead to model inaccuracies. In this paper, we use the Koopman operator to create a linear model of the quadrupedal system in high dimensional space which preserves the nonlinearity of the equations of motion. Then using LMPC, we demonstrate high fidelity tracking and disturbance rejection on a quadrupedal robot. This is the first work that uses the Koopman operator theory for LMPC of quadrupedal locomotion.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt5_06">
             10:20-10:25, Paper ThBT5.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3860'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Kinodynamic Model Predictive Control for Energy Efficient Locomotion of Legged Robots with Parallel Elasticity
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#362632" title="Click to go to the Author Index">
             Zhuang, Yulun
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426037" title="Click to go to the Author Index">
             Wang, Yichen
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#207544" title="Click to go to the Author Index">
             Ding, Yanran
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3860" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#compliant_joints_and_mechanisms" title="Click to go to the Keyword Index">
               Compliant Joints and Mechanisms
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper, we introduce a kinodynamic model predictive control (MPC) framework that exploits unidirectional parallel springs (UPS) to improve the energy efficiency of dynamic legged robots. The proposed method employs a hierarchical control structure, where the solution of MPC with simplified dynamic models is used to warm-start the kinodynamic MPC, which accounts for nonlinear centroidal dynamics and kinematic constraints. The proposed approach enables energy efficient dynamic hopping on legged robots by using UPS to reduce peak motor torques and energy consumption during stance phases. Simulation results demonstrated a 38.8% reduction in the cost of transport (CoT) for a monoped robot equipped with UPS during high-speed hopping. Additionally, preliminary hardware experiments show a 14.8% reduction in energy consumption.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt5_07">
             10:25-10:30, Paper ThBT5.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4201'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Dynamic Bipedal MPC with Foot-Level Obstacle Avoidance and Adjustable Step Timing
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#325994" title="Click to go to the Author Index">
             Wang, Tianze
            </a>
           </td>
           <td class="r">
            Florida State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#148199" title="Click to go to the Author Index">
             Hubicki, Christian
            </a>
           </td>
           <td class="r">
            Florida State University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4201" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#humanoid_and_bipedal_locomotion" title="Click to go to the Keyword Index">
               Humanoid and Bipedal Locomotion
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#whole_body_motion_planning_and_control" title="Click to go to the Keyword Index">
               Whole-Body Motion Planning and Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Collision-free planning is essential for bipedal robots operating within unstructured environments. This paper presents a real-time Model Predictive Control (MPC) framework that addresses both body and foot avoidance for dynamic bipedal robots. Our contribution is two-fold: we introduce (1) a novel formulation for adjusting step timing to facilitate faster body avoidance and (2) a novel 3D foot-avoidance formulation that implicitly selects swing trajectories and footholds that either steps over or navigate around obstacles with awareness of Center of Mass (COM) dynamics. We achieve body avoidance by applying a half-space relaxation of the safe region but introduce a switching heuristic based on tracking error to detect a need to change foot-timing schedules. To enable foot avoidance and viable landing footholds on all sides of foot-level obstacles, we decompose the non-convex safe region on the ground into several convex polygons and use Mixed-Integer Quadratic Programming to determine the optimal candidate. We found that introducing a soft minimum-travel-distance constraint is effective in preventing the MPC from being trapped in local minima that can stall half-space relaxation methods behind obstacles. We demonstrated the proposed algorithms on multibody simulations on the bipedal robot platforms, Cassie and Digit, as well as hardware experiments on Digit.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt6">
             <b>
              ThBT6
             </b>
            </a>
           </td>
           <td class="r">
            307
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thbt6" title="Click to go to the Program at a Glance">
             <b>
              Perception for Manipulation 1
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#223009" title="Click to go to the Author Index">
             Liu, Tengyu
            </a>
           </td>
           <td class="r">
            Beijing Institute for General Artificial Intelligence
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt6_01">
             09:55-10:00, Paper ThBT6.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3483'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Enhancing Robotic Perception with Low-Cost Fast Active Vision Achieving Sub-Millimeter Accurate Marker-Based Pose Estimation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#319102" title="Click to go to the Author Index">
             Knobbe, Dennis
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417128" title="Click to go to the Author Index">
             Standke, Johann Jakob Wilhelm
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#108317" title="Click to go to the Author Index">
             Haddadin, Sami
            </a>
           </td>
           <td class="r">
            Mohamed Bin Zayed University of Artificial Intelligence
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3483" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#visual_servoing" title="Click to go to the Keyword Index">
               Visual Servoing
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#performance_evaluation_and_benchmarking" title="Click to go to the Keyword Index">
               Performance Evaluation and Benchmarking
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robust perception of the environment is a critical challenge for robots, especially those that use mobile platforms or humanoid forms to perform manipulation tasks. Active vision, leveraging strategic camera movements and adaptive imaging parameters, holds great potential for addressing critical challenges such as achieving high accuracy in precise manipulation, ensuring low latency for rapid responsiveness, and overcoming occlusions and illumination variations in dynamic environments. This paper introduces a novel, cost-effective, and easily deployable active vision system designed to enhance visual perception for robotic applications. Integrated with a novel hybrid software setup, the system utilizes ArUco markers to achieve high-accuracy, low-latency performance, boasting sub-millimeter and sub-degree accuracy at 200 Hz with a latency of less than 15 ms. Additionally, a new measurement and evaluation procedure is presented, offering benchmarking for marker-based object detection systems that for the first time includes rotation measurements as well. The benchmarking results for the proposed system indicate that achieving the desired performance levels necessitates specialized active vision measurement strategies. For instance, to ensure high positional accuracy, the system needs precise object centering, while high rotational accuracy requires accounting for lateral or rotational offsets.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt6_02">
             10:00-10:05, Paper ThBT6.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1858'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              PhysPart: Physically Plausible Part Completion for Interactable Objects
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422503" title="Click to go to the Author Index">
             Luo, Rundong
            </a>
           </td>
           <td class="r">
            Cornell University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#341712" title="Click to go to the Author Index">
             Geng, Haoran
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#343263" title="Click to go to the Author Index">
             Deng, Congyue
            </a>
           </td>
           <td class="r">
            Stanford
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#337939" title="Click to go to the Author Index">
             Li, Puhao
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#350947" title="Click to go to the Author Index">
             Wang, Zan
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#355364" title="Click to go to the Author Index">
             Jia, Baoxiong
            </a>
           </td>
           <td class="r">
            Beijing Institute for General Artificial Intelligence
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#118397" title="Click to go to the Author Index">
             Guibas, Leonidas
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#335817" title="Click to go to the Author Index">
             Huang, Siyuan
            </a>
           </td>
           <td class="r">
            Beijing Institute for General Artificial Intelligence
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1858" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#manipulation_planning" title="Click to go to the Keyword Index">
               Manipulation Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Interactable objects are ubiquitous in our daily lives. Recent advances in 3D generative models make it possible to automate the modeling of these objects, benefiting a range of applications from 3D printing to the creation of robot simulation environments. However, while significant progress has been made in modeling 3D shapes and appearances, modeling object physics, particularly for interactable objects, remains challenging due to the physical constraints imposed by inter-part motions. In this paper, we tackle the problem of physically plausible part completion for interactable objects, aiming to generate 3D parts that not only fit precisely into the object but also allow smooth part motions. To this end, we propose a diffusion-based part generation model that utilizes geometric conditioning through classifier-free guidance and formulates physical constraints as a set of stability and mobility losses to guide the sampling process. Additionally, we demonstrate the generation of dependent parts, paving the way toward sequential part generation for objects with complex part-whole hierarchies. Experimentally, we introduce a new metric for measuring physical plausibility based on motion success rates. Our model outperforms existing baselines over shape and physical metrics, especially those that do not adequately model physical constraints. We also demonstrate our applications in 3D printing, robot manipulation, and sequential part generation, showing our strength in realistic tasks with the demand for high physical plausibility.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt6_03">
             10:05-10:10, Paper ThBT6.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1935'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Generalizable Zero-Shot Object Pose Estimation for Bin-Picking
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#382704" title="Click to go to the Author Index">
             Zhang, Zijiang
            </a>
           </td>
           <td class="r">
            Kyushu Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#351349" title="Click to go to the Author Index">
             Huimin, Lu
            </a>
           </td>
           <td class="r">
            Southeast University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#351163" title="Click to go to the Author Index">
             Jintong, Cai
            </a>
           </td>
           <td class="r">
            Southeast University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#358811" title="Click to go to the Author Index">
             Kamiya, Tohru
            </a>
           </td>
           <td class="r">
            Kyushu Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#158802" title="Click to go to the Author Index">
             Serikawa, Seiichi
            </a>
           </td>
           <td class="r">
            Kyushu Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1935" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Abstract—Unordered grasping in industrial robotic manipulation requires precise six-degree-of-freedom (6D) pose estimation. However, existing methods often struggle with unknown objects and require retraining, limiting their practicality. Traditional 3D point-pair feature methods, while training-free, perform poorly with textured symmetric objects. We propose a generalizable approach for zero-shot 6D pose estimation without retraining. Our method consists of two steps: generating CAD-based templates through real-time rendering for coarse pose estimation, and refining poses using semantic point-pair features aligned with the camera viewpoint. We conducted experiments on seven core datasets from the Benchmark for 6D Object Pose Estimation (BOP) challenge, and the results are publicly available on the BOP website. Integration into a robotic grasping system further highlights its high precision and fast execution, making it idealfor applications such as bin-picking.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt6_04">
             10:10-10:15, Paper ThBT6.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3248'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Visuo-Tactile Object Pose Estimation for a Multi-Finger Robot Hand with Low-Resolution In-Hand Tactile Sensing
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#340840" title="Click to go to the Author Index">
             Mack, Lukas
            </a>
           </td>
           <td class="r">
            University of Augsburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#362675" title="Click to go to the Author Index">
             Grüninger, Felix
            </a>
           </td>
           <td class="r">
            Max Planck Institute for Intelligent Systems
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#236046" title="Click to go to the Author Index">
             Richardson, Benjamin A.
            </a>
           </td>
           <td class="r">
            Max Planck Institute for Intelligent Systems
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423351" title="Click to go to the Author Index">
             Lendway, Regine
            </a>
           </td>
           <td class="r">
            University of Tuebingen
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#107509" title="Click to go to the Author Index">
             Kuchenbecker, Katherine J.
            </a>
           </td>
           <td class="r">
            Max Planck Institute for Intelligent Systems
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#114207" title="Click to go to the Author Index">
             Stueckler, Joerg
            </a>
           </td>
           <td class="r">
            University of Augsburg
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3248" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#sensor_fusion" title="Click to go to the Keyword Index">
               Sensor Fusion
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#force_and_tactile_sensing" title="Click to go to the Keyword Index">
               Force and Tactile Sensing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Accurate 3D pose estimation of grasped objects is an important prerequisite for robots to perform assembly or in-hand manipulation tasks, but object occlusion by the robot's own hand greatly increases the difficulty of this perceptual task. Here, we propose that combining visual information and proprioception with binary, low-resolution tactile contact measurements from across the interior surface of an articulated robotic hand can mitigate this issue. The visuo-tactile object-pose-estimation problem is formulated probabilistically in a factor graph. The pose of the object is optimized to align with the three kinds of measurements using a robust cost function to reduce the influence of visual or tactile outlier readings. The advantages of the proposed approach are first demonstrated in simulation: a custom 15-DoF robot hand with one binary tactile sensor per link grasps 17 YCB objects while observed by an RGB-D camera. This low-resolution in-hand tactile sensing significantly improves object-pose estimates under high occlusion and also high visual noise. We also show these benefits through grasping tests with a preliminary real version of our tactile hand, obtaining reasonable visuo-tactile estimates of object pose at approximately 13.3 Hz on average.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt6_05">
             10:15-10:20, Paper ThBT6.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3919'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Proactive Tactile Exploration for Object-Agnostic Shape Reconstruction from Minimal Visual Priors
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#266887" title="Click to go to the Author Index">
             Oikonomou, Paris
            </a>
           </td>
           <td class="r">
            National Technical University of Athens (NTUA)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#377064" title="Click to go to the Author Index">
             Retsinas, George
            </a>
           </td>
           <td class="r">
            National Technical University of Athens
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#114414" title="Click to go to the Author Index">
             Maragos, Petros
            </a>
           </td>
           <td class="r">
            National Technical University of Athens
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#107066" title="Click to go to the Author Index">
             Tzafestas, Costas S.
            </a>
           </td>
           <td class="r">
            ICCS - Inst of Communication and Computer Systems
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3919" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The perception of an object’s surface is important for robotic applications enabling robust object manipulation. The level of accuracy in such a representation affects the outcome of the action planning, especially during tasks that require physical contact, e.g. grasping. In this paper, we propose a novel iterative method for 3D shape reconstruction consisting of two steps. At first, a mesh is fitted on data points acquired from the object’s surface, based on a single primitive template. Subsequently, the mesh is properly adjusted to adequately represent local deformities. Moreover, a novel proactive tactile exploration strategy aims at minimizing the total uncertainty with the least number of contacts, while reducing the risk of contact failure in case the estimated surface differs significantly from the real one. The performance of the methodology is evaluated both in 3D simulation and on a real setup.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt6_06">
             10:20-10:25, Paper ThBT6.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4138'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Multi-Layer Feature Exchange Transformer for Multi-View 6D Object Pose Estimation in Robot Bin Picking
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425081" title="Click to go to the Author Index">
             Khalil, Momen
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#191549" title="Click to go to the Author Index">
             Dietrich, Vincent
            </a>
           </td>
           <td class="r">
            Siemens Corporate Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#133930" title="Click to go to the Author Index">
             Ilic, Slobodan
            </a>
           </td>
           <td class="r">
            Technische Universitat Munchen
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4138" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Accurate 6D object pose estimation is crucial in industrial automation, particularly in robotic bin picking, where objects are often textureless, reflective, and arranged in cluttered environments. Multi-view pose estimation methods offer significant advantages over single-view methods by providing more comprehensive information, effectively handling occlusions and lack of features, and resolving depth ambiguities. However, current multi-view methods often rely on late-stage information fusion, limiting their ability to fully exploit complementary multi-view data.
             <p>
              This paper presents a novel approach to enhance multi-view 6D pose estimation by introducing a Feature Exchange Transformer (FET) for early-stage feature fusion. This approach leverages self-attention and epipolar cross-attention mechanisms to enable multi-layer feature aggregation across views. Additionally, we introduce a coarse-to-fine strategy for an efficient feature exchange at multiple network layers. Our method, implemented on top of EpiSurfEmb, enhances the utilization of multi-view information, leading to significant improvements in pose estimation accuracy and robustness, especially in challenging bin-picking scenarios.
              <p>
               We evaluate our approach on the ROBI dataset, demonstrating that it outperforms both the baseline EpiSurfEmb and other state-of-the-art multi-view pose estimation methods
              </p>
             </p>
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt7">
             <b>
              ThBT7
             </b>
            </a>
           </td>
           <td class="r">
            309
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thbt7" title="Click to go to the Program at a Glance">
             <b>
              Assistive Human-Robot Interaction
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#111458" title="Click to go to the Author Index">
             Yanco, Holly
            </a>
           </td>
           <td class="r">
            UMass Lowell
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#176149" title="Click to go to the Author Index">
             Haring, Kerstin Sophie
            </a>
           </td>
           <td class="r">
            University of Denver
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt7_01">
             09:55-10:00, Paper ThBT7.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('10'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              DRAGON: A Dialogue-Based Robot for Assistive Navigation with Visual Language Grounding
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#267278" title="Click to go to the Author Index">
             Liu, Shuijing
            </a>
           </td>
           <td class="r">
            The University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#283110" title="Click to go to the Author Index">
             Hasan, Aamir
            </a>
           </td>
           <td class="r">
            University of Illinois Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#255003" title="Click to go to the Author Index">
             Hong, Kaiwen
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#365508" title="Click to go to the Author Index">
             Wang, Runxuan
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#267282" title="Click to go to the Author Index">
             Chang, Peixin
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#365829" title="Click to go to the Author Index">
             Mizrachi, Zachary
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#365893" title="Click to go to the Author Index">
             Lin, Justin
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#227000" title="Click to go to the Author Index">
             McPherson, D. Livingston
            </a>
           </td>
           <td class="r">
            University of Illinois
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#176516" title="Click to go to the Author Index">
             Rogers, Wendy
            </a>
           </td>
           <td class="r">
            University of Illinois Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#180408" title="Click to go to the Author Index">
             Driggs-Campbell, Katherine
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab10" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#human_centered_robotics" title="Click to go to the Keyword Index">
               Human-Centered Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#natural_dialog_for_hri" title="Click to go to the Keyword Index">
               Natural Dialog for HRI
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Persons with visual impairments (PwVI) have difficulties understanding and navigating spaces around them. Current wayfinding technologies either focus solely on navigation or provide limited communication about the environment. Motivated by recent advances in visual-language grounding and semantic navigation, we propose DRAGON, a guiding robot powered by a dialogue system and the ability to associate the environment with natural language. By understanding the commands from the user, DRAGON is able to guide the user to the desired landmarks on the map, describe the environment, and answer questions from visual observations. Through effective utilization of dialogue, the robot can ground the user's free-form descriptions to landmarks in the environment, and give the user semantic information through spoken language. We conduct a user study with blindfolded participants in an everyday indoor environment. Our results demonstrate that DRAGON is able to communicate with the user smoothly, provide a good guiding experience, and connect users with their surrounding environment in an intuitive manner. Videos are available at https://sites.google.com/view/dragon-wayfinding/home.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt7_02">
             10:00-10:05, Paper ThBT7.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('748'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Space-Aware Instruction Tuning: Dataset and Benchmark for Guide Dog Robots Assisting the Visually Impaired
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#137116" title="Click to go to the Author Index">
             Han, ByungOk
            </a>
           </td>
           <td class="r">
            ETRI
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#103459" title="Click to go to the Author Index">
             Yun, Woo-han
            </a>
           </td>
           <td class="r">
            Electronics and Telecommunications Research Institute (ETRI)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#108872" title="Click to go to the Author Index">
             Seo, BeomSu
            </a>
           </td>
           <td class="r">
            ETRI
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#130395" title="Click to go to the Author Index">
             Kim, Jaehong
            </a>
           </td>
           <td class="r">
            ETRI
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab748" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#multi_modal_perception_for_hri" title="Click to go to the Keyword Index">
               Multi-Modal Perception for HRI
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#data_sets_for_robot_learning" title="Click to go to the Keyword Index">
               Data Sets for Robot Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#natural_dialog_for_hri" title="Click to go to the Keyword Index">
               Natural Dialog for HRI
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Guide dog robots offer promising solutions to enhance mobility and safety for visually impaired individuals, addressing the limitations of traditional guide dogs, particularly in perceptual intelligence and communication. With the emergence of Vision-Language Models (VLMs), robots are now capable of generating natural language descriptions of their surroundings, aiding in safer decision-making. However, existing VLMs often struggle to accurately interpret and convey spatial relationships, which is crucial for navigation in complex environments such as street crossings. We introduce the Space-Aware Instruction Tuning (SAIT) dataset and the Space-Aware Benchmark (SA-Bench) to address the limitations of current VLMs in understanding physical environments. Our automated data generation pipeline focuses on the virtual path to the destination in 3D space and the surroundings, enhancing environmental comprehension and enabling VLMs to provide more accurate guidance to visually impaired individuals. We also propose an evaluation protocol to assess VLM effectiveness in delivering walking guidance. Comparative experiments demonstrate that our space-aware instruction-tuned model outperforms state-of-the-art algorithms. We have fully open-sourced the SAIT dataset and SA-Bench, along with the related code, at https://github.com/byungokhan/Space-awareVLM.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt7_03">
             10:05-10:10, Paper ThBT7.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2509'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              FitnessAgent: A Unified Agent Framework for Open-Set and Personalized Fitness Evaluation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#414661" title="Click to go to the Author Index">
             Tang, Zhenhui
            </a>
           </td>
           <td class="r">
            Shanghai Jiaotong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423056" title="Click to go to the Author Index">
             jiahao Li, Ljh
            </a>
           </td>
           <td class="r">
            Dalian University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#218339" title="Click to go to the Author Index">
             Guo, Ping
            </a>
           </td>
           <td class="r">
            Intel
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420999" title="Click to go to the Author Index">
             Tian, Bowen
            </a>
           </td>
           <td class="r">
            University of Electronic Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421059" title="Click to go to the Author Index">
             Xing, Qingjun
            </a>
           </td>
           <td class="r">
            Beijing Sport University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420987" title="Click to go to the Author Index">
             Xing, XuYang
            </a>
           </td>
           <td class="r">
            Nanjing University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#289409" title="Click to go to the Author Index">
             Wang, Peng
            </a>
           </td>
           <td class="r">
            Intel
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2509" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#multi_modal_perception_for_hri" title="Click to go to the Keyword Index">
               Multi-Modal Perception for HRI
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#data_sets_for_robotic_vision" title="Click to go to the Keyword Index">
               Data Sets for Robotic Vision
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robotic systems face challenges in performing open-set and personalized fitness evaluations, especially when adapting to new exercises and individual user needs. This paper introduces FitnessAgent, a unified agent framework designed to address these challenges. Unlike traditional systems that rely on pre-trained neural networks or fixed rule-based criteria, FitnessAgent can assess any exercise without prior training, adapting evaluation metrics based on expert knowledge and user-specific requirements. The system breaks down fitness evaluation tasks into combinations of metrics, each calculated using measurable operators such as angles, distances, and positions. By leveraging a set of primitive, exercise-agnostic operators, a large language model (LLM)-based planner dynamically selects and combines these operators for each task. The open-set capability of FitnessAgent is validated through experiments on both the widely-used Functional Movement Screen dataset and a newly collected isometric pose dataset. Results highlight the system's flexibility in handling new movements and its ability to adapt to personalized evaluation criteria without the need for code or algorithm modifications. FitnessAgent offers a scalable and personalized solution for fitness evaluation, making it well-suited for robotic applications that require adaptability to diverse user needs.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt7_04">
             10:10-10:15, Paper ThBT7.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2596'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Reinforcement Learning-Based Social Robot for Personalized Learning in Children with Autism
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#220913" title="Click to go to the Author Index">
             Askari, Farzaneh
            </a>
           </td>
           <td class="r">
            McGill University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#206301" title="Click to go to the Author Index">
             Abdollahi, Hojjat
            </a>
           </td>
           <td class="r">
            University of Denver
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#176149" title="Click to go to the Author Index">
             Haring, Kerstin Sophie
            </a>
           </td>
           <td class="r">
            University of Denver
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#124155" title="Click to go to the Author Index">
             Mahoor, Mohammad
            </a>
           </td>
           <td class="r">
            University of Denver
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2596" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#robot_companions" title="Click to go to the Keyword Index">
               Robot Companions
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This work hypothesizes that a social robot that uses reinforcement learning can effectively adapt to individual differences in teaching imitation skills (e.g., facial expressions) to children with autism spectrum disorder. We developed an active learning method based on reinforcement learning to personalize human-robot interaction sessions based on each child's imitation performance and preference. We evaluated this method with five children with autism spectrum disorder, and the results demonstrated varying responses to different methods of presenting facial expressions to teach imitation skills. We found that the robot consistently promoted increased shared attention, including visual contact and physical proximity during imitation tasks. This suggests that adaptive human-robot interactions can cater to the unique needs of children with autism, offering a promising avenue for personalized intervention. Additionally, we discuss observed qualitative insights from our study and considerations for robot behavior mitigation strategies to sustain engagement.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt7_05">
             10:15-10:20, Paper ThBT7.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3051'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Comparison of User Interface Paradigms for Assistive Robotic Manipulators
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422227" title="Click to go to the Author Index">
             Sinclaire, Amelia
            </a>
           </td>
           <td class="r">
            University of Massachusetts Lowell
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#251763" title="Click to go to the Author Index">
             Wilkinson, Alexander
            </a>
           </td>
           <td class="r">
            University of Massachusetts Lowell
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#189427" title="Click to go to the Author Index">
             Kim, Boyoung
            </a>
           </td>
           <td class="r">
            George Mason University Korea
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#111458" title="Click to go to the Author Index">
             Yanco, Holly
            </a>
           </td>
           <td class="r">
            UMass Lowell
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3051" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#design_and_human_factors" title="Click to go to the Keyword Index">
               Design and Human Factors
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#virtual_reality_and_interfaces" title="Click to go to the Keyword Index">
               Virtual Reality and Interfaces
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents the results of a within-subjects user study with 27 participants over the age of 60, comparing the use of two different user interfaces for an assistive robot scooter. The graphical user interface (GUI) shows a representation of the environment on a 10-inch touchscreen. The tangible user interface (TUI) consists of a joystick, a box of buttons, and a projector -- designed to keep the user's attention in the real world. Trends suggest that the TUI could help mitigate difficulty caused by highly cluttered environments, as well as differences in individual spatial reasoning ability, but additional studies are needed.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt7_06">
             10:20-10:25, Paper ThBT7.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4493'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              VQA-Driven Event Maps for Assistive Navigation for People with Low Vision in Urban Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#339343" title="Click to go to the Author Index">
             Morales, Joseph
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424755" title="Click to go to the Author Index">
             Gebregziabher, Bruk
            </a>
           </td>
           <td class="r">
            Biel Glasses
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424752" title="Click to go to the Author Index">
             Cabañeros, Alex
            </a>
           </td>
           <td class="r">
            Biel Glasses
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#229062" title="Click to go to the Author Index">
             Sanchez-Riera, Jordi
            </a>
           </td>
           <td class="r">
            IRI, CSIC-UPC
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4493" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#multi_modal_perception_for_hri" title="Click to go to the Keyword Index">
               Multi-Modal Perception for HRI
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#human_performance_augmentation" title="Click to go to the Keyword Index">
               Human Performance Augmentation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We introduce a novel framework for assistive urban navigation for individuals with low vision. Utilizing a smart glasses platform developed by Biel Glasses, which provide a continuous stream of stereo images and GPS fixes, we generate an textit{Event Map} based on key semantic elements extracted by carefully prompted visual question-answering (VQA) models. For individuals with blurry or reduced fields of vision (low vision), traversing city streets poses a variety of challenges; they may struggle to perceive construction work, potholes, crowded sidewalks, and other ambiguous obstacles obstructing their paths. Some tasks, such as distinguishing traffic light signals, are nigh impossible without assistance from a companion or city infrastructure aimed towards accessibility. Although the majority of these problems may be solved with individually tailored traditional computer vision algorithms, developing and running a suite of these algorithms is challenging and resource demanding. Therefore, our proposed solution capitalizes on a single underlying implementation that need only be extended by adding queries. We validate our approach using a custom dataset of over 1,300 annotated images from various locations around Barcelona, reporting performance across different urban navigation tasks. We demonstrate the performance of the end to end system on a run of data collected by the Biel Glasses platform.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt8">
             <b>
              ThBT8
             </b>
            </a>
           </td>
           <td class="r">
            311
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thbt8" title="Click to go to the Program at a Glance">
             <b>
              Aerial Robots 4
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#118557" title="Click to go to the Author Index">
             Aloimonos, Yiannis
            </a>
           </td>
           <td class="r">
            University of Maryland
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#107237" title="Click to go to the Author Index">
             Foong, Shaohui
            </a>
           </td>
           <td class="r">
            Singapore University of Technology and Design
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt8_01">
             09:55-10:00, Paper ThBT8.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('223'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Robust High-Strength Multi-Surface Rapid UV-Curable Payload Installation System for Generic Multirotors Via Impact Delivery
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#279409" title="Click to go to the Author Index">
             Lim, Ryan Jon Hui
            </a>
           </td>
           <td class="r">
            Singapore University of Technology &amp; Design
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#404669" title="Click to go to the Author Index">
             Tan, Jeck Chuang
            </a>
           </td>
           <td class="r">
            Singapore University of Technology and Design
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#223470" title="Click to go to the Author Index">
             Ng, Matthew
            </a>
           </td>
           <td class="r">
            Singapore University of Technology and Design
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#237683" title="Click to go to the Author Index">
             Low, Hong Yee
            </a>
           </td>
           <td class="r">
            Singapore University of Technology and Design
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#107237" title="Click to go to the Author Index">
             Foong, Shaohui
            </a>
           </td>
           <td class="r">
            Singapore University of Technology and Design
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab223" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#field_robots" title="Click to go to the Keyword Index">
               Field Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This letter details the design and development of a novel 3D-printed, lightweight and rapid-curing automated payload installation system for aerial robots, using a 3D printed resin-filled adhesive carrier tile (ACT). Its structure is designed to fracture and disperse ultraviolet (UV) curable resin on impact, delivered with a lightweight spring-driven impactor that rams the tile against a target surface. The dispersed resin is then cured with UV light. Shear-testing experiments with 40×40 mm ACTs across common building materials, surface conditions and roughness demonstrate loading exceeding 900N only after 10 seconds of curing, showcasing the strength, robustness and speed of the proposed system. Automated payload installation experiments show potential for applications requiring strong and permanent bonds to wall structures, such as sensor payloads or tether points within urban environments. To the authors’ knowledge, this is the first work employing wet UV adhesives for payload installation via multirotors.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt8_02">
             10:00-10:05, Paper ThBT8.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1244'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Multi-View Stereo with Geometric Encoding for Dense Scene Reconstruction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#351220" title="Click to go to the Author Index">
             Yang, Guidong
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#212047" title="Click to go to the Author Index">
             Cao, Rui
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#319735" title="Click to go to the Author Index">
             Wen, Junjie
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#351216" title="Click to go to the Author Index">
             Zhao, Benyun
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#391025" title="Click to go to the Author Index">
             Li, Qingxiang
            </a>
           </td>
           <td class="r">
            The Chineses University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#375342" title="Click to go to the Author Index">
             Huang, Yijun
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#334961" title="Click to go to the Author Index">
             Lei, Lei
            </a>
           </td>
           <td class="r">
            City University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#351227" title="Click to go to the Author Index">
             Chen, Xi
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420348" title="Click to go to the Author Index">
             Lam, Alan Hiu-Fung
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong,
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#100055" title="Click to go to the Author Index">
             Liu, Yunhui
            </a>
           </td>
           <td class="r">
            Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#171587" title="Click to go to the Author Index">
             Chen, Ben M.
            </a>
           </td>
           <td class="r">
            Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1244" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Multi-view stereo (MVS) implicitly encodes photometric and geometric cues into the cost volume for multi-view correspondence matching, transferring insufficient geometric cues essential to depth estimation and reconstruction. This paper proposes GE-MVS, a novel multi-view stereo network with geometric encoding for more accurate and complete depth estimation and point cloud reconstruction. First, the cross-view adaptive cost volume aggregation module is proposed to strengthen multi-view geometric cues encoding during cost volume construction. Then, the depth consistency optimization is performed in the 3D point space during learning by invoking ground-truth depth cues from adjacent views. Finally, the surface normal geometries are explicitly encoded to refine the sampled depth hypotheses to be consistent in the local neighbor regions. Extensive experiments on the standard MVS benchmarks including DTU, Tanks and Temples, and BlendedMVS demonstrate the state-of-the-art depth estimation and point cloud reconstruction performance of GE-MVS. The GE-MVS is further deployed in real-world experiments for UAV-based large-scale reconstruction, where our method outperforms the prevalent industrial reconstruction solutions concerning reconstruction efficiency and efficacy. Our project page is: https://cuhk-usr-group.github.io/GE-MVS/
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt8_03">
             10:05-10:10, Paper ThBT8.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3564'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              MicroASV: An Affordable 3D-Printed Centimeter-Scale Autonomous Surface Vehicle
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#378539" title="Click to go to the Author Index">
             Macauley, Kevin
            </a>
           </td>
           <td class="r">
            University of Wisconsin-Madison
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#379712" title="Click to go to the Author Index">
             Chen, Zhiheng
            </a>
           </td>
           <td class="r">
            Cornell University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#159640" title="Click to go to the Author Index">
             Wang, Wei
            </a>
           </td>
           <td class="r">
            University of Wisconsin-Madison
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3564" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#marine_robotics" title="Click to go to the Keyword Index">
               Marine Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#swarm_robotics" title="Click to go to the Keyword Index">
               Swarm Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#field_robots" title="Click to go to the Keyword Index">
               Field Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper introduces the design, fabrication, and autonomous control of MicroASV, a low-cost, centimeter-scale autonomous surface Vehicle (ASV). MicroASV has a square footprint with a side length of 85 mm. Its propulsion system consists of four custom water jets arranged in a “Diamond”- shaped actuator configuration, powered by magnetically coupled brushless motors. This setup allows for complete 2D mobility, enabling forward and backward motion, lateral translation, and in-place rotation. The MicroASV is built using commercially available motors and 3D-printed components, creating a modular, appendage-free structure that is simple to assemble. An onboard camera and inertial measurement unit (IMU) are integrated to enable real-time localization, with position and heading controllers developed to provide autonomous feedback control. Preliminary experiments validate the platform’s effectiveness in motion, sensing, and control, establishing MicroASV as a valuable tool for studying centimeter-scale ASV control, both individually and in collective swarm operations.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt8_04">
             10:10-10:15, Paper ThBT8.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4007'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Airflow Source Seeking on Small Quadrotors Using a Single Flow Sensor
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#285524" title="Click to go to the Author Index">
             Thomas, Lenworth
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423002" title="Click to go to the Author Index">
             Bridges, Tjaden
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#103249" title="Click to go to the Author Index">
             Bergbreiter, Sarah
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4007" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#environment_monitoring_and_management" title="Click to go to the Keyword Index">
               Environment Monitoring and Management
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reactive_and_sensor_based_planning" title="Click to go to the Keyword Index">
               Reactive and Sensor-Based Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             As environmental disasters happen more frequently and severely, seeking the source of pollutants or harmful particulates using plume tracking becomes even more important. Plume tracking on small quadrotors would allow these systems to operate around humans and fly in more confined spaces, but can be challenging due to poor sensitivity and long response times from gas sensors that fit on small drones. In this work, we present an approach to complement chemical plume tracking with airflow source-seeking behavior using a custom flow sensor that can sense both airflow magnitude and direction on small quadrotors (&lt;100 g). We use this sensor to implement a modified version of the `Cast and Surge' algorithm that takes advantage of flow direction sensing to find and navigate towards flow sources. A series of characterization experiments verified that the system can detect airflow while in flight and reorient the quadrotor toward the airflow. Several trials with random starting locations and orientations were used to show that our source-seeking algorithm can reliably find a flow source. This work aims to provide a foundation for future platforms that can use flow sensors in concert with other sensors to enable richer plume tracking data collection and source-seeking.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt8_05">
             10:15-10:20, Paper ThBT8.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4294'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Air-FAR: Fast and Adaptable Routing for Aerial Navigation in Large-Scale Complex Unknown Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#288413" title="Click to go to the Author Index">
             He, Botao
            </a>
           </td>
           <td class="r">
            University of Maryland
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#397800" title="Click to go to the Author Index">
             Chen, Guofei
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#136632" title="Click to go to the Author Index">
             Fermuller, Cornelia
            </a>
           </td>
           <td class="r">
            University of Maryland
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#118557" title="Click to go to the Author Index">
             Aloimonos, Yiannis
            </a>
           </td>
           <td class="r">
            University of Maryland
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#125279" title="Click to go to the Author Index">
             Zhang, Ji
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4294" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#field_robots" title="Click to go to the Keyword Index">
               Field Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#task_and_motion_planning" title="Click to go to the Keyword Index">
               Task and Motion Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__perception_and_autonomy" title="Click to go to the Keyword Index">
               Aerial Systems: Perception and Autonomy
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a novel method for real-time 3D navigation in large-scale, complex environments using a hierarchical 3D visibility graph (V-graph). The proposed algorithm addresses the computational challenges of V-graph construction and shortest path search on the graph simulta- neously. By introducing hierarchical 3D V-graph construction with heuristic visibility update, the 3D V-graph is constructed in O(K ·n2logn) time, which guarantees real-time performance. The proposed iterative divide-and-conquer path search method can achieve near-optimal path solutions within the constraints of real-time operations. The algorithm ensures efficient 3D V- graph construction and path search. Extensive simulated and real-world environments validated that our algorithm reduces the travel time by 42%, achieves up to 24.8% higher trajectory efficiency, and runs faster than most benchmarks by orders of magnitude in complex environments. The code and developed simulator have been open-sourced to facilitate future research.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt8_06">
             10:20-10:25, Paper ThBT8.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4900'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Multi-Agent Visual-Inertial Localization for Integrated Aerial Systems with Loose Fusion of Odometry and Kinematics
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#325919" title="Click to go to the Author Index">
             Lai, Ganghua
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#313411" title="Click to go to the Author Index">
             Shi, Chuanbeibei
            </a>
           </td>
           <td class="r">
            Univeristy of Bristol
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#313585" title="Click to go to the Author Index">
             Wang, Kaidi
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#140228" title="Click to go to the Author Index">
             Yu, Yushu
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#203749" title="Click to go to the Author Index">
             Dong, Yiqun
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104988" title="Click to go to the Author Index">
             Franchi, Antonio
            </a>
           </td>
           <td class="r">
            University of Twente / Sapienza University of Rome
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4900" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_slam" title="Click to go to the Keyword Index">
               Multi-Robot SLAM
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Reliably and efficiently estimating the relative pose and global localization of robots in a common reference for Integrated Aerial Platforms (IAPs) is a challenging problem. Unlike unmanned aerial vehicle (UAV) swarms, where the agent individual is able to move freely, IAPs connect UAV agents with mechanical joints, such as spherical joints, and form a rigid central platform, limiting the degree of freedom (DOF) of agents. Traditional methods, which rely on forming loop closures, object detection, or range sensors, suffer from degeneration or inefficiency due to the restricted relative motion between agents. In this paper, we present a centralized multi-agent localization system that fuses the internal kinematic constraints of IAPs and odometry measurements, using only visual-inertial suits for ego-motion estimation for agents and an additional 9-DOF Inertial Measurement Unit (IMU) attached to the central platform for posture estimation. A general formulation for kinematic constraints is derived without requiring knowledge about detailed kinematic parameters. A sliding-window optimization-based state estimator is constructed to estimate the relative transformation between agents. Our proposed approach is validated in our collected dataset. The results show that the proposed method reduces the global localization drift by 27.15% and relative localization error by 53.4% in the translation part and 36.99% in the rotation part compared to the baseline.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt8_07">
             10:25-10:30, Paper ThBT8.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('5071'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Multi Map Visual Localization for Unmanned Aerial Vehicles
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#408165" title="Click to go to the Author Index">
             Lømo, Tobias
            </a>
           </td>
           <td class="r">
            University of Oslo
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#148047" title="Click to go to the Author Index">
             Maffei, Renan
            </a>
           </td>
           <td class="r">
            Federal University of Rio Grande Do Sul
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#165993" title="Click to go to the Author Index">
             Kolberg, Mariana
            </a>
           </td>
           <td class="r">
            UFRGS
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#192025" title="Click to go to the Author Index">
             Torresen, Jim
            </a>
           </td>
           <td class="r">
            University of Oslo
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab5071" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__perception_and_autonomy" title="Click to go to the Keyword Index">
               Aerial Systems: Perception and Autonomy
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             <p>
              Localization has long been an essential area of research within robotics. The popularity of using Unmanned Aerial Vehicles (UAVs) to solve different tasks has increased and is expected to continue. Developing a robust complementary system to the Global Navigation Satellite Systems (GNSS) used today has been researched, and visual localization using cameras and satellite images is a popular choice to use. One of the challenges with using satellite images is that different images over the same area can impact the system’s performance.
              <p>
               This article proposes a novel approach called Multi Map Visual Localization (MMVL), a method to use multiple satellite images simultaneously, which is combined using a weighted average of probability maps. The proposal uses a convolutional neural network (CNN) with a caching strategy together with Monto Carlo Localization (MCL). MMVL achieves excellent robustness compared to other approaches and manages to estimate the correct location on all test flights. At the same time, using multiple satellite images does not significantly impact accuracy and computation time.
              </p>
             </p>
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt9">
             <b>
              ThBT9
             </b>
            </a>
           </td>
           <td class="r">
            312
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thbt9" title="Click to go to the Program at a Glance">
             <b>
              Task and Motion Planning 1
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#103133" title="Click to go to the Author Index">
             Beetz, Michael
            </a>
           </td>
           <td class="r">
            University of Bremen
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt9_01">
             09:55-10:00, Paper ThBT9.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('24'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Task and Motion Planning for Execution in the Real
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#237914" title="Click to go to the Author Index">
             Pan, Tianyang
            </a>
           </td>
           <td class="r">
            Rice University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#176739" title="Click to go to the Author Index">
             Shome, Rahul
            </a>
           </td>
           <td class="r">
            The Australian National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#102678" title="Click to go to the Author Index">
             Kavraki, Lydia
            </a>
           </td>
           <td class="r">
            Rice University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab24" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#task_and_motion_planning" title="Click to go to the Keyword Index">
               Task and Motion Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#manipulation_planning" title="Click to go to the Keyword Index">
               Manipulation Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#task_planning" title="Click to go to the Keyword Index">
               Task Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Task and motion planning represents a powerful set of hybrid planning methods that combine reasoning over discrete task domains and continuous motion generation. Traditional reasoning necessitates task domain models and enough information to ground actions to motion planning queries. Gaps in this knowledge often arise from sources like occlusion or imprecise modeling. This work generates task and motion plans that include actions cannot be fully grounded at planning time. During execution, such an action is handled by a provided human-designed or learned closed-loop behavior. Execution combines offline planned motions and online behaviors till reaching the task goal. Failures of behaviors are fed back as constraints to find new plans. Forty real-robot trials and motivating demonstrations are performed to evaluate the proposed framework and compare against state-of-the-art. Results show faster execution time, less number of actions, and more success in problems where diverse gaps arise. The experiment data is shared for researchers to simulate these settings. The work shows promise in expanding the applicable class of realistic partially grounded problems that robots can address.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt9_02">
             10:00-10:05, Paper ThBT9.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('377'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Automated Planning Domain Inference for Task and Motion Planning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416575" title="Click to go to the Author Index">
             Huang, Jinbang
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#414705" title="Click to go to the Author Index">
             Tao, Allen
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#418844" title="Click to go to the Author Index">
             Marco, Rozilyn
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#226976" title="Click to go to the Author Index">
             Bogdanovic, Miroslav
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#106824" title="Click to go to the Author Index">
             Kelly, Jonathan
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#142623" title="Click to go to the Author Index">
             Shkurti, Florian
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab377" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#task_and_motion_planning" title="Click to go to the Keyword Index">
               Task and Motion Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#integrated_planning_and_learning" title="Click to go to the Keyword Index">
               Integrated Planning and Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Task and motion planning (TAMP) frameworks address long and complex planning problems by integrating high-level task planners with low-level motion planners. However, existing TAMP methods rely heavily on the manual design of planning domains that specify the preconditions and postconditions of all high-level actions. This paper proposes a method to automate planning domain inference from a handful of test-time trajectory demonstrations, reducing the reliance on human design. Our approach incorporates a deep learning-based estimator that predicts the appropriate components of a domain for a new task and a search algorithm that refines this prediction, reducing the size and ensuring the utility of the inferred domain. Our method can generate new domains from minimal demonstrations at test time, enabling robots to handle complex tasks more efficiently. We demonstrate that our approach outperforms behavior cloning baselines, which directly imitate planner behavior, in terms of planning performance and generalization across a variety of tasks. Additionally, our method reduces computational costs and data amount requirements at test time for inferring new planning domains.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt9_03">
             10:05-10:10, Paper ThBT9.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('962'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Shadow Program Inversion with Differentiable Planning: A Framework for Unified Robot Program Parameter and Trajectory Optimization
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#284885" title="Click to go to the Author Index">
             Alt, Benjamin
            </a>
           </td>
           <td class="r">
            ArtiMinds Robotics
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#396314" title="Click to go to the Author Index">
             Kienle, Claudius
            </a>
           </td>
           <td class="r">
            ArtiMinds Robotics GmbH
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#181134" title="Click to go to the Author Index">
             Katic, Darko
            </a>
           </td>
           <td class="r">
            HFT STUTTGART
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#132630" title="Click to go to the Author Index">
             Jäkel, Rainer
            </a>
           </td>
           <td class="r">
            Karlsruhe Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#103133" title="Click to go to the Author Index">
             Beetz, Michael
            </a>
           </td>
           <td class="r">
            University of Bremen
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab962" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#task_and_motion_planning" title="Click to go to the Keyword Index">
               Task and Motion Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#integrated_planning_and_learning" title="Click to go to the Keyword Index">
               Integrated Planning and Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents SPI-DP, a novel first-order optimizer capable of optimizing robot programs with respect to both high-level task objectives and motion-level constraints. To that end, we introduce DGPMP2-ND, a differentiable collision-free motion planner for serial N-DoF kinematics, and integrate it into an iterative, gradient-based optimization approach for generic, parameterized robot program representations. SPI-DP allows first-order optimization of planned trajectories and program parameters with respect to objectives such as cycle time or smoothness subject to e.g. collision constraints, while enabling humans to understand, modify or even certify the optimized programs. We provide a comprehensive evaluation on two practical household and industrial applications.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt9_04">
             10:10-10:15, Paper ThBT9.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2558'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              AlignBot: Aligning VLM-Powered Customized Task Planning with User Reminders through Fine-Tuning for Household Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420968" title="Click to go to the Author Index">
             Zhaxizhuoma, Zhaxizhuoma
            </a>
           </td>
           <td class="r">
            Shanghai Artificial Intelligence Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424210" title="Click to go to the Author Index">
             Chen, Pengan
            </a>
           </td>
           <td class="r">
            The University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#368078" title="Click to go to the Author Index">
             Wu, Ziniu
            </a>
           </td>
           <td class="r">
            University of Bristol
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424401" title="Click to go to the Author Index">
             Sun, Jiawei
            </a>
           </td>
           <td class="r">
            Shanghai Artificial Intelligence Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#345464" title="Click to go to the Author Index">
             Wang, Dong
            </a>
           </td>
           <td class="r">
            Shanghai Artificial Intelligence Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#270209" title="Click to go to the Author Index">
             Zhou, Peng
            </a>
           </td>
           <td class="r">
            Great Bay University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#341557" title="Click to go to the Author Index">
             Cao, Nieqing
            </a>
           </td>
           <td class="r">
            Binghamton University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#278373" title="Click to go to the Author Index">
             Ding, Yan
            </a>
           </td>
           <td class="r">
            SUNY Binghamton
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#372118" title="Click to go to the Author Index">
             Zhao, Bin
            </a>
           </td>
           <td class="r">
            Northwestern Polytechnical University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#372120" title="Click to go to the Author Index">
             Li, Xuelong
            </a>
           </td>
           <td class="r">
            Northwestern Polytechnical University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2558" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#task_and_motion_planning" title="Click to go to the Keyword Index">
               Task and Motion Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#human_centered_robotics" title="Click to go to the Keyword Index">
               Human-Centered Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#learning_from_experience" title="Click to go to the Keyword Index">
               Learning from Experience
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents AlignBot, a novel framework designed to optimize VLM-powered customized task planning for household robots by effectively aligning with user reminders. In domestic settings, aligning task planning with user reminders poses significant challenges due to the limited quantity, diversity, and multimodal nature of the reminders. To address these challenges, AlignBot employs a fine-tuned LLaVA-7B model, functioning as an adapter for GPT-4o. This adapter model internalizes diverse forms of user reminders-such as personalized preferences, corrective guidance, and contextual assistance-into structured that prompt GPT-4o in generating customized task plans. Additionally, AlignBot integrates a dynamic retrieval mechanism that selects task-relevant historical successes as prompts for GPT-4o, further enhancing task planning accuracy. To validate the effectiveness of AlignBot, experiments are conducted in real-world household environments, which are constructed within the laboratory to replicate typical household settings. A multimodal dataset with over 1,500 entries derived from volunteer reminders is used for training and evaluation. The results demonstrate that AlignBot significantly improves customized task planning, outperforming existing LLM- and VLM-powered planners by interpreting and aligning with user reminders, achieving 86.8% success rate compared to the vanilla GPT-4o baseline at 21.6%, reflecting a 65% improvement and over four times greater effectiveness. Supplementary materials are available at: https://yding25.com/AlignBot/
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt9_05">
             10:15-10:20, Paper ThBT9.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3219'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Curiosity-Driven Imagination: Discovering Plan Operators and Learning Associated Policies for Open-World Adaptation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#378310" title="Click to go to the Author Index">
             Lorang, Pierrick
            </a>
           </td>
           <td class="r">
            AIT Austrian Institute of Technology GmbH - Tufts University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#397978" title="Click to go to the Author Index">
             Lu, Hong
            </a>
           </td>
           <td class="r">
            Tufts University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104328" title="Click to go to the Author Index">
             Scheutz, Matthias
            </a>
           </td>
           <td class="r">
            Tufts University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3219" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#integrated_planning_and_learning" title="Click to go to the Keyword Index">
               Integrated Planning and Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#task_and_motion_planning" title="Click to go to the Keyword Index">
               Task and Motion Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#learning_from_experience" title="Click to go to the Keyword Index">
               Learning from Experience
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Adapting quickly to dynamic, uncertain environments—often called ``open worlds"—remains a major challenge in robotics. Traditional Task and Motion Planning (TAMP) approaches struggle to cope with unforeseen changes, are data-inefficient when adapting, and do not leverage world models during learning. We address this issue with a hybrid planning and learning system that integrates two models: a low-level neural network-based model that learns stochastic transitions and drives exploration via an Intrinsic Curiosity Module (ICM), and a high-level symbolic planning model that captures abstract transitions using operators, enabling the agent to plan in an ``imaginary" space and generate reward machines. Our evaluation in a robotic manipulation domain with sequential novelty injections demonstrates that our approach converges faster and outperforms state-of-the-art hybrid methods.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt9_06">
             10:20-10:25, Paper ThBT9.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3456'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Optimization-Based Task and Motion Planning under Signal Temporal Logic Specifications Using Logic Network Flow
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#202382" title="Click to go to the Author Index">
             Lin, Xuan
            </a>
           </td>
           <td class="r">
            UCLA
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#354843" title="Click to go to the Author Index">
             Ren, Jiming
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#147487" title="Click to go to the Author Index">
             Coogan, Samuel
            </a>
           </td>
           <td class="r">
            Georgia Tech
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#158317" title="Click to go to the Author Index">
             Zhao, Ye
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3456" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#task_and_motion_planning" title="Click to go to the Keyword Index">
               Task and Motion Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#path_planning_for_multiple_mobile_robots_or_agents" title="Click to go to the Keyword Index">
               Path Planning for Multiple Mobile Robots or Agents
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#formal_methods_in_robotics_and_automation" title="Click to go to the Keyword Index">
               Formal Methods in Robotics and Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper proposes an optimization-based task and motion planning framework, named "Logic Network Flow", to integrate signal temporal logic (STL) specifications into efficient mixed-binary linear programmings. In this framework, temporal predicates are encoded as polyhedron constraints on each edge of the network flow, instead of as constraints between the nodes as in the traditional Logic Tree formulation. Synthesized with Dynamic Network Flows, Logic Network Flows render a tighter convex relaxation compared to Logic Trees derived from these STL specifications. Our formulation is evaluated on several multi-robot motion planning case studies. Empirical results demonstrate that our formulation outperforms Logic Tree formulation in terms of computation time for several planning problems. As the problem size scales up, our method still discovers better lower and upper bounds by exploring fewer number of nodes during the branches.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt9_07">
             10:25-10:30, Paper ThBT9.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3609'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Integrating Active Sensing and Rearrangement Planning for Efficient Object Retrieval from Unknown, Confined, Cluttered Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425755" title="Click to go to the Author Index">
             Kim, Junyoung
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#335370" title="Click to go to the Author Index">
             Ren, Hanwen
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#201200" title="Click to go to the Author Index">
             Qureshi, Ahmed H.
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3609" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#task_and_motion_planning" title="Click to go to the Keyword Index">
               Task and Motion Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reactive_and_sensor_based_planning" title="Click to go to the Keyword Index">
               Reactive and Sensor-Based Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#task_planning" title="Click to go to the Keyword Index">
               Task Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Retrieving target objects from unknown, confined spaces remains a challenging task that requires integrated, task-driven active sensing and rearrangement planning. Previous approaches have independently addressed active sensing and rearrangement planning, limiting their practicality in real-world scenarios. This paper presents a new, integrated heuristic-based active sensing and Monte-Carlo Tree Search (MCTS)-based retrieval planning approach. These components provide feedback to one another to actively sense critical, unobserved areas suitable for the retrieval planner to plan a sequence for relocating path-blocking obstacles and a collision-free trajectory for retrieving the target object. We demonstrate the effectiveness of our approach using a robot arm equipped with an in-hand camera in both simulated and real-world confined, cluttered scenarios. Our framework is compared against various state-of-the-art methods. The results indicate that our proposed approach outperforms baseline methods by a significant margin in terms of the success rate, the object rearrangement planning time consumption and the number of planning trials before successfully retrieving the target.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt10">
             <b>
              ThBT10
             </b>
            </a>
           </td>
           <td class="r">
            313
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thbt10" title="Click to go to the Program at a Glance">
             <b>
              Multi-Robot Systems 4
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#276792" title="Click to go to the Author Index">
             Keren, Sarah
            </a>
           </td>
           <td class="r">
            Technion - Israel Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#187951" title="Click to go to the Author Index">
             Zhao, Lin
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt10_01">
             09:55-10:00, Paper ThBT10.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2010'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Cooperative Bearing-Rate Approach for Observability-Enhanced Target Motion Estimation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#342781" title="Click to go to the Author Index">
             Zheng, Canlun
            </a>
           </td>
           <td class="r">
            Westlake University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#385688" title="Click to go to the Author Index">
             Guo, Hanqing
            </a>
           </td>
           <td class="r">
            Westlake University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#193378" title="Click to go to the Author Index">
             Zhao, Shiyu
            </a>
           </td>
           <td class="r">
            Westlake University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2010" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#sensor_networks" title="Click to go to the Keyword Index">
               Sensor Networks
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Vision-based target motion estimation is a fundamental problem in many robotic tasks. The existing methods have the limitation of low observability and, hence, face challenges in tracking highly maneuverable targets. Motivated by the aerial target pursuit task where a target may maneuver in 3D space, this paper studies how to further enhance observability by incorporating the emph{bearing rate} information that has not been well explored in the literature. The main contribution of this paper is to propose a new cooperative estimator called STT-R (Spatial-Temporal Triangulation with bearing Rate), which is designed under the framework of distributed recursive least squares. This theoretical result is further verified by numerical simulation and real-world experiments. It is shown that the proposed STT-R algorithm can effectively generate more accurate estimations and effectively reduce the lag in velocity estimation, enabling tracking of more maneuverable targets.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt10_02">
             10:00-10:05, Paper ThBT10.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3385'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Overlapping Free: Anchorless UWB-Assisted Relative Pose Estimation for Multi-Robot Systems
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#384047" title="Click to go to the Author Index">
             Yun, Yanpu
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#253230" title="Click to go to the Author Index">
             Peng, Guohao
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#342006" title="Click to go to the Author Index">
             Zhou, Yichen
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#217946" title="Click to go to the Author Index">
             Zhang, Jun
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#311041" title="Click to go to the Author Index">
             Liu, Yiyao
            </a>
           </td>
           <td class="r">
            NANYANG Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#406611" title="Click to go to the Author Index">
             Mao, Kaimin
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#100565" title="Click to go to the Author Index">
             Wang, Danwei
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3385" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Accurate Relative Pose Estimation (RPE) is critical for effective collaboration of multi-robot systems. Traditional methods using cameras or LiDARs heavily rely on overlapping Fields of View (FoV) between robots, which is highly demanding in practical applications and may hinder collaboration efficiency. To accommodate this issue, we propose Anchorless UWB-Assisted Relative Pose Estimation (AURPE), a novel approach that leverages ultra-wideband (UWB) technology in an anchorless setup to achieve multi-robot RPE without requiring overlapping FoVs or external infrastructure. AURPE first estimates the initial relative poses between robots using inter-robot UWB ranging combined with a Bayesian framework and constrained optimization. During robot operation, AURPE continuously refines the relative poses by integrating UWB measurements with LiDAR-inertial odometry (LIO) and employs a consensus voting mechanism to identify the most reliable pose estimates. Additionally, a pose graph-based back-end optimization is incorporated to enhance the accuracy of both initial and real-time relative pose. Extensive simulations and real-world experiments demonstrate that AURPE achieves accurate RPE even in non-overlapping scenarios where traditional methods fail. Compared to state-of-the-art point cloud registration methods, AURPE shows superior performance in both accuracy and robustness, highlighting its potential to significantly enhance cooperative tasks in multi-robot systems operating in complex environments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt10_03">
             10:05-10:10, Paper ThBT10.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3461'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Maintaining Strong R-Robustness in Reconfigurable Multi-Robot Networks Using Control Barrier Functions
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#386467" title="Click to go to the Author Index">
             Lee, Haejoon
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#103171" title="Click to go to the Author Index">
             Panagou, Dimitra
            </a>
           </td>
           <td class="r">
            University of Michigan, Ann Arbor
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3461" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#networked_robots" title="Click to go to the Keyword Index">
               Networked Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#path_planning_for_multiple_mobile_robots_or_agents" title="Click to go to the Keyword Index">
               Path Planning for Multiple Mobile Robots or Agents
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In leader-follower consensus, strong r-robustness of the communication graph provides a sufficient condition for followers to achieve consensus in the presence of misbehaving agents. Previous studies have assumed that robots can form and/or switch between predetermined network topologies with known robustness properties. However, robots with distance-based communication models may not be able to achieve these topologies while moving through spatially constrained environments, such as narrow corridors, to complete their objectives. This paper introduces a Control Barrier Function (CBF) that ensures robots maintain strong r-robustness of their communication graph above a certain threshold without maintaining any fixed topologies. Our CBF directly addresses robustness, allowing robots to have flexible reconfigurable network structure while navigating to achieve their objectives. The efficacy of our method is tested through various simulation and hardware experiments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt10_04">
             10:10-10:15, Paper ThBT10.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3904'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Online Waypoint Recognition of Controlled Agents in Uncertain Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#342895" title="Click to go to the Author Index">
             Guo, Jia
            </a>
           </td>
           <td class="r">
            Cornell University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#340442" title="Click to go to the Author Index">
             Surve, Sushrut
            </a>
           </td>
           <td class="r">
            Cornell University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426155" title="Click to go to the Author Index">
             He, Zilong
            </a>
           </td>
           <td class="r">
            Cornell University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#132416" title="Click to go to the Author Index">
             Ferrari, Silvia
            </a>
           </td>
           <td class="r">
            Cornell University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#276792" title="Click to go to the Author Index">
             Keren, Sarah
            </a>
           </td>
           <td class="r">
            Technion - Israel Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3904" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#cooperating_robots" title="Click to go to the Keyword Index">
               Cooperating Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#integrated_planning_and_control" title="Click to go to the Keyword Index">
               Integrated Planning and Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#autonomous_agents" title="Click to go to the Keyword Index">
               Autonomous Agents
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             For multi-robot teams with limited communication, the ability to rapidly recognize the intention of a teammate via its exhibited behavior is key to achieving effective collaboration. While current research on plan and goal recognition provide powerful tools, most of them rely on a high-level abstraction of the environment and of its dynamics. We propose online waypoint recognition (OWR) that incorporates knowledge about the dynamic models into the analysis of the observed agent behavior. Our algorithm takes the form of a Kalman filter and performs recognition of the agent's intended waypoint at high frequency. The approach is robust to uncertainties in dynamics and observations. Moreover, it does not require the agent to reach the next waypoint to perform recognition, which saves valuable time. Our empirical evaluation shows the ability of our proposed algorithm to expedite recognition of both simulated and real-world mobile robots.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt10_05">
             10:15-10:20, Paper ThBT10.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4229'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              MARF: Cooperative Multi-Agent Path Finding with Reinforcement Learning and Frenet Lattice in Dynamic Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426084" title="Click to go to the Author Index">
             Hu, Tianyang
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#270935" title="Click to go to the Author Index">
             Zhang, Zhen
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#368276" title="Click to go to the Author Index">
             Zhu, Chengrui
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#314116" title="Click to go to the Author Index">
             Xu, Gang
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#362245" title="Click to go to the Author Index">
             Wu, Yuchen
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416165" title="Click to go to the Author Index">
             Wu, Huifeng
            </a>
           </td>
           <td class="r">
            Hangzhou Dianzi University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#122966" title="Click to go to the Author Index">
             Liu, Yong
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4229" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#path_planning_for_multiple_mobile_robots_or_agents" title="Click to go to the Keyword Index">
               Path Planning for Multiple Mobile Robots or Agents
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Multi-agent path finding (MAPF) in dynamic and complex environments is a highly challenging task. Recent research has often focused on the scalability of the number of robots or the complexity of the environment. Usually, they disregard the robots' physical models or use a differential drive robot. However, this approach fails to adequately capture the kinematic and dynamic constraints of real-world vehicles, particularly those equipped with Ackermann steering in warehousing applications. This paper presents a novel MAPF algorithm that combines reinforcement learning (RL) with a lattice planner. RL provides strong generalization capabilities while maintaining computational efficiency. By incorporating lattice planner trajectories into the action space of the RL framework, agents are capable of generating smooth and feasible paths that respect the kinematic and dynamic constraints. In addition, we adopt a decentralized training and execution framework, where a network of shared value functions enables efficient cooperation among agents during decision-making. Simulation results and real-world experiments in different scenarios demonstrate that our method achieves superior performance in terms of success rate, average speed, extra distance of trajectory, and computing time.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt10_06">
             10:20-10:25, Paper ThBT10.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4441'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Robust Self-Reconfiguration for Fault-Tolerant Control of Modular Aerial Robot Systems
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426146" title="Click to go to the Author Index">
             Huang, Rui
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426209" title="Click to go to the Author Index">
             Tang, Siyu
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426185" title="Click to go to the Author Index">
             Cai, Zhiqian
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#187951" title="Click to go to the Author Index">
             Zhao, Lin
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4441" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#cellular_and_modular_robots" title="Click to go to the Keyword Index">
               Cellular and Modular Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#failure_detection_and_recovery" title="Click to go to the Keyword Index">
               Failure Detection and Recovery
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Modular Aerial Robotic Systems (MARS) consist of multiple drone units assembled into a single, integrated rigid flying platform. With inherent redundancy, MARS can self-reconfigure into different configurations to mitigate rotor or unit failures and maintain stable flight. However, existing works on MARS self-reconfiguration often overlook the practical controllability of intermediate structures formed during the reassembly process, which limits their applicability. In this paper, we address this gap by considering the control-constrained dynamic model of MARS and proposing a robust and efficient self-reconstruction algorithm that maximizes the controllability margin at each intermediate stage. Specifically, we develop algorithms to compute optimal, controllable disassembly and assembly sequences, enabling robust self-reconfiguration. Finally, we validate our method in several challenging fault-tolerant self-reconfiguration scenarios, demonstrating significant improvements in both controllability and trajectory tracking while reducing the number of assembly steps. The videos and source code of this work are available at https://github.com/RuiHuangNUS/MARS-Reconfig/
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt10_07">
             10:25-10:30, Paper ThBT10.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4902'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Where Are You? Unscented Particle Filter for Single Range Relative Pose Estimation in Unobservable Motion Using UWB and VIO
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#335336" title="Click to go to the Author Index">
             Durodié, Yuri
            </a>
           </td>
           <td class="r">
            Vrije Universiteit Brussel
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#207862" title="Click to go to the Author Index">
             Convens, Bryan
            </a>
           </td>
           <td class="r">
            Vrije Universiteit Brussel
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#309962" title="Click to go to the Author Index">
             Liu, Gaoyuan
            </a>
           </td>
           <td class="r">
            Vrije Universiteit Brussel
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#335338" title="Click to go to the Author Index">
             Decoster, Thomas
            </a>
           </td>
           <td class="r">
            Vrije Universiteit Brussel
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#336010" title="Click to go to the Author Index">
             Munteanu, Adrian
            </a>
           </td>
           <td class="r">
            Vrije Universiteit Brussel
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101955" title="Click to go to the Author Index">
             Vanderborght, Bram
            </a>
           </td>
           <td class="r">
            Vrije Universiteit Brussel
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4902" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#sensor_fusion" title="Click to go to the Keyword Index">
               Sensor Fusion
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Real-time relative pose (RP) estimation is a corner- stone for effective multi-agent collaboration. When conventional global positioning infrastructure such as GPS is unavailable, the use of Ultra-Wideband (UWB) technology on each agent provides a practical means to measure inter-agent range, eliminating the need for external hardware installations, due to UWB’s precise range measurements and robust communi- cation capabilities. However, when only a single UWB device per agent is used, the relative pose between the agents can be unobservable, resulting in a complex solution space with multiple possible RPs. In this paper, a novel method is proposed based on an Unscented Particle Filter (UPF) that fuses single UWB ranges with visual-inertial odometry (VIO). The proposed decentralized method solves the multi-modal solution in 3D (4-DoF) for the RP when it is unobservable. Moreover, a pseudo-state is introduced to correct for rotational drift of the agents. Through simulations and experiments involving two robots, the proposed solution was shown to be competitive, but less computationally expensive. Additionally, the proposed solution provides all possible relative poses from the first measurement. The code and link to the video are available https://github.com/y2d2/UPF_RPE.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt11">
             <b>
              ThBT11
             </b>
            </a>
           </td>
           <td class="r">
            314
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thbt11" title="Click to go to the Program at a Glance">
             <b>
              Robot Vision 1
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#106118" title="Click to go to the Author Index">
             Malis, Ezio
            </a>
           </td>
           <td class="r">
            Inria
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt11_01">
             09:55-10:00, Paper ThBT11.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('277'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Asynchronous Blob Tracker for Event Cameras
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#299924" title="Click to go to the Author Index">
             Wang, Ziwei
            </a>
           </td>
           <td class="r">
            Australian National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#159746" title="Click to go to the Author Index">
             Molloy, Timothy L.
            </a>
           </td>
           <td class="r">
            Australian National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#241273" title="Click to go to the Author Index">
             van Goor, Pieter
            </a>
           </td>
           <td class="r">
            University of Twente
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#105364" title="Click to go to the Author Index">
             Mahony, Robert
            </a>
           </td>
           <td class="r">
            Australian National University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab277" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__perception_and_autonomy" title="Click to go to the Keyword Index">
               Aerial Systems: Perception and Autonomy
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#visual_tracking" title="Click to go to the Keyword Index">
               Visual Tracking
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#event_cameras" title="Click to go to the Keyword Index">
               Event Cameras
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Event-based cameras are popular for tracking fast-moving objects due to their high temporal resolution, low latency, and high dynamic range. In this paper, we propose a novel algorithm for tracking event blobs using raw events asynchronously in real time. We introduce the concept of an event blob as a spatio-temporal likelihood of event occurrence where the conditional spatial likelihood is blob-like. Many real-world objects such as car headlights or any quickly moving foreground objects generate event blob data. The proposed algorithm uses a nearest neighbour classifier with a dynamic threshold criteria for data association coupled with an extended Kalman filter to track the event blob state. Our algorithm achieves highly accurate blob tracking, velocity estimation, and shape estimation even under challenging lighting conditions and high-speed motions (&gt; 11000 pixels/s). The microsecond time resolution achieved means that the filter output can be used to derive secondary information such as time-to-contact or range estimation, that will enable applications to real-world problems such as collision avoidance in autonomous driving.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt11_02">
             10:00-10:05, Paper ThBT11.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('806'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Deep Height Decoupling for Precise Vision-Based 3D Occupancy Prediction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#415913" title="Click to go to the Author Index">
             Wu, Yuan
            </a>
           </td>
           <td class="r">
            Nanjing University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416639" title="Click to go to the Author Index">
             Yan, Zhiqiang
            </a>
           </td>
           <td class="r">
            Nanjing University of Science and Tenchnology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416637" title="Click to go to the Author Index">
             Wang, Zhengxue
            </a>
           </td>
           <td class="r">
            Nanjing University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416680" title="Click to go to the Author Index">
             Li, Xiang
            </a>
           </td>
           <td class="r">
            Nankai University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#276202" title="Click to go to the Author Index">
             Hui, Le
            </a>
           </td>
           <td class="r">
            Nanjing University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#203342" title="Click to go to the Author Index">
             Yang, Jian
            </a>
           </td>
           <td class="r">
            Nanjing University of Science &amp; Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab806" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_transportation" title="Click to go to the Keyword Index">
               Computer Vision for Transportation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The task of vision-based 3D occupancy prediction aims to reconstruct 3D geometry and estimate its semantic classes from 2D color images, where the 2D-to-3D view transformation is an indispensable step. Most previous methods conduct forward projection, such as BEVPooling and VoxelPooling, both of which map the 2D image features into 3D grids. However, the current grid representing features within a certain height range usually introduces many confusing features that belong to other height ranges. To address this challenge, we present Deep Height Decoupling (DHD), a novel framework that incorporates explicit height prior to filter out the confusing features. Specifically, DHD first predicts height maps via explicit supervision. Based on the height distribution statistics, DHD designs Mask Guided Height Sampling (MGHS) to adaptively decouple the height map into multiple binary masks. MGHS projects the 2D image features into multiple subspaces, where each grid contains features within reasonable height ranges. Finally, a Synergistic Feature Aggregation (SFA) module is deployed to enhance the feature representation through channel and spatial affinities, enabling further occupancy refinement. On the popular Occ3D-nuScenes benchmark, our method achieves state-of-the-art performance even with minimal input frames. Source code is released at https://github.com/yanzq95/DHD.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt11_03">
             10:05-10:10, Paper ThBT11.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1832'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              RE0: Recognize Everything with 3D Zero-Shot Instance Segmentation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#413315" title="Click to go to the Author Index">
             Yan, Xiaohan
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421983" title="Click to go to the Author Index">
             Jiang, Zijian
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421987" title="Click to go to the Author Index">
             Shuai, Yinghao
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422183" title="Click to go to the Author Index">
             Wang, Nan
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420807" title="Click to go to the Author Index">
             Song, Xiaowei
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417809" title="Click to go to the Author Index">
             Ji, Wenbo
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422368" title="Click to go to the Author Index">
             Wu, Ge
            </a>
           </td>
           <td class="r">
            Nankai University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#413304" title="Click to go to the Author Index">
             He, Jinyu
            </a>
           </td>
           <td class="r">
            Xiamen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422457" title="Click to go to the Author Index">
             Wei, Gang
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422446" title="Click to go to the Author Index">
             Wang, Zhicheng
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1832" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#embodied_cognitive_science" title="Click to go to the Keyword Index">
               Embodied Cognitive Science
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Recognizing objects in the 3D world is a significant challenge for robotics. Due to the lack of high-quality 3D data, directly training a general-purpose segmentation model in 3D is almost infeasible. Meanwhile, vision foundation models (VFM) have revolutionized the 2D computer vision field with outstanding performance, making the use of VFM to assist 3D perception a promising direction. However, most existing VFM-assisted methods do not effectively address the 2D-3D inconsistency problem or adequately provide corresponding semantic information for 3D instance objects. To address these two issues, this paper introduces a novel framework for 3D zero-shot instance segmentation called RE0. For the given 3D point clouds and multi-view RGB-D images with poses, we leverage the 3D geometric information, projection relationships, and CLIP semantic features. Specifically, we utilize CropFormer to extract mask information from multi-view posed images, combined with projection relationships to assign point-level labels to each point in the point cloud, and achieve instance- level consistency through inter-frame information interaction. Then, we employ projection relationships again to assign CLIP semantic features to the point cloud and achieve aggregation of small-scale point clouds. Notably, RE0 does not require any additional training and can be implemented by supporting only one inference of CropFormer and one inference of CLIP. Experiments on ScanNet200 and ScanNet++ show that our method achieves higher quality segmentation than the previous zero-shot methods. Our codes and demos are available at https://recognizeeverything.github.io/, with only one RTX 3090 GPU required.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt11_04">
             10:10-10:15, Paper ThBT11.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1929'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              PTQ4RIS: Post-Training Quantization for Referring Image Segmentation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#334707" title="Click to go to the Author Index">
             Jiang, Xiaoyan
            </a>
           </td>
           <td class="r">
            Shanghai University of Engineering Science
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416304" title="Click to go to the Author Index">
             Yang, Hang
            </a>
           </td>
           <td class="r">
            Shanghai University of Engineering Science
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421761" title="Click to go to the Author Index">
             Zhu, Kaiying
            </a>
           </td>
           <td class="r">
            SenseTime
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#376885" title="Click to go to the Author Index">
             Qiu, Xihe
            </a>
           </td>
           <td class="r">
            Shanghai University of Engineering Science
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#226965" title="Click to go to the Author Index">
             Zhao, Shibo
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#267572" title="Click to go to the Author Index">
             Zhou, Sifan
            </a>
           </td>
           <td class="r">
            Southeast University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1929" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#robotics_in_under_resourced_settings" title="Click to go to the Keyword Index">
               Robotics in Under-Resourced Settings
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Referring Image Segmentation (RIS), aims to segment the object referred by a given sentence in an image by understanding both visual and linguistic information. However, existing RIS methods tend to explore top-performance models, disregarding considerations for practical applications on resources-limited edge devices. This oversight poses a significant challenge for on-device RIS inference. To this end, we propose an effective and efficient post-training quantization framework termed PTQ4RIS. Specifically, we first conduct an in-depth analysis of the root causes of performance degradation in RIS model quantization and propose dual-region quantization (DRQ) and reorder-based outlier-retained quantization (RORQ) to address the quantization difficulties in visual and text encoders. Extensive experiments on three benchmarks with different bits settings (from 8 to 4 bits) demonstrates its superior performance. Importantly, we are the first PTQ method specifically designed for the RIS task, highlighting the feasibility of PTQ in RIS applications.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt11_05">
             10:15-10:20, Paper ThBT11.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2294'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              LeAP: Consistent Multi-Domain 3D Labeling Using Foundation Models
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420326" title="Click to go to the Author Index">
             Gebraad, Simon
            </a>
           </td>
           <td class="r">
            Delft University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#256370" title="Click to go to the Author Index">
             Palffy, Andras
            </a>
           </td>
           <td class="r">
            Delft University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#270117" title="Click to go to the Author Index">
             Caesar, Holger
            </a>
           </td>
           <td class="r">
            TU Delft
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2294" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#data_sets_for_robotic_vision" title="Click to go to the Keyword Index">
               Data Sets for Robotic Vision
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#sensor_fusion" title="Click to go to the Keyword Index">
               Sensor Fusion
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Availability of datasets is a strong driver for research on 3D semantic understanding, and whilst obtaining unlabeled 3D data is straightforward, manually annotating this data with semantic labels is time-consuming and costly. As a result, labeled 3D datasets have largely been confined to the popular automotive domain due to the abundance of labeled data. Recently, Vision Foundation Models (VFMs) enable open-set semantic segmentation, potentially aiding automatic labeling. However, VFMs for 3D data have been limited to adaptations of 2D models, which can introduce inconsistencies to 3D labels. This work introduces Label Any Pointcloud (LeAP), leveraging 2D VFMs to automatically label multi-frame 3D data with any set of classes in any kind of application whilst ensuring label consistency. Using a Bayesian update, point labels are combined into voxels to improve spatio-temporal consistency. A novel 3D Consistency Network (3D-CN) further enhances geometric consistency. Through various experiments, we show that our method can generate high-quality 3D semantic labels across diverse fields without any manual labeling. Further, models adapted to new domains using our labels show a significant mIoU increase in semantic segmentation tasks.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt11_06">
             10:20-10:25, Paper ThBT11.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4595'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              PlaceFormer: Transformer-Based Visual Place Recognition Using Multi-Scale Patch Selection and Fusion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#226469" title="Click to go to the Author Index">
             Kannan, Shyam Sundar
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#164680" title="Click to go to the Author Index">
             Min, Byung-Cheol
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4595" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Visual place recognition is a challenging task in the field of computer vision, and autonomous robotics and vehicles, which aims to identify a location or a place from visual inputs. Contemporary methods in visual place recognition employ convolutional neural networks and utilize every region within the image for the place recognition task. However, the presence of dynamic and distracting elements in the image can impact the effectiveness of the place recognition process. Therefore, it is meaningful to focus on the task-relevant regions of the image for improved recognition. In this paper, we present PlaceFormer, a novel transformer-based approach for visual place recognition. PlaceFormer uses patch tokens from the transformer to create global image descriptors, which are then used for image retrieval. To re-rank the retrieved images, PlaceFormer merges the patch tokens from the transformer to form multi-scale patches. Utilizing the transformer's self-attention mechanism, it selects patches that correspond to task-relevant areas in an image. These selected patches undergo geometric verification, generating similarity scores across different patch sizes. Subsequently, the spatial scores from each patch size are fused to produce a final similarity score. This score is then used to re-rank the images initially retrieved using global image descriptors. Extensive experiments on benchmark datasets demonstrate that PlaceFormer outperforms several state-of-the-art methods in terms of accuracy and computational efficiency, requiring less time and memory.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt11_07">
             10:25-10:30, Paper ThBT11.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('5042'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Motion-Aware Optical Camera Communication with Event Cameras
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#334962" title="Click to go to the Author Index">
             Su, Hang
            </a>
           </td>
           <td class="r">
            ShanghaiTech University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#267634" title="Click to go to the Author Index">
             Gao, Ling
            </a>
           </td>
           <td class="r">
            ShanghaiTech University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#336746" title="Click to go to the Author Index">
             Liu, Tao
            </a>
           </td>
           <td class="r">
            ShanghaiTech University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#123271" title="Click to go to the Author Index">
             Kneip, Laurent
            </a>
           </td>
           <td class="r">
            ShanghaiTech University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab5042" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#visual_tracking" title="Click to go to the Keyword Index">
               Visual Tracking
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#automation_technologies_for_smart_cities" title="Click to go to the Keyword Index">
               Automation Technologies for Smart Cities
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             As the ubiquity of smart mobile devices continues to rise, Optical Camera Communication systems have gained more attention as a solution for efficient and private data streaming. This system utilizes optical cameras to receive data from digital screens via visible light. Despite
             <p>
              their promise, most of them are hindered by dynamic factors such as screen refreshing and rapid camera motion. CMOS cameras, often serving as the receivers, suffer from limited frame rates and motion-induced image blur, which degrade overall performance. To address these challenges, this paper unveils a novel system that utilizes event cameras. We introduce a dynamic visual marker and design event-based tracking algorithms to achieve fast localization and data streaming. Remarkably, the event camera's unique capabilities mitigate issues related to screen refresh rates and camera motion, enabling a high throughput of up to 114 Kbps in static conditions, and a 1 cm localization accuracy with 1% bit error rate under various camera motions. We plan on open-sourcing the code upon acceptance.
             </p>
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt12">
             <b>
              ThBT12
             </b>
            </a>
           </td>
           <td class="r">
            315
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thbt12" title="Click to go to the Program at a Glance">
             <b>
              Applications in the Wild
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#138305" title="Click to go to the Author Index">
             Kelasidi, Eleni
            </a>
           </td>
           <td class="r">
            NTNU
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#114045" title="Click to go to the Author Index">
             Hutter, Marco
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt12_01">
             09:55-10:00, Paper ThBT12.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('695'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Hybrid State Estimation and Mode Identification of an Amphibious Robot
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#296277" title="Click to go to the Author Index">
             Amundsen, Herman Biørn
            </a>
           </td>
           <td class="r">
            NTNU
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#239675" title="Click to go to the Author Index">
             Randeni, Supun
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#407098" title="Click to go to the Author Index">
             Bingham, Russell
            </a>
           </td>
           <td class="r">
            Pliant Energy Systems Inc
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#407099" title="Click to go to the Author Index">
             Civit, Carles
            </a>
           </td>
           <td class="r">
            Pliant Energy Systems Inc
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#407100" title="Click to go to the Author Index">
             Filardo, Benjamin Pietro
            </a>
           </td>
           <td class="r">
            Pliant Energy Systems Inc
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#379062" title="Click to go to the Author Index">
             Føre, Martin
            </a>
           </td>
           <td class="r">
            NTNU
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#138305" title="Click to go to the Author Index">
             Kelasidi, Eleni
            </a>
           </td>
           <td class="r">
            NTNU
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#105323" title="Click to go to the Author Index">
             Benjamin, Michael
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab695" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#discrete_event_dynamic_automation_systems" title="Click to go to the Keyword Index">
               Discrete Event Dynamic Automation Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#biologically_inspired_robots" title="Click to go to the Keyword Index">
               Biologically-Inspired Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             C-Ray is an amphibious robot that is capable of swimming in water and crawling on land using its undulating fins, enabling operations in a wide range of environments. The robot can be modeled as a hybrid dynamical system whose dynamics and propulsion change when the robot transitions between water and land. Most importantly, the direction of wave travel in the robot's fins is reversed between its swimming and crawling locomotion styles. To operate autonomously, C-Ray requires both accurate identification of when transitions between water and land occur and robust state estimation in littoral environments where the transition dynamics are highly discontinuous and transient. This paper presents a hybrid observer for estimating continuous states and identifying state-driven mode switches for C-Ray, enabling autonomous water/land-transitions. The proposed observer is a combination of the multiplicative extended Kalman filter (MEKF) and the salted Kalman filter, a newly proposed Kalman filter for mapping state uncertainty during hybrid transitions. We also propose an altitude and sea floor geometry observer and incorporate this directly into the MEKF. The performance is evaluated in simulations.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt12_02">
             10:00-10:05, Paper ThBT12.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1555'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              LiDARDustX: A LiDAR Dataset for Dusty Unstructured Road Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416735" title="Click to go to the Author Index">
             Wei, Chenfeng
            </a>
           </td>
           <td class="r">
            Wuxi Intelligent Control Research Institute, HNU
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#414235" title="Click to go to the Author Index">
             Wu, Qi
            </a>
           </td>
           <td class="r">
            Wuxi Intelligent Control Research Institute, Hunan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417035" title="Click to go to the Author Index">
             Zuo, Si
            </a>
           </td>
           <td class="r">
            Hunan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421879" title="Click to go to the Author Index">
             Xu, Jiahua
            </a>
           </td>
           <td class="r">
            Wuxi Intelligent Control Research Institute, Hunan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421860" title="Click to go to the Author Index">
             Zhao, Boyang
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421869" title="Click to go to the Author Index">
             Zeyu, Yang
            </a>
           </td>
           <td class="r">
            Hunan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421844" title="Click to go to the Author Index">
             Guotao, Xie
            </a>
           </td>
           <td class="r">
            Hunan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421851" title="Click to go to the Author Index">
             Shenhong, Wang
            </a>
           </td>
           <td class="r">
            Xi'an Jiaotong-Liverpool University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1555" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#data_sets_for_robotic_vision" title="Click to go to the Keyword Index">
               Data Sets for Robotic Vision
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_transportation" title="Click to go to the Keyword Index">
               Computer Vision for Transportation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Abstract— Autonomous driving datasets are essential for validating the progress of intelligent vehicle algorithms, which　include localization, perception, and prediction. However, existing datasets are predominantly focused on structured urban　environments, which limits the exploration of unstructured　and specialized scenarios, particularly those characterized by　significant dust levels. This paper introduces the LiDARDustX　dataset, which is specifically designed for perception tasks under　high-dust conditions, such as those encountered in mining areas.　The LiDARDustX dataset consists of 30,000 LiDAR frames　captured by six different LiDAR sensors, each accompanied by　3D bounding box annotations and point cloud semantic segmentation. Notably, over 80% of the dataset comprises dust-affected　scenes. By utilizing this dataset, we have established a benchmark for evaluating the performance of state-of-the-art 3D detection and segmentation algorithms. Additionally, we have ana- lyzed the impact of dust on perception accuracy and delved into　the causes of these effects. The data and further information can　be accessed at: https://github.com/vincentweikey/LiDARDustX.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt12_03">
             10:05-10:10, Paper ThBT12.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1786'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              How about Them Apples: 3D Pose and Cluster Estimation of Apple Fruitlets in a Commercial Orchard
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#294011" title="Click to go to the Author Index">
             Qureshi, Ans
            </a>
           </td>
           <td class="r">
            University of Auckland
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#353154" title="Click to go to the Author Index">
             Smith, David Anthony James
            </a>
           </td>
           <td class="r">
            University of Auckland
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#339296" title="Click to go to the Author Index">
             Gee, Trevor
            </a>
           </td>
           <td class="r">
            The University of Auckland
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#111352" title="Click to go to the Author Index">
             Ahn, Ho Seok
            </a>
           </td>
           <td class="r">
            The University of Auckland, Auckland
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#353143" title="Click to go to the Author Index">
             McGuinness, Benjamin John
            </a>
           </td>
           <td class="r">
            University of Waikato
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#353180" title="Click to go to the Author Index">
             Downes, Catherine
            </a>
           </td>
           <td class="r">
            University of Waikato
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#353148" title="Click to go to the Author Index">
             Jangali, Rahul
            </a>
           </td>
           <td class="r">
            The University of Waikato
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#353141" title="Click to go to the Author Index">
             Black, Kale
            </a>
           </td>
           <td class="r">
            Black Box Technologies LTD
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#119307" title="Click to go to the Author Index">
             Lim, Shen Hin
            </a>
           </td>
           <td class="r">
            University of Waikato
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#269339" title="Click to go to the Author Index">
             Duke, Mike
            </a>
           </td>
           <td class="r">
            Waikato University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#107803" title="Click to go to the Author Index">
             MacDonald, Bruce
            </a>
           </td>
           <td class="r">
            University of Auckland
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#170065" title="Click to go to the Author Index">
             Williams, Henry
            </a>
           </td>
           <td class="r">
            University of Auckland
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1786" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#robotics_and_automation_in_agriculture_and_forestry" title="Click to go to the Keyword Index">
               Robotics and Automation in Agriculture and Forestry
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#agricultural_automation" title="Click to go to the Keyword Index">
               Agricultural Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#field_robots" title="Click to go to the Keyword Index">
               Field Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Aotearoa’s apple industry struggles to maintain the skilled workforce required for fruitlet thinning each year. Skilled labourers play a pivotal role in managing crop loads by precisely thinning fruitlets to achieve the desired spacing for high-quality apple growth. This complex task requires accurate mapping of the fruitlets along each branch. This paper presents a novel vision system capable of mapping the orientation and clustering information of apple fruitlets as a human expert does. The vision system has been validated against data collected from a real-world commercial apple orchard. The results show an improved counting accuracy of 83.97% over prior implementations, an orientation estimate accuracy of 88.1%, and a clustering accuracy of 94.3%. Future work will utilise this information to determine which fruitlets to remove and then robotically thin them from the canopy.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt12_04">
             10:10-10:15, Paper ThBT12.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1818'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Active Semantic Mapping with Mobile Manipulator in Horticultural Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#341043" title="Click to go to the Author Index">
             Cuaran, Jose
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#411400" title="Click to go to the Author Index">
             Singh Ahluwalia, Kulbir
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana Champaing
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#371470" title="Click to go to the Author Index">
             Koe, Kendall
            </a>
           </td>
           <td class="r">
            University of Illinois Urbana Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#182532" title="Click to go to the Author Index">
             Uppalapati, Naveen Kumar
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#133090" title="Click to go to the Author Index">
             Chowdhary, Girish
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana Champaign
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1818" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#robotics_and_automation_in_agriculture_and_forestry" title="Click to go to the Keyword Index">
               Robotics and Automation in Agriculture and Forestry
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#agricultural_automation" title="Click to go to the Keyword Index">
               Agricultural Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Semantic maps are fundamental for robotics tasks such as navigation and manipulation. They also enable yield prediction and phenotyping in agricultural settings. In this paper, we introduce an efficient and scalable approach for active semantic mapping in horticultural environments, employing a mobile robot manipulator equipped with an RGB-D camera. Our method leverages probabilistic semantic maps to detect semantic targets, generate candidate viewpoints, and compute the corresponding information gain. We present an efficient ray-casting strategy and a novel information utility function that accounts for both semantics and occlusions. The proposed approach reduces total runtime by 8% compared to previous baselines. Furthermore, our information metric surpasses other metrics in reducing multiclass entropy and improving surface coverage, particularly in the presence of segmentation noise. Real-world experiments validate our method's effectiveness but also reveal challenges such as depth sensor noise and varying environmental conditions, requiring further research.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt12_05">
             10:15-10:20, Paper ThBT12.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2466'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Surface Roughness Estimation for Terrain Perception
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424266" title="Click to go to the Author Index">
             Ye, Minxiang
            </a>
           </td>
           <td class="r">
            Zhejiang Lab
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#356706" title="Click to go to the Author Index">
             Zhang, Yifei
            </a>
           </td>
           <td class="r">
            Beihang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#103269" title="Click to go to the Author Index">
             Gu, Jason
            </a>
           </td>
           <td class="r">
            Dalhousie University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424272" title="Click to go to the Author Index">
             Xiang, Senwei
            </a>
           </td>
           <td class="r">
            Hangzhou International Innovation Institute, Beihang University,
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#206266" title="Click to go to the Author Index">
             Kong, Lingyu
            </a>
           </td>
           <td class="r">
            Zhejiang Lab
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#209600" title="Click to go to the Author Index">
             Xie, Anhuan
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2466" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Ground terrain perception has become the primary visual task for the robust navigation of intelligent systems in unstructured outdoor environments. However, complex terrain poses a significant challenge to vision-based perception. This work introduces a novel estimation task using RGB images to facilitate low-cost terrain perception in extracting surface roughness information. The proposed task presents both semantic-aware and edge-aware roughness descriptors at the pixel level instead of a single value for a given image. To promote the research on the proposed novel terrain roughness estimation task, we introduce a multimodal synthetic dataset for terrain perception in outdoor scenes, containing multiple terrain categories, diverse viewpoints, different lighting and weather conditions, as well as semantic and roughness annotations. Additionally, inspired by computer graphics, we introduce TRENet, a roughness estimation architecture to model the intrinsic correlation of depth-normal-roughness. We also perform ablation studies on the effect of each component and diverse types of inputs. Extensive evaluations and comparisons demonstrate that our method can effectively predict pixel-wise terrain surface roughness with high accuracy.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt12_06">
             10:20-10:25, Paper ThBT12.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4816'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Automatic Identification of Individual African Leopards in Unlabeled Camera Trap Images (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#428721" title="Click to go to the Author Index">
             Guo, Cheng
            </a>
           </td>
           <td class="r">
            Colorado State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#429232" title="Click to go to the Author Index">
             Miguel, Agnieszka
            </a>
           </td>
           <td class="r">
            Seattle University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#106168" title="Click to go to the Author Index">
             Maciejewski, Anthony A.
            </a>
           </td>
           <td class="r">
            Colorado State University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4816" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This article describes an algorithm to solve the real-world animal identification problem, i.e., determine the unknown number of K individual animals in a dataset of N unlabeled camera-trap images of African leopards, provided by Panthera. To determine the leopards’ IDs, we propose an effective automated algorithm, that consists of segmenting leopard bodies from images, scoring similarity between image pairs, and clustering followed by verification. To perform clustering, we employ a modified ternary search that uses a novel adaptive k-medoids++ clustering algorithm. The best clustering is determined using an expanded definition of the silhouette score. A new post-clustering verification procedure is used to further improve the quality of a clustering. The algorithm was evaluated using the Panthera dataset that consists of 677 individual leopards taken from 1555 images, and resulted in a clustering with an adjusted mutual information score of 0.958 as compared to 0.864 using a baseline k-medoids++ clustering algorithm.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt12_07">
             10:25-10:30, Paper ThBT12.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4894'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              RoadRunner M&amp;M: Learning Multi-Range Multi-Resolution Traversability Maps for Autonomous Off-Road Navigation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#334640" title="Click to go to the Author Index">
             Patel, Manthan
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#291196" title="Click to go to the Author Index">
             Frey, Jonas
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#325403" title="Click to go to the Author Index">
             Atha, Deegan
            </a>
           </td>
           <td class="r">
            Jet Propulsion Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#256947" title="Click to go to the Author Index">
             Spieler, Patrick
            </a>
           </td>
           <td class="r">
            JPL
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#114045" title="Click to go to the Author Index">
             Hutter, Marco
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#204077" title="Click to go to the Author Index">
             Khattak, Shehryar
            </a>
           </td>
           <td class="r">
            NASA Jet Propulsion Laboratory
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4894" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#field_robots" title="Click to go to the Keyword Index">
               Field Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Autonomous robot navigation in off-road environments requires a comprehensive understanding of the terrain geometry and traversability. The degraded perceptual conditions and sparse geometric information at longer ranges make the problem challenging especially when driving at high speeds. Furthermore, the sensing-to-mapping latency and the look-ahead map range can limit the maximum speed of the vehicle. Building on top of the recent work RoadRunner, in this work, we address the challenge of long-range (±100m) traversability estimation. Our RoadRunner (M&amp;M) is an end-to-end learning-based framework that directly predicts the traversability and elevation maps at multiple ranges (±50m, ±100m) and resolutions (0.2m, 0.8m) taking as input multiple images and a LiDAR voxel map. Our method is trained in a self-supervised manner by leveraging the dense supervision signal generated by fusing predictions from an existing traversability estimation stack (X-Racer) in hindsight and satellite Digital Elevation Maps. RoadRunner M&amp;M achieves a significant improvement of up to 50% for elevation mapping and 30% for traversability estimation over RoadRunner, and is able to predict in 30% more regions compared to X-Racer while achieving real-time performance. Experiments on various out-of-distribution datasets also demonstrate that our data-driven approach starts to generalize to novel unstructured environments. We integrate our proposed framework in closed-loop with the path planner to demonstrate autonomous high-speed off-road robotic navigation in challenging real-world environments. Project Page-https://leggedrobotics.github.io/roadrunner_mm
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt13">
             <b>
              ThBT13
             </b>
            </a>
           </td>
           <td class="r">
            316
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thbt13" title="Click to go to the Program at a Glance">
             <b>
              Perception Systems
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt13_01">
             09:55-10:00, Paper ThBT13.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('939'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              RipGAN: A GAN-Based Rip Current Data Augmentation Method
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419936" title="Click to go to the Author Index">
             Qian, Shenyang
            </a>
           </td>
           <td class="r">
            UNSW Sydney
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419983" title="Click to go to the Author Index">
             Harley, Mitchell Dean
            </a>
           </td>
           <td class="r">
            UNSW Sydney
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419998" title="Click to go to the Author Index">
             Razzak, Imran
            </a>
           </td>
           <td class="r">
            MBZUAI
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#339638" title="Click to go to the Author Index">
             Song, Yang
            </a>
           </td>
           <td class="r">
            University of New South Wales
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab939" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#data_sets_for_robotic_vision" title="Click to go to the Keyword Index">
               Data Sets for Robotic Vision
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Rip currents are a major hazard on beaches worldwide, and their strong, offshore-directed currents can place even experienced beachgoers at risk of drowning. While it is intuitive to consider developing an automated rip current detection system to assist lifeguards in protecting beachgoers, rip current detection is in its infancy due to the lack of high-quality large-scale annotated rip current datasets. Also, the collection and annotation of rip current images require expert knowledge, which makes it more difficult to build datasets. So, this paper proposes a GAN-based rip current data augmentation method, RipGAN, to improve the performance of rip current detectors by increasing representative training data. To create new training images, RipGAN, has two branches. One is a texture generator that enriches the pattern and texture details of waves, making the image more realistic. The other is a rip generator based on FFFM-Unet. FFFM (Fast Fourier Fusion Module) uses Fast Fourier convolution to fuse the features from the low and the high layers, so as to further optimise the generated image. Furthermore, we trained Yolov8, YOLOv10, DINO and RT-DETR as rip current detectors to prove the effectiveness of RipGAN. The detectors' mAP
             <sub>
              50:95
             </sub>
             improved by 2.67% on the test set and AP
             <sub>
              50
             </sub>
             by 4.93% on real-scene videos, outperforming other data augmentation methods. Besides, abundant ablation studies have been conducted to further evaluate each component of RipGAN.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt13_02">
             10:00-10:05, Paper ThBT13.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1360'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Points, Images and Texts: Boosting Point Cloud Completion with Multi-Modal Features
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420498" title="Click to go to the Author Index">
             Xia, ChengKai
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#273028" title="Click to go to the Author Index">
             Lu, Fan
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#322958" title="Click to go to the Author Index">
             Li, Bin
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#402372" title="Click to go to the Author Index">
             Yu, Guo
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#105608" title="Click to go to the Author Index">
             Knoll, Alois
            </a>
           </td>
           <td class="r">
            Tech. Univ. Muenchen TUM
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#157845" title="Click to go to the Author Index">
             Chen, Guang
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1360" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Point cloud completion is crucial for reconstructing accurate shapes in many 3D visual applications. Recent approaches incorporate images into the completion pipeline, introducing geometric clues and global constraints. However, their fusion processes often fail to reconstruct detailed parts and maintain global consistency simultaneously. Except for images, text is another important clue for recognizing the target’s characteristics. Thus, in this work, we propose to combine multiple modalities including points, images and texts for point cloud completion. Specifically, inspired by recently pre-trained large language models, we generate the description texts for images by Visual Question Answering (VQA) models and introduce Visual-Textual Embedding (VTE) models to extract joint features of image-text pairs. Furthermore, we describe the edge geometric patterns by multi-scale edge convolution to guide the refinement of shapes in local areas. Then we adopt cross attention mechanism to effectively fuse multi-modal features and refine the coarse shape. Extensive experiments on the ShapeNet-ViPC benchmark demonstrate our method’s superior performance over previous uni-modal and cross-modal methods.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt13_03">
             10:05-10:10, Paper ThBT13.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1444'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              3DWG: 3D Weakly Supervised Visual Grounding Via Category and Instance-Level Alignment
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#372289" title="Click to go to the Author Index">
             Li, Xiaoqi
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#370243" title="Click to go to the Author Index">
             Liu, Jiaming
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#437795" title="Click to go to the Author Index">
             Han, Nuowei
            </a>
           </td>
           <td class="r">
            Beijing University of Posts and Telecommunications
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#438638" title="Click to go to the Author Index">
             Heng, Liang
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#305181" title="Click to go to the Author Index">
             Guo, Yandong
            </a>
           </td>
           <td class="r">
            OPPO Research Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#280114" title="Click to go to the Author Index">
             Dong, Hao
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#442036" title="Click to go to the Author Index">
             Liu, Yang
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1444" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The 3D weakly-supervised visual grounding task aims to localize oriented 3D boxes in point clouds based on natural language descriptions without requiring annotations to guide model learning. This setting presents two primary challenges: category-level ambiguity and instance-level complexity. Category-level ambiguity arises from representing objects of fine-grained categories in a highly sparse point cloud format, making category distinction challenging. Instance-level complexity stems from multiple instances of the same category coexisting in a scene, leading to distractions during grounding. To address these challenges, we propose a novel weakly-supervised grounding approach that explicitly differentiates between categories and instances. In the category-level branch, we utilize extensive category knowledge from a pre-trained external detector to align object proposal features with sentence-level category features, thereby enhancing category awareness. In the instance-level branch, we utilize spatial relationship descriptions from language queries to refine object proposal features, ensuring clear differentiation among objects. These designs enable our model to accurately identify target-category objects while distinguishing instances within the same category. Compared to previous methods, our approach achieves state-of-the-art performance on three widely used benchmarks: Nr3D, Sr3D, and ScanRef.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt13_04">
             10:10-10:15, Paper ThBT13.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3701'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              MPI-Mamba : Cross Propagation Mamba for Multipath Interference Correction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424537" title="Click to go to the Author Index">
             An, Kang
            </a>
           </td>
           <td class="r">
            ShenZhen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424543" title="Click to go to the Author Index">
             Jiang, ZhaoXiang
            </a>
           </td>
           <td class="r">
            Guangdong Laboratory of Artificial Intelligence and Digital Econ
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424553" title="Click to go to the Author Index">
             Tian, Jindong
            </a>
           </td>
           <td class="r">
            Guangdong Laboratory of Artificial Intelligence and Digital Econ
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3701" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Owing to their compact structure, high stability,and low cost, Indirect Time-of-Fligh (IToF) cameras have gained increasing attention in the fields of robotics and automation. However, in real-world scenarios, IToF cameras are affected by multipath interference, which severely degrades imaging quality. Existing learning-based methods for multipath interference correction are all based on CNN architectures and rely on synthetic datasets, leading to poor generalization in real-world scenarios. We proposed an efficient and accurate real data collection scheme and explored the application of Transformer and Mamba in multipath interference correction tasks. Additionally, we introduced a cross-propagation network that integrates Mamba and CNN modules, reducing system complexity to linear levels while achieving superior multipath interference correction compared to state-of-the-art methods.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt13_05">
             10:15-10:20, Paper ThBT13.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4326'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SurgPLAN++: Universal Surgical Phase Localization Network for Online and Offline Inference
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#286502" title="Click to go to the Author Index">
             Chen, Zhen
            </a>
           </td>
           <td class="r">
            Centre for Artificial Intelligence and Robotics (CAIR), Hong Kon
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#394593" title="Click to go to the Author Index">
             Luo, Xingjian
            </a>
           </td>
           <td class="r">
            Centre for Artificial Intelligence and Robotics (CAIR) Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#381813" title="Click to go to the Author Index">
             Wu, Jinlin
            </a>
           </td>
           <td class="r">
            Institute of Automation, Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#310803" title="Click to go to the Author Index">
             Bai, Long
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426525" title="Click to go to the Author Index">
             Lei, Zhen
            </a>
           </td>
           <td class="r">
            Institute of Automation, Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#106795" title="Click to go to the Author Index">
             Ren, Hongliang
            </a>
           </td>
           <td class="r">
            Chinese Univ Hong Kong (CUHK) &amp; National Univ Singapore(NUS)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#124246" title="Click to go to the Author Index">
             Ourselin, Sebastien
            </a>
           </td>
           <td class="r">
            University College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104575" title="Click to go to the Author Index">
             Liu, Hongbin
            </a>
           </td>
           <td class="r">
            Hong Kong Institute of Science &amp; Innovation, Chinese Academy Of
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4326" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#recognition" title="Click to go to the Keyword Index">
               Recognition
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Surgical phase recognition is critical for assisting surgeons in understanding surgical videos. Existing studies focused more on online surgical phase recognition, by leveraging preceding frames to predict the current frame. Despite great progress, they formulated the task as a series of frame-wise classification, which resulted in a lack of global context of the entire procedure and incoherent predictions. Moreover, besides online analysis, accurate offline surgical phase recognition is also in significant clinical need for retrospective analysis, and existing online algorithms do not fully analyze the entire video, thereby limiting accuracy in offline analysis. To overcome these challenges and enhance both online and offline inference capabilities, we propose a universal Surgical Phase Localization Network, named SurgPLAN++, with the principle of temporal detection. To ensure a global understanding of the surgical procedure, we devise a phase localization strategy for SurgPLAN++ to predict phase segments across the entire video through phase proposals. For online analysis, to generate high-quality phase proposals, SurgPLAN++ incorporates a data augmentation strategy to extend the streaming video into a pseudo-complete video through mirroring, center-duplication, and down-sampling. For offline analysis, SurgPLAN++ capitalizes on its global phase prediction framework to continuously refine preceding predictions during each online inference step, thereby significantly improving the accuracy of phase recognition. We perform extensive experiments to validate the effectiveness, and our SurgPLAN++ achieves remarkable performance in both online and offline modes, which outperforms state-of-the-art methods. The source code is available at https://github.com/franciszchen/SurgPLAN-Plus.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt13_06">
             10:20-10:25, Paper ThBT13.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4501'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Real-Time LiDAR Point Cloud Compression and Transmission for Resource-Constrained Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417170" title="Click to go to the Author Index">
             Cao, Yuhao
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology Shenzhen
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#344022" title="Click to go to the Author Index">
             Wang, Yu
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#121779" title="Click to go to the Author Index">
             Chen, Haoyao
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology, Shenzhen
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4501" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#robotics_in_under_resourced_settings" title="Click to go to the Keyword Index">
               Robotics in Under-Resourced Settings
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#field_robots" title="Click to go to the Keyword Index">
               Field Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             LiDARs are widely used in autonomous robots due to their ability to provide accurate environment structural information. However, the large size of point clouds poses challenges in terms of data storage and transmission. In this paper, we propose a novel point cloud compression and transmission framework for resource-constrained robotic applications, called RCPCC. We iteratively fit the surface of point clouds with a similar range value and eliminate redundancy through their spatial relationships. Then, we use Shape-adaptive DCT (SA-DCT) to transform the unfit points and reduce the data volume by quantizing the transformed coefficients. We design an adaptive bitrate control strategy based on QoE
             <p>
              as the optimization goal to control the quality of the transmitted point cloud. Experiments show that our framework achieves compression rates of 40x to 80x while maintaining high accuracy for downstream applications. our method significantly outperforms other baselines in terms of accuracy when the compression rate exceeds 70x. Fur thermore, in situations of reduced communication bandwidth, our adaptive bitrate control strategy demonstrates significant QoE improvements.
             </p>
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt14">
             <b>
              ThBT14
             </b>
            </a>
           </td>
           <td class="r">
            402
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thbt14" title="Click to go to the Program at a Glance">
             <b>
              Language Guided Manipulation
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#114908" title="Click to go to the Author Index">
             Walter, Matthew
            </a>
           </td>
           <td class="r">
            Toyota Technological Institute at Chicago
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#267284" title="Click to go to the Author Index">
             Chen, Haonan
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana-Champaign
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt14_01">
             09:55-10:00, Paper ThBT14.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('89'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Shared Autonomy System for Precise and Efficient Remote Underwater Manipulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#354994" title="Click to go to the Author Index">
             Phung, Amy
            </a>
           </td>
           <td class="r">
            MIT-WHOI Joint Program
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#238129" title="Click to go to the Author Index">
             Billings, Gideon
            </a>
           </td>
           <td class="r">
            University of Sydney, Australian Center for Field Robotics
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#237986" title="Click to go to the Author Index">
             Daniele, Andrea F
            </a>
           </td>
           <td class="r">
            Toyota Technological Institute at Chicago
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#114908" title="Click to go to the Author Index">
             Walter, Matthew
            </a>
           </td>
           <td class="r">
            Toyota Technological Institute at Chicago
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#117912" title="Click to go to the Author Index">
             Camilli, Richard
            </a>
           </td>
           <td class="r">
            Woods Hole Oceanographic Institution
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab89" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#cognitive_human_robot_interaction" title="Click to go to the Keyword Index">
               Cognitive Human-Robot Interaction
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#virtual_reality_and_interfaces" title="Click to go to the Keyword Index">
               Virtual Reality and Interfaces
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#shared_autonomy_and_field_robotics" title="Click to go to the Keyword Index">
               Shared Autonomy and Field Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Conventional underwater intervention operations using robotic vehicles require expert teleoperators and limit interaction with remote scientists. We present the SHared Autonomy for Remote Collaboration (SHARC) framework that enables novice operators to cooperatively conduct underwater sampling and manipulation tasks. With SHARC, operators can plan and complete manipulation tasks using natural language or hand gestures through a virtual reality (SHARC-VR) interface. The interface provides remote operators with a contextual 3D scene understanding that is updated according to bandwidth availability. Evaluation of the SHARC framework through controlled lab experiments demonstrates that SHARC-VR enables novice operators to complete manipulation tasks in framerate-limited conditions (i.e., 0.1–0.5 frames per second) faster than expert pilots using a conventional topside controller. For both novice and expert users, the SHARC-VR interface also increases the task completion rate and improves sampling precision. The SHARC framework is readily extensible to other hardware architectures, including terrestrial and space systems.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt14_02">
             10:00-10:05, Paper ThBT14.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('256'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              E2Map: Experience-And-Emotion Map for Self-Reflective Robot Navigation with Language Models
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#266457" title="Click to go to the Author Index">
             Kim, Chan
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#414901" title="Click to go to the Author Index">
             Kim, Keonwoo
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#413786" title="Click to go to the Author Index">
             Oh, Mintaek
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#413800" title="Click to go to the Author Index">
             Baek, Hanbi
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#251287" title="Click to go to the Author Index">
             Lee, Jiyang
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#278946" title="Click to go to the Author Index">
             Jung, Donghwi
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#378258" title="Click to go to the Author Index">
             Woo, Soojin
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#413774" title="Click to go to the Author Index">
             Woo, Younkyung
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#381471" title="Click to go to the Author Index">
             Tucker, John
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#180374" title="Click to go to the Author Index">
             Firoozi, Roya
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#205194" title="Click to go to the Author Index">
             Seo, Seung-Woo
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#106233" title="Click to go to the Author Index">
             Schwager, Mac
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#157251" title="Click to go to the Author Index">
             Kim, Seong-Woo
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab256" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#learning_from_experience" title="Click to go to the Keyword Index">
               Learning from Experience
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#emotional_robotics" title="Click to go to the Keyword Index">
               Emotional Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Large language models (LLMs) have shown significant potential in guiding embodied agents to execute language instructions across a range of tasks, including robotic manipulation and navigation. However, existing methods are primarily designed for static environments and do not leverage the agent's own experiences to refine its initial plans. Given that real-world environments are inherently stochastic, initial plans based solely on LLMs' general knowledge may fail to achieve their objectives, unlike in static scenarios. To address this limitation, this study introduces the Experience-and-Emotion Map (E2Map), which integrates not only LLM knowledge but also the agent's real-world experiences, drawing inspiration from human emotional responses. The proposed methodology enables one-shot behavior adjustments by updating the E2Map based on the agent's experiences. Our evaluation in stochastic navigation environments, including both simulations and real-world scenarios, demonstrates that the proposed method significantly enhances performance in stochastic environments compared to existing LLM-based approaches.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt14_03">
             10:05-10:10, Paper ThBT14.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1754'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Improving Zero-Shot ObjectNav with Generative Communication
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#233874" title="Click to go to the Author Index">
             Dorbala, Vishnu Sashank
            </a>
           </td>
           <td class="r">
            University of Maryland, College Park
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#192654" title="Click to go to the Author Index">
             Sharma, Vishnu D.
            </a>
           </td>
           <td class="r">
            Nokia Bell Labs
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#128263" title="Click to go to the Author Index">
             Tokekar, Pratap
            </a>
           </td>
           <td class="r">
            University of Maryland
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#106235" title="Click to go to the Author Index">
             Manocha, Dinesh
            </a>
           </td>
           <td class="r">
            University of Maryland
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1754" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#agent_based_systems" title="Click to go to the Keyword Index">
               Agent-Based Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#domestic_robotics" title="Click to go to the Keyword Index">
               Domestic Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We propose a new method for improving Zero-Shot ObjectNav that aims to utilize potentially available environmental percepts for navigational assistance. Our approach takes into account that the ground agent may have limited and sometimes obstructed view. Our formulation encourages Generative Communication (GC) between an assistive overhead agent with a global view containing the target object and the ground agent with an obfuscated view; both equipped with Vision-Language Models (VLMs) for vision-to-language translation. In this assisted setup, the embodied agents communicate environmental information before the ground agent executes actions towards a target. Despite the overhead agent having a global view with the target, we note a drop in performance (-13% in OSR and -13% in SPL) of a fully cooperative assistance scheme over an unassisted baseline. In contrast, a selective assistance scheme where the ground agent retains its independent exploratory behaviour shows a 10% OSR and 7.65% SPL improvement. To explain navigation performance, we analyze the GC for unique traits, quantifying the presence of hallucination and cooperation. Specifically, we identify the novel linguistic trait of preemptive hallucination in our embodied setting, where the overhead agent assumes that the ground agent has executed an action in the dialogue when it is yet to move, and note its strong correlation with navigation performance. We conduct real-world experiments and present some qualitative examples where we mitigate hallucinations via prompt finetuning to improve ObjectNav performance.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt14_04">
             10:10-10:15, Paper ThBT14.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1904'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Commonsense Reasoning for Legged Robot Adaptation with Vision-Language Models
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#286001" title="Click to go to the Author Index">
             Chen, Annie
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421737" title="Click to go to the Author Index">
             Lessing, Alec
            </a>
           </td>
           <td class="r">
            Stanford
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422163" title="Click to go to the Author Index">
             Tang, Andy
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422171" title="Click to go to the Author Index">
             Chada, Govind
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#268839" title="Click to go to the Author Index">
             Smith, Laura
            </a>
           </td>
           <td class="r">
            UC Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#156706" title="Click to go to the Author Index">
             Levine, Sergey
            </a>
           </td>
           <td class="r">
            UC Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#178551" title="Click to go to the Author Index">
             Finn, Chelsea
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1904" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#ai_based_methods" title="Click to go to the Keyword Index">
               AI-Based Methods
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#autonomous_agents" title="Click to go to the Keyword Index">
               Autonomous Agents
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Legged robots are physically capable of navigating a diverse variety of environments and overcoming a wide range of obstructions. For example, in a search and rescue mission, a legged robot could climb over debris, crawl through gaps, and navigate out of dead ends. However, the robot’s controller needs to respond intelligently to such varied obstacles, and this requires handling unexpected and unusual scenarios successfully. This presents an open challenge to current learning methods, which often struggle with generalization to the long tail of unexpected situations without heavy human supervision. To address this issue, we investigate how to leverage the broad knowledge about the structure of the world and commonsense reasoning capabilities of vision-language models (VLMs) to aid legged robots in handling difficult, ambiguous situations. We propose a system, VLM-Predictive Control (VLM-PC), combining two key components that we find to be crucial for eliciting on-the-fly, adaptive behavior selection with VLMs: (1) in-context adaptation over previous robot interactions and (2) planning multiple skills into the future and replanning. We evaluate VLM-PC on several challenging real-world obstacle courses, involving dead ends and climbing and crawling, on a Go1 quadruped robot. Our experiments show that by reasoning over the history of interactions and future plans, VLMs enable the robot to autonomously perceive, navigate, and act in a wide range of complex scenarios that would otherwise require environment- specific engineering or human guidance.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt14_05">
             10:15-10:20, Paper ThBT14.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2819'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Language-Guided Object-Centric Diffusion Policy for Generalizable and Collision-Aware Manipulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#411964" title="Click to go to the Author Index">
             Li, Hang
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#268498" title="Click to go to the Author Index">
             Feng, Qian
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424691" title="Click to go to the Author Index">
             Zheng, Zhi
            </a>
           </td>
           <td class="r">
            TUM
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#251798" title="Click to go to the Author Index">
             Feng, Jianxiang
            </a>
           </td>
           <td class="r">
            Technical University of Munich (TUM)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#137632" title="Click to go to the Author Index">
             Chen, Zhaopeng
            </a>
           </td>
           <td class="r">
            University of Hamburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#105608" title="Click to go to the Author Index">
             Knoll, Alois
            </a>
           </td>
           <td class="r">
            Tech. Univ. Muenchen TUM
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2819" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#manipulation_planning" title="Click to go to the Keyword Index">
               Manipulation Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Learning from demonstrations faces challenges in generalizing beyond the training data and often lacks collision awareness. This paper introduces Lan-o3dp, a language-guided object-centric diffusion policy framework that can adapt to unseen situations such as cluttered scenes, shifting camera views, ambiguous similar objects, while offering training-free collision avoidance and achieving high success rate with few demonstrations. We train diffusion model conditioned on 3D point clouds of task-relevant objects to predict the robot's end-effector trajectories, enabling it to complete the tasks. During inference we incorporate cost optimization into denoising steps to guide the generated trajectory to be collision free. We leverage open-set segmentation to obtain the 3D point clouds of related objects and use a large language model to identify the target objects and possible obstacles by interpreting the user's natural language instructions. To effectively guide the conditional diffusion model using time-independent cost function, we proposed a novel guided generation mechanism based on the estimated clean trajectories. In simulation, we showed that diffusion policy based on the object-centric 3D representation achieves a much higher success rate (68.7%) compared to baselines with simple 2D (39.3%) and 3D scene (43.6%) representations, across 21 challenging RLBench tasks with only 40 demonstrations. In real-world experiments, we extensively evaluated the generalization in various unseen situations and validated the effectiveness of the proposed zero-shot cost-guided collision avoidance.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt14_06">
             10:20-10:25, Paper ThBT14.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3125'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              This&amp;That: Language-Gesture Controlled Video Generation for Robot Planning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419655" title="Click to go to the Author Index">
             Wang, Boyang
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424838" title="Click to go to the Author Index">
             Sridhar, Nikhil
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424662" title="Click to go to the Author Index">
             Feng, Chao
            </a>
           </td>
           <td class="r">
            University of Michigan - Ann Arbor
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#268127" title="Click to go to the Author Index">
             Van der Merwe, Mark
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#257018" title="Click to go to the Author Index">
             Fishman, Adam
            </a>
           </td>
           <td class="r">
            OpenAI
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#191587" title="Click to go to the Author Index">
             Fazeli, Nima
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424592" title="Click to go to the Author Index">
             Park, Jeong Joon
            </a>
           </td>
           <td class="r">
            University of Michigan, Ann Arbor
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3125" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#embodied_cognitive_science" title="Click to go to the Keyword Index">
               Embodied Cognitive Science
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Clear, interpretable instructions are invaluable for complex tasks, helping to clarify goals and anticipate necessary steps. In this work, we propose a robot learning framework for communicating, planning, and executing a wide range of tasks, dubbed This&amp;That. This&amp;That solves general tasks by leveraging video generative models, which, through training on internet-scale data, contain rich physical and semantic context. Through this work, we tackle three fundamental challenges in video-based planning: 1) unambiguous task communication with simple human instructions, 2) controllable video generation that respects user intent, and 3) translating visual plans into robot actions. This&amp;That adds gesture conditioning alongside language to generate video predictions, as a succinct and unambiguous alternative to existing language-only methods, especially in complex and uncertain environments. These video predictions are then fed into a behavior cloning architecture dubbed Diffusion Video to Action (DiVA), which outperforms prior state-of-the-art behavior cloning and video-based planning methods by substantial margins.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt15">
             <b>
              ThBT15
             </b>
            </a>
           </td>
           <td class="r">
            403
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thbt15" title="Click to go to the Program at a Glance">
             <b>
              Robot Safety
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#106981" title="Click to go to the Author Index">
             Vela, Patricio
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#285376" title="Click to go to the Author Index">
             Koga, Shumon
            </a>
           </td>
           <td class="r">
            Kobe University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt15_01">
             09:55-10:00, Paper ThBT15.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('75'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Quantifying the Risk of Unmapped Associations for Mobile Robot Localization Safety
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#276311" title="Click to go to the Author Index">
             Chen, Yihe
            </a>
           </td>
           <td class="r">
            Illinois Insitute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#282165" title="Click to go to the Author Index">
             Pervan, Boris
            </a>
           </td>
           <td class="r">
            Illinois Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#106228" title="Click to go to the Author Index">
             Spenko, Matthew
            </a>
           </td>
           <td class="r">
            Illinois Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab75" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#robot_safety" title="Click to go to the Keyword Index">
               Robot Safety
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#integrity_risk" title="Click to go to the Keyword Index">
               Integrity Risk
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#probability_and_statistical_methods" title="Click to go to the Keyword Index">
               Probability and Statistical Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Integrity risk is a measure of localization safety that accounts for the presence of undetected sensor faults. The metric has been used for decades in aviation and has recently been applied to terrestrial robots
             <p>
              operating in life-critical missions. For ground vehicles, integrity risk can be quantified for systems using lidar measurements, where two
              <p>
               specific fault types have been identified: miss-association and unmapped association. While miss-association faults, which occur when a
               <p>
                correctly extracted feature is associated to the wrong landmark, have been well-studied, the probability of an unmapped association fault, where an incorrectly extracted feature is associated to a landmark, is not well-understood. Namely, previous research has never quantified this value and instead relies on an assumed value, one whose value has not been properly justified. This work is the first to provide a methodology that estimates the risk of unmapped association for each mapped landmark; the paper demonstrates the effect of this probability for both the chi-squared and fixed-lag smoothing methods for integrity monitoring. Data collected in downtown Chicago, IL USA was used to tes
               </p>
              </p>
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt15_02">
             10:00-10:05, Paper ThBT15.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3949'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Control Strategies for Pursuit-Evasion under Occlusion Using Visibility and Safety Barrier Functions
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425468" title="Click to go to the Author Index">
             Zhou, Minnan
            </a>
           </td>
           <td class="r">
            University of California, San Diego
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425471" title="Click to go to the Author Index">
             Shaikh, Mustafa
            </a>
           </td>
           <td class="r">
            University of California, San Diego
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425476" title="Click to go to the Author Index">
             Chaubey, Vatsalya
            </a>
           </td>
           <td class="r">
            University of California, San Diego
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425528" title="Click to go to the Author Index">
             Haggerty, Patrick
            </a>
           </td>
           <td class="r">
            General Dynamics Mission Systems
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#285376" title="Click to go to the Author Index">
             Koga, Shumon
            </a>
           </td>
           <td class="r">
            Honda Research and Development
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#103171" title="Click to go to the Author Index">
             Panagou, Dimitra
            </a>
           </td>
           <td class="r">
            University of Michigan, Ann Arbor
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#149893" title="Click to go to the Author Index">
             Atanasov, Nikolay
            </a>
           </td>
           <td class="r">
            University of California, San Diego
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3949" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#sensor_based_control" title="Click to go to the Keyword Index">
               Sensor-based Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#robot_safety" title="Click to go to the Keyword Index">
               Robot Safety
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper develops a control strategy for pursuit-evasion problems in environments with occlusions. We address the challenge of a mobile pursuer keeping a mobile evader within its field of view (FoV) despite line-of-sight obstructions. The signed distance function (SDF) of the FoV is used to formulate visibility as a control barrier function (CBF) constraint on the pursuer's control inputs. Similarly, obstacle avoidance is formulated as a CBF constraint based on the SDF of the obstacle set. While the visibility and safety CBFs are Lipschitz continuous, they are not differentiable everywhere, necessitating the use of generalized gradients. To achieve non-myopic pursuit, we generate reference control trajectories leading to evader visibility using a sampling-based kinodynamic planner. The pursuer then tracks this reference via convex optimization under the CBF constraints. We validate our approach in CARLA simulations and real-world robot experiments, demonstrating successful visibility maintenance using only onboard sensing, even under severe occlusions and dynamic evader movements.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt15_03">
             10:05-10:10, Paper ThBT15.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3983'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Dynamic Gap: Safe Gap-Based Navigation in Dynamic Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#268738" title="Click to go to the Author Index">
             Asselmeier, Maxwell
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424735" title="Click to go to the Author Index">
             Ahuja, Dhruv
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#365898" title="Click to go to the Author Index">
             Zaro, Abdel
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#355147" title="Click to go to the Author Index">
             Abuaish, Ahmad
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#158317" title="Click to go to the Author Index">
             Zhao, Ye
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#106981" title="Click to go to the Author Index">
             Vela, Patricio
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3983" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#collision_avoidance" title="Click to go to the Keyword Index">
               Collision Avoidance
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper extends the family of gap-based local planners to unknown dynamic environments through generating provably collision-free properties for hierarchical navigation systems. Existing perception-informed local planners that operate in dynamic environments rely on emergent or empirical robustness for collision avoidance as opposed to performing formal analysis of dynamic obstacles. In addition to this, the obstacle tracking that is performed in these existent planners is often achieved with respect to a global inertial frame, subjecting such tracking estimates to transformation errors from odometry drift. The proposed local planner, dynamic gap, shifts the tracking paradigm to modeling how the free space, represented as gaps, evolves over time. Gap crossing and closing conditions are developed to aid in determining the feasibility of passage through gaps, and a breadth of simulation benchmarking is performed against other navigation planners in the literature where the proposed dynamic gap planner achieves the highest success rate out of all planners tested in all environments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt15_04">
             10:10-10:15, Paper ThBT15.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4459'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Conformalized Reachable Sets for Obstacle Avoidance with Spheres
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426814" title="Click to go to the Author Index">
             Kwon, Yong Seok
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#304094" title="Click to go to the Author Index">
             Michaux, Jonathan
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#348647" title="Click to go to the Author Index">
             Isaacson, Seth
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#279054" title="Click to go to the Author Index">
             Zhang, Bohao
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#429241" title="Click to go to the Author Index">
             Ejakov, Matthew
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#180427" title="Click to go to the Author Index">
             Skinner, Katherine
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#187525" title="Click to go to the Author Index">
             Vasudevan, Ram
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4459" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#robot_safety" title="Click to go to the Keyword Index">
               Robot Safety
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#planning_under_uncertainty" title="Click to go to the Keyword Index">
               Planning under Uncertainty
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#constrained_motion_planning" title="Click to go to the Keyword Index">
               Constrained Motion Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Safe motion planning algorithms are necessary for deploying autonomous robots in unstructured environments. Motion plans must be safe to ensure that the robot does not harm humans or damage any nearby objects. Generating these motion plans in real-time is also important to ensure that the robot can adapt to sudden changes in its environment. Many trajectory optimization methods introduce heuristics that balance safety and real-time performance, potentially increasing the risk of the robot colliding with its environment. This paper addresses this challenge by proposing Conformalized Reachable Sets for Obstacle Avoidance With Spheres (CROWS). CROWS is a novel real-time, receding-horizon trajectory planner that generates probablistically-safe motion plans. Offline, CROWS learns a novel neural network-based representation of a sphere-based reachable set that overapproximates the swept volume of the robot's motion. CROWS then uses conformal prediction to compute a confidence bound that provides a probabilistic safety guarantee on the learned reachable set. At runtime, CROWS performs trajectory optimization to select a trajectory that is probabilstically-guaranteed to be collision-free. We demonstrate that CROWS outperforms a variety of state-of-the-art methods in solving challenging motion planning tasks in cluttered environments while remaining collision-free. Code, data, and video demonstrations can be found at url{https://roahmlab.github.io/crows/}.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt15_05">
             10:15-10:20, Paper ThBT15.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4466'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              System-Level Safety Monitoring and Recovery for Perception Failures in Autonomous Vehicles
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#318538" title="Click to go to the Author Index">
             Chakraborty, Kaustav
            </a>
           </td>
           <td class="r">
            University of Southern California
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#309792" title="Click to go to the Author Index">
             Feng, Zeyuan
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#180771" title="Click to go to the Author Index">
             Veer, Sushant
            </a>
           </td>
           <td class="r">
            NVIDIA
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#238643" title="Click to go to the Author Index">
             Sharma, Apoorva
            </a>
           </td>
           <td class="r">
            NVIDIA
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#226946" title="Click to go to the Author Index">
             Ivanovic, Boris
            </a>
           </td>
           <td class="r">
            NVIDIA
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#123466" title="Click to go to the Author Index">
             Pavone, Marco
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#220606" title="Click to go to the Author Index">
             Bansal, Somil
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4466" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#intelligent_transportation_systems" title="Click to go to the Keyword Index">
               Intelligent Transportation Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#failure_detection_and_recovery" title="Click to go to the Keyword Index">
               Failure Detection and Recovery
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#autonomous_vehicle_navigation" title="Click to go to the Keyword Index">
               Autonomous Vehicle Navigation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The safety-critical nature of autonomous vehicle(AV) operation necessitates development of task-relevant algorithms that can reason about safety at the system level and not just at the component level. To reason about the impact of a perception failure on the entire system performance, such task-relevant algorithms must contend with various challenges: complexity of AV stacks, high uncertainty in the operating environments, and the need for real-time performance. To overcome these challenges, in this work, we introduce a Q-network called SPARQ (abbreviation for Safety evaluation for Perception And Recovery Q-network) that evaluates the safety of a plan generated by a planning algorithm, accounting for perception failures that the planning process may have overlooked. This Q-network can be queried during system runtime to assess whether a proposed plan is safe for execution or poses potential safety risks. If a violation is detected, the network can then recommend a corrective plan while accounting for the perceptual failure. We validate our algorithm using the NuPlan-Vegas dataset, demonstrating its ability to handle cases where a perception failure compromises a proposed plan, while the corrective plan remains safe. We observe an overall accuracy and recall of 90% while sustaining a frequency of 42HZ on the unseen testing dataset. We compare our performance to a popular reachability based baseline and analysed some interesting properties of our approach in improving the safety properties of an AV pipeline.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt15_06">
             10:20-10:25, Paper ThBT15.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4993'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Safety Filtering While Training: Improving the Performance and Sample Efficiency of Reinforcement Learning Agents
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#309224" title="Click to go to the Author Index">
             Pizarro Bejarano, Federico
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#307671" title="Click to go to the Author Index">
             Brunke, Lukas
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#124265" title="Click to go to the Author Index">
             Schoellig, Angela P.
            </a>
           </td>
           <td class="r">
            TU Munich
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4993" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#robot_safety" title="Click to go to the Keyword Index">
               Robot Safety
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#machine_learning_for_robot_control" title="Click to go to the Keyword Index">
               Machine Learning for Robot Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Reinforcement learning (RL) controllers are flexible and performant but rarely guarantee safety. Safety filters impart hard safety guarantees to RL controllers while maintaining flexibility. However, safety filters can cause undesired behaviours due to the separation between the controller and the safety filter, often degrading performance and robustness. In this paper, we analyze several modifications to incorporating the safety filter in training RL controllers rather than solely applying it during evaluation. The modifications allow the RL controller to learn to account for the safety filter. This paper presents a comprehensive analysis of training RL with safety filters, featuring simulated and real-world experiments with a Crazyflie 2.0 drone. We examine how various training modifications and hyperparameters impact performance, sample efficiency, safety, and chattering. Our findings serve as a guide for practitioners and researchers focused on safety filters and safe RL.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt16">
             <b>
              ThBT16
             </b>
            </a>
           </td>
           <td class="r">
            404
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thbt16" title="Click to go to the Program at a Glance">
             <b>
              Soft Robotics 1
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#240024" title="Click to go to the Author Index">
             Dorsey, Kristen
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt16_01">
             09:55-10:00, Paper ThBT16.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('889'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Pneumatic Logic Systems for Selectively Operating Distributed Pneumatic Elements
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419741" title="Click to go to the Author Index">
             Ferrin Pozuelo, Rafael
            </a>
           </td>
           <td class="r">
            National Institute of Advanced Industrial Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#110681" title="Click to go to the Author Index">
             Tomita, Kohji
            </a>
           </td>
           <td class="r">
            National Institute of Advanced Industrial Science AndTechnology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#110687" title="Click to go to the Author Index">
             Kamimura, Akiya
            </a>
           </td>
           <td class="r">
            National Institute of Advanced Industrial Science and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab889" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#soft_robot_materials_and_design" title="Click to go to the Keyword Index">
               Soft Robot Materials and Design
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#soft_robot_applications" title="Click to go to the Keyword Index">
               Soft Robot Applications
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#hydraulic_pneumatic_actuators" title="Click to go to the Keyword Index">
               Hydraulic/Pneumatic Actuators
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Microfluidic and pneumatic logic systems are valuable for applications such as lab-on-a-chip devices, soft robotics, and factory automation. These systems are particularly advantageous when metal or electronic components are impractical or when there are constraints on the control system volume or weight. This paper introduces a novel individual membrane valve that functions as a set-reset latch and can reduce the number of valves required for some pneumatic or microfluidic logic systems. An application of pneumatic logic systems in soft robotics is the access to multiple tethered pneumatic elements through a reduced number of pneumatic lines. To this end, this paper proposes two pneumatic logic systems capable of selecting among multiple distributed sets of pneumatic elements and operating the elements of the set simultaneously and independently through the different pneumatic lines. The selection is achieved via a sequence of pressure pulses applied on the same lines used afterwards for operation. Two prototypes of these pneumatic logic systems were built and successfully demonstrated, consisting primarily of set-reset membrane valves and powered by binary high/low pressure sources. The first prototype features a hierarchical network with four lines and five sets of three pneumatic elements each; the second prototype features a non-hierarchical network with five lines and twelve sets of four pneumatic elements each.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt16_02">
             10:00-10:05, Paper ThBT16.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('909'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Helical Structured Soft Growing Robot for Hazardous Gas Suction in Inaccessible Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#391616" title="Click to go to the Author Index">
             Lee, Sanghun
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#295695" title="Click to go to the Author Index">
             Kim, Nam Gyun
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#334402" title="Click to go to the Author Index">
             Seo, Dongoh
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#334501" title="Click to go to the Author Index">
             Park, Shinwoo
            </a>
           </td>
           <td class="r">
            KAIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#102369" title="Click to go to the Author Index">
             Ryu, Jee-Hwan
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab909" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#soft_robot_applications" title="Click to go to the Keyword Index">
               Soft Robot Applications
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#soft_robot_materials_and_design" title="Click to go to the Keyword Index">
               Soft Robot Materials and Design
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#modeling__control__and_learning_for_soft_robots" title="Click to go to the Keyword Index">
               Modeling, Control, and Learning for Soft Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Immediate removal of hazardous gases is critical for ensuring safety. Traditional methods, such as portable ventilation equipment, are difficult to use when hazardous gases are released in inaccessible environments. In this paper, we propose a novel mechanism that integrates an inflatable helical structure into a soft growing robot. The proposed mechanism is capable of performing suction through its inner channel after navigating complex environments, while maintaining the inherent advantages of the soft growing robot as it grows. The mechanism operates in two phases: a growing phase, in which the robot extends by eversion, and a suction phase, in which suction is performed through the inner channel of the robot. Experiments and demonstrations were conducted to evaluate the performance of the proposed mechanism. The experimental results confirmed the ability to maintain the passageway shape of the inner channel during suction operations and provided a design guideline. The demonstration validated that the mechanism can effectively navigate inaccessible environments and perform suction to remove hazardous gases.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt16_03">
             10:05-10:10, Paper ThBT16.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2866'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Shape-Programming Robotic Reflectors for Wireless Networks
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#315792" title="Click to go to the Author Index">
             Liu, Yawen
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#319411" title="Click to go to the Author Index">
             Prabhakara, Akarsh
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420501" title="Click to go to the Author Index">
             Zhu, Jiangyifei
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420461" title="Click to go to the Author Index">
             Qiao, Shenyi
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#198248" title="Click to go to the Author Index">
             Kumar, Swarun
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2866" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#soft_robot_applications" title="Click to go to the Keyword Index">
               Soft Robot Applications
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#automation_technologies_for_smart_cities" title="Click to go to the Keyword Index">
               Automation Technologies for Smart Cities
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#sensor_networks" title="Click to go to the Keyword Index">
               Sensor Networks
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             With the increasing use of wireless technologies in robotics for communication, sensing, and localization, the potential benefits of how robotics can complement and enhance wireless systems remain underexplored. This paper explores a novel application of the existing inflatable robots for wireless communication systems by forming a shape-programming, reflective waveguide that enhances the received signal quality for wireless devices. Our primary target is enhancing Low-Power Wide-Area Networks (LP WANs) – where 10-year battery-powered client devices (e.g. energy meters or smart home sensors) connect to cellular-like powered base stations to deliver data. Devices in these networks often experience significant seasonal variability in battery life – even simple obstructions between the device and base station (e.g. due to construction) can shave off years of battery life. We propose MetaMorph, a programmable robotic reflector attached to base stations that enhances signal quality from client devices by enhancing received signal energy with controlled reflections. We investigate the design of the reflector, and our experiments show the ability to improve the signal quality for LP-WAN(LoRa) communication systems demonstrating signal quality and battery-benefits. To our best knowledge, MetaMorph is the first paper to explore how flexible robotics can serve as virtuous reflectors for wireless communication systems.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt16_04">
             10:10-10:15, Paper ThBT16.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3491'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              MORF: Magnetic Origami Reprogramming and Folding System for Repeatably Reconfigurable Structures with Fold Angle Control
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424518" title="Click to go to the Author Index">
             Unger, Gabriel
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424540" title="Click to go to the Author Index">
             Shenoy, Sridhar
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#377644" title="Click to go to the Author Index">
             Li, Tianyu
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#151054" title="Click to go to the Author Index">
             Figueroa, Nadia
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#154404" title="Click to go to the Author Index">
             Sung, Cynthia
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3491" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#soft_robot_materials_and_design" title="Click to go to the Keyword Index">
               Soft Robot Materials and Design
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#soft_robot_applications" title="Click to go to the Keyword Index">
               Soft Robot Applications
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present the Magnetic Origami Reprogramming and Folding System (MORF), a magnetically reprogrammable system capable of precise shape control, repeated transformations, and adaptive functionality for robotic applications. Unlike current self-folding systems, which often lack re-programmability or lose rigidity after folding, MORF generates stiff structures over multiple folding cycles without degradation in performance. The ability to reconfigure and maintain structural stability is crucial for tasks such as reconfigurable tooling. The system utilizes a thermoplastic layer sandwiched within a thin magnetically responsive laminate sheet, enabling structures to self-fold in response to a combination of external magnetic field and heating. We demonstrate that the resulting folded structures can bear loads over 40 times their own weight and can undergo up to 50 cycles of repeated transformations without losing structural integrity. We showcase these strengths in a reconfigurable tool for unscrewing and screwing bolts and screws of various sizes, allowing the tool to adapt its shape to different bolt sizes while withstanding the mechanical stresses involved. This capability highlights the system’s potential for task-varying, load-bearing applications in robotics, where both versatility and durability are essential.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt16_05">
             10:15-10:20, Paper ThBT16.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4327'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Tunable Leg Stiffness in a Monopedal Hopper for Energy-Efficient Vertical Hopping across Varying Ground Profiles
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#341620" title="Click to go to the Author Index">
             Chen, Rongqian
            </a>
           </td>
           <td class="r">
            George Washington University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#383381" title="Click to go to the Author Index">
             Kwon, Jun
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422668" title="Click to go to the Author Index">
             Wu, Kefan
            </a>
           </td>
           <td class="r">
            University of Connecticut
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#152712" title="Click to go to the Author Index">
             Chen, Wei-Hsi
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4327" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#soft_robot_applications" title="Click to go to the Keyword Index">
               Soft Robot Applications
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mechanism_design" title="Click to go to the Keyword Index">
               Mechanism Design
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present the design and implementation of HASTA (Hopper with Adjustable Stiffness for Terrain Adaption), a vertical hopping robot with real-time tunable leg stiffness, aimed at optimizing energy efficiency across various ground profiles (a pair of ground stiffness and damping conditions). By adjusting leg stiffness, we aim to maximize apex hopping height, a key metric for energy-efficient vertical hopping. We hypothesize that softer legs perform better on soft, damped ground by minimizing penetration and energy loss, while stiffer legs excel on hard, less damped ground by reducing limb deformation and energy dissipation. Through experimental tests and simulations, we find the best leg stiffness within our selection for each combination of ground stiffness and damping, enabling the robot to achieve maximum steady-state hopping height with a constant energy input. These results support our hypothesis that tunable stiffness improves energy-efficient locomotion in controlled experimental conditions. In addition, the simulation provides insights that could aid in future development of controllers for selecting leg stiffness.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt16_06">
             10:20-10:25, Paper ThBT16.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4813'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Online Learning Based Shape Control for a Soft Manipulator Based on Spatial Features Feedback
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#238966" title="Click to go to the Author Index">
             Shen, Yi
            </a>
           </td>
           <td class="r">
            Huazhong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#402534" title="Click to go to the Author Index">
             Zhang, Jinghao
            </a>
           </td>
           <td class="r">
            Huazhong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#122749" title="Click to go to the Author Index">
             Yuan, Ye
            </a>
           </td>
           <td class="r">
            Huazhong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#107391" title="Click to go to the Author Index">
             Zhang, Fumin
            </a>
           </td>
           <td class="r">
            Hong Kong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#195115" title="Click to go to the Author Index">
             Ding, Han
            </a>
           </td>
           <td class="r">
            Huazhong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4813" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#modeling__control__and_learning_for_soft_robots" title="Click to go to the Keyword Index">
               Modeling, Control, and Learning for Soft Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#soft_robot_applications" title="Click to go to the Keyword Index">
               Soft Robot Applications
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Although soft manipulators are endowed with compliance and flexibility, most control strategies focus on end-effector control and lack shape control ability. This letter aims to design a shape controller for the soft manipulator. Firstly, we establish a modified forward kinematics model (FKM) based on the long-short-term-memory (LSTM) neural network to describe the mapping between actuation inputs and spatial features. The spatial features consist of the backbone curve and contour features. The backbone curve is represented by the piecewise Bézier curve under geometrically continuous constraint. The contour features are extracted from the camera-generated point cloud. Besides, an adaptive online learning based shape controller (OLSC) is designed by online back-propagating shape error. The stability of OLSC is proved based on the Lyapunov theorem. Finally, the random excitation model validation experiment demonstrates the prediction accuracy of the proposed modified FKM, and the shape control experiments in air and water validate the effectiveness of the proposed OLSC.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt16_07">
             10:25-10:30, Paper ThBT16.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4825'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Augmenting Compliance with Motion Generation through Imitation Learning Using Drop-Stitch Reinforced Inflatable Robot Arm with Rigid Joints
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#401542" title="Click to go to the Author Index">
             Gubbala, Gangadhara Naga Sai
            </a>
           </td>
           <td class="r">
            Waseda University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#401799" title="Click to go to the Author Index">
             Nagashima, Masato
            </a>
           </td>
           <td class="r">
            Waseda University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#158485" title="Click to go to the Author Index">
             Mori, Hiroki
            </a>
           </td>
           <td class="r">
            Waseda University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#221709" title="Click to go to the Author Index">
             Seong, Young Ah
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#145000" title="Click to go to the Author Index">
             Sato, Hiroki
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#103555" title="Click to go to the Author Index">
             Niiyama, Ryuma
            </a>
           </td>
           <td class="r">
            Meiji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#107685" title="Click to go to the Author Index">
             Suga, Yuki
            </a>
           </td>
           <td class="r">
            Waseda University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101828" title="Click to go to the Author Index">
             Ogata, Tetsuya
            </a>
           </td>
           <td class="r">
            Waseda University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4825" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#modeling__control__and_learning_for_soft_robots" title="Click to go to the Keyword Index">
               Modeling, Control, and Learning for Soft Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#soft_robot_materials_and_design" title="Click to go to the Keyword Index">
               Soft Robot Materials and Design
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Safe physical human-robot collaboration can be possible with soft robots due to their inherent compliance and low inertia. Soft bodies provide passive compliance and adaptability due to their deformations, but these same characteristics also lead to difficulty in dynamic control and mathematical modeling. We focus on motion generation for a 3-DOF (Degree of freedom) inflatable robot arm, consisting of soft inflatable body links and rigid joints. This research explores the limitations of relying only on soft robot compliance for contact-based tasks. Our goal is to generate adaptive motion for contact-based tasks by exploiting the compliance of the soft links. We compare contact-based tasks for the inflatable robot with and without a learning model. This shows improved performance when soft robot compliance is augmented with imitation learning. The combination of soft robot compliance and the machine learning model's adaptability shows the potential for collaborative robots to interact with humans and their surroundings safely.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt17">
             <b>
              ThBT17
             </b>
            </a>
           </td>
           <td class="r">
            405
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thbt17" title="Click to go to the Program at a Glance">
             <b>
              Planning, Scheduling and Coordination
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#135594" title="Click to go to the Author Index">
             Pecora, Federico
            </a>
           </td>
           <td class="r">
            Amazon Robotics
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#170338" title="Click to go to the Author Index">
             Rastgoftar, Hossein
            </a>
           </td>
           <td class="r">
            University of Arizona
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt17_01">
             09:55-10:00, Paper ThBT17.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('706'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Safe Human-UAS Collaboration from High-Level Planning to Low-Level Tracking (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#170338" title="Click to go to the Author Index">
             Rastgoftar, Hossein
            </a>
           </td>
           <td class="r">
            University of Arizona
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab706" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#planning__scheduling_and_coordination" title="Click to go to the Keyword Index">
               Planning, Scheduling and Coordination
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#intention_recognition" title="Click to go to the Keyword Index">
               Intention Recognition
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper studies the problem of safe human-uncrewed aerial system (UAS) collaboration in a shared work environment. By considering human and UAS as co-workers, we use Petri Nets to abstractly model evolution of shared tasks assigned to human and UAS co-workers. Particularly, the Petri Nets’ “places” represent work stations; therefore, the Petri Nets’ transitions can formally specify displacements between the work stations. The paper’s first objective is to incorporate uncertainty regarding the intentions of human co-workers into motion planning for UAS, when UAS closely interacts with human co-workers. To this end, the proposed Petri Nets model uses “conflict” constructs to represent situations at which UAS deals with incomplete knowledge about human co-worker intention. The paper’s second objective is then to plan the motion of the UAS in a resilient and safe manner, in the presence of non-cooperative human co-workers. In order to achieve this objective, UAS equipped with onboard perception and decision-making capabilities are able to, through real-time processing of in-situ observation, predict human intention, quantify human distraction, and apply a non-stationary Markov Decision Process (MDP) model to safely plan UAS motion in the presence of uncertainty. Given the current and next UAS waypoints, the paper applies Potryagin’s minimal principle to plan the desired trajectory of the UAS and uses feedback linearaztion method for trajectory tracking control.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt17_02">
             10:00-10:05, Paper ThBT17.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1155'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Reliable and Efficient Multi-Agent Coordination Via Graph Neural Network Variational Autoencoders
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#236419" title="Click to go to the Author Index">
             Meng, Yue
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#221094" title="Click to go to the Author Index">
             Majcherczyk, Nathalie
            </a>
           </td>
           <td class="r">
            Worcester Polytechnic Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#312712" title="Click to go to the Author Index">
             Liu, Wenliang
            </a>
           </td>
           <td class="r">
            Amazon
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#274888" title="Click to go to the Author Index">
             Kiesel, Scott
            </a>
           </td>
           <td class="r">
            Amazon
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#217837" title="Click to go to the Author Index">
             Fan, Chuchu
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#135594" title="Click to go to the Author Index">
             Pecora, Federico
            </a>
           </td>
           <td class="r">
            Amazon Robotics
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1155" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#planning__scheduling_and_coordination" title="Click to go to the Keyword Index">
               Planning, Scheduling and Coordination
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Multi-agent coordination is crucial for reliable multi-robot navigation in shared spaces such as automated warehouses. In regions of dense robot traffic, local coordination methods may fail to find a deadlock-free solution. In these scenarios, it is appropriate to let a central unit generate a global schedule that decides the passing order of robots. However, the runtime of such centralized coordination methods increases significantly with the problem scale. In this paper, we propose to leverage Graph Neural Network Variational Autoencoders (GNN-VAE) to solve the multi-agent coordination problem faster than through centralized optimization at scale.	We formulate the coordination problem as a graph problem and collect ground truth data using a Mixed-Integer Linear Program (MILP) solver. During training, our learning framework encodes good quality solutions of the graph problem into a latent space. At inference time, solution samples are decoded from the sampled latent variables, and the lowest-cost sample is selected for coordination. By construction, our GNN-VAE framework returns solutions that always respect the constraints of the considered coordination problem. Numerical results show that our approach trained on small-scale problems can achieve high-quality solutions even for large-scale problems with 250 robots, being much faster than other baselines.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt17_03">
             10:05-10:10, Paper ThBT17.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2048'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Efficient Cross-Boundary Grasping in Stacked Clutter with Single-Visual Mapping Multi-Step
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#157899" title="Click to go to the Author Index">
             Luo, Yudong
            </a>
           </td>
           <td class="r">
            Dalian Maritime University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#370430" title="Click to go to the Author Index">
             Wang, Tong
            </a>
           </td>
           <td class="r">
            Dalian Martime University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#370426" title="Click to go to the Author Index">
             Xie, Feiyu
            </a>
           </td>
           <td class="r">
            Dalian Maritime University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#198540" title="Click to go to the Author Index">
             Zhao, Na
            </a>
           </td>
           <td class="r">
            Dalian Maritime University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#271366" title="Click to go to the Author Index">
             Fu, Xianping
            </a>
           </td>
           <td class="r">
            Dalian Maritime University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#100889" title="Click to go to the Author Index">
             Shen, Yantao
            </a>
           </td>
           <td class="r">
            University of Nevada, Reno
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2048" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#logistics" title="Click to go to the Keyword Index">
               Logistics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#factory_automation" title="Click to go to the Keyword Index">
               Factory Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In logistics applications, the vision-based technology for grasping target objects in the air is relatively mature. However, when operating across the air and water, such as grasping marine products from the water, the visual information collected by the camera will be disturbed by ripples and bubbles on the water surface, resulting in low grasping efficiency. Therefore, we introduce a grasping strategy based on single-visual mapping for multi-step (SVMMS) operations, which is suitable for cross-medium operations involving stacked objects. Specifically, we design a multifunctional integrated network model based on Deep Q-learning, which extracts visual features from the scene to detect stacked objects and outputs their hierarchical relationships effectively. Moreover, we quantify the potential relationship between motion logic during action execution and changes in RGB-D information to help the robot achieve efficient and collision-free operations. Our approach also incorporates a time-series design with prioritized experience replay to optimize the action sequence globally. Additionally, we propose a novel sim2real method by combining domain randomization to address the difference in object sizes between the simulation and the real world. Extensive experiments in both simulation and physical environments show that SVMMS-Grasp significantly outperforms existing methods regarding task success rate, stability, and operational efficiency.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt17_04">
             10:10-10:15, Paper ThBT17.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2527'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Efficient Second-Order Cone Programming for the Close Enough Traveling Salesman Problem
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#256318" title="Click to go to the Author Index">
             Gutow, Geordan
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104011" title="Click to go to the Author Index">
             Choset, Howie
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2527" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#planning__scheduling_and_coordination" title="Click to go to the Keyword Index">
               Planning, Scheduling and Coordination
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             When agents must execute multiple tasks at spatially distinct locations, it is common to formulate and solve a Traveling Salesman Problem (TSP) to find the order of locations (targets) that requires the smallest travel cost. Approaching such task sequencing problems as a TSP is restrictive, as it requires that unique locations be specified for each task. In reality a set of acceptable locations might be available. The Close Enough Traveling Salesman Problem (CETSP) is a generalization of the Traveling Salesman Problem in which the agent needs only visit a spherical neighborhood surrounding each target, and can thus address this task sequencing problem when any location in a sphere is acceptable. Prior work has developed a branch-and-bound approach that finds globally optimal solutions to instances of the CETSP by solving a sequence of Second-Order Cone Programs (SOCP). We demonstrate it is possible to eliminate 2/3 of the variables and 1/2 of the constraints in these SOCPs, show how to reuse computation and memory allocation across multiple SOCPs in the sequence, and propose a strategy to warm-start the SOCPs using solutions obtained earlier in the sequence. Collectively, these three changes halve the time required to solve 210 random CETSP instances to optimality. We also obtained improved lower bounds on 73 instances from the literature, including solving one instance to optimality for the first time.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt17_05">
             10:15-10:20, Paper ThBT17.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2633'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Decoupled Training Neural Solver for Dynamic Traveling Salesman Problem
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419513" title="Click to go to the Author Index">
             Lin, Shaoheng
            </a>
           </td>
           <td class="r">
            South China University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419510" title="Click to go to the Author Index">
             Cui, Hanyun
            </a>
           </td>
           <td class="r">
            South China University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419509" title="Click to go to the Author Index">
             Yang, Wang
            </a>
           </td>
           <td class="r">
            South China University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419775" title="Click to go to the Author Index">
             Jia, Ya-Hui
            </a>
           </td>
           <td class="r">
            South China University of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2633" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#planning__scheduling_and_coordination" title="Click to go to the Keyword Index">
               Planning, Scheduling and Coordination
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#planning_under_uncertainty" title="Click to go to the Keyword Index">
               Planning under Uncertainty
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#task_planning" title="Click to go to the Keyword Index">
               Task Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Deep reinforcement learning (DRL) methods have achieved remarkable success in solving static traveling salesman problems (TSP). However, dynamic TSP (DTSP), with the random appearance of new customers over time, introduces additional complexities that challenge DRL methods by the difficulty of obtaining optimized routing policy which lead to sub-optimal results and reduced training efficiency. To address these issues, we propose a decoupled training neural solver (DTNS) based on the encoder-decoder architecture, which is a novel approach that decouples the optimization of encoder and decoder, enhancing the model's ability to handle dynamic changes. Our method involves training under an Fore-Reveal condition first where the information of all customers nodes are known in advance to obtain optimized encoder and initialization for decoder and then fine-tuning the decoder in dynamic scenarios where dynamic customers are revealed over time. This training paradigm results in a flexible and globally optimized routing policy. Experimental results demonstrate that DTNS efficiently adapts to new customer requests in dynamic scenario, outperforming existing methods in dynamic routing environments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt17_06">
             10:20-10:25, Paper ThBT17.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3696'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Multi-Drone-Truck Collaborative Delivery with En Route Operations: A Hierarchical MARL-Based Approach
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424220" title="Click to go to the Author Index">
             Hu, Shun
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421725" title="Click to go to the Author Index">
             Li, Bing
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#354167" title="Click to go to the Author Index">
             Zhang, Rongqing
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3696" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#planning__scheduling_and_coordination" title="Click to go to the Keyword Index">
               Planning, Scheduling and Coordination
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#distributed_robot_systems" title="Click to go to the Keyword Index">
               Distributed Robot Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#path_planning_for_multiple_mobile_robots_or_agents" title="Click to go to the Keyword Index">
               Path Planning for Multiple Mobile Robots or Agents
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The multi-drone-truck collaborative delivery, where unmanned trucks serve as mobile supply stations for drones, effectively combines the strengths of both vehicles and presents wide application prospects. But the majority of existing literature restricts drone launch and retrieve operations (LARO) to stationary truck, and potential drone route collisions are mostly ignored. This leads to inability to fully exploit the capability of drones. We address these gaps and introduce a new variant of multi-drone-truck collaborative delivery. However, the scheduling for drones and truck faces high-dimensional solution space and complex constraints, making it almost impossible for centralized solving. To this end, we develop a hierarchical solution framework that decomposes the complete problem into two levels of subproblem. The upper solver centrally allocates tasks and schedules when drones to launch, while the lower solver, based on multi-agent reinforcement learning (MARL), plans paths for each drone agent in a decentralized but cooperative manner. In addition, we validate the effectiveness of our method by benchmarking it against three state-of-the-art approaches, demonstrating its superiority in terms of both efficiency and collision avoidance.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt17_07">
             10:25-10:30, Paper ThBT17.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3981'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Risk-Aware Energy-Constrained UAV-UGV Cooperative Routing Using Attention-Guided Reinforcement Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#354972" title="Click to go to the Author Index">
             Mondal, Mohammad Safwan
            </a>
           </td>
           <td class="r">
            University of Illinois Chicago
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#329588" title="Click to go to the Author Index">
             Ramasamy, Subramanian
            </a>
           </td>
           <td class="r">
            University of Illinois at Chicago
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425093" title="Click to go to the Author Index">
             Rownak, Ragib
            </a>
           </td>
           <td class="r">
            University of Illinois Chicago
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#450996" title="Click to go to the Author Index">
             Russo, Luca
            </a>
           </td>
           <td class="r">
            University of Illinois at Chicago
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#328786" title="Click to go to the Author Index">
             Humann, James
            </a>
           </td>
           <td class="r">
            DEVCOM Army Research Laboratory,
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#415366" title="Click to go to the Author Index">
             James, Dotterweich, Jim
            </a>
           </td>
           <td class="r">
            Army Research Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#166583" title="Click to go to the Author Index">
             Bhounsule, Pranav
            </a>
           </td>
           <td class="r">
            University of Illinois at Chicago
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3981" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#planning__scheduling_and_coordination" title="Click to go to the Keyword Index">
               Planning, Scheduling and Coordination
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#autonomous_agents" title="Click to go to the Keyword Index">
               Autonomous Agents
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Maximizing the endurance of unmanned aerial vehicles (UAVs) in large-scale monitoring missions spanning over large areas requires addressing their limited battery capacity. Deploying unmanned ground vehicles (UGVs) as mobile recharging stations offers a practical solution, extending UAVs’ operational range. This introduces the challenge of optimizing UAV-UGV routes for efficient mission point coverage and seamless recharging coordination. In this paper, we present a risk-aware deep reinforcement learning (Ra-DRL) framework with a multi-head attention mechanism within an encoder-decoder transformer architecture to solve this cooperative routing problem for a UAV-UGV team. Our model minimizes mission time while accounting for the stochastic fuel consumption of the UAV, influenced by environmental factors like wind velocity, ensuring adherence to a risk threshold to avoid mid-mission energy depletion. Extensive evaluations on various problem sizes show that our method significantly outperforms nearest-neighbor heuristics in both solution quality and risk management. We validate the Ra-DRL policy in a Gazebo-ROS SITL environment with a PX4-based custom UAV and Clearpath Husky UGV. The results demonstrate the robustness and adaptability of our policy, making it highly effective for mission planning in dynamic, uncertain scenarios.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt18">
             <b>
              ThBT18
             </b>
            </a>
           </td>
           <td class="r">
            406
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thbt18" title="Click to go to the Program at a Glance">
             <b>
              RADAR-Based Navigation
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#204077" title="Click to go to the Author Index">
             Khattak, Shehryar
            </a>
           </td>
           <td class="r">
            NASA Jet Propulsion Laboratory
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#174388" title="Click to go to the Author Index">
             Heidingsfeld, Michael
            </a>
           </td>
           <td class="r">
            CARIAD SE
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt18_01">
             09:55-10:00, Paper ThBT18.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('820'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Ground-Aware Automotive Radar Odometry
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#340128" title="Click to go to the Author Index">
             Casado Herraez, Daniel
            </a>
           </td>
           <td class="r">
            University of Bonn &amp; CARIAD SE
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419452" title="Click to go to the Author Index">
             Kaschner, Franz
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#333422" title="Click to go to the Author Index">
             Zeller, Matthias
            </a>
           </td>
           <td class="r">
            CARIAD SE
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#418839" title="Click to go to the Author Index">
             Muhle, Dominik
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#137262" title="Click to go to the Author Index">
             Behley, Jens
            </a>
           </td>
           <td class="r">
            University of Bonn
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#174388" title="Click to go to the Author Index">
             Heidingsfeld, Michael
            </a>
           </td>
           <td class="r">
            CARIAD SE
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#150816" title="Click to go to the Author Index">
             Cremers, Daniel
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101642" title="Click to go to the Author Index">
             Stachniss, Cyrill
            </a>
           </td>
           <td class="r">
            University of Bonn
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab820" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#autonomous_vehicle_navigation" title="Click to go to the Keyword Index">
               Autonomous Vehicle Navigation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Odometry is crucial for the navigation of autonomous vehicles in unknown environments. While cameras and LiDARs are commonly used to estimate the ego-motion of a vehicle, these sensors face limitations under bad lighting and severe weather conditions. Automotive radars overcome these challenges, but radar point clouds are generally sparse and noisy, making it difficult to identify useful features within a radar scan. In this paper, we address the problem of ego-motion estimation using a single automotive radar sensor. We propose a simple, yet effective, heuristic-based method to extract the ground plane from single radar scans and perform ground plane matching between consecutive scans. Additionally, we perform a windowed factor-graph optimization of the poses together with the ground plane, improving the accuracy of the pose estimation. We put our work to the test using the 4DRadarDataset. Our findings illustrate the state-of-the-art performance of our odometry approach compared to existing alternatives that use radar point clouds.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt18_02">
             10:00-10:05, Paper ThBT18.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1999'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              CAO-RONet: A Robust 4D Radar Odometry with Exploring More Information from Low-Quality Points
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#325553" title="Click to go to the Author Index">
             Li, Zhiheng
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#251832" title="Click to go to the Author Index">
             Cui, Yubo
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#377449" title="Click to go to the Author Index">
             Huang, Ningyuan
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#356388" title="Click to go to the Author Index">
             Pang, Chenglin
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#121608" title="Click to go to the Author Index">
             Fang, Zheng
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1999" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Recently, 4D millimetre-wave radar exhibits more stable perception ability than LiDAR and camera under adverse conditions (e.g. rain and fog). However, low-quality radar points hinder its application, especially the odometry task that requires a dense and accurate matching. To fully explore the potential of 4D radar, we introduce a learning-based odometry framework, enabling robust ego-motion estimation from finite and uncertain geometry information. First, for sparse radar points, we propose a local completion to supplement missing structures and provide denser guideline for aligning two frames. Then, a context-aware association with a hierarchical structure flexibly matches points of different scales aided by feature similarity, and improves local matching consistency through correlation balancing. Finally, we present a window-based optimizer that uses historical priors to establish a coupling state estimation and correct errors of inter-frame matching. The superiority of our algorithm is confirmed on View-of-Delft dataset, achieving around a 50% performance improvement over previous approaches and delivering accuracy on par with LiDAR odometry. The code will be released at https://github.com/NEU-REAL/CAO-RONet.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt18_03">
             10:05-10:10, Paper ThBT18.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2909'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Radar Teach and Repeat: Architecture and Initial Field Testing
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#351161" title="Click to go to the Author Index">
             Qiao, Xinyuan
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#365704" title="Click to go to the Author Index">
             Krawciw, Alec
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#225265" title="Click to go to the Author Index">
             Lilge, Sven
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#115139" title="Click to go to the Author Index">
             Barfoot, Timothy
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2909" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#field_robots" title="Click to go to the Keyword Index">
               Field Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#autonomous_vehicle_navigation" title="Click to go to the Keyword Index">
               Autonomous Vehicle Navigation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Frequency-modulated continuous-wave (FMCW) scanning radar has emerged as an alternative to spinning LiDAR for state estimation on mobile robots. Radar's longer wavelength is less affected by small particulates, providing operational advantages in challenging environments such as dust, smoke, and fog. This paper presents Radar Teach and Repeat (RT&amp;R): a full-stack radar system for long-term off-road robot autonomy. RT&amp;R can drive routes reliably in off-road cluttered areas without any GPS. We benchmark the radar system's closed-loop path-tracking performance and compare it to its 3D LiDAR counterpart. 11.8 km of autonomous driving was completed without interventions using only radar and gyro for navigation. RT&amp;R was evaluated on four different routes with progressively less structured scene geometry. RT&amp;R achieved lateral path-tracking root mean squared errors (RMSE) of 5.6 cm, 7.5 cm, and 12.1 cm as the routes became more challenging. These RMSE values are less than half of the width of one tire (24 cm) on our robot testing platform. These same routes have worst-case errors of 21.7 cm, 24.0 cm, and 43.8 cm. We conclude that radar is a viable alternative to LiDAR for long-term autonomy in challenging off-road scenarios. The implementation of RT&amp;R is open-source and available at: https://github.com/utiasASRL/vtr3.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt18_04">
             10:10-10:15, Paper ThBT18.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3889'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Structure-Aware Radar-Camera Depth Estimation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#409596" title="Click to go to the Author Index">
             Zhang, Fuyi
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#355060" title="Click to go to the Author Index">
             Yu, Zhu
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416199" title="Click to go to the Author Index">
             Li, ChunHao
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416202" title="Click to go to the Author Index">
             Zhang, Runmin
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#415069" title="Click to go to the Author Index">
             Bai, Xiaokai
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416213" title="Click to go to the Author Index">
             Zhou, Zili
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#287535" title="Click to go to the Author Index">
             Cao, Siyuan
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417205" title="Click to go to the Author Index">
             Wang, Fang
            </a>
           </td>
           <td class="r">
            Hangzhou City University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#287552" title="Click to go to the Author Index">
             Shen, Hui-liang
            </a>
           </td>
           <td class="r">
            Zhejaing University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3889" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Radar has gained much attention in autonomous driving due to its accessibility and robustness. However, its standalone application for depth perception is constrained by issues of sparsity and noise. Radar-camera depth estimation offers a more promising complementary solution. Despite significant progress, current approaches fail to produce satisfactory dense depth maps, due to the unsatisfactory processing of the sparse and noisy radar data. They constrain the regions of interest for radar points in rigid rectangular regions, which may introduce unexpected errors and confusions. To address these issues, we develop a structure-aware strategy for radar depth enhancement, which provides more targeted regions of interest by leveraging the structural priors of RGB images. Furthermore, we design a Multi-Scale Structure Guided Network to enhance radar features and preserve detailed structures, achieving accurate and structure-detailed dense metric depth estimation. Building on these, we propose a structure-aware radar-camera depth estimation framework, named SA-RCD. Extensive experiments demonstrate that our SA-RCD achieves state-of-the-art performance on the nuScenes dataset. Our code will be available at https://github.com/FreyZhangYeh/SA-RCD.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt18_05">
             10:15-10:20, Paper ThBT18.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4333'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Doppler Former: Velocity Supervision of Raw Radar Data
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416122" title="Click to go to the Author Index">
             Zhao, Shuo
            </a>
           </td>
           <td class="r">
            Megvii
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426004" title="Click to go to the Author Index">
             Sun, Wei
            </a>
           </td>
           <td class="r">
            Fvidar
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416673" title="Click to go to the Author Index">
             Li, Huadong
            </a>
           </td>
           <td class="r">
            MEGVII Technique
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426074" title="Click to go to the Author Index">
             Jiang, Zhaoying
            </a>
           </td>
           <td class="r">
            Southeast University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4333" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_manufacturing" title="Click to go to the Keyword Index">
               Computer Vision for Manufacturing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Thanks to the high robustness of 4D millimeter-wave radar in various environments, it has been widely applied in the field of autonomous driving. Recent research has increasingly focused on utilizing raw data, as a substitute for the sparse and noisy point cloud data. However, these approaches have not fully exploited the Doppler features present in the raw data. In this paper, we introduce the Doppler Former (DPF) module to efficiently extract velocity information from the target environment. DPF can be seamlessly integrated into most radar perception backbone and enhance their performance in downstream tasks. Additionally, we propose a new backbone, Fully Complex Convolutional Network (FCCN), which is more suitable for raw data. By incorporating the DPF module into FCCN, we achieved state-of-the-art (SOTA) performance on the RADIal dataset, with code available at https://github.com/coconut-zs/Fvidar-DopplerFormer.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt18_06">
             10:20-10:25, Paper ThBT18.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('5060'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Robust High-Speed State Estimation for Off-Road Navigation Using Radar Velocity Factors
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#341082" title="Click to go to the Author Index">
             Nissov, Morten
            </a>
           </td>
           <td class="r">
            NTNU
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#130394" title="Click to go to the Author Index">
             Edlund, Jeffrey
            </a>
           </td>
           <td class="r">
            Jet Propulsion Lab
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#256947" title="Click to go to the Author Index">
             Spieler, Patrick
            </a>
           </td>
           <td class="r">
            JPL
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#107007" title="Click to go to the Author Index">
             Padgett, Curtis
            </a>
           </td>
           <td class="r">
            JPL
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#132933" title="Click to go to the Author Index">
             Alexis, Kostas
            </a>
           </td>
           <td class="r">
            NTNU - Norwegian University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#204077" title="Click to go to the Author Index">
             Khattak, Shehryar
            </a>
           </td>
           <td class="r">
            NASA Jet Propulsion Laboratory
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab5060" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#field_robots" title="Click to go to the Keyword Index">
               Field Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#sensor_fusion" title="Click to go to the Keyword Index">
               Sensor Fusion
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Enabling robot autonomy in complex environments for mission critical application requires robust state estimation. Particularly under conditions where the exteroceptive sensors, which the navigation depends on, can be degraded by environmental challenges thus, leading to mission failure. It is precisely in such challenges where the potential for Frequency Modulated Continuous Wave (FMCW) radar sensors is highlighted: as a complementary exteroceptive sensing modality with direct velocity measuring capabilities. In this work we integrate radial speed measurements from a FMCW radar sensor, using a radial speed factor, to provide linear velocity updates into a sliding–window state estimator for fusion with LiDAR pose and IMU measurements. We demonstrate that this augmentation increases the robustness of the state estimator to challenging conditions present in the environment and the negative effects they can pose to vulnerable exteroceptive modalities. The proposed method is extensively evaluated using robotic field experiments conducted using an autonomous, full-scale, off-road vehicle operating at high-speeds (~12 m/s) in complex desert environments. Furthermore, the robustness of the approach is demonstrated for cases of both simulated and real-world degradation of the LiDAR odometry performance along with comparison against state-of-the-art methods for radar-inertial odometry on public datasets.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt19">
             <b>
              ThBT19
             </b>
            </a>
           </td>
           <td class="r">
            407
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thbt19" title="Click to go to the Program at a Glance">
             <b>
              Active Sensing
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#203745" title="Click to go to the Author Index">
             Abraham, Ian
            </a>
           </td>
           <td class="r">
            Yale University
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#196715" title="Click to go to the Author Index">
             Yau, Wei-Yun
            </a>
           </td>
           <td class="r">
            I2R
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt19_01">
             09:55-10:00, Paper ThBT19.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('54'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Graph-Based SLAM-Aware Exploration with Prior Topo-Metric Information
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#296723" title="Click to go to the Author Index">
             Bai, Ruofei
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#238454" title="Click to go to the Author Index">
             Guo, Hongliang
            </a>
           </td>
           <td class="r">
            Agency for Science Technology and Research
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#196715" title="Click to go to the Author Index">
             Yau, Wei-Yun
            </a>
           </td>
           <td class="r">
            I2R
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#115410" title="Click to go to the Author Index">
             Xie, Lihua
            </a>
           </td>
           <td class="r">
            NanyangTechnological University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab54" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#planning_under_uncertainty" title="Click to go to the Keyword Index">
               Planning under Uncertainty
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#autonomous_vehicle_navigation" title="Click to go to the Keyword Index">
               Autonomous Vehicle Navigation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Autonomous exploration requires a robot to explore an unknown environment while constructing an accurate map using SLAM (Simultaneous Localization and Mapping) techniques. Without prior information, the exploration performance is usually conservative due to the limited planning horizon. This paper exploits a prior topo-metric graph of the environment to benefit both the exploration efficiency and the pose graph reliability in SLAM. Based on the relationship between pose graph reliability and graph topology, we formulate a SLAM-aware path planning problem over the prior graph, which finds a fast exploration path enhanced with the globally informative loop-closing actions to stabilize the SLAM pose graph. A greedy algorithm is proposed to solve the problem, in which we derive theoretical thresholds that significantly prune non-optimal loop-closing actions without affecting the potential informative ones. Furthermore, we incorporate the proposed planner into a hierarchical exploration framework, with flexible features including path replanning, and online prior graph update that adds additional information to the prior graph. Simulation and real-world experiments indicate that the proposed method can reliably achieve higher mapping accuracy than compared methods when exploring environments with rich topologies, while maintaining comparable exploration efficiency. Our method is open-sourced on GitHub.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt19_02">
             10:00-10:05, Paper ThBT19.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('833'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Dynamic Multi-Objective Ergodic Path Planning Using Decomposition Methods
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#339434" title="Click to go to the Author Index">
             Breitfeld, Abigail
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#107777" title="Click to go to the Author Index">
             Wettergreen, David
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab833" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#space_robotics_and_automation" title="Click to go to the Keyword Index">
               Space Robotics and Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#field_robots" title="Click to go to the Keyword Index">
               Field Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robots are often employed in hazardous or inaccessible environments, such as disaster sites, extraterrestrial terrains, agricultural fields, and ocean floors. Autonomous operation is crucial in these scenarios to reduce reliance on human operators and enable real-time decision-making. However, robots must balance multiple, often conflicting, objectives. These objectives are subject to change based on new data or evolving conditions. This paper presents a novel approach to dynamic multi-objective trajectory planning. The proposed method leverages the boundary intersection decomposition technique to adaptively plan trajectories that balance multiple evolving objectives. Our approach ensures efficient and effective exploration by continuously optimizing the trade-offs between changing objectives. We show that our method performs on average 34% better in terms of solution quality on the dynamic multi-objective trajectory planning problem as compared to prior work.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt19_03">
             10:05-10:10, Paper ThBT19.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2273'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Rapid Autonomous Exploration of Large-Scale Environments for Ground Robots Based on Region Partitioning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419682" title="Click to go to the Author Index">
             Wen, Zhi
            </a>
           </td>
           <td class="r">
            Xidian University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#385398" title="Click to go to the Author Index">
             Liu, Xiaotao
            </a>
           </td>
           <td class="r">
            Xidian University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419139" title="Click to go to the Author Index">
             Lu, GaoJie
            </a>
           </td>
           <td class="r">
            Xidian University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#212814" title="Click to go to the Author Index">
             Liu, Jing
            </a>
           </td>
           <td class="r">
            Xidian University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2273" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#wheeled_robots" title="Click to go to the Keyword Index">
               Wheeled Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Autonomous exploration in large environments often leads to inefficient long backtracking, as distant targets are prioritized over closer ones. To address this issue, in this work, we propose a hierarchical planning method based on region partitioning. The space is dynamically partitioned at a coarse resolution, and as exploration progresses, regions with sufficient known areas are further subdivided to locate unknown areas more precisely. A utility function considering unknown area size, travel distance, and sequence similarity is used, and the simulated annealing algorithm generates a subregion sequence for global guidance. Within each subregion, a linear acceleration model helps select target points. This method reduces computational load and minimizes long-distance backtracking, enabling more efficient high-frequency planning. Extensive simulations and real world tests show that our method significantly improves exploration efficiency compared to existing vision-based techniques.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt19_04">
             10:10-10:15, Paper ThBT19.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2833'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              MapEx: Indoor Structure Exploration with Probabilistic Information Gain from Global Map Predictions
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#215359" title="Click to go to the Author Index">
             Ho, Cherie
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#314400" title="Click to go to the Author Index">
             Kim, Seungchan
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#277419" title="Click to go to the Author Index">
             Moon, Brady
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#399081" title="Click to go to the Author Index">
             Parandekar, Aditya
            </a>
           </td>
           <td class="r">
            Birla Institute of Technology and Science, Pilani - Goa Campus
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417974" title="Click to go to the Author Index">
             Harutyunyan, Narek
            </a>
           </td>
           <td class="r">
            Brown University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#200000" title="Click to go to the Author Index">
             Wang, Chen
            </a>
           </td>
           <td class="r">
            University at Buffalo
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#117857" title="Click to go to the Author Index">
             Sycara, Katia
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#185051" title="Click to go to the Author Index">
             Best, Graeme
            </a>
           </td>
           <td class="r">
            University of Technology Sydney
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104304" title="Click to go to the Author Index">
             Scherer, Sebastian
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2833" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#planning_under_uncertainty" title="Click to go to the Keyword Index">
               Planning under Uncertainty
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#integrated_planning_and_learning" title="Click to go to the Keyword Index">
               Integrated Planning and Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Exploration is a critical challenge in robotics, centered on understanding unknown environments. In this work, we focus on structured indoor environments, which often exhibit predictable, repeating patterns. Conventional frontier-based exploration approaches have difficulty leveraging this predictability, relying on simple heuristics such as 'closest first' for exploration. More recent deep learning-based methods predict unknown regions of the map for information gain computation, but these approaches are often sensitive to the predicted map quality or fail to account for sensor coverage. To overcome these issues, our key insight is to jointly reason over what the robot can observe and its uncertainty to calculate probabilistic information gain. We introduce MapEx, a new exploration framework that uses predicted maps to form probabilistic sensor model for information gain estimation. MapEx generates multiple predicted maps based on observed information, and takes into consideration both the computed variances of predicted maps and estimated visible area to estimate the information gain of a given viewpoint. Experiments on the real-world KTH dataset showed on average 12.4% improvement than representative map-prediction based exploration and 25.4% improvement than nearest frontier approach. Website: mapex-explorer.github.io
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt19_05">
             10:15-10:20, Paper ThBT19.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3147'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Ergodic Trajectory Optimization on Generalized Domains Using Maximum Mean Discrepancy
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425145" title="Click to go to the Author Index">
             Hughes, Christian
            </a>
           </td>
           <td class="r">
            Yale University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425136" title="Click to go to the Author Index">
             Warren, Houston
            </a>
           </td>
           <td class="r">
            University of Sydney
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425134" title="Click to go to the Author Index">
             Lee, Darrick
            </a>
           </td>
           <td class="r">
            Univ. of Edinburgh
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101688" title="Click to go to the Author Index">
             Ramos, Fabio
            </a>
           </td>
           <td class="r">
            University of Sydney, NVIDIA
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#203745" title="Click to go to the Author Index">
             Abraham, Ian
            </a>
           </td>
           <td class="r">
            Yale University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3147" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#integrated_planning_and_control" title="Click to go to the Keyword Index">
               Integrated Planning and Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present a novel formulation of ergodic trajectory optimization that can be specified over general domains using kernel maximum mean discrepancy. Ergodic trajectory optimization is an effective approach that generates coverage paths for problems related to robotic inspection, information gathering problems, and search and rescue. These optimization schemes compel the robot to spend time in a region proportional to the expected utility of visiting that region. Current methods for ergodic trajectory optimization rely on domain-specific knowledge, e.g., a defined utility map, and well-defined spatial basis functions to produce ergodic trajectories. Here, we present a generalization of ergodic trajectory optimization based on maximum mean discrepancy that requires only samples from the search domain. We demonstrate the ability of our approach to produce coverage trajectories on a variety of problem domains including robotic inspection of objects with differential kinematics constraints and on Lie groups without having access to domain specific knowledge. Furthermore, we show favorable computational scaling compared to existing state-of-the-art methods for ergodic trajectory optimization with a trade-off between domain specific knowledge and computational scaling, thus extending the versatility of ergodic coverage on a wider application domain
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt19_06">
             10:20-10:25, Paper ThBT19.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3261'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Ergodic Exploration Over Meshable Surfaces
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#341792" title="Click to go to the Author Index">
             Dong, Dayi, E
            </a>
           </td>
           <td class="r">
            University of California Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#289432" title="Click to go to the Author Index">
             Xu, Albert
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#256318" title="Click to go to the Author Index">
             Gutow, Geordan
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104011" title="Click to go to the Author Index">
             Choset, Howie
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#203745" title="Click to go to the Author Index">
             Abraham, Ian
            </a>
           </td>
           <td class="r">
            Yale University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3261" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#search_and_rescue_robots" title="Click to go to the Keyword Index">
               Search and Rescue Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computational_geometry" title="Click to go to the Keyword Index">
               Computational Geometry
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robotic search and rescue, exploration, and inspection require trajectory planning across a variety of domains. A popular approach to trajectory planning for these types of missions is ergodic search, which biases a trajectory to spend time in parts of the exploration domain that are believed to contain more information. Most prior work on ergodic search has been limited to searching simple surfaces, like a 2D Euclidean plane or a sphere, as they rely on projecting functions defined on the exploration domain onto analytically obtained Fourier basis functions. In this paper, we extend ergodic search to any surface that can be approximated by a triangle mesh. The basis functions are approximated through finite element methods on a triangle mesh of the domain. We formally prove that this approximation converges to the continuous case as the mesh approximation converges to the true domain. We demonstrate that on domains where analytical basis functions are available (plane, sphere), the proposed method obtains equivalent results, and while on other domains (torus, bunny, wind turbine), the approach is versatile enough to still search effectively. Lastly, we also compare with an existing ergodic search technique that can handle complex domains and show that our method results in a higher quality exploration.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt19_07">
             10:25-10:30, Paper ThBT19.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4995'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              FALCON: Fast Autonomous Aerial Exploration Using Coverage Path Guidance
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#269478" title="Click to go to the Author Index">
             Zhang, Yichen
            </a>
           </td>
           <td class="r">
            The Hong Kong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#285331" title="Click to go to the Author Index">
             Chen, Xinyi
            </a>
           </td>
           <td class="r">
            The Hong Kong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#340168" title="Click to go to the Author Index">
             Feng, Chen
            </a>
           </td>
           <td class="r">
            Hong Kong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#225160" title="Click to go to the Author Index">
             Zhou, Boyu
            </a>
           </td>
           <td class="r">
            Southern University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#142354" title="Click to go to the Author Index">
             Shen, Shaojie
            </a>
           </td>
           <td class="r">
            Hong Kong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4995" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__perception_and_autonomy" title="Click to go to the Keyword Index">
               Aerial Systems: Perception and Autonomy
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#autonomous_exploration" title="Click to go to the Keyword Index">
               Autonomous Exploration
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper introduces FALCON, a novel Fast Autonomous expLoration framework using COverage path guidaNce, which aims at setting a new performance benchmark in the field of autonomous aerial exploration. FALCON effectively harnesses the full potential of online generated coverage paths in enhancing exploration efficiency. The framework begins with an incremental connectivity-aware space decomposition and connectivity graph construction. Subsequently, a hierarchical planner generates a coverage path spanning the entire unexplored space, serving as a global guidance. Then, a local planner optimizes the frontier visitation order, consciously incorporating the intention of the global guidance. For fair and comprehensive benchmark experiments, we introduce a lightweight exploration planner evaluation environment that allows for comparing exploration planners across a variety of testing scenarios using an identical quadrotor simulator. Extensive benchmark experiments and ablation studies demonstrate the significant performance of FALCON. Real-world experiments conducted fully onboard further validate FALCON’s practical capability in complex and challenging environments.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt20">
             <b>
              ThBT20
             </b>
            </a>
           </td>
           <td class="r">
            408
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thbt20" title="Click to go to the Program at a Glance">
             <b>
              Agricultural Automation 2
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#133090" title="Click to go to the Author Index">
             Chowdhary, Girish
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana Champaign
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#105300" title="Click to go to the Author Index">
             Cappelleri, David
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt20_01">
             09:55-10:00, Paper ThBT20.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('27'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Improving Robotic Fruit Harvesting within Cluttered Environments through 3D Shape Completion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#274849" title="Click to go to the Author Index">
             Magistri, Federico
            </a>
           </td>
           <td class="r">
            University of Bonn
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#284469" title="Click to go to the Author Index">
             Pan, Yue
            </a>
           </td>
           <td class="r">
            University of Bonn
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#385229" title="Click to go to the Author Index">
             Bartels, Jake
            </a>
           </td>
           <td class="r">
            Queensland University of Technology (QUT)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#137262" title="Click to go to the Author Index">
             Behley, Jens
            </a>
           </td>
           <td class="r">
            University of Bonn
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101642" title="Click to go to the Author Index">
             Stachniss, Cyrill
            </a>
           </td>
           <td class="r">
            University of Bonn
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#133477" title="Click to go to the Author Index">
             Lehnert, Christopher
            </a>
           </td>
           <td class="r">
            Queensland University of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab27" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#robotics_and_automation_in_agriculture_and_forestry" title="Click to go to the Keyword Index">
               Robotics and Automation in Agriculture and Forestry
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#agricultural_automation" title="Click to go to the Keyword Index">
               Agricultural Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The world population is increasing and will, by 2050, nearly double its demand for food, feed, fuel, and fiber. Be sides environmental challenges, labor shortage also poses crucial challenges to the agricultural production system. Automation of manual tasks in crop production can potentially increase efficiency but also lead to a change in agricultural practices for more effective usage of available land. In this paper, we address the problem of robotic fruit harvesting in challenging real-world scenarios such as vertical farms, where robotic sensing and acting need to cope with a cluttered environment. Robotic fruit harvesting is typically done by directly detecting a grasp point in the sensor reading, which can lie on the fruit itself or on its peduncle depending on crop harvesting requirements. However, grasp point detection is not always possible as the ideal grasp point may be hidden behind leaves or other fruits. Our approach exploits shape completion techniques allowing us to estimate the complete 3D shape of a target fruit together with its pose even under strong occlusions. In this way, we can estimate a grasp point even when the fruit is only partially visible. We evaluate our approach on a real robotic manipulator operating in a vertical farm growing different fruit species and employing different harvesting tools. Our experiments show that, on average, our proposed pipeline increases the success rate by 18.5 percentage points, in terms of end-effector positioning, compared to the most competitive baseline among the ones reported in this work, that does not rely on shape completion.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt20_02">
             10:00-10:05, Paper ThBT20.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('39'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              P-AgSLAM: In-Row and Under-Canopy SLAM for Agricultural Monitoring in Cornfields
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#242489" title="Click to go to the Author Index">
             Kim, Kitae
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#321230" title="Click to go to the Author Index">
             Deb, Aarya
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#105300" title="Click to go to the Author Index">
             Cappelleri, David
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab39" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#robotics_and_automation_in_agriculture_and_forestry" title="Click to go to the Keyword Index">
               Robotics and Automation in Agriculture and Forestry
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#agricultural_automation" title="Click to go to the Keyword Index">
               Agricultural Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper, we present an in-row and under-canopy Simultaneous Localization and Mapping (SLAM) framework called the Purdue AgSLAM or P-AgSLAM which is designed for robot pose estimation and agricultural monitoring in cornfields. Our SLAM approach is primarily based on a 3D light detection and ranging (LiDAR) sensor and it is designed for the extraction of unique morphological features of cornfields which have significantly different characteristics from structured indoor and outdoor urban environments. The performance of the proposed approach has been validated with experiments in simulation and in real cornfield environments. P-AgSLAM outperforms existing state-of-the-art LiDAR-based state estimators in robot pose estimations and mapping.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt20_03">
             10:05-10:10, Paper ThBT20.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('228'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Robotic Mushroom Harvesting with Real2Sim2Real and Model Predictive Path Integral (MPPI) Based Planning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#356879" title="Click to go to the Author Index">
             Vasios, Konstantinos
            </a>
           </td>
           <td class="r">
            University of Essex
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#374345" title="Click to go to the Author Index">
             Porichis, Antonis
            </a>
           </td>
           <td class="r">
            University of Essex
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#126877" title="Click to go to the Author Index">
             Mohan, Vishwanathan
            </a>
           </td>
           <td class="r">
            University of Essex
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#113959" title="Click to go to the Author Index">
             Chatzakos, Panagiotis
            </a>
           </td>
           <td class="r">
            University of Essex AI Innovation Centre
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab228" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#agricultural_automation" title="Click to go to the Keyword Index">
               Agricultural Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#manipulation_planning" title="Click to go to the Keyword Index">
               Manipulation Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#dexterous_manipulation" title="Click to go to the Keyword Index">
               Dexterous Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present a strategy for the problem of robotic button mushroom harvesting (Agaricus Bisporus) that involves a Real2Sim2Real pipeline with dynamic scene reconstruction and a Model Predictive Path Integral (MPPI) control &amp; planning architecture for generating optimal uprooting motion primitives based on a physics engine simulation framework. Given the complex, nonlinear, anisotropic material properties of the mushrooms in combination with the multiple failure-mode modalities involved, we design a simulation framework around the PyBullet rigid-body physics engine by utilizing first-order approximations of the equivalent continuum mechanics models. By exploiting the computational efficiency of the aforementioned simulation framework, we directly apply the MPPI control framework to generate offline optimal mushroom uprooting motion primitives, defining a set of cost objectives for an optimal and within-constraint harvesting plan. We show that with this planning strategy, the ``root-bending'' action emerges autonomously for the single mushroom case as an optimal uprooting maneuver, which corresponds well to empirical knowledge obtained by expert pickers. A video demonstration of the proposed architecture can be found in https://youtu.be/k38ePBsBego.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt20_04">
             10:10-10:15, Paper ThBT20.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1748'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Collision-Aware Traversability Analysis for Autonomous Vehicles in the Context of Agricultural Robotics
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#376724" title="Click to go to the Author Index">
             Philippe, Florian
            </a>
           </td>
           <td class="r">
            Université De Haute-Alsace
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#236390" title="Click to go to the Author Index">
             Laconte, Johann
            </a>
           </td>
           <td class="r">
            French National Research Institute for Agriculture, Food and The
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420262" title="Click to go to the Author Index">
             Lapray, Pierre-Jean
            </a>
           </td>
           <td class="r">
            Université De Haute-Alsace
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#376716" title="Click to go to the Author Index">
             Spisser, Matthias
            </a>
           </td>
           <td class="r">
            Technology &amp; Strategy Engineering SAS
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#118648" title="Click to go to the Author Index">
             Lauffenburger, Jean-Philippe
            </a>
           </td>
           <td class="r">
            Université De Haute-Alsace
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1748" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#agricultural_automation" title="Click to go to the Keyword Index">
               Agricultural Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#sensor_fusion" title="Click to go to the Keyword Index">
               Sensor Fusion
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#collision_avoidance" title="Click to go to the Keyword Index">
               Collision Avoidance
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper, we introduce a novel method for safe navigation in agricultural robotics. As global environmental challenges intensify, robotics offers a powerful solution to reduce chemical usage while meeting the increasing demands for food production. However, significant challenges remain in ensuring the autonomy and resilience of robots operating in unstructured agricultural environments. Obstacles such as crops and tall grass, which are deformable, must be identified as safely traversable, compared to rigid obstacles. To address this, we propose a new traversability analysis method based on a 3D spectral map reconstructed using a LIDAR and a multispectral camera. This approach enables the robot to distinguish between safe and unsafe collisions with deformable obstacles. We perform a comprehensive evaluation of multispectral metrics for vegetation detection and incorporate these metrics into an augmented environmental map. Utilizing this map, we compute a physics-based traversability metric that accounts for the robot’s weight and size, ensuring safe navigation over deformable obstacles.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt20_05">
             10:15-10:20, Paper ThBT20.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3277'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Enhanced View Planning for Robotic Harvesting: Tackling Occlusions with Imitation Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#379085" title="Click to go to the Author Index">
             Li, Lun
            </a>
           </td>
           <td class="r">
            University of Groningen
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#169824" title="Click to go to the Author Index">
             Kasaei, Hamidreza
            </a>
           </td>
           <td class="r">
            University of Groningen
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3277" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#agricultural_automation" title="Click to go to the Keyword Index">
               Agricultural Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#robotics_and_automation_in_agriculture_and_forestry" title="Click to go to the Keyword Index">
               Robotics and Automation in Agriculture and Forestry
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In agricultural automation, inherent occlusion presents a major challenge for robotic harvesting. We propose an imitation learning-based viewpoint planning approach to actively adjust camera viewpoint and capture unobstructed images of the target crop. Traditional viewpoint planners and existing learning-based methods, depend on manually designed evaluation metrics or reward functions, often struggle to generalize to complex, unseen scenarios. Our method employs the Action Chunking with Transformer (ACT) algorithm to learn effective camera motion policies from expert demonstrations. This enables continuous six-degree-of-freedom (6-DoF) viewpoint adjustments that are smoother, more precise and reveal occluded targets. Extensive experiments in both simulated and real-world environments, featuring agricultural scenarios and a 6-DoF collaborative robot arm equipped with an RGB-D camera, demonstrate our method's superior success rate and efficiency, especially in complex occlusion conditions, as well as its ability to generalize across different crops without reprogramming. This study advances robotic harvesting by providing a practical “learn from demonstration” (LfD) solution to occlusion challenges, ultimately enhancing autonomous harvesting performance and productivity.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt20_06">
             10:20-10:25, Paper ThBT20.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4153'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Precision Harvesting in Cluttered Environments: Integrating End Effector Design with Dual Camera Perception
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#371470" title="Click to go to the Author Index">
             Koe, Kendall
            </a>
           </td>
           <td class="r">
            University of Illinois Urbana Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#399120" title="Click to go to the Author Index">
             Shah, Poojan Kalpeshbhai
            </a>
           </td>
           <td class="r">
            University of Illinois
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#315543" title="Click to go to the Author Index">
             Walt, Benjamin
            </a>
           </td>
           <td class="r">
            University of Illinois Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#399113" title="Click to go to the Author Index">
             Westphal, Jordan
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#293488" title="Click to go to the Author Index">
             Marri, Samhita
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#315566" title="Click to go to the Author Index">
             Kamtikar, Shivani Kiran
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#378514" title="Click to go to the Author Index">
             Nam, James Seungbum
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#182532" title="Click to go to the Author Index">
             Uppalapati, Naveen Kumar
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#133090" title="Click to go to the Author Index">
             Chowdhary, Girish
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#151228" title="Click to go to the Author Index">
             Krishnan, Girish
            </a>
           </td>
           <td class="r">
            University of Illinois Urbana Champaign
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4153" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#agricultural_automation" title="Click to go to the Keyword Index">
               Agricultural Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#robotics_and_automation_in_agriculture_and_forestry" title="Click to go to the Keyword Index">
               Robotics and Automation in Agriculture and Forestry
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#field_robots" title="Click to go to the Keyword Index">
               Field Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Due to labor shortages in specialty crop industries, a need for robotic automation to increase agricultural efficiency and productivity has arisen. Previous manipulation systems harvest well in uncluttered and structured environments. High tunnel environments are more compact and cluttered in nature, requiring a rethinking of the large form factor systems and grippers. We propose a novel co-designed framework incorporating a global detection camera and a local eye-in-hand camera that demonstrates precise localization of small fruits via closed-loop visual feedback and reliable error handling. Field experiments in high tunnels show that our system can reach 85.0% of cherry tomato fruit in 10.98s on average.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt20_07">
             10:25-10:30, Paper ThBT20.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4443'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              S^2BEV: Lightweight, Robust, and Precise SLAM-Oriented Segmentation Bird Eye’s View Mapping Approach
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#400551" title="Click to go to the Author Index">
             Sun, Yefeng
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#142453" title="Click to go to the Author Index">
             Gong, Liang
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425632" title="Click to go to the Author Index">
             Dai, Jialing
            </a>
           </td>
           <td class="r">
            University of Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#400548" title="Click to go to the Author Index">
             Bishu, Gao
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425650" title="Click to go to the Author Index">
             Cai, Jinghan
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#340073" title="Click to go to the Author Index">
             Lin, Gengjie
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#191260" title="Click to go to the Author Index">
             Moutarde, Fabien
            </a>
           </td>
           <td class="r">
            MINES ParisTech - PSL University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#149623" title="Click to go to the Author Index">
             Lu, Junguo
            </a>
           </td>
           <td class="r">
            Shanghai Jiaotong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#135074" title="Click to go to the Author Index">
             Liu, Chengliang
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4443" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#agricultural_automation" title="Click to go to the Keyword Index">
               Agricultural Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#robotics_and_automation_in_agriculture_and_forestry" title="Click to go to the Keyword Index">
               Robotics and Automation in Agriculture and Forestry
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             As modern agriculture progresses, the swift deployment of accurate maps becomes essential for the autonomous navigation and operation of orchard robots. Traditional mapping techniques often fall short in addressing the challenges posed by orchards, which are characterized by unstructured, dynamically changing environments with complex spatial and temporal dynamics due to seasonal and continuous operations.	This paper proposes a new approach to orchard map construction that merges topological maps with semantic SLAM, which leverages semantic segmentation to discriminate the topological invariant against volatile orchard scenes during mapping.	Meanwhile, this integration enables the creation, optimization, and rapid deployment of maps that are not only lightweight and robust but also precise. To evaluate the effectiveness of our method, we performed navigation tests in orchard environments using the newly developed maps. The experimental outcomes demonstrated a significant reduction in CPU usage, with maximum and average reductions of 7.6% and 4.5%, respectively. This approach not only enhances navigation efficiency but also facilitates quicker map deployment, effectively freeing computational resources for other critical tasks.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt21">
             <b>
              ThBT21
             </b>
            </a>
           </td>
           <td class="r">
            410
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thbt21" title="Click to go to the Program at a Glance">
             <b>
              Manipulation Planning and Control 2
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#139609" title="Click to go to the Author Index">
             Hu, Ai-Ping
            </a>
           </td>
           <td class="r">
            Georgia Tech Research Institute
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt21_01">
             09:55-10:00, Paper ThBT21.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('127'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Non-Prehensile Object Transport by Nonholonomic Robots Connected by Linear Deformable Elements
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#343401" title="Click to go to the Author Index">
             Zhi, Hui
            </a>
           </td>
           <td class="r">
            The Hong Kong Polytechnic University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#315057" title="Click to go to the Author Index">
             Zhang, Bin
            </a>
           </td>
           <td class="r">
            The Hong Kong Polytechnic University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#270207" title="Click to go to the Author Index">
             Qi, Jiaming
            </a>
           </td>
           <td class="r">
            Centre for Transformative Garment Production, HongKong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#159452" title="Click to go to the Author Index">
             Romero Velazquez, Jose Guadalupe
            </a>
           </td>
           <td class="r">
            ITAM
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#283315" title="Click to go to the Author Index">
             Shao, Xiaodong
            </a>
           </td>
           <td class="r">
            Beihang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#107047" title="Click to go to the Author Index">
             Yang, Chenguang
            </a>
           </td>
           <td class="r">
            University of Liverpool
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#123555" title="Click to go to the Author Index">
             Navarro-Alarcon, David
            </a>
           </td>
           <td class="r">
            The Hong Kong Polytechnic University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab127" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#motion_control" title="Click to go to the Keyword Index">
               Motion Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#constrained_motion_planning" title="Click to go to the Keyword Index">
               Constrained Motion Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#soft_robot_applications" title="Click to go to the Keyword Index">
               Soft Robot Applications
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a new method to automatically transport objects with mobile robots via non-prehensile actions. Our proposed approach utilizes a pair of nonholonomic robots connected by a deformable tube to efficiently manipulate objects of irregular shapes toward target locations. To autonomously perform this task, we develop a local integrated planning and control strategy that solves the problem in two steps (viz. enveloping and transport) based on the model predictive control (MPC) framework. The deformable underactuated system is simplified by a linear kinematic model. The enveloping problem is formulated as the minimization of multiple criteria that represent the enclosing error of the object by the variable morphology system. The transport problem is tackled by formulating the non-prehensile dragging action as an inequality constraint specified by the body frame of the deformable system. Reactive obstacle avoidance is ensured by a maximum margin-based term that utilizes the system's geometry and the feedback proximity to the environment. To validate the performance of the proposed methodology, we report a detailed experimental study with vision-guided robotic prototypes conducting multiple autonomous object transport tasks.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt21_02">
             10:00-10:05, Paper ThBT21.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1631'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Implicit Physics-Aware Policy for Dynamic Manipulation of Rigid Objects Via Soft Body Tools
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#256108" title="Click to go to the Author Index">
             Wang, Zixing
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#201200" title="Click to go to the Author Index">
             Qureshi, Ahmed H.
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1631" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#sensorimotor_learning" title="Click to go to the Keyword Index">
               Sensorimotor Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Recent advancements in robot tool use have unlocked their usage for novel tasks, yet the predominant focus is on rigid-body tools, while the investigation of soft-body tools and their dynamic interaction with rigid bodies remains unexplored. This paper takes a pioneering step towards dynamic one-shot soft tool use for manipulating rigid objects, a challenging problem posed by complex interactions and unobservable physical properties. To address these problems, we propose the Implicit Physics-aware (IPA) policy, designed to facilitate effective soft tool use across various environmental configurations. The IPA policy conducts system identification to implicitly identify physics information and predict goal-conditioned, one-shot actions accordingly. We validate our approach through a challenging task, i.e., transporting rigid objects using soft tools such as ropes to distant target positions in a single attempt under unknown environment physics parameters. Our experimental results indicate the effectiveness of our method in efficiently identifying physical properties, accurately predicting actions, and smoothly generalizing to real-world environments. The related video is available at: https://youtu.be/4hPrUDTc4Rg?si=WUZrT2vjLMt8qRWA
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt21_03">
             10:05-10:10, Paper ThBT21.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2301'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              General-Purpose Clothes Manipulation with Semantic Keypoints
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#246451" title="Click to go to the Author Index">
             Deng, Yuhong
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101988" title="Click to go to the Author Index">
             Hsu, David
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2301" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#representation_learning" title="Click to go to the Keyword Index">
               Representation Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Clothes manipulation is a critical capability for household robots; yet, existing methods are often confined to specific tasks, such as folding or flattening, due to the complex high-dimensional geometry of deformable fabric. This paper presents CLothes mAnipulation with Semantic keyPoints (CLASP) for general-purpose clothes manipulation, which enables the robot to perform diverse manipulation tasks over different types of clothes. The key idea of CLASP is semantic keypoints---e.g., "right shoulder", "left sleeve", etc.---a sparse spatial-semantic representation that is salient for both perception and action. Semantic keypoints of clothes can be effectively extracted from depth images and are sufficient to represent a broad range of clothes manipulation policies. CLASP leverages semantic keypoints to bridge LLM-powered task planning and low-level action execution in a two-level hierarchy. Extensive simulation experiments show that CLASP outperforms baseline methods across diverse clothes types in both seen and unseen tasks. Further, experiments with a Kinova dual-arm system on four distinct tasks---folding, flattening, hanging, and placing---confirm CLASP's performance on a real robot.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt21_04">
             10:10-10:15, Paper ThBT21.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2582'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Robust Optical Transceiver Manipulation in Cluttered Cable Environments Using 3D Scene Understanding and Planning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#211057" title="Click to go to the Author Index">
             Sarantopoulos, Iason
            </a>
           </td>
           <td class="r">
            Microsoft Research
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420059" title="Click to go to the Author Index">
             Liu, Chenyu
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420183" title="Click to go to the Author Index">
             Weng, Bohong
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420108" title="Click to go to the Author Index">
             Xu, Sicheng
            </a>
           </td>
           <td class="r">
            Microsoft Research Asia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#148987" title="Click to go to the Author Index">
             Zhang, Yizhong
            </a>
           </td>
           <td class="r">
            Microsoft
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#145292" title="Click to go to the Author Index">
             Yang, Jiaolong
            </a>
           </td>
           <td class="r">
            Microsoft Research
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#234427" title="Click to go to the Author Index">
             Tong, Xin
            </a>
           </td>
           <td class="r">
            MICROSOFT
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#314529" title="Click to go to the Author Index">
             Otto, Fabian
            </a>
           </td>
           <td class="r">
            Microsoft Research
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#374167" title="Click to go to the Author Index">
             Sweeney, David
            </a>
           </td>
           <td class="r">
            Microsoft Research
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#373102" title="Click to go to the Author Index">
             Chatzieleftheriou, Andromachi
            </a>
           </td>
           <td class="r">
            Microsoft
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#374242" title="Click to go to the Author Index">
             Rowstron, Antony
            </a>
           </td>
           <td class="r">
            Microsoft Research
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2582" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#constrained_motion_planning" title="Click to go to the Keyword Index">
               Constrained Motion Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robotic manipulation in cluttered environments presents significant challenges, particularly when the clutter includes thin, deformable objects like cables, which complicate perception and decision-making processes. In the context of datacenters, the automation of networking tasks often involves the manipulation of optical transceivers within densely packed cable configurations. Such environments are characterized by an abundance of delicate, overlapping, and intersecting cables, leading to frequent occlusions. This paper introduces an innovative system designed for the manipulation of optical transceivers in environments cluttered by cables. Our integrated approach combines advanced 3D scene understanding with a heuristic-based pushing policy to effectively manipulate optical transceivers amidst clutter. The system's perception component utilizes image segmentation and 3D reconstruction to accurately model the transceivers and surrounding cables. Meanwhile, the planning aspect employs a search algorithm with task-specific heuristics, to navigate the gripper, displace obstructing cables, and safely achieve a precise pre-grasp position in front of the target transceiver. We have conducted extensive evaluations of our methodology in both simulated and real-world settings, demonstrating its high success rates, robustness, and proficiency in addressing the unique challenges posed by cable-occluded environments within datacenters.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt21_05">
             10:15-10:20, Paper ThBT21.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2842'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              ReloPush: Multi-Object Rearrangement in Confined Spaces with a Nonholonomic Mobile Robot Pusher
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#219144" title="Click to go to the Author Index">
             Ahn, Jeeho
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#159826" title="Click to go to the Author Index">
             Mavrogiannis, Christoforos
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2842" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#mobile_manipulation" title="Click to go to the Keyword Index">
               Mobile Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#task_and_motion_planning" title="Click to go to the Keyword Index">
               Task and Motion Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#manipulation_planning" title="Click to go to the Keyword Index">
               Manipulation Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We focus on the problem of rearranging a set of objects within a confined space with a nonholonomically constrained mobile robot pusher. This problem is relevant to many real-world domains, including warehouse automation and construction. These domains give rise to instances involving a combination of geometric, kinematic, and physics constraints, which make planning particularly challenging. Prior work often makes simplifying assumptions like the use of holonomic mobile robots or dexterous manipulators capable of unconstrained overhand reaching. Our key insight is we can empower even a constrained mobile pusher to tackle complex rearrangement tasks by enabling it to modify the environment to its favor in a constraint-aware fashion. To this end, we describe a Push-Traversability graph, whose vertices represent poses that the pusher can push objects from and edges represent optimal, kinematically feasible, and stable push-rearrangements of objects. Based on this graph, we develop ReloPush, a planning framework that leverages Dubins curves and standard graph search techniques to generate an efficient sequence of object rearrangements to be executed by the pusher. We evaluate ReloPush across a series of challenging scenarios, involving the rearrangement of densely cluttered workspaces with up to eight objects by a 1tenth mobile robot pusher. ReloPush exhibits orders of magnitude faster runtimes and significantly more robust execution in the real world, evidenced in lower execution times and fewer losses of object contact, compared to two baselines lacking our proposed graph structure.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt21_06">
             10:20-10:25, Paper ThBT21.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2976'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Non-Prehensile Shape Manipulation of Elastoplastic Objects with Reinforcement Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#375959" title="Click to go to the Author Index">
             Herland, Sverre
            </a>
           </td>
           <td class="r">
            Norwegian University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#204295" title="Click to go to the Author Index">
             Misimi, Ekrem
            </a>
           </td>
           <td class="r">
            SINTEF Ocean
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2976" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present a novel framework for non-prehensile shape manipulation of deformable objects using Deep Reinforcement Learning. Unlike previous approaches that rely on grasping, our method employs a sequence of gentle pushing actions to deform objects into target shapes. We introduce a continuous parametrization of pushing actions that allows for precise control over pushing trajectories, enabling more flexible and efficient manipulation. The framework is applicable to a wide range of objects by representing them as sampled boundary coordinates, removing the need for predefined object partitions. Trained entirely in simulation, our controller demonstrates zero-shot transfer to real-world scenarios without additional training. Extensive evaluations show that our approach not only matches but substantially exceeds the performance of previous methods, while being more gentle and efficient. We demonstrate successful manipulation across various deformable objects and materials, including food items like salmon and pork loin. This work represents a significant advancement in robotic manipulation of deformable objects, with potential applications in food processing, manufacturing, and beyond.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt21_07">
             10:25-10:30, Paper ThBT21.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3133'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              ORLA*: Mobile Manipulator-Based Object Rearrangement with Lazy A*
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#251355" title="Click to go to the Author Index">
             Gao, Kai
            </a>
           </td>
           <td class="r">
            Rutgers University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420968" title="Click to go to the Author Index">
             Zhaxizhuoma, Zhaxizhuoma
            </a>
           </td>
           <td class="r">
            Shanghai Artificial Intelligence Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#278373" title="Click to go to the Author Index">
             Ding, Yan
            </a>
           </td>
           <td class="r">
            SUNY Binghamton
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#141408" title="Click to go to the Author Index">
             Zhang, Shiqi
            </a>
           </td>
           <td class="r">
            SUNY Binghamton
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#114720" title="Click to go to the Author Index">
             Yu, Jingjin
            </a>
           </td>
           <td class="r">
            Rutgers University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3133" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#mobile_manipulation" title="Click to go to the Keyword Index">
               Mobile Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#task_planning" title="Click to go to the Keyword Index">
               Task Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#manipulation_planning" title="Click to go to the Keyword Index">
               Manipulation Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Effectively performing object rearrangement is an essential skill for mobile manipulators, e.g., setting up a dinner table. A key challenge in such problems is deciding an appropriate ordering to effectively untangle object-object dependencies while considering the necessary motions for realizing manipulation tasks (e.g., pick and place). Computing time-optimal multi-object rearrangement solutions for mobile manipulators remains a largely untapped research direction. In this work, we propose ORLA*, which leverages delayed/lazy evaluation in searching for a high-quality object pick-n-place sequence that considers both end-effector and mobile robot base travel. ORLA* readily handles multi-layered rearrangement tasks powered by learning-based stability predictions. Employing an optimal solver for finding temporary locations for displacing objects, ORLA* can achieve global optimality. Through extensive simulation and ablation study, we confirm the effectiveness of ORLA* delivering quality solutions for challenging rearrangement instances. Supplementary materials are available at: https://gaokai15.github.io/ORLA-Star/
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt22">
             <b>
              ThBT22
             </b>
            </a>
           </td>
           <td class="r">
            411
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thbt22" title="Click to go to the Program at a Glance">
             <b>
              Imitation Learning for Manipulation 1
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#147280" title="Click to go to the Author Index">
             Hoffman, Judy
            </a>
           </td>
           <td class="r">
            Georgia Tech
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#180296" title="Click to go to the Author Index">
             Ravichandar, Harish
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt22_01">
             09:55-10:00, Paper ThBT22.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('144'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Learning Prehensile Dexterity by Imitating and Emulating State-Only Observations
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#285721" title="Click to go to the Author Index">
             Han, Yunhai
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#316530" title="Click to go to the Author Index">
             Chen, Zhenyang
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#319634" title="Click to go to the Author Index">
             Williams, Kyle
            </a>
           </td>
           <td class="r">
            Sandia National Labs
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#180296" title="Click to go to the Author Index">
             Ravichandar, Harish
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab144" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#dexterous_manipulation" title="Click to go to the Keyword Index">
               Dexterous Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             When human acquire physical skills (e.g., tennis) from experts, we tend to first learn from merely observing the expert. But this is often insufficient. We then engage in practice, where we try to emulate the expert and ensure that our actions produce similar effects on our environment. Inspired by this observation, we introduce Combining IMitation and Emulation for Motion Refinement (CIMER) -- a two-stage framework to learn dexterous prehensile manipulation skills from state-only observations. CIMER's first stage involves imitation: simultaneously encode the complex interdependent motions of the robot hand and the object in a structured dynamical system. This results in a reactive motion generation policy that provides a reasonable motion prior, but lacks the ability to reason about contact effects due to the lack of action labels. The second stage involves emulation: learn a motion refinement policy via reinforcement that adjusts the robot hand's motion prior such that the desired object motion is reenacted. CIMER is both task-agnostic (no task-specific reward design or shaping) and intervention-free (no additional teleoperated or labeled demonstrations). Detailed experiments with prehensile dexterity reveal that i) imitation alone is insufficient, but adding emulation drastically improves performance, ii) CIMER outperforms existing methods in terms of sample efficiency and the ability to generate realistic and stable motions, iii) CIMER can either zero-shot generalize or learn to adapt to novel objects from the YCB dataset, even outperforming expert policies trained with action labels in most cases. Source code and videos are available at https://sites.google.com/view/cimer-2024/.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt22_02">
             10:00-10:05, Paper ThBT22.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4179'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              EgoMimic: Scaling Imitation Learning Via Egocentric Video
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#339109" title="Click to go to the Author Index">
             Kareer, Simar
            </a>
           </td>
           <td class="r">
            Georgia Tech
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#339071" title="Click to go to the Author Index">
             Patel, Dhruv
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422018" title="Click to go to the Author Index">
             Punamiya, Ryan
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#376113" title="Click to go to the Author Index">
             Mathur, Pranay
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#285427" title="Click to go to the Author Index">
             Cheng, Shuo
            </a>
           </td>
           <td class="r">
            Gatech
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#222003" title="Click to go to the Author Index">
             Wang, Chen
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#147280" title="Click to go to the Author Index">
             Hoffman, Judy
            </a>
           </td>
           <td class="r">
            Georgia Tech
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#158823" title="Click to go to the Author Index">
             Xu, Danfei
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4179" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#big_data_in_robotics_and_automation" title="Click to go to the Keyword Index">
               Big Data in Robotics and Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#transfer_learning" title="Click to go to the Keyword Index">
               Transfer Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The scale and diversity of demonstration data required for imitation learning is a significant challenge. We present EgoMimic, a full-stack framework that scales manipulation through egocentric-view human demonstrations. EgoMimic achieves this through: (1) an ergonomic human data collection system using the Project Aria glasses, (2) a low-cost bimanual manipulator that minimizes the kinematic gap to human data, (3) cross-domain data alignment techniques, and (4) an imitation learning architecture that co-trains on hand and robot data. Compared to prior works that only extract high-level intent from human videos, our approach treats human and robot data equally as embodied demonstration data and learns a unified policy from both data sources. EgoMimic achieves significant improvement on a diverse set of long-horizon, single-arm and bimanual manipulation tasks over state-of-the-art imitation learning methods and enables generalization to entirely new scenes. Finally, we show a favorable scaling trend for EgoMimic, where adding 1 hour of additional hand data is significantly more valuable than 1 hour of additional robot data. Videos and additional information can be found at https://egomimic.github.io/.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt22_03">
             10:05-10:10, Paper ThBT22.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4204'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Neural Dynamics Augmented Diffusion Policy
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#299203" title="Click to go to the Author Index">
             Wu, Ruihai
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425442" title="Click to go to the Author Index">
             Chen, Haozhe
            </a>
           </td>
           <td class="r">
            University of Illinois Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#378728" title="Click to go to the Author Index">
             Zhang, Mingtong
            </a>
           </td>
           <td class="r">
            UIUC
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#377662" title="Click to go to the Author Index">
             Lu, Haoran
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425555" title="Click to go to the Author Index">
             Li, Yitong
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#237816" title="Click to go to the Author Index">
             Li, Yunzhu
            </a>
           </td>
           <td class="r">
            Columbia University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4204" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#model_learning_for_control" title="Click to go to the Keyword Index">
               Model Learning for Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#machine_learning_for_robot_control" title="Click to go to the Keyword Index">
               Machine Learning for Robot Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Imitation learning has been proven effective in mimicking demonstrations across various robotic manipulation tasks. However, to develop robust policies, current imitation methods, such as diffusion policy, require training on extensive demonstrations, making data collection labor-intensive. In contrast, model-based planning with dynamics models can effectively cover a sufficient range of configurations using only off-policy data. Yet, without the guidance of expert demonstrations, many tasks are difficult and time-consuming to plan using the dynamics models. Therefore, we take the best of both model learning and imitation learning, and propose neural dynamics augmented imitation learning that covers a large scene configurations with few-shot demonstrations. This method trains a robust diffusion policy in a local support region using few-shot demonstrations and rearranges objects outside this region into it using offline-trained neural dynamics models. Extensive experiments across various tasks in both simulations and real-world scenarios, including granular manipulation, contact-rich task and multi-object interaction task, have demonstrated that trained with only 1 to 30 demonstrations, our proposed method can robustly cover a significantly larger area than the policy trained purely from the demonstrations. Our project page is available at: https://dynamics-dp.github.io/.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt22_04">
             10:10-10:15, Paper ThBT22.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4434'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              CAGE: Causal Attention Enables Data-Efficient Generalizable Robotic Manipulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423620" title="Click to go to the Author Index">
             Xia, Shangning
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#300234" title="Click to go to the Author Index">
             Fang, Hongjie
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#224610" title="Click to go to the Author Index">
             Lu, Cewu
            </a>
           </td>
           <td class="r">
            ShangHai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#288067" title="Click to go to the Author Index">
             Fang, Hao-Shu
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4434" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Generalization in robotic manipulation remains a critical challenge, particularly when scaling to new environments with limited demonstrations. This paper introduces CAGE, a novel robotic manipulation policy designed to overcome these generalization barriers by integrating the pre-trained visual representation with causal attention mechanism. CAGE utilizes the powerful feature extraction capabilities of the vision foundation model DINOv2, combined with LoRA fine-tuning for robust environment understanding. The policy further employs a causal perceiver for effective token compression and a diffusion-based action head with attention to enhance task-specific fine-grained conditioning. With as few as 50 demonstrations from a single training environment, CAGE achieves robust generalization across diverse visual changes in objects, backgrounds, and viewpoints. Extensive experiments validate that CAGE significantly outperforms existing state-of-the-art RGB/RGB-D-based approaches in various manipulation tasks, especially under large distribution shifts. In similar environments, CAGE offers an average of 42% increase in task completion rate. While all baselines fail in unseen environments, CAGE manages to obtain a 43% completion rate and a 51% success rate in average, marking a substantial advancement toward the practical deployment of robots in real-world settings. Project website: cage-policy.github.io.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt22_05">
             10:15-10:20, Paper ThBT22.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4541'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              RoCoDA: Counterfactual Data Augmentation for Data-Efficient Robot Learning from Demonstrations
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#190259" title="Click to go to the Author Index">
             Ameperosa, Ezra
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#309954" title="Click to go to the Author Index">
             Collins, Jeremy
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416944" title="Click to go to the Author Index">
             Jain, Mrinal
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#155114" title="Click to go to the Author Index">
             Garg, Animesh
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4541" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#bimanual_manipulation" title="Click to go to the Keyword Index">
               Bimanual Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Imitation learning in robotics faces significant challenges in generalization due to the complexity of robotic environments and the high cost of data collection. We introduce RoCoDA, a novel method that unifies the concepts of invariance, equivariance, and causality within a single framework to enhance data augmentation for imitation learning. RoCoDA leverages causal invariance by modifying task-irrelevant subsets of the environment state without affecting the policy's output. Simultaneously, we exploit SE(3) equivariance by applying rigid body transformations to object poses and adjusting corresponding actions to generate synthetic demonstrations. We validate RoCoDA through extensive experiments on five robotic manipulation tasks, demonstrating improvements in policy performance, generalization, and sample efficiency compared to state-of-the-art data augmentation methods. Our policies exhibit robust generalization to unseen object poses, textures, and the presence of distractors. Furthermore, we observe emergent behavior such as re-grasping, indicating policies trained with RoCoDA possess a deeper understanding of task dynamics. By leveraging invariance, equivariance, and causality, RoCoDA provides a principled approach to data augmentation in imitation learning, bridging the gap between geometric symmetries and causal reasoning. Project Page: https://rocoda.github.io
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt22_06">
             10:20-10:25, Paper ThBT22.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4801'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Conditional Neural Expert Processes for Learning Movement Primitives from Demonstration
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#306731" title="Click to go to the Author Index">
             Yildirim, Yigit
            </a>
           </td>
           <td class="r">
            Bogazici University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#106000" title="Click to go to the Author Index">
             Ugur, Emre
            </a>
           </td>
           <td class="r">
            Bogazici University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4801" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Learning from Demonstration (LfD) is a widely used technique for skill acquisition in robotics. However, demonstrations of the same skill may exhibit significant variances, or learning systems may attempt to acquire different means of the same skill simultaneously, making it challenging to encode these motions into movement primitives. To address these challenges, we propose an LfD framework, namely the Conditional Neural Expert Processes (CNEP), that learns to assign demonstrations from different modes to distinct expert networks utilizing the inherent information within the latent space to match experts with the encoded representations. CNEP does not require supervision on which mode the trajectories belong to. We compare the performance of CNEP against widely used and powerful LfD methods such as Gaussian Mixture Models, Probabilistic Movement Primitives, and Stable Movement Primitives and show that our method outperforms these baselines on multimodal trajectory datasets. The results reveal enhanced modeling performance for movement primitives, leading to the synthesis of trajectories that more accurately reflect those demonstrated by experts, particularly when the skill demonstrations include intersection points from various trajectories. We evaluated the CNEP model on two real-robot tasks, namely obstacle avoidance and pick-and-place tasks, that require the robot to learn multi-modal motion trajectories and execute the correct primitives given target environment conditions. We also showed that our system is capable of on-the-fly adaptation to environmental changes via an online conditioning mechanism. Lastly, we believe that CNEP offers improved explainability and interpretability by autonomously finding discrete behavior primitives and providing probability values about its expert selection decisions.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt22_07">
             10:25-10:30, Paper ThBT22.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4909'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              PRIME: Scaffolding Manipulation Tasks with Behavior Primitives for Data-Efficient Imitation Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#336551" title="Click to go to the Author Index">
             Gao, Tian
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#291297" title="Click to go to the Author Index">
             Nasiriany, Soroush
            </a>
           </td>
           <td class="r">
            The University of Austin at Texas
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#313831" title="Click to go to the Author Index">
             Liu, Huihan
            </a>
           </td>
           <td class="r">
            University of Texas, Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#283805" title="Click to go to the Author Index">
             Yang, Quantao
            </a>
           </td>
           <td class="r">
            KTH Royal Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#203352" title="Click to go to the Author Index">
             Zhu, Yuke
            </a>
           </td>
           <td class="r">
            The University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4909" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Imitation learning has shown great potential for enabling robots to acquire complex manipulation behaviors. However, these algorithms suffer from high sample complexity in long-horizon tasks, where compounding errors accumulate over the task horizons. We present PRIME (PRimitive-based IMitation with data Efficiency), a behavior primitive-based framework designed for improving the data efficiency of imitation learning. PRIME scaffolds robot tasks by decomposing task demonstrations into primitive sequences, followed by learning a high-level control policy to sequence primitives through imitation learning. Our experiments demonstrate that PRIME achieves a significant performance improvement in multi-stage manipulation tasks, with 10-34% higher success rates in simulation over state-of-the-art baselines and 20-48% on physical hardware.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt23">
             <b>
              ThBT23
             </b>
            </a>
           </td>
           <td class="r">
            412
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thbt23" title="Click to go to the Program at a Glance">
             <b>
              Diffusion-Based Visual Perception and Learning
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#425192" title="Click to go to the Author Index">
             Brandt, Laura Eileen
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#140145" title="Click to go to the Author Index">
             Nalpantidis, Lazaros
            </a>
           </td>
           <td class="r">
            Technical University of Denmark
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt23_01">
             09:55-10:00, Paper ThBT23.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('57'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Towards Dense and Accurate Radar Perception Via Efficient Cross-Modal Diffusion Model
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#275156" title="Click to go to the Author Index">
             Zhang, Ruibin
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#392768" title="Click to go to the Author Index">
             Xue, Donglai
            </a>
           </td>
           <td class="r">
            Huzhou Institude of Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#392115" title="Click to go to the Author Index">
             Wang, Yuhan
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#393126" title="Click to go to the Author Index">
             Geng, Ruixu
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#200893" title="Click to go to the Author Index">
             Gao, Fei
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab57" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#range_sensing" title="Click to go to the Keyword Index">
               Range Sensing
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Millimeter wave (mmWave) radars have attracted significant attention from both academia and industry due to their capability to operate in extreme weather conditions. However, they face challenges in terms of sparsity and noise interference, which hinder their application in the field of micro aerial vehicle (MAV) autonomous navigation. To this end, this paper proposes a novel approach to dense and accurate mmWave radar point cloud construction via cross-modal learning. Specifically, we introduce diffusion models, which possess state-of-the-art performance in generative modeling, to predict LiDAR-like point clouds from paired raw radar data. We also incorporate the most recent diffusion model inference accelerating techniques to ensure that the proposed method can be implemented on MAVs. We validate the proposed method through extensive benchmark comparisons and real-world experiments, demonstrating its superior performance and generalization ability. Code and pre-trained models will be available at https://github.com/ZJU-FAST-Lab/Radar-Diffusion.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt23_02">
             10:00-10:05, Paper ThBT23.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('252'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              DiffMap: Enhancing Map Segmentation with Map Prior Using Diffusion Model
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#331577" title="Click to go to the Author Index">
             Jia, Peijin
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#285502" title="Click to go to the Author Index">
             Wen, Tuopu
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#403299" title="Click to go to the Author Index">
             Luo, Ziang
            </a>
           </td>
           <td class="r">
            TsingHua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#288524" title="Click to go to the Author Index">
             Yang, Mengmeng
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#286888" title="Click to go to the Author Index">
             Jiang, Kun
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#405906" title="Click to go to the Author Index">
             Liu, ZiYuan
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#405937" title="Click to go to the Author Index">
             Tang, Xuewei
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#405908" title="Click to go to the Author Index">
             Lei, Zhiquan
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#405923" title="Click to go to the Author Index">
             Cui, Le
            </a>
           </td>
           <td class="r">
            DIdi Inc
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#405925" title="Click to go to the Author Index">
             Sheng, Kehua
            </a>
           </td>
           <td class="r">
            DIdi Inc
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#405926" title="Click to go to the Author Index">
             Zhang, Bo
            </a>
           </td>
           <td class="r">
            DIdi Inc
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#286889" title="Click to go to the Author Index">
             Yang, Diange
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab252" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_transportation" title="Click to go to the Keyword Index">
               Computer Vision for Transportation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Constructing high-definition (HD) maps is a crucial requirement for enabling autonomous driving. In recent years, several map segmentation algorithms have been developed to address this need, leveraging advancements in Bird's-Eye View (BEV) perception. However, existing models still encounter challenges in producing realistic and consistent
             <p>
              semantic map layouts. One prominent issue is the limited utilization of
              <p>
               structured priors inherent in map segmentation masks. In light of this, we propose DiffMap, a novel approach specifically designed to model the structured priors of map segmentation masks using
               <p>
                latent diffusion model. By incorporating this technique, the performance of existing semantic segmentation methods can be significantly enhanced and certain structural errors present in the segmentation outputs can be effectively rectified. Notably, the proposed module can be seamlessly integrated into any map segmentation model, thereby augmenting its capability to accurately delineate semantic information. Furthermore, through extensive visualization analysis, our model demonstrates superior proficiency in generating results that more accurately reflect real-world map layouts, further validating its efficacy in improving the quality of the generated maps.
               </p>
              </p>
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt23_03">
             10:05-10:10, Paper ThBT23.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('593'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              AVD2: Accident Video Diffusion for Accident Video Description
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#414447" title="Click to go to the Author Index">
             Li, Cheng
            </a>
           </td>
           <td class="r">
            The Hong Kong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#414452" title="Click to go to the Author Index">
             Zhou, Keyuan
            </a>
           </td>
           <td class="r">
            Jilin University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#414450" title="Click to go to the Author Index">
             Liu, Tong
            </a>
           </td>
           <td class="r">
            Nanjing University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#414454" title="Click to go to the Author Index">
             Wang, Yu
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417976" title="Click to go to the Author Index">
             Zhuang, Mingqiao
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#339018" title="Click to go to the Author Index">
             Gao, Huan-ang
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#340733" title="Click to go to the Author Index">
             Jin, Bu
            </a>
           </td>
           <td class="r">
            Institute of Automation, Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#223795" title="Click to go to the Author Index">
             Zhao, Hao
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab593" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_transportation" title="Click to go to the Keyword Index">
               Computer Vision for Transportation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Traffic accidents present complex challenges for autonomous driving, often creating unpredictable scenarios that hinder accurate system interpretation and responses. Therefore, understanding accident scenarios is crucial for improving safety and gaining public trust. However, current methods struggle to fully explain accident causes and preventive actions. In this work, we introduce AVD2 (Accident Video Diffusion for Accident Video Description), a novel framework that enhances accident scene understanding by generating detailed natural language descriptions and reasoning. Additionally, we propose a new approach for augmenting accident video datasets by generating accident videos with a customized diffusion model, resulting in the EMM-AU (Enhanced Multi-Modal Accident Video Understanding) dataset, a higher-quality, more diverse version of MM-AU. Experimental results demonstrate that using the AVD2 system and training on the EMM-AU dataset achieves state-of-the-art performance in both automated metrics and human evaluations, significantly advancing accident analysis and prevention. Project resources are available at https://an-answer-tree.github.io
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt23_04">
             10:10-10:15, Paper ThBT23.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('905'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              LDM-ISP: Enhancing Neural ISP for Low Light with Latent Diffusion Models
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#336141" title="Click to go to the Author Index">
             Wen, Qiang
            </a>
           </td>
           <td class="r">
            The Hong Kong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#415173" title="Click to go to the Author Index">
             Rao, Zhefan
            </a>
           </td>
           <td class="r">
            HKUST
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#218458" title="Click to go to the Author Index">
             Xing, Yazhou
            </a>
           </td>
           <td class="r">
            The Hong Kong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#206397" title="Click to go to the Author Index">
             Chen, Qifeng
            </a>
           </td>
           <td class="r">
            HKUST
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab905" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Enhancing a low-light noisy RAW image into a well-exposed and clean sRGB image is a significant challenge for modern digital cameras. Prior approaches have difficulties in recovering fine-grained details and true colors of the scene under extremely low-light environments due to near-to-zero SNR. Meanwhile, diffusion models have shown significant progress towards general domain image generation. In this paper, we propose to leverage the pre-trained latent diffusion model to perform the neural ISP for enhancing extremely low-light images. Specifically, to tailor the pre-trained latent diffusion model to operate on the RAW domain, we train a set of lightweight taming modules to inject the RAW information into the diffusion denoising process via modulating the intermediate features of UNet. We further observe different roles of UNet denoising and decoder reconstruction in the latent diffusion model, which inspires us to decompose the low-light image enhancement task into latent-space low-frequency content generation and decoding-phase high-frequency detail maintenance. Through extensive experiments on representative datasets, we demonstrate our simple design not only achieves state-of-the-art performance in quantitative evaluations but also shows significant superiority in visual comparisons over strong baselines, which highlight the effectiveness of powerful generative priors for neural ISP under extremely low-light environments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt23_05">
             10:15-10:20, Paper ThBT23.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1604'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SteeredMarigold: Steering Diffusion towards Depth Completion of Largely Incomplete Depth Maps
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419512" title="Click to go to the Author Index">
             Gregorek, Jakub
            </a>
           </td>
           <td class="r">
            DTU - Technical University of Denmark
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#140145" title="Click to go to the Author Index">
             Nalpantidis, Lazaros
            </a>
           </td>
           <td class="r">
            Technical University of Denmark
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1604" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Even if the depth maps captured by RGB-D sensors deployed in real environments are often characterized by large areas missing valid depth measurements, the vast majority of depth completion methods still assumes depth values covering all areas of the scene. To address this limitation, we introduce SteeredMarigold, a training-free, zero-shot depth completion method capable of producing metric dense depth, even for largely incomplete depth maps. SteeredMarigold achieves this by using the available sparse depth points as conditions to steer a denoising diffusion probabilistic model. Our method outperforms relevant top-performing methods on the NYUv2 dataset, in tests where no depth was provided for a large area, achieving state-of-art performance and exhibiting remarkable robustness against depth map incompleteness. Our source code is publicly available at https://steeredmarigold.github.io.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt23_06">
             10:20-10:25, Paper ThBT23.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2938'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              DualDiff: Dual-Branch Diffusion Model for Autonomous Driving with Semantic Fusion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419487" title="Click to go to the Author Index">
             Li, Haoteng
            </a>
           </td>
           <td class="r">
            Xi'an Jiaotong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#413721" title="Click to go to the Author Index">
             Yang, Zhao
            </a>
           </td>
           <td class="r">
            Xi'an Jiaotong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#413804" title="Click to go to the Author Index">
             Qian, Zezhong
            </a>
           </td>
           <td class="r">
            Xi'an Jiaotong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#413951" title="Click to go to the Author Index">
             Zhao, Gongpeng
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419526" title="Click to go to the Author Index">
             Huang, Yuqi
            </a>
           </td>
           <td class="r">
            Xi'an Jiaotong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#367323" title="Click to go to the Author Index">
             Yu, Jun
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#450689" title="Click to go to the Author Index">
             Zhou, Huazheng
            </a>
           </td>
           <td class="r">
            Xi'an Jiaotong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416118" title="Click to go to the Author Index">
             Liu, Longjun
            </a>
           </td>
           <td class="r">
            Xi'an Jiaotong University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2938" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_transportation" title="Click to go to the Keyword Index">
               Computer Vision for Transportation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Accurate and high-fidelity driving scene reconstruction relies on fully leveraging scene information as conditioning. However, existing approaches, which primarily use 3D bounding boxes and binary maps for foreground and background control, fall short in capturing the complexity of the scene and integrating multi-modal information. In this paper, we propose DualDiff, a dual-branch conditional diffusion model designed to enhance multi-view driving scene generation. We introduce Occupancy Ray Sampling (ORS), a semantic-rich 3D representation, alongside numerical driving scene representation, for comprehensive foreground and background control. To improve cross-modal information integration, we propose a Semantic Fusion Attention (SFA) mechanism that aligns and fuses features across modalities. Furthermore, we design a foreground-aware masked (FGM) loss to enhance the generation of tiny objects. DualDiff achieves state-of-the-art performance in FID score, as well as consistently better results in downstream BEV segmentation and 3D object detection tasks.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt23_07">
             10:25-10:30, Paper ThBT23.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3172'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Anomalies-By-Synthesis: Anomaly Detection Using Generative Diffusion Models for Off-Road Navigation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#239383" title="Click to go to the Author Index">
             Ancha, Siddharth
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425146" title="Click to go to the Author Index">
             Jiang, Sunshine
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#119747" title="Click to go to the Author Index">
             Manderson, Travis
            </a>
           </td>
           <td class="r">
            McGill University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425192" title="Click to go to the Author Index">
             Brandt, Laura Eileen
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#311624" title="Click to go to the Author Index">
             Du, Yilun
            </a>
           </td>
           <td class="r">
            MIT
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#150496" title="Click to go to the Author Index">
             Osteen, Philip
            </a>
           </td>
           <td class="r">
            U.S. Army Research Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101682" title="Click to go to the Author Index">
             Roy, Nicholas
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3172" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In order to navigate safely and reliably in off-road environments, robots must detect anomalies that are out-of- distribution (OOD) with respect to the training data. We present an analysis-by-synthesis approach for pixel-wise anomaly detection without making any assumptions about the nature of OOD data. Given an input image, we use a generative diffusion model to synthesize an edited image that removes anomalies while keeping the remaining image unchanged. Then, we formulate anomaly detection as analyzing which image segments were modified by the diffusion model. We propose a novel inference approach for guided diffusion by analyzing the ideal guidance gradient and deriving a principled approximation that bootstraps the diffusion model to predict guidance gradients. Our editing technique is purely test-time that can be integrated into existing workflows without the need for retraining or fine-tuning. Finally, we use a combination of vision-language foundation models to compare pixels between the original and synthesized images in a learned feature space and detect semantically meaningful edits. Our diffusion-based analysis-by-synthesis method enables accurate anomaly detections for off-road navigation.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct1">
             <b>
              ThCT1
             </b>
            </a>
           </td>
           <td class="r">
            302
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thct1" title="Click to go to the Program at a Glance">
             <b>
              Mobile Manipulation: Planning and Control
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#142950" title="Click to go to the Author Index">
             Martín-Martín, Roberto
            </a>
           </td>
           <td class="r">
            University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#105401" title="Click to go to the Author Index">
             Tsagarakis, Nikos
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct1_01">
             11:15-11:20, Paper ThCT1.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('818'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              EHC-MM: Embodied Holistic Control for Mobile Manipulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#350817" title="Click to go to the Author Index">
             Wang, Jiawen
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#354081" title="Click to go to the Author Index">
             Jin, Yixiang
            </a>
           </td>
           <td class="r">
            Samsung Research China – Beijing (SRC-B)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#353919" title="Click to go to the Author Index">
             Shi, Jun
            </a>
           </td>
           <td class="r">
            Samsung Research China – Beijing (SRC-B)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#354018" title="Click to go to the Author Index">
             A, Yong
            </a>
           </td>
           <td class="r">
            Samsung Research China – Beijing (SRC-B)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#291827" title="Click to go to the Author Index">
             Li, Dingzhe
            </a>
           </td>
           <td class="r">
            Beihang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#216966" title="Click to go to the Author Index">
             Sun, Fuchun
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#152071" title="Click to go to the Author Index">
             Luo, Dingsheng
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#141789" title="Click to go to the Author Index">
             Fang, Bin
            </a>
           </td>
           <td class="r">
            Beijing University of Posts and Telecommunications / Tsinghua Un
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab818" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#mobile_manipulation" title="Click to go to the Keyword Index">
               Mobile Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#embodied_cognitive_science" title="Click to go to the Keyword Index">
               Embodied Cognitive Science
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#whole_body_motion_planning_and_control" title="Click to go to the Keyword Index">
               Whole-Body Motion Planning and Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Mobile manipulation typically entails the base for mobility, the arm for accurate manipulation, and the camera for perception. The principle of Distant Mobility, Close Grasping(DMCG) is essential for holistic control. We propose Embodied Holistic Control for Mobile Manipulation(EHC-MM) with the embodied function of sig(w): By formulating the DMCG principle as a Quadratic Programming (QP) problem, sig(w) dynamically balances the robot’s emphasis between movement and manipulation with the consideration of the robot's state and environment. In addition, we propose the Monitor-Position-Based Servoing(MPBS) with sig(w), enabling the tracking of the target during the operation. This approach enables coordinated control among the robot's base, arm, and camera, enhancing task efficiency. Through extensive simulations and real-world experiments, our approach significantly improves both the success rate and efficiency of mobile manipulation tasks, achieving a 95.6% success rate in real-world scenarios and a 52.8% increase in time efficiency.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct1_02">
             11:20-11:25, Paper ThCT1.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1267'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              BUMBLE: Unifying Reasoning and Acting with Vision-Language Models for Building-Wide Mobile Manipulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#344192" title="Click to go to the Author Index">
             Shah, Rutav
            </a>
           </td>
           <td class="r">
            The University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420899" title="Click to go to the Author Index">
             Yu, Albert
            </a>
           </td>
           <td class="r">
            UT Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#232957" title="Click to go to the Author Index">
             Zhu, Yifeng
            </a>
           </td>
           <td class="r">
            The University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#203352" title="Click to go to the Author Index">
             Zhu, Yuke
            </a>
           </td>
           <td class="r">
            The University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#142950" title="Click to go to the Author Index">
             Martín-Martín, Roberto
            </a>
           </td>
           <td class="r">
            University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1267" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#mobile_manipulation" title="Click to go to the Keyword Index">
               Mobile Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#big_data_in_robotics_and_automation" title="Click to go to the Keyword Index">
               Big Data in Robotics and Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#continual_learning" title="Click to go to the Keyword Index">
               Continual Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             To operate at a building scale, service robots must perform long-horizon mobile manipulation tasks by navigating to different rooms, accessing multiple floors, and interacting with a wide and unseen range of everyday objects. We refer to these tasks as Building-wide Mobile Manipulation. To tackle these inherently long-horizon tasks, we introduce BUMBLE, a unified Vision-Language Model (VLM)-based framework integrating open-world RGB-D perception, a wide spectrum of gross-to-fine motor skills, and dual-layered memory. Our extensive evaluation (90+ hours) indicates that BUMBLE outperforms competitive baselines in long-horizon building-wide tasks that require sequencing up to 12 skills, spanning 15 minutes per trial. BUMBLE achieves 47.1% success rate averaged over 70 trials in different buildings, tasks, and scene layouts from various starting locations. Our user study shows 22% higher task satisfaction using our framework compared to state-of-the-art VLM-based mobile manipulation methods. Finally, we show the potential of using increasingly capable foundation models to improve the system performance further. For more information, see https://robin-lab.cs.utexas.edu/BUMBLE/
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct1_03">
             11:25-11:30, Paper ThCT1.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1606'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#393666" title="Click to go to the Author Index">
             Liu, Peiqi
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422011" title="Click to go to the Author Index">
             Guo, Zhanqiu
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425018" title="Click to go to the Author Index">
             Warke, Mohit
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#291566" title="Click to go to the Author Index">
             Chintala, Soumith
            </a>
           </td>
           <td class="r">
            Facebook AI Research
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#171475" title="Click to go to the Author Index">
             Paxton, Chris
            </a>
           </td>
           <td class="r">
            Meta AI
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#314022" title="Click to go to the Author Index">
             Shafiullah, Nur Muhammad (Mahi)
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#161575" title="Click to go to the Author Index">
             Pinto, Lerrel
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1606" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mobile_manipulation" title="Click to go to the Keyword Index">
               Mobile Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#continual_learning" title="Click to go to the Keyword Index">
               Continual Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Significant progress has been made in open-vocabulary mobile manipulation, where the goal is for a robot to perform tasks in any environment given a natural language description. However, most current systems assume a static environment, which limits the system’s applicability in real-world scenarios where environments frequently change due to human intervention or the robot’s own actions. In this work, we present DynaMem, a new approach to open-world mobile manipulation that uses a dynamic spatio-semantic memory to represent a robot’s environment. DynaMem constructs a 3D data structure to maintain a dynamic memory of point clouds, and answers open-vocabulary object localization queries using multimodal LLMs or open-vocabulary features generated by state-of-the-art vision-language models. Powered by DynaMem, our robots can explore novel environments, search for objects not found in memory, and continuously update the memory as objects move, appear, or disappear in the scene. We run extensive experiments on the Stretch SE3 robots in three real and nine offline scenes, and achieve an average pick-and-drop success rate of 70% on non-stationary objects, a 3X improvement over state-of-the-art static systems.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct1_04">
             11:30-11:35, Paper ThCT1.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2235'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Whole-Body Model Predictive Control for Mobile Manipulation with Task Priority Transition
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#411908" title="Click to go to the Author Index">
             Wang, Yushi
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422412" title="Click to go to the Author Index">
             Chen, Ruoqu
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#105099" title="Click to go to the Author Index">
             Zhao, Mingguo
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2235" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#mobile_manipulation" title="Click to go to the Keyword Index">
               Mobile Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#whole_body_motion_planning_and_control" title="Click to go to the Keyword Index">
               Whole-Body Motion Planning and Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Mobile manipulators enable a wide range of operations with mobility and advanced manipulation capabilities. Despite their potential, existing approaches typically treat the mobile base and the manipulator separately, thereby limiting the optimality of the system for composite whole-body behaviors. In this work, we present a Whole-Body Model Predictive Control framework for mobile manipulation involving tasks with varying timelines. We integrate task priorities across both task and time dimensions, bringing inherent transition ability with enhanced performance. Our approach improves the trajectory tracking performance by up to 36% in terms of manipulability and reduces the maximum velocity during task priority transitions by 53% compared to the existing approach while maintaining a low computational cost of 4.3ms, allowing for high reactivity in real-world applications. We demonstrate its effectiveness through a door-opening and traversing behavior, showcasing the first successful implementation of a non-holonomic mobile manipulator in such a scenario. See https://wbmpc.github.io/ for supplemental materials.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct1_05">
             11:35-11:40, Paper ThCT1.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2569'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Dynamic Object Goal Pushing with Mobile Manipulators through Model-Free Constrained Reinforcement Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#315859" title="Click to go to the Author Index">
             Dadiotis, Ioannis
            </a>
           </td>
           <td class="r">
            Italian Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#251790" title="Click to go to the Author Index">
             Mittal, Mayank
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#105401" title="Click to go to the Author Index">
             Tsagarakis, Nikos
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#114045" title="Click to go to the Author Index">
             Hutter, Marco
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2569" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#mobile_manipulation" title="Click to go to the Keyword Index">
               Mobile Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Non-prehensile pushing to move and reorient objects to a goal is a versatile loco-manipulation skill. In the real world, the object's physical properties and friction with the floor contain significant uncertainties, which makes the task challenging for a mobile manipulator. In this paper, we develop a learning-based controller for a mobile manipulator to move an unknown object to a desired position and yaw orientation through a sequence of pushing actions. The proposed controller for the robotic arm and the mobile base motion is trained using a constrained Reinforcement Learning (RL) formulation. We demonstrate its capability in experiments with a quadrupedal robot equipped with an arm. The learned policy achieves a success rate of 91.35% in simulation and at least 80% on hardware in challenging scenarios. Through our extensive hardware experiments, we show that the approach demonstrates high robustness against unknown objects of different masses, materials, sizes, and shapes. It reactively discovers the pushing location and direction, thus achieving contact-rich behavior while observing only the pose of the object. Additionally, we demonstrate the adaptive behavior of the learned policy towards preventing the object from toppling.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct1_06">
             11:40-11:45, Paper ThCT1.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4814'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Door-To-Door Parcel Delivery from Supply Point to Users Home with Heterogeneous Robot Team: EuROBIN First Year Robotics Hackathon
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#169970" title="Click to go to the Author Index">
             Suarez, Alejandro
            </a>
           </td>
           <td class="r">
            University of Seville
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#223045" title="Click to go to the Author Index">
             Kartmann, Rainer
            </a>
           </td>
           <td class="r">
            Karslruhe Institute of Technology (KIT)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#135778" title="Click to go to the Author Index">
             Leidner, Daniel
            </a>
           </td>
           <td class="r">
            German Aerospace Center (DLR)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#252912" title="Click to go to the Author Index">
             Rossini, Luca
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#338725" title="Click to go to the Author Index">
             Huber, Johann
            </a>
           </td>
           <td class="r">
            ISIR, Sorbonne Université
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#274178" title="Click to go to the Author Index">
             Azevedo, Carlos
            </a>
           </td>
           <td class="r">
            Instituto Superior Técnico - Institute for Systems and Robotics
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#188333" title="Click to go to the Author Index">
             Rouxel, Quentin
            </a>
           </td>
           <td class="r">
            INRIA
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#195453" title="Click to go to the Author Index">
             Bjelonic, Marko
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#267920" title="Click to go to the Author Index">
             Gonzalez-Morgado, Antonio
            </a>
           </td>
           <td class="r">
            Universidad De Sevilla
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#216000" title="Click to go to the Author Index">
             Dreher, Christian R. G.
            </a>
           </td>
           <td class="r">
            Karlsruhe Institute of Technology (KIT)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#176754" title="Click to go to the Author Index">
             Schmaus, Peter
            </a>
           </td>
           <td class="r">
            German Aerospace Center (DLR)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#208752" title="Click to go to the Author Index">
             Laurenzi, Arturo
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#266764" title="Click to go to the Author Index">
             Hélénon, François
            </a>
           </td>
           <td class="r">
            Sorbonne Université
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#340251" title="Click to go to the Author Index">
             Serra, Rodrigo
            </a>
           </td>
           <td class="r">
            Institute for Systems and Robotics / Instituto Superior Técnico
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#385651" title="Click to go to the Author Index">
             Rochel, Olivier
            </a>
           </td>
           <td class="r">
            INRIA Institut National De Recherche En Sciences Et Technologies
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#182706" title="Click to go to the Author Index">
             Wellhausen, Lorenz
            </a>
           </td>
           <td class="r">
            ETH Zürich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#266595" title="Click to go to the Author Index">
             Perez Sanchez, Vicente
            </a>
           </td>
           <td class="r">
            University of Seville, GRVC
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#232674" title="Click to go to the Author Index">
             Gao, Jianfeng
            </a>
           </td>
           <td class="r">
            Karlsruhe Institute of Technology (KIT)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#223641" title="Click to go to the Author Index">
             Bauer, Adrian Simon
            </a>
           </td>
           <td class="r">
            German Aerospace Center (DLR)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#275331" title="Click to go to the Author Index">
             De Luca, Alessio
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#359611" title="Click to go to the Author Index">
             Abrini, Mouad
            </a>
           </td>
           <td class="r">
            Sorbonne University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#293447" title="Click to go to the Author Index">
             Bettencourt, Rui
            </a>
           </td>
           <td class="r">
            Institute for Systems and Robotics / Instituto Superior Técnico
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#127834" title="Click to go to the Author Index">
             Mouret, Jean-Baptiste
            </a>
           </td>
           <td class="r">
            Inria
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#217937" title="Click to go to the Author Index">
             Lee, Joonho
            </a>
           </td>
           <td class="r">
            Neuromeka
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#385629" title="Click to go to the Author Index">
             Viana Servan, Pablo
            </a>
           </td>
           <td class="r">
            GRVC
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#277637" title="Click to go to the Author Index">
             Pohl, Christoph
            </a>
           </td>
           <td class="r">
            Karlsruhe Institute of Technology (KIT)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#385381" title="Click to go to the Author Index">
             Batti, Nesrine
            </a>
           </td>
           <td class="r">
            German Aerospace Center (DLR)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#385889" title="Click to go to the Author Index">
             Vedelago, Diego
            </a>
           </td>
           <td class="r">
            Fondazione Istituto Italiano Di Tecnologia (IIT)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#385145" title="Click to go to the Author Index">
             Guda, Vamsi Krishna
            </a>
           </td>
           <td class="r">
            Sorbonne University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#362662" title="Click to go to the Author Index">
             Carlos, Alvarez Cia
            </a>
           </td>
           <td class="r">
            Universidad De Sevilla
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#292001" title="Click to go to the Author Index">
             Reister, Fabian
            </a>
           </td>
           <td class="r">
            Karlsruhe Institute of Technology (KIT)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#133089" title="Click to go to the Author Index">
             Friedl, Werner
            </a>
           </td>
           <td class="r">
            German AerospaceCenter (DLR)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#385890" title="Click to go to the Author Index">
             Burchielli, Corrado
            </a>
           </td>
           <td class="r">
            Fondazione Istituto Italiano Di Tecnologia (IIT)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#219827" title="Click to go to the Author Index">
             Baudry, Aline
            </a>
           </td>
           <td class="r">
            CNRS
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#232702" title="Click to go to the Author Index">
             Peller-Konrad, Fabian
            </a>
           </td>
           <td class="r">
            Karlsruhe Institute of Technology (KIT)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#170214" title="Click to go to the Author Index">
             Gumpert, Thomas
            </a>
           </td>
           <td class="r">
            German Aerospace Center (DLR)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#201773" title="Click to go to the Author Index">
             Muratore, Luca
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#303580" title="Click to go to the Author Index">
             Gauthier, Philippe
            </a>
           </td>
           <td class="r">
            Sorbonne Université
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#385382" title="Click to go to the Author Index">
             Schedl-Warpup, Rebecca
            </a>
           </td>
           <td class="r">
            German Aerospace Center (DLR)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#114045" title="Click to go to the Author Index">
             Hutter, Marco
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#108790" title="Click to go to the Author Index">
             Ivaldi, Serena
            </a>
           </td>
           <td class="r">
            INRIA
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#107437" title="Click to go to the Author Index">
             Lima, Pedro U.
            </a>
           </td>
           <td class="r">
            Instituto Superior Técnico - Institute for Systems and Robotics
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#110325" title="Click to go to the Author Index">
             Doncieux, Stéphane
            </a>
           </td>
           <td class="r">
            Sorbonne University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#105401" title="Click to go to the Author Index">
             Tsagarakis, Nikos
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#102922" title="Click to go to the Author Index">
             Asfour, Tamim
            </a>
           </td>
           <td class="r">
            Karlsruhe Institute of Technology (KIT)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104455" title="Click to go to the Author Index">
             Ollero, Anibal
            </a>
           </td>
           <td class="r">
            AICIA. G41099946
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101628" title="Click to go to the Author Index">
             Albu-Schäffer, Alin
            </a>
           </td>
           <td class="r">
            DLR - German Aerospace Center
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4814" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#cooperating_robots" title="Click to go to the Keyword Index">
               Cooperating Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mobile_manipulation" title="Click to go to the Keyword Index">
               Mobile Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#service_robotics" title="Click to go to the Keyword Index">
               Service Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Logistics and service operations involving parcel preparation, delivery, and unpacking from a supply point to the user's home could be carried out completely by robots in the near future, taking benefit of the capabilities of the different robot morphologies for the logistics, outdoors, and domestic environments. The use of robots for parcel delivery can contribute to the goals of sustainability and reduced emissions by exploiting the different locomotion modalities (wheeled, legged, and aerial). This paper reports the development and results obtained from the first robotics hackathon celebrated as part of the European Robotics and Artificial Intelligence Network (euROBIN) involving eight robotic platforms in three domains: 1) an industrial robotic arm for parcel preparation at the supply point, 2) a Centauro robot, a dual-arm aerial manipulator, and a wheeled-legged quadruped for parcel transportation, and 3) two humanoid robots and two commercial mobile manipulators for parcel delivery and unpacking in domestic scenarios. The paper describes the joint operation and the evaluation scenario, the features and capabilities of the robots, particularly those involved in the realization of the tasks, and the lessons learned.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct2">
             <b>
              ThCT2
             </b>
            </a>
           </td>
           <td class="r">
            301
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thct2" title="Click to go to the Program at a Glance">
             <b>
              Bio-Inspired Robot Learning
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#246678" title="Click to go to the Author Index">
             Tucker, Maegan
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#107321" title="Click to go to the Author Index">
             Krichmar, Jeffrey
            </a>
           </td>
           <td class="r">
            University of California, Irvine
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct2_01">
             11:15-11:20, Paper ThCT2.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1071'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              HSRL: A Hierarchical Control System Based on Spiking Deep Reinforcement Learning for Robot Navigation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#351649" title="Click to go to the Author Index">
             Yang, Bo
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419048" title="Click to go to the Author Index">
             Zhou, Shibo
            </a>
           </td>
           <td class="r">
            Zhejiang Lab
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#418997" title="Click to go to the Author Index">
             Lin, Chaohui
            </a>
           </td>
           <td class="r">
            Zhejiang Lab
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#418989" title="Click to go to the Author Index">
             Chai, Qingao
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#405598" title="Click to go to the Author Index">
             Yan, Rui
            </a>
           </td>
           <td class="r">
            Zhejiang University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420156" title="Click to go to the Author Index">
             Ma, De
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#366616" title="Click to go to the Author Index">
             Pan, Gang
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#150562" title="Click to go to the Author Index">
             Tang, Huajin
            </a>
           </td>
           <td class="r">
            Zhejiang University, China
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1071" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#bioinspired_robot_learning" title="Click to go to the Keyword Index">
               Bioinspired Robot Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_control" title="Click to go to the Keyword Index">
               Motion Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Reinforcement Learning (RL) has shown promise in robotic navigation tasks, yet applying it to real-world environments remains challenging due to dynamic complexities and the need for dynamically feasible actions. We propose a hierarchical control framework based on Spiking Deep Reinforcement Learning (SDRL) for robust robot navigation in real environments. Our approach utilizes a two-layer architecture: a high-level decision layer powered by a Spiking GRU network for handling partially observable environments, and a low-level executive layer employing Continuous Attractor Neural Networks (CANNs) to ensure precise and continuous actions. This hierarchical structure allows real-time decision-making that respects the physical constraints of the robot. Experimental results show that our method adapts effectively to new environments without fine-tuning and surpasses existing methods in performance. We also explore the implementation on the Darwin3 chip, paving the way for biologically inspired motion control in future robotic applications.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct2_02">
             11:20-11:25, Paper ThCT2.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1687'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Materials Matter: Investigating Functional Advantages of Bio-Inspired Materials Via Simulated Robotic Hopping
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#386098" title="Click to go to the Author Index">
             Schulz, Andrew
            </a>
           </td>
           <td class="r">
            Max Planck Institute for Intelligent System
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#355425" title="Click to go to the Author Index">
             Ahmad, Ayah
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#246678" title="Click to go to the Author Index">
             Tucker, Maegan
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1687" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#methods_and_tools_for_robot_system_design" title="Click to go to the Keyword Index">
               Methods and Tools for Robot System Design
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#biologically_inspired_robots" title="Click to go to the Keyword Index">
               Biologically-Inspired Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#simulation_and_animation" title="Click to go to the Keyword Index">
               Simulation and Animation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In contrast with the diversity of materials found in nature, most robots are designed with some combination of aluminum, stainless steel, and 3D-printed filament. Additionally, robotic systems are typically assumed to follow basic rigid-body dynamics. However, several examples in nature illustrate how changes in physical material properties yield functional advantages. In this paper, we explore how physical materials (non-rigid bodies) affect the functional performance of a hopping robot. In doing so, we address the practical question of how to model and simulate material properties. Through these simulations we demonstrate that material gradients in the leg of a single-limb hopper provide functional advantages compared to homogeneous designs. For example, when considering incline ramp hopping, a material gradient with increasing density provides a 35% reduction in tracking error and a 23% reduction in power consumption compared to homogeneous stainless steel.
             <p>
              By providing bio-inspiration to the rigid limbs in a robotic system, we seek to show that future fabrication of robots should look to leverage the material anisotropies of moduli and density found in nature. This would allow for reduced vibrations in the system and would provide offsets of joint torques and vibrations while protecting their structural integrity against reduced fatigue and wear. This simulation system could inspire future intelligent material gradients of custom-fabricated robotic locomotive devices.
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct2_03">
             11:25-11:30, Paper ThCT2.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2837'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SHIRE: Enhancing Sample Efficiency Using Human Intuition in REinforcement Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#391577" title="Click to go to the Author Index">
             Joshi, Amogh
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#273795" title="Click to go to the Author Index">
             Kosta, Adarsh Kumar
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#299278" title="Click to go to the Author Index">
             Roy, Kaushik
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2837" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#bioinspired_robot_learning" title="Click to go to the Keyword Index">
               Bioinspired Robot Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#probabilistic_inference" title="Click to go to the Keyword Index">
               Probabilistic Inference
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The ability of neural networks to perform robotic perception and control tasks such as depth and optical flow estimation, simultaneous localization and mapping (SLAM), and automatic control has led to their widespread adoption in recent years. Deep Reinforcement Learning (DeepRL) has been used extensively in these settings, as it does not have the unsustainable training costs associated with supervised learning. However, DeepRL suffers from poor sample efficiency, i.e., it requires a large number of environmental interactions to converge to an acceptable solution. Modern RL algorithms such as Deep Q Learning and Soft Actor-Critic attempt to remedy this shortcoming but can not provide the explainability required in applications such as autonomous robotics. Humans intuitively understand the long-time-horizon sequential tasks common in robotics. Properly using such intuition can make RL policies more explainable while enhancing their sample efficiency. In this work, we propose SHIRE, a novel framework for encoding human intuition using Probabilistic Graphical Models (PGMs) and using it in the Deep RL training pipeline to enhance sample efficiency. Our framework achieves 25−78% sample efficiency gains across the environments we evaluate at negligible overhead cost. Additionally, by teaching RL agents the encoded elementary behavior, SHIRE enhances policy explainability. A real-world demonstration further highlights the efficacy of policies trained using our framework.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct2_04">
             11:30-11:35, Paper ThCT2.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3298'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Hyperdimensional Computing-Based Federated Learning in Mobile Robots through Synthetic Oversampling
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#373011" title="Click to go to the Author Index">
             Lee, Hyunsei
            </a>
           </td>
           <td class="r">
            DGIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417032" title="Click to go to the Author Index">
             Han, WoongJae
            </a>
           </td>
           <td class="r">
            DGIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419847" title="Click to go to the Author Index">
             Kim, Hojeong
            </a>
           </td>
           <td class="r">
            DGIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#372663" title="Click to go to the Author Index">
             Kwon, Hyukjun
            </a>
           </td>
           <td class="r">
            DGIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417091" title="Click to go to the Author Index">
             Jang, Shinhyoung
            </a>
           </td>
           <td class="r">
            DGIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#102631" title="Click to go to the Author Index">
             Suh, Il Hong
            </a>
           </td>
           <td class="r">
            Hanyang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#372674" title="Click to go to the Author Index">
             Kim, Yeseong
            </a>
           </td>
           <td class="r">
            DGIST
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3298" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#bioinspired_robot_learning" title="Click to go to the Keyword Index">
               Bioinspired Robot Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#networked_robots" title="Click to go to the Keyword Index">
               Networked Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Traditional federated learning frameworks, often reliant on deep neural networks, face challenges related to computational demands and privacy risks. In this paper, we present a novel Hyperdimensional (HD) Computing-based federated learning framework designed for resource-constrained mobile robots. Unlike other HD-based learning, our approach introduces dynamic encoding, which improves both model accuracy and privacy by continuously updating hypervector representations. To further address the issue of imbalanced data, especially prevalent in robotics tasks, we propose a hypervector oversampling technique, enhancing model robustness. Extensive evaluations on LiDAR-equipped mobile robots demonstrate that our oversampling method outperforms state-of-the-art HD computing frameworks, achieving up to a 22.9% increase in accuracy while maintaining computational efficiency and privacy protection.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct2_05">
             11:35-11:40, Paper ThCT2.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4251'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Brain-Inspired Spatial Continuous State Encoding for Efficient Spiking-Based Navigation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#418989" title="Click to go to the Author Index">
             Chai, Qingao
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421934" title="Click to go to the Author Index">
             Wang, Jiashuo
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#409724" title="Click to go to the Author Index">
             Jiang, Runhao
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#351649" title="Click to go to the Author Index">
             Yang, Bo
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#405598" title="Click to go to the Author Index">
             Yan, Rui
            </a>
           </td>
           <td class="r">
            Zhejiang University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#150562" title="Click to go to the Author Index">
             Tang, Huajin
            </a>
           </td>
           <td class="r">
            Zhejiang University, China
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4251" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#bioinspired_robot_learning" title="Click to go to the Keyword Index">
               Bioinspired Robot Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#cognitive_modeling" title="Click to go to the Keyword Index">
               Cognitive Modeling
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Spiking neural networks (SNNs) show great potential in mapless navigation tasks due to their low power consumption, but the continuous representation of spatial information poses a challenge to SNN training. Neuroscience findings reveal that spatial cognition cells encode spatial information through population spike patterns. Inspired by this, we propose a navigation method based on SNNs, leveraging spatial cognition cells, which include grid cells (GCs), head direction cells (HDCs), and boundary vector cells (BVCs). Our method integrates spike-based information to achieve precise navigation goal encoding and egocentric environment perception, significantly improving SNN navigation capabilities in complex environments. Simulation and real-world experiments demonstrate that our method achieves significant improvements in navigation success rate and energy efficiency, showcasing superior adaptability across environments. Our work provides a novel approach to developing efficient brain-inspired navigation systems.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct2_06">
             11:40-11:45, Paper ThCT2.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4795'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Rapid Adapting and Continual Learning Spiking Neural Network Path Planning Algorithm for Mobile Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#355217" title="Click to go to the Author Index">
             Espino, Harrison
            </a>
           </td>
           <td class="r">
            University of California, Irvine
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#355284" title="Click to go to the Author Index">
             Bain, Robert
            </a>
           </td>
           <td class="r">
            University of California Irvine
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#107321" title="Click to go to the Author Index">
             Krichmar, Jeffrey
            </a>
           </td>
           <td class="r">
            University of California, Irvine
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4795" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#neurorobotics" title="Click to go to the Keyword Index">
               Neurorobotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#learning_from_experience" title="Click to go to the Keyword Index">
               Learning from Experience
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Mapping traversal costs in an environment and planning paths based on this map are important for autonomous navigation. We present a neurorobotic navigation system that utilizes a Spiking Neural Network (SNN) Wavefront Planner and E-prop learning to concurrently map and plan paths in a large and complex environment. We incorporate a novel method for mapping which, when combined with the Spiking Wavefront Planner (SWP), allows for adaptive planning by selectively considering any combination of costs. The system is tested on a mobile robot platform in an outdoor environment with obstacles and varying terrain. Results indicate that the system is capable of discerning features in the environment using three measures of cost, (1) energy expenditure by the wheels, (2) time spent in the presence of obstacles, and (3) terrain slope. In just twelve hours of online training, E-prop learns and incorporates traversal costs into the path planning maps by updating the delays in the SWP. On simulated paths, the SWP plans significantly shorter and lower cost paths than A* and RRT*. The SWP is compatible with neuromorphic hardware and could be used for applications requiring low size, weight, and power.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct3">
             <b>
              ThCT3
             </b>
            </a>
           </td>
           <td class="r">
            303
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thct3" title="Click to go to the Program at a Glance">
             <b>
              Space Robotics 1
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#157520" title="Click to go to the Author Index">
             Beksi, William J.
            </a>
           </td>
           <td class="r">
            The University of Texas at Arlington
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct3_01">
             11:15-11:20, Paper ThCT3.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('648'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              LuVo: Lunar Visual Odometry Using Homography-Based Image Feature Matching
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#233386" title="Click to go to the Author Index">
             Soussan, Ryan
            </a>
           </td>
           <td class="r">
            Aerodyne Industries
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#418354" title="Click to go to the Author Index">
             McCaffery, John
            </a>
           </td>
           <td class="r">
            KBR, Inc
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417951" title="Click to go to the Author Index">
             McMichael, Scott
            </a>
           </td>
           <td class="r">
            NASA Ames Research Center, KBR Inc
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#103703" title="Click to go to the Author Index">
             Deans, Matthew
            </a>
           </td>
           <td class="r">
            NASA Ames Research Center
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab648" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#space_robotics_and_automation" title="Click to go to the Keyword Index">
               Space Robotics and Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present LuVo, an initialization-free stereo visual odometry (VO) method developed for the VIPER lunar rover. We provide a novel stereo registration method using LightGlue image feature matching in a warped, locally planar space that improves matching robustness to larger baseline stereo sequences and repetitive terrain that traditionally challenge odometry approaches. We additionally introduce methods that increase the usable image region for matching by estimating a horizon cutoff in image space and enhance robustness to stereo correspondence failures using a Manhattan distance search for valid stereo points during cloud alignment. We evaluate the performance of LuVo on a dataset of 155 simulated lunar stereo sequences and show that it significantly improves registration accuracy and success rates for clouds separated by both expected driving ranges below eight meters and longer distance translations of up to 16 meters. While LuVo is developed for VIPER, it can be used in other environments featuring slip-prone and repetitive terrain that limit rover travel.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct3_02">
             11:20-11:25, Paper ThCT3.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1123'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Instance Segmentation-Based Hazard Detection with Lunar South Pole Lighting
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#238365" title="Click to go to the Author Index">
             Cloud, Joseph
            </a>
           </td>
           <td class="r">
            NASA Kennedy Space Center
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420564" title="Click to go to the Author Index">
             Buckles, Bradley
            </a>
           </td>
           <td class="r">
            NASA Kennedy Space Center
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420565" title="Click to go to the Author Index">
             Muller, Thomas
            </a>
           </td>
           <td class="r">
            Bennett Aerospace, NASA Kennedy Space Center
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#157520" title="Click to go to the Author Index">
             Beksi, William J.
            </a>
           </td>
           <td class="r">
            The University of Texas at Arlington
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420566" title="Click to go to the Author Index">
             Schuler, Jason
            </a>
           </td>
           <td class="r">
            NASA Kennedy Space Center
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1123" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#space_robotics_and_automation" title="Click to go to the Keyword Index">
               Space Robotics and Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mining_robotics" title="Click to go to the Keyword Index">
               Mining Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper addresses rock hazard detection for in-situ resource utilization (ISRU) robotic navigation in the challenging visual environment of the lunar south pole (LSP). We evaluate three state-of-the-art instance segmentation models—Mask~R-CNN, YOLOv8, and SAM—using a novel, synthetically generated dataset that simulates LSP-specific illumination challenges at sun angles of 2.5°, 5°, and 7.5°. Additionally, we evaluate these approaches in both up and down-sun driving with low solar angle light. This study highlights the potential of deep learning-based approaches for improving ISRU operations by reliably identifying visual surface hazards, such as rocks, which may impede robotic navigation and excavation in future lunar missions.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct3_03">
             11:25-11:30, Paper ThCT3.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1641'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Resettable Land Anchor Launcher for Unmanned Rover Rescue and Slope Climbing
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422048" title="Click to go to the Author Index">
             Kainth, Aaryan
            </a>
           </td>
           <td class="r">
            University of California Santa Barbara
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422052" title="Click to go to the Author Index">
             Krohn, Andrew R.
            </a>
           </td>
           <td class="r">
            University of California Santa Barbara
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#184736" title="Click to go to the Author Index">
             Johnson, Kyle A.
            </a>
           </td>
           <td class="r">
            NASA Glenn Research Center
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#123317" title="Click to go to the Author Index">
             Schepelmann, Alexander
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#114825" title="Click to go to the Author Index">
             Hawkes, Elliot Wright
            </a>
           </td>
           <td class="r">
            University of California, Santa Barbara
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#226661" title="Click to go to the Author Index">
             Naclerio, Nicholas
            </a>
           </td>
           <td class="r">
            University of California, Santa Barbara
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1641" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#space_robotics_and_automation" title="Click to go to the Keyword Index">
               Space Robotics and Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mechanism_design" title="Click to go to the Keyword Index">
               Mechanism Design
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Unmanned planetary rovers have traversed kilometers of Lunar and Martian terrain while performing valuable science. However, they still face mobility challenges including steep slopes and unstable soil that can entrap vehicles, as demonstrated by NASA’s Spirit rover. Vehicles on Earth can depend on a human operator or rescue vehicle to tow them out of an entrapment, but remote rovers cannot, limiting their route to highly conservative path selections. To increase rover mobility on slopes and unstable soils, we present a resettable anchor launcher for independent self-rescue. The device launches a tethered land anchor away from the rover and then uses a winch to tow the rover up a hill or out of an entrapment. This paper presents the design of the launcher and its integration into a half-meter-long rover mobility platform with field testing at the NASA Glenn Research Center SLOPE Lab. We demonstrate repeatable launching and winching to help the rover climb a 17° slope of loose GRC-1 Lunar regolith simulant that it otherwise could not climb. Our work presents an alternative method to increase rover mobility, especially up slopes, and enables independent rover rescue, which could eventually increase mission duration and reduce risk of entrapment during extraterrestrial exploration.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct3_04">
             11:30-11:35, Paper ThCT3.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3894'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SOF-E: An Energy Efficient Robot for Collaborative Transport and Placement of Mechanical Meta-Material Modules
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425973" title="Click to go to the Author Index">
             Moon, Inchul
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#367609" title="Click to go to the Author Index">
             Sebastianelli, Frank
            </a>
           </td>
           <td class="r">
            NASA
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#340920" title="Click to go to the Author Index">
             Gregg, Christine
            </a>
           </td>
           <td class="r">
            NASA Ames Research Center
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#131537" title="Click to go to the Author Index">
             Cheung, Kenneth C.
            </a>
           </td>
           <td class="r">
            National Aeronautics and Space Administration (NASA)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3894" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#space_robotics_and_automation" title="Click to go to the Keyword Index">
               Space Robotics and Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#robotics_and_automation_in_construction" title="Click to go to the Keyword Index">
               Robotics and Automation in Construction
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#cooperating_robots" title="Click to go to the Keyword Index">
               Cooperating Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In-space assembly is a key capability to enable construction of large-scale structures required for sustained human presence in space. Robotic assembly is critical to reduce required crew time and risk, while modularity ensures that solutions are versatile and adaptive to complex mission concepts. NASA’s Automated Reconfigurable Mission Adaptive Digital Assembly Systems (ARMADAS) project demonstrated that robots with relatively low cost, size, and degrees-of-freedom (DoFs) can be used for large-scale modular lattice structure assembly. This is possible by using the structural modules for robotic systems metrology and error mitigation. Robots with reduced complexity may lead to advantages in initial and maintenance cost, offering an alternative to large, complex, and expensive robots. In this paper, we describe the Structure Omni-directional Foldable Explorer (SOF-E), a robot with significantly lower mass and DoF compared to the previous ARMADAS robot architecture. Although SOF-E is a five DoF robot with only two or three control states per actuator, it is capable of transporting and placing structural modules by collaborating with other instances of itself. We discuss the mechanical design and architecture of SOF-E, including analysis of energy usage during each operation. Experiments demonstrate that during locomotion and module transport tasks, SOF-E requires significantly lower energy than the previous cargo transport robot architecture, the Scaling Omni-directional Lattice Locomoting Explorer (SOLL-E). The cost of transport metric is used to compare the energy efficiency of the operation.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct3_05">
             11:35-11:40, Paper ThCT3.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4144'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Quarry-Bot: A Reconfigurable Cable-Suspended Robot for Lunar Site Engineering
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#317471" title="Click to go to the Author Index">
             Castrejon, Zahir
            </a>
           </td>
           <td class="r">
            University of Nevada Las Vegas
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#100181" title="Click to go to the Author Index">
             Oh, Paul Y.
            </a>
           </td>
           <td class="r">
            University of Nevada, Las Vegas (UNLV)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4144" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#field_robots" title="Click to go to the Keyword Index">
               Field Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#robotics_and_automation_in_agriculture_and_forestry" title="Click to go to the Keyword Index">
               Robotics and Automation in Agriculture and Forestry
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#space_robotics_and_automation" title="Click to go to the Keyword Index">
               Space Robotics and Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper introduces Quarry-Bot, a Reconfigurable Cable-Suspended Robot developed to support the NASA Artemis program’s efforts in preparing for the long-term colonization of the Moon and Mars. Quarry-Bot autonomously clears debris on the lunar surface, a key step in site preparation for future habitats and infrastructure. The system utilizes active control strategies, combined with the Moon’s lower gravity, to perform underhand rock tosses as a scalable approach to extraterrestrial site preparation. Its reconfigurable structure, including motorized anchor points and a lightweight tripod design, adjusts cable tensions to generate swing motions for debris displacement. The system is driven by two Dynamixel MX-106 motors for movement and steering, along with a NEMA 17 stepper motor for cable adjustments. A decentralized control system, managed by Raspberry Pi units, coordinates these components. Simulations and experiments conducted under both Earth and lunar gravity conditions demonstrate the effectiveness of Linear Quadratic Regulator (LQR) and Model Predictive Control (MPC) strategies in achieving rock throws. Quarry-Bot reaches swing angles and projects rocks over distances that may support lunar site clearing and overall engineering purposes. The paper concludes by discussing potential areas for further system refinement, including adjustments for different terrain conditions and im- proved actuation strategies for lunar missions.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct3_06">
             11:40-11:45, Paper ThCT3.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4494'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Tugging Controller That Maximizes Lateral Resistive Force by Mounding Sandy Terrain
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#185095" title="Click to go to the Author Index">
             Moon, Deaho
            </a>
           </td>
           <td class="r">
            University of California Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426858" title="Click to go to the Author Index">
             Huang, Chris
            </a>
           </td>
           <td class="r">
            University of California Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#331121" title="Click to go to the Author Index">
             Page, Justin
            </a>
           </td>
           <td class="r">
            UC Berkeley Mechanical Engineering
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#168436" title="Click to go to the Author Index">
             Stuart, Hannah
            </a>
           </td>
           <td class="r">
            UC Berkeley
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4494" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#space_robotics_and_automation" title="Click to go to the Keyword Index">
               Space Robotics and Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#field_robots" title="Click to go to the Keyword Index">
               Field Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#sensor_based_control" title="Click to go to the Keyword Index">
               Sensor-based Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Sandy environments present challenges for robotic space rovers and systems due to reduced traction, limiting mobility and tugging force. This paper presents an anchoring method that utilizes a winching system to create a sand mound in front of a mobile agent dragged through the media. The proposed controller is designed to consistently achieve real-time capture of close-to-maximal lateral sand mound resistive force, even when applied to varied uneven terrains, like holes or waves. Notably, tugging is non-reversible, so suitable peaks should be captured before breakdown and without necessarily knowing the global optimum a priori. The controller logic tracks both tugging force and agent pitch gradients to detect terrain conditions and peak force trends. Results show that the controller captures an average 92% of the maximum forces, within the previously winched workspace tested, across three different granular media with four varying structured terrain features. The controller achieves higher resistive force peaks on terrains with geometric features, as opposed to flat sand. We conclude that sand mounding through tugging is a viable means to generate robotic resistive forces for unknown sandy terrains, a simple yet effective anchoring mechanism.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct4">
             <b>
              ThCT4
             </b>
            </a>
           </td>
           <td class="r">
            304
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thct4" title="Click to go to the Program at a Glance">
             <b>
              Image and 3D Segmentation 2
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#168728" title="Click to go to the Author Index">
             Marron, Pedro Jose
            </a>
           </td>
           <td class="r">
            University of Duisburg-Essen
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct4_01">
             11:15-11:20, Paper ThCT4.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1454'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              RMSeg-UDA: Unsupervised Domain Adaptation for Road Marking Segmentation under Adverse Conditions
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#393706" title="Click to go to the Author Index">
             Cai, Yi-Chang
            </a>
           </td>
           <td class="r">
            National Chung Cheng University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#393676" title="Click to go to the Author Index">
             Hsiao, Heng Chih
            </a>
           </td>
           <td class="r">
            National Chung Cheng University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#238086" title="Click to go to the Author Index">
             Chiu, Wei-Chen
            </a>
           </td>
           <td class="r">
            National Chiao Tung University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#116421" title="Click to go to the Author Index">
             Lin, Huei-Yung
            </a>
           </td>
           <td class="r">
            National Taipei University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#353649" title="Click to go to the Author Index">
             Chiao-Tung, Chan
            </a>
           </td>
           <td class="r">
            Mechanical and Mechatronics Systems Research Laboratories, Indus
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1454" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_transportation" title="Click to go to the Keyword Index">
               Computer Vision for Transportation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#intelligent_transportation_systems" title="Click to go to the Keyword Index">
               Intelligent Transportation Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The segmentation of road markings plays a crucial role in visual perception for the autonomous driving system. It enables vehicles to recognize road markings at the pixel-level, and facilitates subsequent path planning, localization, and map construction tasks. Current techniques mainly focus on normal driving scenes (i.e., clear daytime), and the performance would decrease significantly for adverse weather conditions. This work proposes RMSeg-UDA: an unsupervised domain adaptive road marking segmentation framework. By combining schedule self- training and class-conditioned adversarial training, the network utilizes both labeled normal data and unlabeled data from other domains to train a road marking segmentation model. For the evaluation on adverse conditions, a new image dataset, RLMD- AC, is established with rainy and nighttime driving scenes. The experiments conducted using both public and our datasets have demonstrated the effectiveness of the proposed technique. Code and dataset are available at https://github.com/stu9113611/RM Seg-UDA.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct4_02">
             11:20-11:25, Paper ThCT4.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1986'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Enhancing the Utilization of Color Information in Point Cloud Semantic Segmentation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419194" title="Click to go to the Author Index">
             Guo, Xinyu
            </a>
           </td>
           <td class="r">
            Wuhan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#193655" title="Click to go to the Author Index">
             Gao, Zhi
            </a>
           </td>
           <td class="r">
            Temasek Laboratories @ NUS
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#378929" title="Click to go to the Author Index">
             Zhou, Zhiyu
            </a>
           </td>
           <td class="r">
            Wuhan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#439839" title="Click to go to the Author Index">
             Wang, Jingshi
            </a>
           </td>
           <td class="r">
            1.School of Aeronautics and Astronautics, Shanghai Jiao Tong Uni
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422788" title="Click to go to the Author Index">
             Tang, Luliang
            </a>
           </td>
           <td class="r">
            Wuhan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#439840" title="Click to go to the Author Index">
             Cao, Min
            </a>
           </td>
           <td class="r">
            Wuhan Guanggu Zoyon Science and Technology Company Ltd., Wuhan 4
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1986" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#recognition" title="Click to go to the Keyword Index">
               Recognition
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#sensor_fusion" title="Click to go to the Keyword Index">
               Sensor Fusion
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Point cloud semantic segmentation is crucial in various applications such as autonomous driving, robotics, and virtual reality, aiming to assign labels to each point in a cloud to reflect spatial relationships and boundaries. While previous methods primarily focus on geometric features, they often overlook the auxiliary role of color information, especially in scenes where geometric structures are less distinct. In this paper, we propose the Color Point Cloud Enhancement (CPCE) method to effectively leverage color information for improved 3D scene understanding. CPCE introduces a color information enhancement module with multi-scale consistency, enriching point features throughout the encoder stages. Additionally, we develop a novel contrastive learning module that uses relative color coordinates for point cloud serialization, allowing for the capture of positive and negative samples from distant points with similar color textures. Furthermore, we design a contrastive learning module tailored for scenes with weak geometric structures, enhancing feature representation through color-augmented contrast. Our method achieved a 78.1% mIoU on the ScanNet dataset, outperforming existing models trained on a single dataset. These results highlight the effectiveness of CPCE in scenarios where traditional methods struggle, particularly in enhancing segmentation accuracy by utilizing color as a critical feature.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct4_03">
             11:25-11:30, Paper ThCT4.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2425'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              UltraFastCrackSeg: A Lightweight Real-Time Crack Segmentation Model with Task-Oriented Pretraining
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#367558" title="Click to go to the Author Index">
             Qi, Weiqing
            </a>
           </td>
           <td class="r">
            HKUST
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#365001" title="Click to go to the Author Index">
             Zhao, Guoyang
            </a>
           </td>
           <td class="r">
            HKUST(GZ)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#276186" title="Click to go to the Author Index">
             Ma, Fulong
            </a>
           </td>
           <td class="r">
            The Hong Kong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#125556" title="Click to go to the Author Index">
             Liu, Ming
            </a>
           </td>
           <td class="r">
            Hong Kong University of Science and Technology (Guangzhou)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#438714" title="Click to go to the Author Index">
             Yang, Yang
            </a>
           </td>
           <td class="r">
            Hong Kong University of Science and Technology (Guangzhou)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2425" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#robotics_in_under_resourced_settings" title="Click to go to the Keyword Index">
               Robotics in Under-Resourced Settings
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Crack segmentation is pivotal for structural health monitoring, enabling the timely maintenance of critical infrastructure such as bridges and roads. However, existing deep learning models are often too computationally intensive for deployment on resource-constrained devices. To address this limitation, we introduce UltraFastCrackSeg, a lightweight model designed for real-time crack segmentation that effectively balances high accuracy with low computational demands. Featuring an efficient encoder-decoder architecture, our model significantly reduces parameter count and floating point operations (FLOPs) compared to current methods. We further enhance performance through a self-supervised pretraining approach that employs a novel, task-oriented masking strategy, thereby improving feature extraction. Experiments across multiple datasets demonstrate that UltraFastCrackSeg achieves state-of-the-art Intersection over Union (IoU) and F1 scores while maintaining a compact model size and high inference speed. Evaluations on a low-power CPU device confirm its capability to achieve up to 80 frames per second (FPS) with ONNX runtime optimization, making it highly suitable for real-time, on-site applications. These findings establish UltraFastCrackSeg as a robust and efficient solution for practical crack detection tasks.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct4_04">
             11:30-11:35, Paper ThCT4.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2563'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Enhancing 3D Scene Graphs with Real-Time Room Classification
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#337849" title="Click to go to the Author Index">
             Janzon, Simon
            </a>
           </td>
           <td class="r">
            University of Duisburg Essen
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#252339" title="Click to go to the Author Index">
             Medina Sanchez, Carlos
            </a>
           </td>
           <td class="r">
            Duisburg Essen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#338135" title="Click to go to the Author Index">
             Golkowski, Alexander Julian
            </a>
           </td>
           <td class="r">
            University of Duisburg-Essen
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#338170" title="Click to go to the Author Index">
             Handte, Marcus
            </a>
           </td>
           <td class="r">
            University of Duisburg-Essen
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#168728" title="Click to go to the Author Index">
             Marron, Pedro Jose
            </a>
           </td>
           <td class="r">
            University of Duisburg-Essen
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2563" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#software_architecture_for_robotic_and_automation" title="Click to go to the Keyword Index">
               Software Architecture for Robotic and Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In recent years, 3D scene graphs have become a critical tool in robotics and computer vision for enabling systems to understand both the geometric and semantic aspects of their surroundings. These data structures represent spatial and semantic relationships between objects in a three-dimensional environment, supporting tasks like navigation, object manipulation, and scene understanding. This paper presents a real-time pipeline for 3D scene graph generation that offers flexibility in image segmentation techniques while incorporating room classification that is based on a Random Forest model. Our work enables robots to dynamically update their understanding of complex and large-scale environments in real-time. We evaluate our approach systematically on a dataset and in a real-life experiment. The results demonstrate the capability of running our solution at over 10 Hz on an Nvidia Jetson AGX Orin SoC while also scaling favorably in larger environments. Our proposed room classification approach predicts classes with an average accuracy of 80%.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct4_05">
             11:35-11:40, Paper ThCT4.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4306'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              MFSeg: Efficient Multi-Frame 3D Semantic Segmentation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#279249" title="Click to go to the Author Index">
             Huang, Chengjie
            </a>
           </td>
           <td class="r">
            University of Waterloo
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#244317" title="Click to go to the Author Index">
             Czarnecki, Krzysztof
            </a>
           </td>
           <td class="r">
            University of Waterloo
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4306" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We propose MFSeg, an efficient multi-frame 3D semantic segmentation framework. By aggregating point cloud sequences at the feature level and regularizing the feature extraction and aggregation process, MFSeg reduces computational overhead while maintaining high accuracy. Moreover, by employing a lightweight MLP-based point decoder, our method eliminates the need to upsample redundant points from past frames. Experiments on the nuScenes and Waymo datasets show that MFSeg outperforms existing methods, demonstrating its effectiveness and efficiency.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct4_06">
             11:40-11:45, Paper ThCT4.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4983'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Good Foundation Is Worth Many Labels: Label-Efficient Panoptic Segmentation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#276994" title="Click to go to the Author Index">
             Vödisch, Niclas
            </a>
           </td>
           <td class="r">
            University of Freiburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#274286" title="Click to go to the Author Index">
             Petek, Kürsat
            </a>
           </td>
           <td class="r">
            University of Freiburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#372367" title="Click to go to the Author Index">
             Käppeler, Markus
            </a>
           </td>
           <td class="r">
            University of Freiburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#160427" title="Click to go to the Author Index">
             Valada, Abhinav
            </a>
           </td>
           <td class="r">
            University of Freiburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101785" title="Click to go to the Author Index">
             Burgard, Wolfram
            </a>
           </td>
           <td class="r">
            University of Technology Nuremberg
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4983" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#robotics_and_automation_in_agriculture_and_forestry" title="Click to go to the Keyword Index">
               Robotics and Automation in Agriculture and Forestry
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             A key challenge for the widespread application of learning-based models for robotic perception is to significantly reduce the required amount of annotated training data while achieving accurate predictions. This is essential not only to decrease operating costs but also to speed up deployment time. In this work, we address this challenge for PAnoptic SegmenTation with fEw Labels (PASTEL) by exploiting the groundwork paved by visual foundation models. We leverage descriptive image features from such a model to train two lightweight network heads for semantic segmentation and object boundary detection, using very few annotated training samples. We then merge their predictions via a novel fusion module that yields panoptic maps based on normalized cut. To further enhance the performance, we utilize self-training on unlabeled images selected by a feature-driven similarity scheme. We underline the relevance of our approach by employing PASTEL to important robot perception use cases from autonomous driving and agricultural robotics. In extensive experiments, we demonstrate that PASTEL significantly outperforms previous methods for label-efficient segmentation even when using fewer annotations. The code of our work is publicly available at https://pastel.cs.uni-freiburg.de.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct5">
             <b>
              ThCT5
             </b>
            </a>
           </td>
           <td class="r">
            305
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thct5" title="Click to go to the Program at a Glance">
             <b>
              Explainable AI in Robotics
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#104305" title="Click to go to the Author Index">
             Chernova, Sonia
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#115415" title="Click to go to the Author Index">
             Feil-Seifer, David
            </a>
           </td>
           <td class="r">
            University of Nevada, Reno
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct5_01">
             11:15-11:20, Paper ThCT5.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('392'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              CE-MRS: Contrastive Explanations for Multi-Robot Systems
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#405225" title="Click to go to the Author Index">
             Schneider, Ethan
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#405337" title="Click to go to the Author Index">
             Wu, Daniel
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#299569" title="Click to go to the Author Index">
             Das, Devleena
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104305" title="Click to go to the Author Index">
             Chernova, Sonia
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab392" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#design_and_human_factors" title="Click to go to the Keyword Index">
               Design and Human Factors
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#human_factors_and_human_in_the_loop" title="Click to go to the Keyword Index">
               Human Factors and Human-in-the-Loop
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             As the complexity of multi-robot systems grows to incorporate a greater number of robots, more complex tasks, and longer time horizons, the solutions to such problems often become too complex to be fully intelligible to human users. In this work, we introduce an approach for generating natural language explanations that justify the validity of the system's solution to the user, or else aid the user in correcting any errors that led to a suboptimal system solution. Toward this goal, we first contribute a generalizable formalism of contrastive explanations for multi-robot systems, and then introduce a holistic approach to generating contrastive explanations for multi-robot scenarios that selectively incorporates data from multi-robot task allocation, scheduling, and motion-planning to explain system behavior. Through user studies with human operators we demonstrate that our integrated contrastive explanation approach leads to significant improvements in user ability to identify and solve system errors, leading to significant improvements in overall multi-robot team performance.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct5_02">
             11:20-11:25, Paper ThCT5.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('823'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Affordance-Based Explanations of Robot Navigation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#323691" title="Click to go to the Author Index">
             Halilovic, Amar
            </a>
           </td>
           <td class="r">
            Ulm University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#194762" title="Click to go to the Author Index">
             Krivic, Senka
            </a>
           </td>
           <td class="r">
            University of Sarajevo
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab823" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#social_hri" title="Click to go to the Keyword Index">
               Social HRI
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#human_centered_robotics" title="Click to go to the Keyword Index">
               Human-Centered Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper introduces affordance-based explanations of robot navigational decisions. The rationale behind affordance-based explanations draws on the theory of affordances, a principle rooted in ecological psychology that describes potential actions the objects in the environment offer to the robot. We demonstrate how affordances can be incorporated into visual and textual explanations for common robot navigation and path-planning scenarios. Furthermore, we formalize and categorize the concept of affordance-based explanations and connect it to existing explanation types in robotics. We present the results of a user study that shows participants to be, on average, highly satisfied with visual-textual, i.e., multimodal, affordance-based explanations of robot navigation. Furthermore, we investigate the complexity of different types of textual affordance-based explanations. Our research contributes to the expanding domain of explainable robotics, focusing on explaining robot actions in navigation.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct5_03">
             11:25-11:30, Paper ThCT5.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1619'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Explainable Reinforcement Learning Via Dynamic Mixture Policies
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#339022" title="Click to go to the Author Index">
             Schier, Maximilian
            </a>
           </td>
           <td class="r">
            Leibniz Universität Hannover
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421060" title="Click to go to the Author Index">
             Schubert, Frederik
            </a>
           </td>
           <td class="r">
            Leibniz University Hannover
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#204884" title="Click to go to the Author Index">
             Rosenhahn, Bodo
            </a>
           </td>
           <td class="r">
            Institute of Information Processing, Leibniz Universität Hannove
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1619" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#acceptability_and_trust" title="Click to go to the Keyword Index">
               Acceptability and Trust
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Learning control policies using deep reinforcement learning has shown great success for a variety of applications, including robotics and automated driving. A key area limiting the adaptation of RL in the real world is the lack of trust in the decision-making process of such policies. Therefore, explainability is a requirement of any RL agent operating in the real world. In this work, we propose a family of control policies that are explainable-by-design regarding individual observation components on object-based scene representations. By estimating diagonal squashed Gaussian and categorical mixture distributions on sub-spaces of the decomposed observations, we develop stochastic policies with easy-to-read explanations of the decision-making process. Our design is generally applicable to any RL algorithm using stochastic policies. We showcase the explainability on an extensive suite of single- and multi-agent simulations, set- and sequence-based high-level scenes, and discrete and continuous action spaces, with performance at least on-par or better compared to standard policy architectures. In additional experiments, we analyze the robustness of our approach to its single additional hyper-parameter and examine its potential for very low computational requirements with tiny policies.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct5_04">
             11:30-11:35, Paper ThCT5.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1778'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              3D Spatial Understanding in MLLMs: Disambiguation and Evaluation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416719" title="Click to go to the Author Index">
             Chang, Chun-Peng
            </a>
           </td>
           <td class="r">
            DFKI
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#187379" title="Click to go to the Author Index">
             Pagani, Alain
            </a>
           </td>
           <td class="r">
            German Research Center for Artificial Intelligence
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#189516" title="Click to go to the Author Index">
             Stricker, Didier
            </a>
           </td>
           <td class="r">
            German Research Center for Artificial Intelligence
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1778" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#multi_modal_perception_for_hri" title="Click to go to the Keyword Index">
               Multi-Modal Perception for HRI
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Multimodal Large Language Models (MLLMs) have made significant progress in tasks such as image captioning and question answering. However, while these models can generate realistic captions, they often struggle with providing precise instructions, particularly when it comes to localizing and disambiguating objects in complex 3D environments. This capability is critical as MLLMs become more integrated with collaborative robotic systems. In scenarios where a target object is surrounded by similar objects (distractors), robots must deliver clear, spatially-aware instructions to guide humans effectively. We refer to this challenge as contextual object localization and disambiguation, which imposes stricter constraints than conventional 3D dense captioning, especially regarding ensuring target exclusivity. In response, we propose simple yet effective techniques to enhance the model's ability to localize and disambiguate target objects. Our approach not only achieves state-of-the-art performance on conventional metrics that evaluate sentence similarity, but also demonstrates improved 3D spatial understanding through 3D visual grounding model.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct5_05">
             11:35-11:40, Paper ThCT5.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2467'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Towards Transparent Multi-Agent Autonomous Systems through Principled Multi-Source Knowledge Distillation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419166" title="Click to go to the Author Index">
             Zhongzheng, Guo
            </a>
           </td>
           <td class="r">
            Chinese Academy of Military Science
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423840" title="Click to go to the Author Index">
             Chaoran, Wang
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#319699" title="Click to go to the Author Index">
             Zhu, Xiaozhou
            </a>
           </td>
           <td class="r">
            Chinese Academy of Military Science
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423884" title="Click to go to the Author Index">
             Changju, Wu
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#296419" title="Click to go to the Author Index">
             Deng, Baosong
            </a>
           </td>
           <td class="r">
            Academy of Military Science
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#319704" title="Click to go to the Author Index">
             Yao, Wen
            </a>
           </td>
           <td class="r">
            Chinese Academy of Military Science
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2467" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#ai_based_methods" title="Click to go to the Keyword Index">
               AI-Based Methods
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#behavior_based_systems" title="Click to go to the Keyword Index">
               Behavior-Based Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#path_planning_for_multiple_mobile_robots_or_agents" title="Click to go to the Keyword Index">
               Path Planning for Multiple Mobile Robots or Agents
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Many real-world robotic applications can be formulated as Multi-Agent Path-Finding (MAPF) problems and approximated using Multi-Agent Reinforcement Learning (MARL) algorithms. However, the opaque nature of the black-box neural network models employed by MARL algorithms has impeded their widespread adoption due to concerns over interpretability, debugging, and user trust.To address these limitations, we propose an interpretable MAPF framework that emulates a group of n path-finding agents optimized through reinforcement learning (RL) using behavior trees (BTs), where n is the number of agents in path-finding scenarios. Expert behavior datasets consisting of state-action trajectories from MARL algorithms are generated, and a knowledge distillation approach is employed to reduce the size of the datasets and extract implicit rules.Additionally, a principled rules factorization technique based on Boolean algebra theory is utilized to prune the behavior rules and create more compact BTs representations.The proposed framework is evaluated on randomly generated MAPF scenarios and demonstrates superior performance compared to conventional BTs generation methods. This paper advances the field of interpretable AI by enabling the extraction of understandable decision-making processes from complex reinforcement learning models in multi-agent systems.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct5_06">
             11:40-11:45, Paper ThCT5.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2957'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Through the Clutter: Exploring the Impact of Complex Environments on the Legibility of Robot Motion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#321062" title="Click to go to the Author Index">
             Schmidt-Wolf, Melanie
            </a>
           </td>
           <td class="r">
            University of Nevada, Reno
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#366900" title="Click to go to the Author Index">
             Becker, Tyler J
            </a>
           </td>
           <td class="r">
            University of Nevada, Reno
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#375847" title="Click to go to the Author Index">
             Oliva, Denielle
            </a>
           </td>
           <td class="r">
            University of Nevada, Reno
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#107467" title="Click to go to the Author Index">
             Nicolescu, Monica
            </a>
           </td>
           <td class="r">
            University of Nevada, Reno
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#115415" title="Click to go to the Author Index">
             Feil-Seifer, David
            </a>
           </td>
           <td class="r">
            University of Nevada, Reno
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2957" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#intention_recognition" title="Click to go to the Keyword Index">
               Intention Recognition
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#social_hri" title="Click to go to the Keyword Index">
               Social HRI
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The environments in which the collaboration of a robot would be the most helpful to a person are frequently uncontrolled and cluttered with many objects present. Legible robot arm motion is crucial in tasks like these in order to avoid possible collisions, improve the workflow and help ensure the safety of the person. Prior work in this area, however, focuses on solutions that are tested only in uncluttered environments and there are not many results taken from cluttered environments. In this research we present a measure for clutteredness based on an entropic measure of the environment, and a novel motion planner based on potential fields. Both our measure and the planner were tested in a cluttered environment meant to represent a more typical tool-sorting task for which the person would collaborate with a robot. The in-person validation study with Baxter robots shows a significant improvement in legibility of our proposed legible motion planner compared to the current state-of-the-art legible motion planner in cluttered environments. Further, the results show a significant difference in the performance of the planners in cluttered and uncluttered environments, and the need to further explore legible motion in cluttered environments. We argue that the inconsistency of our results in cluttered environments with those obtained from uncluttered environments points out several important issues with the current research performed in the area of legible motion planners.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct6">
             <b>
              ThCT6
             </b>
            </a>
           </td>
           <td class="r">
            307
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thct6" title="Click to go to the Program at a Glance">
             <b>
              Perception for Manipulation 2
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct6_01">
             11:15-11:20, Paper ThCT6.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2398'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              OpenSU3D: Open World 3D Scene Understanding Using Foundation Models
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#411515" title="Click to go to the Author Index">
             Mohiuddin, Rafay
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#170130" title="Click to go to the Author Index">
             Prakhya, Sai Manoj
            </a>
           </td>
           <td class="r">
            Huawei Technologies Deutscheland GmbH
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#411516" title="Click to go to the Author Index">
             Collins, Fiona
            </a>
           </td>
           <td class="r">
            TUM
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#146987" title="Click to go to the Author Index">
             Liu, Ziyuan
            </a>
           </td>
           <td class="r">
            Huawei Group
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#411518" title="Click to go to the Author Index">
             Borrmann, Andre
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2398" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper, we present a novel, scalable approach for constructing open set, instance-level 3D scene representations, advancing open world understanding of 3D environments. Existing methods require pre-constructed 3D scenes and face scalability issues due to per-point feature representation, additionally struggle with contextual queries. Our method overcomes these limitations by incrementally building instance level 3D scene representations using 2D foundation models, and efficiently aggregating instance-level details such as masks, feature vectors, names, and captions. We introduce fusion schemes for feature vectors to enhance their contextual knowledge and performance on complex queries. Additionally, we explore large language models for robust automatic annotation and spatial reasoning tasks. We evaluate our proposed approach on multiple scenes from ScanNet and Replica datasets demonstrating zero-shot generalization capabilities, exceeding current state-of-the-art methods in open world 3D scene understanding. Project page: https://opensu3d.github.io/
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct6_02">
             11:20-11:25, Paper ThCT6.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2523'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Task-Aware Semantic Map: Autonomous Robot Task Assignment Beyond Commands
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416821" title="Click to go to the Author Index">
             Choi, Daewon
            </a>
           </td>
           <td class="r">
            Hanyang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416828" title="Click to go to the Author Index">
             Lee, Ho Sung
            </a>
           </td>
           <td class="r">
            Hanyang Univ
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416827" title="Click to go to the Author Index">
             Hwang, Soeun
            </a>
           </td>
           <td class="r">
            Hanyang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#154127" title="Click to go to the Author Index">
             Oh, Yoonseon
            </a>
           </td>
           <td class="r">
            Hanyang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2523" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#task_planning" title="Click to go to the Keyword Index">
               Task Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#perception_action_coupling" title="Click to go to the Keyword Index">
               Perception-Action Coupling
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             With recent advancements in Large Language Models, task planning methods that interpret human commands have garnered significant attention. However, as home robots become more common, specifying every daily task could become impractical. This paper introduces a novel semantic map called the Task-Aware Semantic Map (TASMap), which enables robots to autonomously assign and propose necessary tasks in a scene without explicit human commands. The core innovation of this approach is the ability of TASMap to comprehend the context of objects within a scene and autonomously generate task proposals. This capability significantly advances autonomous robotic assistance, reducing the dependency on specific commands and enhancing interaction with environments. We present two key applications of TASMap: contextual task proposal and spatial task proposal. Our results, verified across 35 diverse and realistically disordered scenes, underscore the effectiveness of TASMap in both simulation and real-world environments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct6_03">
             11:25-11:30, Paper ThCT6.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2608'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              High-Quality Unknown Object Instance Segmentation Via Quadruple Boundary Error Refinement
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#270223" title="Click to go to the Author Index">
             Back, Seunghyeok
            </a>
           </td>
           <td class="r">
            Korea Institute of Machinery &amp; Materials
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#365246" title="Click to go to the Author Index">
             Lee, Sangbeom
            </a>
           </td>
           <td class="r">
            Gwangju Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#365245" title="Click to go to the Author Index">
             Kim, Kangmin
            </a>
           </td>
           <td class="r">
            Gwangju Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#274396" title="Click to go to the Author Index">
             Lee, Joosoon
            </a>
           </td>
           <td class="r">
            Gwangju Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#270196" title="Click to go to the Author Index">
             Shin, Sungho
            </a>
           </td>
           <td class="r">
            Hyundai Motors
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#349591" title="Click to go to the Author Index">
             Maeng, Jemo
            </a>
           </td>
           <td class="r">
            Gwangju Institute of Science and Technology(GIST)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#100825" title="Click to go to the Author Index">
             Lee, Kyoobin
            </a>
           </td>
           <td class="r">
            Gwangju Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2608" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Accurate and efficient segmentation of unknown objects in unstructured environments is essential for robotic manipulation. Unknown Object Instance Segmentation (UOIS), which aims to identify all objects in unknown categories and backgrounds, has become a key capability for various robotic tasks. However, existing methods struggle with over-segmentation and under-segmentation, leading to failures in manipulation tasks such as grasping. To address these challenges, we propose QuBER (Quadruple Boundary Error Refinement), a novel error-informed refinement approach for high-quality UOIS. QuBER first estimates quadruple boundary errors—true positive, true negative, false positive, and false negative pixels—at the instance boundaries of the initial segmentation. It then refines the segmentation using an error-guided fusion mechanism, effectively correcting both fine-grained and instance-level segmentation errors. Extensive evaluations on three public benchmarks demonstrate that QuBER outperforms state-of-the-art methods and consistently improves various UOIS methods while maintaining a fast inference time of less than 0.1 seconds. Furthermore, we show that QuBER improves the success rate of grasping target objects in cluttered environments. Code and supplementary materials are available at https://sites.google.com/view/uois-quber.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct6_04">
             11:30-11:35, Paper ThCT6.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2965'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Beyond Bare Queries: Open-Vocabulary Object Grounding with 3D Scene Graph
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#309932" title="Click to go to the Author Index">
             Linok, Sergey
            </a>
           </td>
           <td class="r">
            MIPT
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#418726" title="Click to go to the Author Index">
             Zemskova, Tatiana
            </a>
           </td>
           <td class="r">
            AIRI, MIPT
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417869" title="Click to go to the Author Index">
             Ladanova, Svetlana
            </a>
           </td>
           <td class="r">
            MIPT
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#418741" title="Click to go to the Author Index">
             Titkov, Roman
            </a>
           </td>
           <td class="r">
            Moscow Institute of Physics and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#299169" title="Click to go to the Author Index">
             Yudin, Dmitry
            </a>
           </td>
           <td class="r">
            Moscow Institute of Physics and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422793" title="Click to go to the Author Index">
             Monastyrny, Maxim
            </a>
           </td>
           <td class="r">
            Sberbank of Russia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#398682" title="Click to go to the Author Index">
             Valenkov, Aleksei
            </a>
           </td>
           <td class="r">
            Sberbank of Russia
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2965" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Locating objects described in natural language presents a significant challenge for autonomous agents. Existing CLIP-based open-vocabulary methods successfully perform 3D object grounding with simple (bare) queries, but cannot cope with ambiguous descriptions that demand an understanding of object relations. To tackle this problem, we propose a modular approach called BBQ (Beyond Bare Queries), which constructs 3D scene graph representation with metric and semantic edges and utilizes a large language model as a human-to-agent interface through our deductive scene reasoning algorithm. BBQ employs robust DINO-powered associations to construct 3D object-centric map and an advanced raycasting algorithm with a 2D vision-language model to describe them as graph nodes. On the Replica and ScanNet datasets, we have demonstrated that BBQ takes a leading place in open-vocabulary 3D semantic segmentation compared to other zero-shot methods. Also, we show that leveraging spatial relations is especially effective for scenes containing multiple entities of the same semantic class. On challenging Sr3D+, Nr3D and ScanRefer benchmarks, our deductive approach demonstrates a significant improvement, enabling objects grounding by complex queries compared to other state-of-the-art methods. The combination of our design choices and software implementation has resulted in significant data processing speed in experiments on the robot on-board computer. This promising performance enables the application of our approach in intelligent robotics projects. We made the code publicly available at linukc.github.io/BeyondBareQueries.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct6_05">
             11:35-11:40, Paper ThCT6.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4321'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Light-Weight Framework for Open-Set Object Detection with Decoupled Feature Alignment in Joint Space
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426588" title="Click to go to the Author Index">
             He, Yonghao
            </a>
           </td>
           <td class="r">
            D-Robotics
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#189350" title="Click to go to the Author Index">
             Su, Hu
            </a>
           </td>
           <td class="r">
            Institute of Automation, Chinese Academy of Science
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426604" title="Click to go to the Author Index">
             Yu, Haiyong
            </a>
           </td>
           <td class="r">
            D-Robotics
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#336495" title="Click to go to the Author Index">
             Yang, Cong
            </a>
           </td>
           <td class="r">
            Soochow University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#287085" title="Click to go to the Author Index">
             Sui, Wei
            </a>
           </td>
           <td class="r">
            Soochow University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426596" title="Click to go to the Author Index">
             Wang, Cong
            </a>
           </td>
           <td class="r">
            D-Robotics
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#285160" title="Click to go to the Author Index">
             Liu, Song
            </a>
           </td>
           <td class="r">
            ShanghaiTech University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4321" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Open-set object detection (OSOD) is highly desirable for robotic manipulation in unstructured environments. However, existing OSOD methods often fail to meet the requirements of robotic applications due to their high computational burden and complex deployment. To address this issue, this paper proposes a light-weight framework called Decoupled OSOD (DOSOD), which is a practical and highly efficient solution for supporting real-time OSOD tasks in robotic systems. Specifically, DOSOD builds upon the YOLO-World pipeline by integrating a vision-language model (VLM) with a detector. A Multilayer Perceptron (MLP) adaptor is developed to transform text embeddings extracted by the VLM into a joint space, within which the detector learns the region representations of class-agnostic proposals. Cross-modality features are directly aligned in the joint space, avoiding the complex feature interactions and thereby improving computational efficiency. DOSOD operates like a traditional closed-set detector during the testing
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct6_06">
             11:40-11:45, Paper ThCT6.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('5066'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              LBSNet: Lightweight Joint Boundary Detection and Semantic Segmentation for Transparent and Reflective Objects
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#332124" title="Click to go to the Author Index">
             Tong, Ling
            </a>
           </td>
           <td class="r">
            Southeast University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#113640" title="Click to go to the Author Index">
             Qian, Kun
            </a>
           </td>
           <td class="r">
            Southeast University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#289927" title="Click to go to the Author Index">
             Jing, Xingshuo
            </a>
           </td>
           <td class="r">
            Southeast University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab5066" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Accurate visual detection of transparent and reflective objects remains a challenging issue for mobile manipulators. For the most common depth cameras and LiDAR sensors, the distinctive optical attributes inherent in both transparent and reflective objects pose a significant challenge. To address this problem, this study proposes a lightweight joint boundary detection and semantic segmentation network named LBSNet. LBSNet aims to enhance the perception of transparent and reflective objects in complex and dynamic environments, using RGB images only. It leverages the synergy between boundary detection and semantic segmentation through feature fusion and a multitask learning mechanism. The encoder consists of two paths: one captures category-aware semantic information, while the other discerns boundaries from multi-scale features. The gated channel adaptive (GCA) module enhances boundary features by learning channel parameters. The dynamic adaptive feature fusion (DAFF) module dynamically adjusts semantic and boundary information through cross-feature fusion. These methods effectively capture the distinctive characteristics of transparent and reflective objects, such as light refraction, boundary blurring and low contrast. Experimental results show that LBSNet achieves higher accuracy and faster processing speed on multiple public datasets compared with existing methods. Moreover, its lightweight design makes it suitable for resource-constrained mobile manipulators.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct7">
             <b>
              ThCT7
             </b>
            </a>
           </td>
           <td class="r">
            309
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thct7" title="Click to go to the Program at a Glance">
             <b>
              Marine Robotics 6
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct7_01">
             11:15-11:20, Paper ThCT7.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1295'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Stonefish: Supporting Machine Learning Research in Marine Robotics
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#367715" title="Click to go to the Author Index">
             Grimaldi, Michele
            </a>
           </td>
           <td class="r">
            University of Girona
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#179975" title="Click to go to the Author Index">
             Cieslak, Patryk
            </a>
           </td>
           <td class="r">
            Universitat De Girona
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#267344" title="Click to go to the Author Index">
             Ochoa, Eduardo
            </a>
           </td>
           <td class="r">
            Universitat De Girona
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#234067" title="Click to go to the Author Index">
             Bharti, VIbhav
            </a>
           </td>
           <td class="r">
            Heriot Watt University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420341" title="Click to go to the Author Index">
             Rajani, Hayat
            </a>
           </td>
           <td class="r">
            University of Girona
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#293055" title="Click to go to the Author Index">
             Carlucho, Ignacio
            </a>
           </td>
           <td class="r">
            University of Edinburgh
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#202239" title="Click to go to the Author Index">
             Koskinopoulou, Maria
            </a>
           </td>
           <td class="r">
            Heriot-Watt University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#113794" title="Click to go to the Author Index">
             Petillot, Yvan R.
            </a>
           </td>
           <td class="r">
            Heriot-Watt University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#106949" title="Click to go to the Author Index">
             Gracias, Nuno
            </a>
           </td>
           <td class="r">
            University of Girona
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1295" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#marine_robotics" title="Click to go to the Keyword Index">
               Marine Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#simulation_and_animation" title="Click to go to the Keyword Index">
               Simulation and Animation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Simulations are highly valuable in marine robotics, offering a cost-effective and controlled environment for testing in the challenging conditions of underwater and surface operations. Given the high costs and logistical difficulties of real-world trials, simulators capable of capturing the operational conditions of subsea environments have become key in developing and refining remotely-operated and autonomous underwater vehicles. This paper highlights recent enhancements to the Stonefish simulator, an advanced open-source platform supporting development and testing of marine robotics solutions. Key updates include a suite of additional sensors, such as an event-based camera, a thermal camera, and an optical flow camera, as well as, visual light com- munication, support for tethered operations, improved thruster modelling, more flexible hydrodynamics, and enhanced sonar accuracy. These developments and an automated annotation tool significantly bolster Stonefish’s role in marine robotics research, especially in the field of deep learning, where training data with a known ground truth is hard or impossible to collect. https://github.com/patrykcieslak/stonefish
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct7_02">
             11:20-11:25, Paper ThCT7.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2118'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Sea-U-Whale: A Reconfigurable Marine Robot with Multi-Modal Motion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#377724" title="Click to go to the Author Index">
             Ding, Wendi
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#323166" title="Click to go to the Author Index">
             Zhao, Zuoquan
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#339386" title="Click to go to the Author Index">
             Yan, Ruixin
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#350081" title="Click to go to the Author Index">
             Gao, Songqun
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#343258" title="Click to go to the Author Index">
             Guo, Zixuan
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#325015" title="Click to go to the Author Index">
             Liu, Xuchen
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#171587" title="Click to go to the Author Index">
             Chen, Ben M.
            </a>
           </td>
           <td class="r">
            Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2118" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#marine_robotics" title="Click to go to the Keyword Index">
               Marine Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#actuation_and_joint_mechanisms" title="Click to go to the Keyword Index">
               Actuation and Joint Mechanisms
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#product_design__development_and_prototyping" title="Click to go to the Keyword Index">
               Product Design, Development and Prototyping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             As marine exploration becomes increasingly important, marine robots have been extensively studied in recent years. Despite some well-designed robots have already achieved to various successful missions, most existing robots struggle to adapt to diverse demands or tasks due to their fixed structure and complexity of the marine environment. To address these challenges, we present a novel reconfigurable marine robot named Sea-U-Whale. This system can dynamically adjust its actuator configuration in the marine environment, providing superior environmental adaptability, maneuverability, and versatile mobility. Considering the demands of unmanned ocean exploration, an active reconfiguration mechanism and three distinct vehicle modes are designed for optimal actuation in various marine scenarios. The multi-modal mobility of our system and its robust performance have been validated through extensive field tests and water tank experiments, demonstrating its potential in handling a wide range of mission profiles.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct7_03">
             11:25-11:30, Paper ThCT7.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3920'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              MERLION: Marine ExploRation with Language guIded Online iNformative Visual Sampling and Enhancement
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#381808" title="Click to go to the Author Index">
             Thengane, Shrutika
            </a>
           </td>
           <td class="r">
            Singapore University of Technology and Design
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#332057" title="Click to go to the Author Index">
             Prasetyo, Marcel Bartholomeus
            </a>
           </td>
           <td class="r">
            Singapore University of Technology and Design
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#336397" title="Click to go to the Author Index">
             Tan, Yu Xiang
            </a>
           </td>
           <td class="r">
            Singapore University of Technology and Design
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#133857" title="Click to go to the Author Index">
             Meghjani, Malika
            </a>
           </td>
           <td class="r">
            Singapore University of Technology and Design
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3920" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#marine_robotics" title="Click to go to the Keyword Index">
               Marine Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#environment_monitoring_and_management" title="Click to go to the Keyword Index">
               Environment Monitoring and Management
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Autonomous and targeted underwater visual monitoring and exploration using Autonomous Underwater Vehicles (AUVs) can be a challenging task due to both online and offline constraints. The online constraints comprise limited onboard storage capacity and communication bandwidth to the surface, whereas the offline constraints entail the time and effort required for the selection of desired keyframes from the video data. An example use case of targeted underwater visual monitoring is finding the most interesting visual frames of fish in a long sequence of an AUV's visual experience. This challenge of targeted informative sampling is further aggravated in murky waters with poor visibility. In this paper, we present MERLION, a novel framework that provides semantically aligned and visually enhanced summaries for murky underwater marine environment monitoring and exploration. Specifically, our framework integrates (a) an image-text model for semantically aligning the visual samples to the user's needs, (b) an image enhancement model for murky water visual data and (c) an informative sampler for summarizing the monitoring experience. We validate our proposed MERLION framework on real-world data with user studies and present qualitative and quantitative results using our evaluation metric and show improved results compared to the state-of-the-art approaches. The code is available at https://github.com/MARVL-Lab/MERLION.git
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct7_04">
             11:30-11:35, Paper ThCT7.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4196'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              PoLaRIS Dataset: A Maritime Object Detection and Tracking Dataset in Pohang Canal
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#393761" title="Click to go to the Author Index">
             Choi, Jiwon
            </a>
           </td>
           <td class="r">
            Inha University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424661" title="Click to go to the Author Index">
             Cho, Dongjin
            </a>
           </td>
           <td class="r">
            Inha University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420896" title="Click to go to the Author Index">
             Lee, Gihyeon
            </a>
           </td>
           <td class="r">
            Inha University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#334472" title="Click to go to the Author Index">
             Kim, Hogyun
            </a>
           </td>
           <td class="r">
            Inha University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#334470" title="Click to go to the Author Index">
             Yang, Geonmo
            </a>
           </td>
           <td class="r">
            Inha University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#219087" title="Click to go to the Author Index">
             Kim, Joowan
            </a>
           </td>
           <td class="r">
            Samsung Heavy Industries
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#203908" title="Click to go to the Author Index">
             Cho, Younggun
            </a>
           </td>
           <td class="r">
            Inha University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4196" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#marine_robotics" title="Click to go to the Keyword Index">
               Marine Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#data_sets_for_robotic_vision" title="Click to go to the Keyword Index">
               Data Sets for Robotic Vision
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#sensor_fusion" title="Click to go to the Keyword Index">
               Sensor Fusion
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Maritime environments often present hazardous situations due to factors such as moving ships or buoys, which become obstacles under the influence of waves. In such challenging conditions, the ability to detect and track potentially hazardous objects is critical for the safe navigation of marine robots, but datasets capturing these scenarios remain limited. To address this limitation, we introduce a new multi-modal dataset that includes image and point-wise annotations of maritime obstacles. Our dataset provides detailed ground truth for obstacle detection and tracking, including objects as small as 10×10 pixels, which are crucial for maritime safety. To validate the dataset’s effectiveness as a reliable benchmark, we conducted evaluations using various methodologies, including state-of-the-art (SOTA) techniques for object detection and tracking. These evaluations are expected to contribute to improving performance, particularly in the complex maritime environment. This represents the first demonstration of a dataset offering multi-modal annotations specifically tailored to maritime environments. Our dataset is available at https: //github.com/sparolab/PoLaRIS.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct7_05">
             11:35-11:40, Paper ThCT7.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4818'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Confidence-Aware Object Capture for a Manipulator Subject to Floating-Base Disturbances
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#243281" title="Click to go to the Author Index">
             Xu, Ruoyu
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong, Shenzhen
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#312729" title="Click to go to the Author Index">
             Jiang, Zixing
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#363670" title="Click to go to the Author Index">
             Liu, Beibei
            </a>
           </td>
           <td class="r">
            The Chinese University of Hongkong, Shenzhen
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#142145" title="Click to go to the Author Index">
             Wang, Yuquan
            </a>
           </td>
           <td class="r">
            Tencent
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#113091" title="Click to go to the Author Index">
             Qian, Huihuan (Alex)
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong, Shenzhen
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4818" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#marine_robotics" title="Click to go to the Keyword Index">
               Marine Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#field_robots" title="Click to go to the Keyword Index">
               Field Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#robotics_in_hazardous_fields" title="Click to go to the Keyword Index">
               Robotics in Hazardous Fields
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#floating_base_manipulator" title="Click to go to the Keyword Index">
               Floating-Base Manipulator
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Capturing stationary aerial objects on unmanned surface vehicles (USVs) is challenging due to quasiperiodic and fast floating-base motions caused by wave-induced disturbances. It is hard to (1) maintain high motion prediction accuracy due to the stochastic nature of these disturbances and (2) perform object capture through real-time tracking due to the limited active torque. We introduce confidence analysis in predictive capture. To address the inaccuracy predictions, we calculate a real-time confidence tube to evaluate the prediction quality. To overcome tracking difficulties, we plan a trajectory to capture the object at a future moment while maximizing the confidence of the capture position on the predicted trajectory. All calculations are completed within 0.2 seconds to ensure a timely response. We validate our approach through experiments, where we simulate disturbances by executing real USV motions using a servo platform. The results demonstrate that our method achieves an 80% success rate.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct7_06">
             11:40-11:45, Paper ThCT7.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('5007'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              RecGS: Removing Water Caustic with Recurrent Gaussian Splatting
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#266906" title="Click to go to the Author Index">
             Zhang, Tianyi
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#238613" title="Click to go to the Author Index">
             Zhi, Weiming
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#413271" title="Click to go to the Author Index">
             Meyers, Braden
            </a>
           </td>
           <td class="r">
            Brigham Young University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#413286" title="Click to go to the Author Index">
             Durrant, Sterling Nelson
            </a>
           </td>
           <td class="r">
            Brigham Young University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#396946" title="Click to go to the Author Index">
             Huang, Kaining
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#180889" title="Click to go to the Author Index">
             Mangelson, Joshua
            </a>
           </td>
           <td class="r">
            Brigham Young University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#175790" title="Click to go to the Author Index">
             Barbalata, Corina
            </a>
           </td>
           <td class="r">
            Louisiana State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#118789" title="Click to go to the Author Index">
             Johnson-Roberson, Matthew
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab5007" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#marine_robotics" title="Click to go to the Keyword Index">
               Marine Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Water caustics are commonly observed in seafloor imaging data from shallow-water areas. Traditional methods that remove caustic patterns from images often rely on 2D filtering or pre-training on an annotated dataset, hindering the performance when generalizing to real-world seafloor data with 3D structures. In this paper, we present a novel method Recurrent Gaussian Splatting (RecGS), which takes advantage of today’s photorealistic 3D reconstruction technology, 3D Gaussian Splatting (3DGS), to separate caustics from seafloor imagery. With a sequence of images taken by an underwater robot, we build 3DGS recurrently and decompose the caustic with low-pass filtering in each iteration. In the experiments, we analyze and compare with different methods, including joint optimization, 2D filtering, and deep learning approaches. The results show that our proposed RecGS paradigm can effectively separate the caustic from the seafloor, improving the visual appearance, and can be potentially applied on more problems with inconsistent illumination.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct8">
             <b>
              ThCT8
             </b>
            </a>
           </td>
           <td class="r">
            311
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thct8" title="Click to go to the Program at a Glance">
             <b>
              Aerial Robots: Learning 2
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#103169" title="Click to go to the Author Index">
             Robuffo Giordano, Paolo
            </a>
           </td>
           <td class="r">
            Irisa Cnrs Umr6074
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct8_01">
             11:15-11:20, Paper ThCT8.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('13'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Learning to Fly in Seconds
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#266338" title="Click to go to the Author Index">
             Eschmann, Jonas
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#210977" title="Click to go to the Author Index">
             Albani, Dario
            </a>
           </td>
           <td class="r">
            Technology Innovation Institure
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#142369" title="Click to go to the Author Index">
             Loianno, Giuseppe
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab13" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#machine_learning_for_robot_control" title="Click to go to the Keyword Index">
               Machine Learning for Robot Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Learning-based methods, particularly Reinforcement Learning (RL), hold great promise for streamlining deployment, enhancing performance, and achieving generalization in the control of autonomous multirotor aerial vehicles. Deep RL has been able to control complex systems with impressive fidelity and agility in simulation but the simulation-to-reality transfer often brings a hard-to-bridge reality gap. Moreover, RL is commonly plagued by prohibitively long training times. In this work, we propose a novel asymmetric actor-critic-based architecture coupled with a highly reliable RL-based training paradigm for end-to-end quadrotor control. We show how curriculum learning and a highly optimized simulator enhance sample complexity and lead to fast training times. To precisely discuss the challenges related to low-level/end-to-end multirotor control, we also introduce a taxonomy that classifies the existing levels of control abstractions as well as non-linearities and domain parameters. Our framework enables Simulation-to-Reality (Sim2Real) transfer for direct RPM control after only 18 seconds of training on a consumer-grade laptop as well as its deployment on microcontrollers to control a multirotor under real-time guarantees. Finally, our solution exhibits competitive performance in trajectory tracking, as demonstrated through various experimental comparisons with existing state-of-the-art control solutions using a real Crazyflie nano quadrotor. We open source the code including a very fast multirotor dynamics simulator that can simulate about 5 months of flight per second on a laptop GPU. The fast training times and deployment to a cheap, off-the-shelf quadrotor lower the barriers to entry and help democratize the research and development of these systems.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct8_02">
             11:20-11:25, Paper ThCT8.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('68'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Multi-UAVs End-To-End Distributed Trajectory Generation Over Point Cloud Data
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#275356" title="Click to go to the Author Index">
             Marino, Antonio
            </a>
           </td>
           <td class="r">
            University of Rennes
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#138065" title="Click to go to the Author Index">
             Pacchierotti, Claudio
            </a>
           </td>
           <td class="r">
            Centre National De La Recherche Scientifique (CNRS)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#103169" title="Click to go to the Author Index">
             Robuffo Giordano, Paolo
            </a>
           </td>
           <td class="r">
            Irisa Cnrs Umr6074
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab68" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__mechanics_and_control" title="Click to go to the Keyword Index">
               Aerial Systems: Mechanics and Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper introduces an end-to-end trajectory planning algorithm tailored for multi-UAV systems that gener- ates collision-free trajectories in environments populated with both static and dynamic obstacles, leveraging point cloud data. Our approach consists of a 2-branch neural network fed with sensing and localization data, able to communicate intermediate learned features among the agents. One network branch crafts an initial collision-free trajectory estimate, while the other devises a neural collision constraint for subsequent optimiza- tion, ensuring trajectory continuity and adherence to physical actuation limits. Extensive simulations in challenging cluttered environments, involving up to 25 robots and 25% obstacle density, show a collision avoidance success rate in the range of 100 − 85%. Finally, we introduce a saliency map computation method acting on the point cloud data, offering qualitative insights into our methodology.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct8_03">
             11:25-11:30, Paper ThCT8.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1880'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Lightweight yet High-Performance Defect Detector for UAV-Based Large-Scale Infrastructure Real-Time Inspection
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#351216" title="Click to go to the Author Index">
             Zhao, Benyun
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421995" title="Click to go to the Author Index">
             Duan, Qigeng
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#351220" title="Click to go to the Author Index">
             Yang, Guidong
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#275702" title="Click to go to the Author Index">
             Tang, Haoyun (Jerry)
            </a>
           </td>
           <td class="r">
            UC Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#266220" title="Click to go to the Author Index">
             Song, Zhenbo
            </a>
           </td>
           <td class="r">
            Nanjing University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#319735" title="Click to go to the Author Index">
             Wen, Junjie
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#325015" title="Click to go to the Author Index">
             Liu, Xuchen
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#391025" title="Click to go to the Author Index">
             Li, Qingxiang
            </a>
           </td>
           <td class="r">
            The Chineses University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#334961" title="Click to go to the Author Index">
             Lei, Lei
            </a>
           </td>
           <td class="r">
            City University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#289427" title="Click to go to the Author Index">
             Zhang, Jihan
            </a>
           </td>
           <td class="r">
            Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#351227" title="Click to go to the Author Index">
             Chen, Xi
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#146039" title="Click to go to the Author Index">
             Mueller, Mark Wilfried
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#171587" title="Click to go to the Author Index">
             Chen, Ben M.
            </a>
           </td>
           <td class="r">
            Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1880" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__perception_and_autonomy" title="Click to go to the Keyword Index">
               Aerial Systems: Perception and Autonomy
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Defect diagnosis in urban infrastructure is crucial for public safety. Traditional manual inspections face significant challenges in terms of accuracy and cost-effectiveness. In this paper, we propose a lightweight and hardware-friendly large-scale infrastructure detector, CUPID, highly suitable for unmanned aerial vehicles (UAVs). Given the significant challenges in automatically detecting defects of varying intensity and size within complex infrastructure, along with the tendency of lightweight models to lose detail and fail to fully capture features during the defect extraction process, we propose the CUPID_Block, a multi-level information fusion block to construct the backbone, featuring the CUPID_Conv module equipped with our proposed CCA (CrissCross Attention). Furthermore, CUPID features an auxiliary training branch that assimilates lower feature maps, helping to recover details lost in deeper convolutional layers. To verify the effectiveness of CUPID and to address the lack of a suitable dataset in the community, we establish a multi-scenario infrastructure defect dataset, CUBIT2024, to conduct extensive experiments. Finally, to assess the efficiency and adaptability of CUPID in UAV for online infrastructure inspection, we design a compact autonomous drone, CU-Astro, where the proposed CUPID is deployed on the Jetson Orin NX computer onboard to evaluate the speed and power consumption of the inference.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct8_04">
             11:30-11:35, Paper ThCT8.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2109'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              ProxFly: Robust Control for Close Proximity Quadcopter Flight Via Residual Reinforcement Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#311776" title="Click to go to the Author Index">
             Zhang, Ruiqi
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#341118" title="Click to go to the Author Index">
             Zhang, Dingqi
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#146039" title="Click to go to the Author Index">
             Mueller, Mark Wilfried
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2109" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__mechanics_and_control" title="Click to go to the Keyword Index">
               Aerial Systems: Mechanics and Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#robust_adaptive_control" title="Click to go to the Keyword Index">
               Robust/Adaptive Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper proposes the ProxFly, a residual deep Reinforcement Learning (RL)-based controller for close proximity quadcopter flight. Specifically, we design a residual module on top of a cascaded controller (denoted as basic controller) to generate high-level control commands, which compensate for external disturbances and thrust loss caused by downwash effects from other quadcopters. First, our method takes only the ego state and controllers' commands as inputs and does not rely on any communication between quadcopters, thereby reducing the bandwidth requirement. Through domain randomization, our method relaxes the requirement for accurate system identification and fine-tuned controller parameters, allowing it to adapt to changing system models. Meanwhile, our method not only reduces the proportion of unexplainable signals from the black box in control commands but also enables the RL training to skip the time-consuming exploration from scratch via guidance from the basic controller. We validate the effectiveness of the residual module in the simulation with different proximities. Moreover, we conduct the real close proximity flight test to compare ProxFly with the basic controller and an advanced model-based controller with complex aerodynamic compensation. Finally, we show that ProxFly can be used for challenging quadcopter mid-air docking, where two quadcopters fly in extreme proximity, and strong airflow significantly disrupts flight. However, our method can stabilize the quadcopter in this case and accomplish docking. The resources are available at https://github.com/ruiqizhang99/ProxFly.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct8_05">
             11:35-11:40, Paper ThCT8.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4658'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              TempFuser: Learning Agile, Tactical, and Acrobatic Flight Maneuvers Using a Long Short-Term Temporal Fusion Transformer
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#277516" title="Click to go to the Author Index">
             Seong, Hyunki
            </a>
           </td>
           <td class="r">
            KAIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104593" title="Click to go to the Author Index">
             Shim, David Hyunchul
            </a>
           </td>
           <td class="r">
            KAIST
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4658" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#machine_learning_for_robot_control" title="Click to go to the Keyword Index">
               Machine Learning for Robot Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Dogfighting is a challenging scenario in aerial applications that requires a comprehensive understanding of both strategic maneuvers and the aerodynamics of agile aircraft. The aerial agent needs to not only understand tactically evolving maneuvers of fighter jets from a long-term perspective but also react to rapidly changing aerodynamics of aircraft from a short-term viewpoint. In this paper, we introduce TempFuser, a novel long short-term temporal fusion transformer architecture that can learn agile, tactical, and acrobatic flight maneuvers in complex dogfight problems. Our approach integrates two distinct temporal transition embeddings into a transformer-based network to comprehensively capture both the long-term tactics and short-term agility of aerial agents. By incorporating these perspectives, our policy network generates end-to-end flight commands that secure dominant positions over the long term and effectively outmaneuver agile opponents. After training in a high-fidelity flight simulator, our model successfully learns to execute strategic maneuvers, outperforming baseline policy models against various types of opponent aircraft. Notably, our model exhibits human-like acrobatic maneuvers even when facing adversaries with superior specifications, all without relying on prior knowledge. Moreover, it demonstrates robust pursuit performance in challenging supersonic and low-altitude situations. Demo videos are available at https://sites.google.com/view/tempfuser.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct8_06">
             11:40-11:45, Paper ThCT8.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4949'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Modular Reinforcement Learning for a Quadrotor UAV with Decoupled Yaw Control
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#216516" title="Click to go to the Author Index">
             Yu, Beomyeol
            </a>
           </td>
           <td class="r">
            The George Washington University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#161383" title="Click to go to the Author Index">
             Lee, Taeyoung
            </a>
           </td>
           <td class="r">
            George Washington University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4949" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__mechanics_and_control" title="Click to go to the Keyword Index">
               Aerial Systems: Mechanics and Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents modular reinforcement learning (RL) frameworks for the low-level control of a quadrotor, enabling direct control of yawing motion. While traditional monolithic RL approaches have demonstrated success in real-world autonomous flight, they often struggle to precisely control both the translational and yawing motions due to their distinct dynamic characteristics and strong coupling. Moreover, training a large-scale monolithic network typically demands a wealth of training data for broad generalization. To address these issues, we decompose the quadrotor dynamics into translational and yawing subsystems and assign dedicated modular RL agents for each. This design significantly improves performance, as each RL agent is trained for its specific purpose, and they are integrated in a synergistic way. It further enhances robustness, as potential failures within one module have minimal impact on the other, promoting fault tolerance. These improvements are illustrated by flight experiments achieved via zero-shot sim-to-real transfer. It is shown that the proposed modular policies substantially enhance training efficiency, tracking performance, and adaptability to real-world conditions.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct9">
             <b>
              ThCT9
             </b>
            </a>
           </td>
           <td class="r">
            312
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thct9" title="Click to go to the Program at a Glance">
             <b>
              Task and Motion Planning 2
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct9_01">
             11:15-11:20, Paper ThCT9.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('717'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              HBTP: Heuristic Behavior Tree Planning with Large Language Model Reasoning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#341194" title="Click to go to the Author Index">
             Cai, Yishuai
            </a>
           </td>
           <td class="r">
            National University of Defense Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#341263" title="Click to go to the Author Index">
             Chen, Xinglin
            </a>
           </td>
           <td class="r">
            National University of Defense Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#341228" title="Click to go to the Author Index">
             Mao, Yunxin
            </a>
           </td>
           <td class="r">
            National University of Defense Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#269415" title="Click to go to the Author Index">
             Li, Minglong
            </a>
           </td>
           <td class="r">
            National University of Defense Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#122474" title="Click to go to the Author Index">
             Yang, Shaowu
            </a>
           </td>
           <td class="r">
            National University of Defense Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#226679" title="Click to go to the Author Index">
             Yang, Wenjing
            </a>
           </td>
           <td class="r">
            State Key Laboratory of High Performance Computing (HPCL), Schoo
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#269479" title="Click to go to the Author Index">
             Wang, Ji
            </a>
           </td>
           <td class="r">
            National University of Defense Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab717" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Behavior Trees (BTs) are increasingly becoming a popular control structure in robotics due to their modularity, reactivity, and robustness. In terms of BT generation methods, BT planning shows promise for generating reliable BTs. However, the scalability of BT planning is often constrained by prolonged planning times in complex scenarios, largely due to a lack of domain knowledge. In contrast, pre-trained Large Language Models (LLMs) have demonstrated task reasoning capabilities across various domains, though the correctness and safety of their planning remain uncertain. This paper proposes integrating BT planning with LLM reasoning, introducing Heuristic Behavior Tree Planning (HBTP)—a reliable and efficient framework for BT generation. The key idea in HBTP is to leverage LLMs for task-specific reasoning to generate a heuristic path, which BT planning can then follow to expand efficiently. We first introduce the heuristic BT expansion process, along with two heuristic variants designed for optimal planning and satisficing planning, respectively. Then, we propose methods to address the inaccuracies of LLM reasoning, including action space pruning and reflective feedback, to further enhance both reasoning accuracy and planning efficiency. Experiments demonstrate the theoretical bounds of HBTP, and results from four datasets confirm its practical effectiveness in everyday service robot applications.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct9_02">
             11:20-11:25, Paper ThCT9.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2721'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SPINE: Online Semantic Planning for Missions with Incomplete Natural Language Specifications in Unstructured Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#286020" title="Click to go to the Author Index">
             Ravichandran, Zachary
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#189452" title="Click to go to the Author Index">
             Murali, Varun
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#298407" title="Click to go to the Author Index">
             Tzes, Mariliza
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#102357" title="Click to go to the Author Index">
             Pappas, George J.
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104342" title="Click to go to the Author Index">
             Kumar, Vijay
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2721" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#autonomous_agents" title="Click to go to the Keyword Index">
               Autonomous Agents
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#field_robots" title="Click to go to the Keyword Index">
               Field Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             As robots become increasingly capable, users will want to describe high-level missions and have robots infer the relevant details. Because pre-built maps are difficult to obtain in many realistic settings, accomplishing such missions will require the robot to map and plan online. While many semantic planning methods operate online, they are typically designed for well specified missions such as object search or exploration. Recently, Large Language Models (LLMs) have demonstrated powerful contextual reasoning abilities over a range of robotic tasks described in natural language. However, existing LLM-enabled planners typically do not consider online planning or complex missions; rather, relevant subtasks and semantics are provided by a pre-built map or a user. We address these limitations via SPINE, an online planner for missions with incomplete mission specifications provided in natural language. The planner uses an LLM to reason about subtasks implied by the mission specification and then realizes these subtasks in a receding horizon framework. Tasks are automatically validated for safety and refined online with new map observations. We evaluate SPINE in simulation and real-world settings with missions that require multiple steps of semantic reasoning and exploration in cluttered outdoor environments of over 20,000 square meters. Compared to baselines that use existing LLM-enabled planning approaches, our method is over twice as efficient in terms of time and distance, requires less user interactions, and does not require a full map. Additional resources are provided at https://zacravichandran.github.io/SPINE.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct9_03">
             11:25-11:30, Paper ThCT9.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2917'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Closed Loop Interactive Embodied Reasoning for Robot Manipulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#268945" title="Click to go to the Author Index">
             Nazarczuk, Michal
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#214555" title="Click to go to the Author Index">
             Behrens, Jan Kristof
            </a>
           </td>
           <td class="r">
            Czech Technical University in Prague, CIIRC
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#165953" title="Click to go to the Author Index">
             Stepanova, Karla
            </a>
           </td>
           <td class="r">
            Czech Technical University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#114104" title="Click to go to the Author Index">
             Hoffmann, Matej
            </a>
           </td>
           <td class="r">
            Czech Technical University in Prague, Faculty of Electrical Engi
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#111738" title="Click to go to the Author Index">
             Mikolajczyk, Krystian
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2917" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#manipulation_planning" title="Click to go to the Keyword Index">
               Manipulation Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reactive_and_sensor_based_planning" title="Click to go to the Keyword Index">
               Reactive and Sensor-Based Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Embodied reasoning systems integrate robotic hardware and cognitive processes to perform complex tasks, typically in response to a natural language query about a specific physical environment. This usually involves changing the belief about the scene or physically interacting and changing the scene (e.g., sort the objects from lightest to heaviest). In order to facilitate the development of such systems we introduce a new modular Closed Loop Interactive Embodied Reasoning (CLIER) approach that takes into account the measurements of non-visual object properties, changes in the scene caused by external disturbances as well as uncertain outcomes of robotic actions. CLIER performs multi-modal reasoning and action planning and generates a sequence of primitive actions that can be executed by a robot manipulator. Our method operates in a closed loop, responding to changes in the environment. Our approach is developed with the use of MuBle simulation environment and tested in 10 interactive benchmark scenarios. We extensively evaluate our reasoning approach in simulation and in real-world manipulation tasks with a success rate above 76% and 64%, respectively.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct9_04">
             11:30-11:35, Paper ThCT9.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3169'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SayComply: Grounding Field Robotic Tasks in Operational Compliance through Retrieval-Based Language Models
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#278868" title="Click to go to the Author Index">
             Ginting, Muhammad Fadhil
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#199323" title="Click to go to the Author Index">
             Kim, Dong Ki
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Tech
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#118338" title="Click to go to the Author Index">
             Kim, Sung-Kyun
            </a>
           </td>
           <td class="r">
            NASA Jet Propulsion Laboratory, Caltech
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#368607" title="Click to go to the Author Index">
             Bandi, Jai Krishna
            </a>
           </td>
           <td class="r">
            Field AI
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#170846" title="Click to go to the Author Index">
             Kochenderfer, Mykel
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#172598" title="Click to go to the Author Index">
             Omidshafiei, Shayegan
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#114852" title="Click to go to the Author Index">
             Agha-mohammadi, Ali-akbar
            </a>
           </td>
           <td class="r">
            NASA-JPL, Caltech
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3169" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#field_robots" title="Click to go to the Keyword Index">
               Field Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#task_and_motion_planning" title="Click to go to the Keyword Index">
               Task and Motion Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper addresses the problem of task planning for robots that must comply with operational manuals in real-world settings. Task planning under these constraints is essential for enabling autonomous robot operation in domains that require adherence to domain-specific knowledge. Current methods for generating robot goals and plans rely on common sense knowledge encoded in large language models. However, these models lack grounding of robot plans to domain-specific knowledge and are not easily transferable between multiple sites or customers with different compliance needs. In this work, we present SayComply, which enables grounding robotic task planning with operational compliance using retrieval-based language models. We design a hierarchical database of operational, environment, and robot embodiment manuals and procedures to enable efficient retrieval of the relevant context under the limited context length of the LLMs. We then design a task planner using a tree-based retrieval augmented generation (RAG) technique to generate robot tasks that follow user instructions while simultaneously complying with the domain knowledge in the database. We demonstrate the benefits of our approach through simulations and hardware experiments in real-world scenarios that require precise context retrieval across various types of context, outperforming the standard RAG method. Our approach bridges the gap in deploying robots that consistently adhere to operational protocols, offering a scalable and edge-deployable solution for ensuring compliance across varied and complex real-world environments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct9_05">
             11:35-11:40, Paper ThCT9.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('5050'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              LiP-LLM: Integrating Linear Programming and Dependency Graph with Large Language Models for Multi-Robot Task Planning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#411532" title="Click to go to the Author Index">
             Obata, Kazuma
            </a>
           </td>
           <td class="r">
            Osaka University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#195844" title="Click to go to the Author Index">
             Aoki, Tatsuya
            </a>
           </td>
           <td class="r">
            Osaka University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#176109" title="Click to go to the Author Index">
             Horii, Takato
            </a>
           </td>
           <td class="r">
            Osaka University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#107751" title="Click to go to the Author Index">
             Taniguchi, Tadahiro
            </a>
           </td>
           <td class="r">
            Ritsumeikan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#111403" title="Click to go to the Author Index">
             Nagai, Takayuki
            </a>
           </td>
           <td class="r">
            Osaka University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab5050" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#task_planning" title="Click to go to the Keyword Index">
               Task Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#cooperating_robots" title="Click to go to the Keyword Index">
               Cooperating Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This study proposes LiP-LLM: integrating linear programming and dependency graph with large language models (LLMs) for multi-robot task planning. In order for multiple robots to perform tasks more efficiently, it is necessary to manage the precedence dependencies between tasks. Although multi-robot decentralized and centralized task planners using LLMs have been proposed, none of these studies focus on precedence dependencies from the perspective of task efficiency or leverage traditional optimization methods. It addresses key challenges in managing dependencies between skills and optimizing task allocation. LiP-LLM consists of three steps: skill list generation and dependency graph generation by LLMs, and task allocation using linear programming. The LLMs are utilized to generate a comprehensive list of skills and to construct a dependency graph that maps the relationships and sequential constraints among these skills. To ensure the feasibility and efficiency of skill execution, the skill list is generated by calculated likelihood, and linear programming is used to optimally allocate tasks to each robot. Experimental evaluations in simulated environments demonstrate that this method outperforms existing task planners, achieving higher success rates and efficiency in executing complex, multi-robot tasks. The results indicate the potential of combining LLMs with optimization techniques to enhance the capabilities of multi-robot systems in executing coordinated tasks accurately and efficiently. In an environment with two robots, a maximum success rate difference of 0.82 is observed in the language instruction group with a change in the object name.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct9_06">
             11:40-11:45, Paper ThCT9.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4913'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Transformer-Based Model Predictive Control: Trajectory Optimization Via Sequence Modeling
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#321403" title="Click to go to the Author Index">
             Celestini, Davide
            </a>
           </td>
           <td class="r">
            Politecnico Di Torino
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#360960" title="Click to go to the Author Index">
             Gammelli, Daniele
            </a>
           </td>
           <td class="r">
            Stanford
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#406817" title="Click to go to the Author Index">
             Guffanti, Tommaso
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#370291" title="Click to go to the Author Index">
             D’Amico, Simone
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#193353" title="Click to go to the Author Index">
             Capello, Elisa
            </a>
           </td>
           <td class="r">
            Politecnico Di Torino CNR IEIIT
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#123466" title="Click to go to the Author Index">
             Pavone, Marco
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4913" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#machine_learning_for_robot_control" title="Click to go to the Keyword Index">
               Machine Learning for Robot Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Model predictive control (MPC) has established itself as the primary methodology for constrained control, enabling general-purpose robot autonomy in diverse real-world scenarios. However, for most problems of interest, MPC relies on the recursive solution of highly non-convex trajectory optimization problems, leading to high computational complexity and strong dependency on initialization. In this work, we present a unified framework to combine the main strengths of optimization-based and learning-based methods for MPC. Our approach entails embedding high-capacity, transformer-based neural network models within the optimization process for trajectory generation, whereby the transformer provides a near-optimal initial guess, or target plan, to a non-convex optimization problem. Our experiments, performed in simulation and the real world onboard a free flyer platform, demonstrate the capabilities of our framework to improve MPC convergence and runtime. Compared to purely optimization-based approaches, results show that our approach can improve trajectory generation performance by up to 75%, reduce the number of solver iterations by up to 45%, and improve overall MPC runtime by 7x without loss in performance.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct10">
             <b>
              ThCT10
             </b>
            </a>
           </td>
           <td class="r">
            313
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thct10" title="Click to go to the Program at a Glance">
             <b>
              Multi-Robot Systems 5
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#135321" title="Click to go to the Author Index">
             Saeedi, Sajad
            </a>
           </td>
           <td class="r">
            Toronto Metropolitan University
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#122186" title="Click to go to the Author Index">
             Sabattini, Lorenzo
            </a>
           </td>
           <td class="r">
            University of Modena and Reggio Emilia
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct10_01">
             11:15-11:20, Paper ThCT10.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1803'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Method for Constructing Building Structure Grid Map Based on a Climbing Algorithm
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#331083" title="Click to go to the Author Index">
             Zhou, Xidong
            </a>
           </td>
           <td class="r">
            Hunan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#210996" title="Click to go to the Author Index">
             Zhong, Hang
            </a>
           </td>
           <td class="r">
            Hunan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#244397" title="Click to go to the Author Index">
             Zhang, Hui
            </a>
           </td>
           <td class="r">
            Hunan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421044" title="Click to go to the Author Index">
             Chen, MingYuan
            </a>
           </td>
           <td class="r">
            Hunan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421118" title="Click to go to the Author Index">
             Yu, Haoyang
            </a>
           </td>
           <td class="r">
            Hunan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421174" title="Click to go to the Author Index">
             Wang, Weizheng
            </a>
           </td>
           <td class="r">
            Hunan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#157693" title="Click to go to the Author Index">
             Wang, Yaonan
            </a>
           </td>
           <td class="r">
            Hunan University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1803" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__perception_and_autonomy" title="Click to go to the Keyword Index">
               Aerial Systems: Perception and Autonomy
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Aerial-terrestrial amphibious robots excel in search and rescue tasks in unstructured terrains but face challenges in autonomous navigation indoors. Traditional full-mapping methods can degrade global path planning performance, especially when semi-static obstacles shift, leading to suboptimal paths. We propose a method for constructing building structure grid maps that are unaffected by semi-static obstacles. Our approach includes a building structure recognition algorithm based on an octree structure to differentiate between occupied and free grid cells. Experimental results demonstrate that coverage path planning on building structure grid maps produces superior global paths compared to traditional grid maps, offering a more streamlined and robust solution for autonomous navigation of aerial-terrestrial amphibious robots in indoor environments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct10_02">
             11:20-11:25, Paper ThCT10.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1825'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Efficient Scale-Uniform 3D Visual Coverage Algorithm for UAV Based on Elastic Photogrammetric Constraints
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#311992" title="Click to go to the Author Index">
             Zong, Jianping
            </a>
           </td>
           <td class="r">
            Nankai University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#365358" title="Click to go to the Author Index">
             Cao, Zhongzhi
            </a>
           </td>
           <td class="r">
            Nankai University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421242" title="Click to go to the Author Index">
             Chen, Qi
            </a>
           </td>
           <td class="r">
            Nankai University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#387339" title="Click to go to the Author Index">
             Sun, Chuanyu
            </a>
           </td>
           <td class="r">
            Nankai University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#386062" title="Click to go to the Author Index">
             Shao, Xiuli
            </a>
           </td>
           <td class="r">
            Nankai University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#132383" title="Click to go to the Author Index">
             Li, Haifeng
            </a>
           </td>
           <td class="r">
            Civil Aviation University of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#122638" title="Click to go to the Author Index">
             Wang, Hongpeng
            </a>
           </td>
           <td class="r">
            Nankai University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1825" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#environment_monitoring_and_management" title="Click to go to the Keyword Index">
               Environment Monitoring and Management
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#search_and_rescue_robots" title="Click to go to the Keyword Index">
               Search and Rescue Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Unmanned aerial vehicles equipped with modern vision algorithms are crucial for missions such as reconstruction and target acquisition. However, when deployed in the field, undulating terrain can cause significant fluctuations in image scale and degrade the performance of vision algorithms. Instead of developing specialized image processing schemes with limited adaptability, this paper presents a novel 3D visual coverage algorithm that is compatible with existing generic vision algorithms and maintains a uniform image scale for ground targets. In detail, photogrammetric constraints are initially introduced to generate aerial waypoints, and then the negative effects of valley clustering are addressed. Elastic Photogrammetric Constraints (EPC) are further proposed to eliminate valley clustering effects induced by saddle terrain. The experimental results demonstrate that EPC reduces the traversal path length by up to 37.38% compared to the previous work, but with a minor trade-off in scale variations.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct10_03">
             11:25-11:30, Paper ThCT10.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2426'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Target-Aware Viewpoint Generation for Active Robotic Exploration in Unknown Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#378255" title="Click to go to the Author Index">
             Xu, Pu
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#379064" title="Click to go to the Author Index">
             Liu, Haoming
            </a>
           </td>
           <td class="r">
            Northeastern University(CN)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#325553" title="Click to go to the Author Index">
             Li, Zhiheng
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#378250" title="Click to go to the Author Index">
             Bai, Zhaoqiang
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#121608" title="Click to go to the Author Index">
             Fang, Zheng
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2426" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#search_and_rescue_robots" title="Click to go to the Keyword Index">
               Search and Rescue Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#constrained_motion_planning" title="Click to go to the Keyword Index">
               Constrained Motion Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             When entering an unfamiliar environment, animals usually sweep off their surroundings to identify points of interest. In search and rescue robotics, autonomous exploration requires both coarse mapping of unknown areas and detailed target detection, which poses a significant challenge in balancing these tasks. To that end, we propose a target-aware robotic exploration framework that prioritizes both exploration efficiency and search completeness through three components: First, considering the computational limitations of robotic platforms, a lightweight 3D target detection method with post-fusion is introduced to detect target positions in real time. Secondly, we propose a target-aware viewpoint generation approach that integrates information gain and inspection gain to identify promising viewpoints for thorough target searches. Lastly, since a detailed examination of the environment demands numerous viewpoints, we propose a heuristic-based active exploration framework that employs a hierarchical structure to optimize exploration gain, traveling distance, and path smoothness to maximize the utility function of viewpoint sequences and ultimately find the optimal path. Extensive simulations and real-world experiments demonstrate our framework significantly enhances target search capabilities, achieving a 13% average improvement in exploration efficiency over existing methods.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct10_04">
             11:30-11:35, Paper ThCT10.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3066'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Online Multi-Robot Federated Learning for Distributed Coverage Control of Unknown Spatial Processes
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#363355" title="Click to go to the Author Index">
             Mantovani, Mattia
            </a>
           </td>
           <td class="r">
            University of Modena and Reggio Emilia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#250167" title="Click to go to the Author Index">
             Pratissoli, Federico
            </a>
           </td>
           <td class="r">
            Università Degli Studi Di Modena E Reggio Emilia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#122186" title="Click to go to the Author Index">
             Sabattini, Lorenzo
            </a>
           </td>
           <td class="r">
            University of Modena and Reggio Emilia
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3066" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#distributed_robot_systems" title="Click to go to the Keyword Index">
               Distributed Robot Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#networked_robots" title="Click to go to the Keyword Index">
               Networked Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Distributed multi-robot teams are increasingly used for optimal coverage of domains with unknown density distributions, often modeled with Gaussian Processes (GPs). However, current methods rely on data sharing, raising privacy concerns and computational issues. We propose a Federated Learning (FL) approach that enables collaborative training of GP models without sharing raw data. To enhance scalability and efficiency, we introduce a filtering strategy that selects relevant data samples, minimizing computational load. Realistic simulations emulating real scenarios demonstrate the effectiveness of our method in achieving robust environmental estimates with minimal data sharing and reduced complexity.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct10_05">
             11:35-11:40, Paper ThCT10.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4126'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Constrained Learning for Decentralized Multi-Objective Coverage Control
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425423" title="Click to go to the Author Index">
             Cervino, Juan
            </a>
           </td>
           <td class="r">
            MIT
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#220123" title="Click to go to the Author Index">
             Agarwal, Saurav
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104342" title="Click to go to the Author Index">
             Kumar, Vijay
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#134263" title="Click to go to the Author Index">
             Ribeiro, Alejandro
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4126" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#autonomous_vehicle_navigation" title="Click to go to the Keyword Index">
               Autonomous Vehicle Navigation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The multi-objective coverage control problem requires a robot swarm to collaboratively provide sensor coverage to multiple heterogeneous importance density fields (IDFs) simultaneously. We pose this as an optimization problem with constraints and study two different formulations: (1) Fair coverage, where we minimize the maximum coverage cost for any field, promoting equitable resource distribution among all fields; and (2) Constrained coverage, where each field must be covered below a certain cost threshold, ensuring that critical areas receive adequate coverage according to predefined importance levels. We study the decentralized setting where robots have limited communication and local sensing capabilities, making the system more realistic, scalable, and robust. Given the complexity, we propose a novel decentralized constrained learning approach that combines primal-dual optimization with a Learnable Perception-Action-Communication (LPAC) neural network architecture. We show that the Lagrangian of the dual problem can be reformulated as a linear combination of the IDFs, enabling the LPAC policy to serve as a primal solver. We empirically demonstrate that the proposed method (i) significantly outperforms state-of-the-art decentralized controllers by 30% on average in terms of coverage cost, (ii) transfers well to larger environments with more robots, and (iii) is scalable in the number of IDFs and robots in the swarm.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct10_06">
             11:40-11:45, Paper ThCT10.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4777'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Di-NeRF: Distributed NeRF for Collaborative Learning with Relative Pose Refinement
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#359439" title="Click to go to the Author Index">
             Asadi, Mahboubeh
            </a>
           </td>
           <td class="r">
            Toronto Metropolitan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#159424" title="Click to go to the Author Index">
             Zareinia, Kourosh
            </a>
           </td>
           <td class="r">
            Toronto Metropolitan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#135321" title="Click to go to the Author Index">
             Saeedi, Sajad
            </a>
           </td>
           <td class="r">
            Toronto Metropolitan University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4777" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#distributed_robot_systems" title="Click to go to the Keyword Index">
               Distributed Robot Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_slam" title="Click to go to the Keyword Index">
               Multi-Robot SLAM
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Collaborative mapping of unknown environments can be done faster and more robustly than a single robot. However, a collaborative approach requires a distributed paradigm to be scalable and deal with communication issues. This work presents a fully distributed algorithm enabling a group of robots to collectively optimize the parameters of a Neural Radiance Field (NeRF). The algorithm involves the communication of each robot's trained NeRF parameters over a mesh network, where each robot trains its NeRF and has access to its own visual data only. Additionally, the relative poses of all robots are jointly optimized alongside the model parameters, enabling mapping with less accurate relative camera poses. We show that multi-robot systems can benefit from differentiable and robust 3D reconstruction optimized from multiple NeRFs. Experiments on real-world and synthetic data demonstrate the efficiency of the proposed algorithm. See the website of the project for videos of the experiments and supplementary material https://sites.google.com/view/di-nerf/home.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct11">
             <b>
              ThCT11
             </b>
            </a>
           </td>
           <td class="r">
            314
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thct11" title="Click to go to the Program at a Glance">
             <b>
              Haptics 2
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#118310" title="Click to go to the Author Index">
             Kyung, Ki-Uk
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science &amp; Technology (KAIST)
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct11_01">
             11:15-11:20, Paper ThCT11.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('15'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Hybrid Haptic Device for Virtual Car Door Interactions: Design and Implementation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#244519" title="Click to go to the Author Index">
             Ma, Jihyeong
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#363812" title="Click to go to the Author Index">
             Kim, Ji-Sung
            </a>
           </td>
           <td class="r">
            KAIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#118310" title="Click to go to the Author Index">
             Kyung, Ki-Uk
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science &amp; Technology (KAIST)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab15" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#haptics_and_haptic_interfaces" title="Click to go to the Keyword Index">
               Haptics and Haptic Interfaces
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#virtual_reality_and_interfaces" title="Click to go to the Keyword Index">
               Virtual Reality and Interfaces
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#compliance_and_impedance_control" title="Click to go to the Keyword Index">
               Compliance and Impedance Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             As cars evolve from mere modes of transportation into living spaces, the importance of haptic interaction with vehicles is increasing. Here, we introduce a hybrid haptic device for the virtual prototyping of car doors, employing both the motor and brake. Physical prototyping, which is a conventional method for product designing, is often expensive and time-consuming. As a valuable alternative, virtual prototyping with a haptic device that delivers realistic haptic feedback can be utilized. However, replicating the substantial torque of a car door requires a high torque capacity motor, which can potentially pose safety risks to the user during haptic interaction. The proposed hybrid haptic device, combining a servo motor and a magnetic powder brake, effectively renders the dynamics of car doors. We experimentally measured the door's torque profile and confirmed significant friction from the door check mechanism and hinge. The torque profile was divided into active and passive torque, and each torque was distributed to the motor and brake, respectively. Finally, the proposed device and control method demonstrate the capability to accurately render the car door's kinesthetic haptic feedback, confirming its potential as an efficient tool for virtual prototyping in automotive design.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct11_02">
             11:20-11:25, Paper ThCT11.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2606'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              RAR-6: An Optimized Reconfigurable Asymmetric 6-DOF Haptic Robot for Gross and Fine Motor Tasks
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#272752" title="Click to go to the Author Index">
             Zhang, Changqi
            </a>
           </td>
           <td class="r">
            SINOPEC Research Institute of Petroleum Engineering Co., Ltd
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#292191" title="Click to go to the Author Index">
             Wang, Cui
            </a>
           </td>
           <td class="r">
            Southern University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#299182" title="Click to go to the Author Index">
             Wang, Congzhe
            </a>
           </td>
           <td class="r">
            Chongqing University of Posts and Telecommunications
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#181799" title="Click to go to the Author Index">
             Zhang, Mingming
            </a>
           </td>
           <td class="r">
            Southern University of Science and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2606" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#haptics_and_haptic_interfaces" title="Click to go to the Keyword Index">
               Haptics and Haptic Interfaces
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mechanism_design" title="Click to go to the Keyword Index">
               Mechanism Design
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robot-assisted task-oriented training demonstrates immense potential in rehabilitation area. Parallel robots, with advantages such as low inertia and high stiffness, facilitate precise haptic feedback, yet their application in rehabilitation is limited by workspace constraints. To this end, we propose a design scheme for a haptic robot based on a reconfigurable asymmetric parallel mechanism. We first introduce a two-stage multi-objective optimization method to obtain the optimal parameter configurations. Then, to achieve precise assembling of the reconfigurable mechanism in each configuration, corresponding positioning mechanisms are designed. System performance tests validate the robot’s capabilities under different configurations: workspace meets design requirements, stiffness output reaches 30 N/mm, force output is 40 N, RMS of maximum back-driven force along x, y, and z axes is 7.5 N, and RMS of maximum back-driven torque around x and y axes is 567.4 N∙mm. Target tracking and virtual channel trajectory tracking experiments demonstrate the system’s haptic rendering ability for gross motor tasks (GMTs) and fine motor tasks (FMTs), respectively. The developed 6-DOF haptic robot holds promise for versatile task-oriented rehabilitation training.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct11_03">
             11:25-11:30, Paper ThCT11.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3765'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Design, Implementation, and Validation of an Ungrounded Visuo-Tactile Haptic Interface for Robotic Teleoperation in High-Risk Steel Production
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#346406" title="Click to go to the Author Index">
             Park, Jaehyun
            </a>
           </td>
           <td class="r">
            Pohang University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#171886" title="Click to go to the Author Index">
             Choi, Il Seop
            </a>
           </td>
           <td class="r">
            POSCO HOLDINGS
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#394335" title="Click to go to the Author Index">
             Choi, Sang-Woo
            </a>
           </td>
           <td class="r">
            PoscoHoldings
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#103228" title="Click to go to the Author Index">
             Kim, Keehoon
            </a>
           </td>
           <td class="r">
            POSTECH, Pohang University of Science and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3765" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#telerobotics_and_teleoperation" title="Click to go to the Keyword Index">
               Telerobotics and Teleoperation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#haptics_and_haptic_interfaces" title="Click to go to the Keyword Index">
               Haptics and Haptic Interfaces
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#robotics_in_hazardous_fields" title="Click to go to the Keyword Index">
               Robotics in Hazardous Fields
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Haptic devices are widely used as control interfaces for robotic teleoperation, offering intuitive rendering of interactions between remote robot and environment. In particular, cutaneous feedback devices provide intrinsic stability and reduced form factor compared to kinesthetic feedback interfaces. However, the implementation of cutaneous feedback devices in industrial settings must be rigorously validated to prevent potential equipment accidents, which could lead to substantial economic losses due to unskilled robot manipulation. This paper presents a novel ungrounded haptic control interface (POstick-VF), designed specifically for high-risk steel production tasks. POstick-VF offers visuo-tactile feedback within an extensive workspace, enabling intuitive robot manipulation through its kinematic similarity with real tools ensuring safety. The performance of the developed POstick is rigorously validated and compared with conventional joystick controller through experiments conducted with an on-site hydraulic robot.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct11_04">
             11:30-11:35, Paper ThCT11.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4874'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Enhanced Tiny Haptic Dial with T-Shaped Shaft Based on Magnetorheological Fluid
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#351350" title="Click to go to the Author Index">
             Heo, Yong Hae
            </a>
           </td>
           <td class="r">
            Korea University of Technology and Education
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#406360" title="Click to go to the Author Index">
             Kim, Seongho
            </a>
           </td>
           <td class="r">
            Korea University of Technology and Education
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#124475" title="Click to go to the Author Index">
             Kim, Sang-Youn
            </a>
           </td>
           <td class="r">
            Korea Univ. Technology &amp; Education
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4874" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#haptics_and_haptic_interfaces" title="Click to go to the Keyword Index">
               Haptics and Haptic Interfaces
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#touch_in_hri" title="Click to go to the Keyword Index">
               Touch in HRI
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper introduces a tiny haptic dial utilizing magnetorheological fluid (MRF) to enhance its resistive torque feedback. Moreover, we design the T-shaped rotary shaft with bumps and embed it into the haptic dial to enhance its haptic performance (resistive torque). This structure enables two operation modes (shear and flow) of MRF that contribute to the actuation simultaneously in the proposed haptic dial. This structure allows the magnetic flux to flow towards the MRF, helping further maximize the resistive torque. We conduct a simulation to confirm that the magnetic flux generated from a solenoid forms a closed-loop magnetic path without magnetic saturation or leakage in the proposed haptic dial. The resistive torque of the proposed haptic dial varied from 8 N·mm to 47 N·mm as the input current changed from 0 to 300 mA, thus indicating that the proposed haptic dial can create a variety of haptic sensations in a tiny size (diameter: 20 mm; height:20 mm).
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct11_05">
             11:35-11:40, Paper ThCT11.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('5031'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Path-Constrained Haptic Motion Guidance Via Adaptive Phase-Based Admittance Control
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#180824" title="Click to go to the Author Index">
             Shahriari, Erfan
            </a>
           </td>
           <td class="r">
            Boston Dynamics AI Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#226424" title="Click to go to the Author Index">
             Svarny, Petr
            </a>
           </td>
           <td class="r">
            CTU in Prague, FEE
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#232852" title="Click to go to the Author Index">
             Baradaran Birjandi, Seyed Ali
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#114104" title="Click to go to the Author Index">
             Hoffmann, Matej
            </a>
           </td>
           <td class="r">
            Czech Technical University in Prague, Faculty of Electrical Engi
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#108317" title="Click to go to the Author Index">
             Haddadin, Sami
            </a>
           </td>
           <td class="r">
            Mohamed Bin Zayed University of Artificial Intelligence
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab5031" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#physical_human_robot_interaction" title="Click to go to the Keyword Index">
               Physical Human-Robot Interaction
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#robust_adaptive_control_of_robotic_systems" title="Click to go to the Keyword Index">
               Robust/Adaptive Control of Robotic Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_control" title="Click to go to the Keyword Index">
               Motion Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robots have surpassed humans in terms of strength and precision, yet humans retain an unparalleled ability for decision-making in the face of unpredictable disturbances. This article aims to combine the strengths of both entities within a singular task: human motion guidance under strict geometric constraints, particularly adhering to predetermined paths. To tackle this challenge, a modular haptic guidance law is proposed that takes the human-applied wrench as an input. Using an auxiliary variable called phase, the generated desired motion is guaranteed to consistently adhere to the constraint path. It is demonstrated how the guidance policy can be generalized into physically interpretable terms, adjustable either prior to initiating the task or dynamically while the task is in progress. Additionally, an illustrative guidance adaptation policy is showcased that takes into account the human's manipulability. Leveraging passivity analysis, potential sources of instability are pinpointed, and subsequently, overall system stability is ensured by incorporating an augmented virtual energy tank. Lastly, a comprehensive set of experiments, including a 20-participant user study, explores various aspects of the approach in practice, encompassing both technical and usability consideration.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct11_06">
             11:40-11:45, Paper ThCT11.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('5069'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Pneumatic-Actuated Feel-Through Wearable Haptic Display for Multi-Cue Delivery
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#347188" title="Click to go to the Author Index">
             Pagnanelli, Giulia
            </a>
           </td>
           <td class="r">
            University of Pisa
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#412477" title="Click to go to the Author Index">
             Latella, Giovanni
            </a>
           </td>
           <td class="r">
            University of Pisa
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#128228" title="Click to go to the Author Index">
             Catalano, Manuel Giuseppe
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#125068" title="Click to go to the Author Index">
             Bianchi, Matteo
            </a>
           </td>
           <td class="r">
            University of Pisa
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab5069" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#haptics_and_haptic_interfaces" title="Click to go to the Keyword Index">
               Haptics and Haptic Interfaces
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#wearable_robotics" title="Click to go to the Keyword Index">
               Wearable Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mechanism_design" title="Click to go to the Keyword Index">
               Mechanism Design
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Compared to the ”Seeing-through” paradigm for the concurrent display of both real and virtual images in vision-enabled Augmented Reality (AR), its haptic counterpart, i.e., the ”Feeling-through” via wearable tactile systems, which enables to experience simultaneously physical objects and haptically rendered virtual properties, is still largely unexplored. In a previous work, we introduced the Wearable-Fabric Yielding Display (W-FYD), which uses an elastic thin fabric as the interaction surface with the finger, allowing the delivery of softness-related cues both in active and passive exploration mode, together with sliding stimuli. The device was proven effective, but the current design faces form factor issues related to the dimensions and weight of the device, due to the actuation strategy of the lifting mechanism in the passive mode. To tackle this issue, we propose a miniaturized version of the system, named the W-FYD AIR, which allows reducing the overall dimensions of the device, from 100 × 60 × 36 mm to 78 × 45 × 37 mm, and its weight, from 100 g to 54 g, by exploiting pneumatically-actuated chambers for the lifting mechanism. Through careful sizing of each component and a process of characterization and identification, we demonstrated that the new system attained the same characteristics and functionality as the original one.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct12">
             <b>
              ThCT12
             </b>
            </a>
           </td>
           <td class="r">
            315
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thct12" title="Click to go to the Program at a Glance">
             <b>
              Big Data
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#158823" title="Click to go to the Author Index">
             Xu, Danfei
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct12_01">
             11:15-11:20, Paper ThCT12.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('205'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              How Generalizable Is My Behavior Cloning Policy? a Statistical Approach to Trustworthy Performance Evaluation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#291157" title="Click to go to the Author Index">
             Vincent, Joseph
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#215912" title="Click to go to the Author Index">
             Nishimura, Haruki
            </a>
           </td>
           <td class="r">
            Toyota Research Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#286255" title="Click to go to the Author Index">
             Itkina, Masha
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#282828" title="Click to go to the Author Index">
             Shah, Paarth
            </a>
           </td>
           <td class="r">
            University of Oxford
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#106233" title="Click to go to the Author Index">
             Schwager, Mac
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#107892" title="Click to go to the Author Index">
             Kollar, Thomas
            </a>
           </td>
           <td class="r">
            Toyota Research Institute
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab205" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#performance_evaluation_and_benchmarking" title="Click to go to the Keyword Index">
               Performance Evaluation and Benchmarking
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#probability_and_statistical_methods" title="Click to go to the Keyword Index">
               Probability and Statistical Methods
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             With the rise of stochastic generative models in robot policy learning, end-to-end visuomotor policies are increasingly successful at solving complex tasks by learning from human demonstrations. Nevertheless, since real-world evaluation costs afford users only a small number of policy rollouts, it remains a challenge to accurately gauge the performance of such policies. This is exacerbated by distribution shifts causing unpredictable changes in performance during deployment. To rigorously evaluate behavior cloning policies, we present a framework that provides a tight lower-bound on robot performance in an arbitrary environment, using a minimal number of experimental policy rollouts. Notably, by applying the standard stochastic ordering to robot performance distributions, we provide a worst-case bound on the entire distribution of performance (via bounds on the cumulative distribution function) for a given task. We build upon established statistical results to ensure that the bounds hold with a user-specified confidence level and tightness, and are constructed from as few policy rollouts as possible. In experiments we evaluate policies for visuomotor manipulation in both simulation and hardware. Specifically, we (i) empirically validate the guarantees of the bounds in simulated manipulation settings, (ii) find the degree to which a learned policy deployed on hardware generalizes to new real-world environments, and (iii) rigorously compare two policies tested in out-of-distribution settings. Our experimental data, code, and implementation of confidence bounds are open-source.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct12_02">
             11:20-11:25, Paper ThCT12.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1063'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Fine-Grained Open-Vocabulary Object Detection with Fined-Grained Prompts: Task, Dataset and Benchmark
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417559" title="Click to go to the Author Index">
             Liu, Ying
            </a>
           </td>
           <td class="r">
            Northeastern University, China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#451585" title="Click to go to the Author Index">
             Hua, Yijing
            </a>
           </td>
           <td class="r">
            Northeastern University, China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#451584" title="Click to go to the Author Index">
             Chai, Haojiang
            </a>
           </td>
           <td class="r">
            Northeastern University, China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#451591" title="Click to go to the Author Index">
             Wang, Yanbo
            </a>
           </td>
           <td class="r">
            Northeastern University, China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#413057" title="Click to go to the Author Index">
             TengQi, Ye
            </a>
           </td>
           <td class="r">
            Articul8 AI
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1063" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#data_sets_for_robotic_vision" title="Click to go to the Keyword Index">
               Data Sets for Robotic Vision
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             <p>
              Open-vocabulary detectors are proposed to locate and recognize objects in novel classes. However, variations in vision-aware language vocabulary data used for open-vocabulary learning can lead to unfair and unreliable evaluations. Recent evaluation methods have attempted to address this issue by incorporating object properties or adding locations and characteristics to the captions. Nevertheless, since these properties and locations depend on the specific details of the images instead of classes, detectors can not make accurate predictions without precise descriptions provided through human annotation.
             </p>
             <p>
              <p>
               This paper introduces
               <strong>
                3F-OVD
               </strong>
               , a novel task that extends supervised fine-grained object detection to the open-vocabulary setting. Our task is intuitive and challenging, requiring a deep understanding of
               <em>
                fine-grained captions
               </em>
               and careful attention to
               <em>
                fine-grained details
               </em>
               in images in order to accurately detect
               <em>
                fine-grained objects
               </em>
               . Additionally, due to the scarcity of qualified fine-grained object detection datasets, we have created a new dataset,
               <strong>
                NEU-171K
               </strong>
               , tailored for both supervised and open-vocabulary settings. We benchmark state-of-the-art object detectors on our dataset for both settings. Furthermore, we propose a simple yet effective post-processing technique. Our data, annotations, and codes are available at
               <a href='"https://github.com/tengerye/3FOVD"'>
                https://github.com/tengerye/3FOVD
               </a>
               .
              </p>
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct12_03">
             11:25-11:30, Paper ThCT12.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2280'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              GPU-Accelerated Subsystem-Based ADMM for Large-Scale Interactive Simulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#396285" title="Click to go to the Author Index">
             Ji, Harim
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#367839" title="Click to go to the Author Index">
             Kim, Hyunsu
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#274086" title="Click to go to the Author Index">
             Lee, Jeongmin
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#319698" title="Click to go to the Author Index">
             Lee, Somang
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#395769" title="Click to go to the Author Index">
             An, Seoki
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#237295" title="Click to go to the Author Index">
             Heo, Jinuk
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#319683" title="Click to go to the Author Index">
             Lee, Youngseon
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#178091" title="Click to go to the Author Index">
             Lee, Yongseok
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104207" title="Click to go to the Author Index">
             Lee, Dongjun
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2280" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#simulation_and_animation" title="Click to go to the Keyword Index">
               Simulation and Animation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#virtual_reality_and_interfaces" title="Click to go to the Keyword Index">
               Virtual Reality and Interfaces
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#haptics_and_haptic_interfaces" title="Click to go to the Keyword Index">
               Haptics and Haptic Interfaces
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper, we implement the GPU-accelerated subsystem-based Alternating Direction Method of Multipliers (SubADMM) for interactive simulation. The challenging objective for interactive simulations is to deliver realistic results under tight performance, even for large-scale scenarios. We aim to achieve this by exploiting the parallelizable nature of SubADMM to the fullest extent. We introduce a new subsystem division strategy to make SubADMM `GPU friendly' along with custom kernel designs and optimization regarding efficient memory access patterns. We successfully implement the GPU-accelerated SubADMM and show the accuracy and speed of the framework for large-scale scenarios, highlighted with an interactive `Hand demo' scenario. We also show improved robustness and accuracy compared to other state-of-the-art interactive simulators with several challenging scenarios that introduce large-scale ill-conditioned dynamics problems.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct12_04">
             11:30-11:35, Paper ThCT12.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3045'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Local Policies Enable Zero-Shot Long-Horizon Manipulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#220382" title="Click to go to the Author Index">
             Dalal, Murtaza
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#371657" title="Click to go to the Author Index">
             Liu, Min
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#151023" title="Click to go to the Author Index">
             Talbott, Walter
            </a>
           </td>
           <td class="r">
            Apple
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420706" title="Click to go to the Author Index">
             Chen, Chen
            </a>
           </td>
           <td class="r">
            Apple
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#239260" title="Click to go to the Author Index">
             Pathak, Deepak
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#160774" title="Click to go to the Author Index">
             Zhang, Jian
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#166699" title="Click to go to the Author Index">
             Salakhutdinov, Ruslan
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3045" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#big_data_in_robotics_and_automation" title="Click to go to the Keyword Index">
               Big Data in Robotics and Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#machine_learning_for_robot_control" title="Click to go to the Keyword Index">
               Machine Learning for Robot Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Sim2real for robotic manipulation is difficult due to the challenges of simulating complex contacts and generating realistic task distributions. To tackle the latter problem, we introduce ManipGen, which leverages a new class of policies for sim2real transfer: local policies. Locality enables a variety of appealing properties including invariances to absolute robot and object pose, skill ordering, and global scene configuration. We combine these policies with foundation models for vision, language and motion planning and demonstrate SOTA zero-shot performance of our method to Robosuite benchmark tasks in simulation (97%). We transfer our local policies from simulation to reality and observe they can solve unseen long-horizon manipulation tasks with up to 8 stages with significant pose, object and scene configuration variation. ManipGen outperforms SOTA approaches such as SayCan, OpenVLA and LLMTrajGen across 50 real-world manipulation tasks by 36%, 76% and 62% respectively. All code, models and datasets will be released. Video results at manipgen.github.io
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct12_05">
             11:35-11:40, Paper ThCT12.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4290'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              DART: Dexterous Augmented Reality Teleoperation Platform for Large-Scale Robot Data Collection in Simulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#314699" title="Click to go to the Author Index">
             Park, Younghyo
            </a>
           </td>
           <td class="r">
            MIT
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426369" title="Click to go to the Author Index">
             Bhatia, Jagdeep
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#389867" title="Click to go to the Author Index">
             Ankile, Lars
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#204887" title="Click to go to the Author Index">
             Agrawal, Pulkit
            </a>
           </td>
           <td class="r">
            MIT
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4290" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#data_sets_for_robot_learning" title="Click to go to the Keyword Index">
               Data Sets for Robot Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#telerobotics_and_teleoperation" title="Click to go to the Keyword Index">
               Telerobotics and Teleoperation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#virtual_reality_and_interfaces" title="Click to go to the Keyword Index">
               Virtual Reality and Interfaces
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The scarcity of diverse and high-quality data impedes the quest to build a generalist robotic system. Current robotics data collection efforts face many challenges: the need for physical robotic hardware, setting up the environment, frequent resets, and the fatigue for data collectors operating real robots. We introduce DART, a teleoperation platform designed for crowdsourcing that reimagines robotic data collection by leveraging cloud-based simulation and augmented reality (AR) to address many limitations of prior data collection efforts. User studies show that DART enables higher data collection throughput and lower physical fatigue than real-world teleoperation. We also demonstrate that policies trained using DART-collected datasets successfully transfer to reality and are robust to unseen visual disturbances. All data collected through DART is automatically stored in a cloud-hosted database, DexHub, paving the path for an ever-growing data hub for robot learning.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct13">
             <b>
              ThCT13
             </b>
            </a>
           </td>
           <td class="r">
            316
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thct13" title="Click to go to the Program at a Glance">
             <b>
              Motion Prediction
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#275077" title="Click to go to the Author Index">
             Liang, Xiao
            </a>
           </td>
           <td class="r">
            Texas A&amp;M University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct13_01">
             11:15-11:20, Paper ThCT13.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('408'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              TransFusion: A Practical and Effective Transformer-Based Diffusion Model for 3D Human Motion Prediction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#365750" title="Click to go to the Author Index">
             Tian, Sibo
            </a>
           </td>
           <td class="r">
            Texas A&amp;M University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#190695" title="Click to go to the Author Index">
             Zheng, Minghui
            </a>
           </td>
           <td class="r">
            Texas A&amp;M University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#275077" title="Click to go to the Author Index">
             Liang, Xiao
            </a>
           </td>
           <td class="r">
            Texas A&amp;M University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab408" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Predicting human motion plays a crucial role in ensuring a safe and effective human-robot close collaboration in intelligent remanufacturing systems of the future. Existing works can be categorized into two groups: those focusing on accuracy, predicting a single future motion, and those generating diverse predictions based on
             <p>
              observations. The former group fails to address the uncertainty and multi-modal nature of human motion, while the latter group often produces motion sequences that deviate too far from the ground truth or
              <p>
               become unrealistic within historical contexts. To tackle these issues, we propose TransFusion, an innovative and practical diffusion-based model for 3D human motion prediction which can generate samples that are more likely to happen while maintaining a certain level of diversity. Our model leverages Transformer as the backbone with long skip connections between shallow and deep layers. Additionally, we employ the discrete cosine transform to model motion sequences in the frequency space, thereby improving performance. In contrast to prior diffusion-based models that utilize extra modules like cross-attention and adaptive layer normalization to condition the prediction on past observed motion, we treat all inputs, including conditions, as tokens to create a more practical and effective model compared to existing approaches. Extensive experimental studies are conducted on benchmark datasets to validate the effectiveness of our human motion prediction model. The project page is available at https://github.com/sibotian96/TransFusion.
              </p>
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct13_02">
             11:20-11:25, Paper ThCT13.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('409'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              DE-TGN: Uncertainty-Aware Human Motion Forecasting Using Deep Ensembles
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#310540" title="Click to go to the Author Index">
             Eltouny, Kareem
            </a>
           </td>
           <td class="r">
            Simpson Gumpertz &amp; Heger
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#286323" title="Click to go to the Author Index">
             Liu, Wansong
            </a>
           </td>
           <td class="r">
            University at Buffalo
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#365750" title="Click to go to the Author Index">
             Tian, Sibo
            </a>
           </td>
           <td class="r">
            Texas A&amp;M University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#190695" title="Click to go to the Author Index">
             Zheng, Minghui
            </a>
           </td>
           <td class="r">
            Texas A&amp;M University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#275077" title="Click to go to the Author Index">
             Liang, Xiao
            </a>
           </td>
           <td class="r">
            Texas A&amp;M University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab409" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Ensuring the safety of human workers in a collaborative environment with robots is of utmost importance. Although accurate pose prediction models can help prevent collisions between human workers and robots, they are still susceptible to critical errors. In this study, we propose a novel approach called deep ensembles of temporal graph neural
             <p>
              networks (DE-TGN) that not only accurately forecast human motion but also provide a measure of prediction uncertainty. By leveraging deep ensembles and employing stochastic Monte-Carlo dropout sampling, we construct a volumetric field representing a range of potential future human poses based on covariance ellipsoids. To validate our framework, we conducted experiments using three motion capture datasets including Human3.6M, and two human-robot interaction scenarios, achieving state-of-the-art prediction error. Moreover, we discovered that deep ensembles not only enable us to quantify uncertainty but also improve the accuracy of our predictions.
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct13_03">
             11:25-11:30, Paper ThCT13.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('835'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Large-Scale Dataset for Humanoid Robotics Enabling a Novel Data-Driven Fall Prediction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#149525" title="Click to go to the Author Index">
             Urbann, Oliver
            </a>
           </td>
           <td class="r">
            Fraunhofer IML
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#294486" title="Click to go to the Author Index">
             Eßer, Julian
            </a>
           </td>
           <td class="r">
            Fraunhofer IML
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419547" title="Click to go to the Author Index">
             Kleingarn, Diana
            </a>
           </td>
           <td class="r">
            TU Dortmund University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#245701" title="Click to go to the Author Index">
             Moos, Arne
            </a>
           </td>
           <td class="r">
            Robotics Research Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#311433" title="Click to go to the Author Index">
             Brämer, Dominik
            </a>
           </td>
           <td class="r">
            Fraunhofer IML
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#311432" title="Click to go to the Author Index">
             Brömmel, Piet
            </a>
           </td>
           <td class="r">
            Fraunhofer IML
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#324285" title="Click to go to the Author Index">
             Bach, Nicolas
            </a>
           </td>
           <td class="r">
            Fraunhofer IML
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#324353" title="Click to go to the Author Index">
             Jestel, Christian
            </a>
           </td>
           <td class="r">
            Fraunhofer IML
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419549" title="Click to go to the Author Index">
             Larisch, Aaron
            </a>
           </td>
           <td class="r">
            TU Dortmund University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#155925" title="Click to go to the Author Index">
             Kirchheim, Alice
            </a>
           </td>
           <td class="r">
            TU Dortmund
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab835" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#humanoid_and_bipedal_locomotion" title="Click to go to the Keyword Index">
               Humanoid and Bipedal Locomotion
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#failure_detection_and_recovery" title="Click to go to the Keyword Index">
               Failure Detection and Recovery
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#data_sets_for_robot_learning" title="Click to go to the Keyword Index">
               Data Sets for Robot Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper, we present a comprehensive dataset comprising 37.9 hours of sensor data collected from humanoid robots, including 18.3 hours of walking and 2,519 recorded falls. This extensive dataset is a valuable resource for various robotics and machine learning applications. Leveraging this data, we propose RePro-TCN, a Temporal Convolutional Network (TCN) enhanced with two novel extensions: Relaxed Loss Formulation and Progressive Forecasting. Predicting falls is a critical capability in humanoid robotics for implementing countermeasures such as lunging or stopping the walk. Thanks to the new dataset, we train RePro-TCN and demonstrate its superiority over previous approaches under real-world conditions that were previously unattainable.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct13_04">
             11:30-11:35, Paper ThCT13.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2285'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Social-MAE: Social Masked Autoencoder for Multi-Person Motion Representation Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423978" title="Click to go to the Author Index">
             Ehsanpour, Mahsa
            </a>
           </td>
           <td class="r">
            University of Adelaide
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#106056" title="Click to go to the Author Index">
             Reid, Ian
            </a>
           </td>
           <td class="r">
            University of Adelaide
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#212097" title="Click to go to the Author Index">
             Rezatofighi, Hamid
            </a>
           </td>
           <td class="r">
            Monash University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2285" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#recognition" title="Click to go to the Keyword Index">
               Recognition
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#human_centered_robotics" title="Click to go to the Keyword Index">
               Human-Centered Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             For seamless robot navigation, it’s vital to thoroughly understand multi-person scenes, which requires moving beyond simple tasks such as detection and tracking. Higher-level tasks, such as understanding the interactions and social activities among individuals, are also crucial. Progress towards models that can fully understand scenes involving multiple people is hindered by a lack of sufficient annotated data for such high-level tasks. To address this challenge, we introduce Social-MAE, a simple yet effective transformer-based masked autoencoder framework for multi-person human motion data. The framework uses masked modeling to pre-train the encoder to reconstruct masked human joint trajectories, enabling it to learn generalizable representations of motion in human crowded scenes. Social-MAE comprises a transformer as the MAE encoder and a lighter-weight transformer as the MAE decoder which operates on multi-person joints’ trajectory. After the reconstruction task, the MAE decoder is replaced with a task-specific decoder and the model is fine-tuned end-to-end for a variety of high-level social tasks. Our proposed model combined with our pre-training approach achieves the state-of-the-art results on various high-level social tasks, including multi-person pose forecasting, social grouping, and social action understanding. These improvements are demonstrated across four popular multi-person datasets encompassing both human 2D and 3D body pose.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct13_05">
             11:35-11:40, Paper ThCT13.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2404'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Depth-Temporal Attention with Dual Modality Data for Walking Intention Prediction in Close-Proximity Front-Following
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#275660" title="Click to go to the Author Index">
             Zhao, Chongyu
            </a>
           </td>
           <td class="r">
            The University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423625" title="Click to go to the Author Index">
             Guo, Lingyu
            </a>
           </td>
           <td class="r">
            The University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#313120" title="Click to go to the Author Index">
             Wen, Rongwei
            </a>
           </td>
           <td class="r">
            The University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#377441" title="Click to go to the Author Index">
             Wang, Yanrui
            </a>
           </td>
           <td class="r">
            The University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#247630" title="Click to go to the Author Index">
             Wu, Chuan
            </a>
           </td>
           <td class="r">
            The University of Hong Kong
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2404" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#human_detection_and_tracking" title="Click to go to the Keyword Index">
               Human Detection and Tracking
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#intention_recognition" title="Click to go to the Keyword Index">
               Intention Recognition
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The role of robot following is crucial for effective human-robot collaboration. Traditional methods often rely on maintaining a significant distance between the robot and the human, which limits interaction and responsiveness. In contrast, close-proximity front-following facilitates immediate engagement, enhancing user experience and improving human-robot interaction. Nonetheless, it presents challenges in accurately interpreting human walking intentions due to a restricted observational field. In our paper, we introduce an innovative Depth-Temporal Attention Network that takes lower-limb depth images and robot motor signals as input, to accurately predict human walking intentions. This network leverages a depth attention module to capture essential spatial features and integrates a temporal attention mechanism to analyze movement dynamics. To enhance generalization, we use a domain adversarial module that focuses on shared features across diverse walking data, ensuring consistent performance across users. Experimental results demonstrate that our approach achieves an impressive average intention prediction accuracy of 91.09%, significantly surpassing baseline models by 12.59% to 23.66%. Additionally, an ablation study reveals that the depth-attention module substantially improves the model's understanding of depth features, resulting in an 11.44% increase in accuracy. With this high prediction accuracy, smooth front-following is achieved at close-proximity.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct13_06">
             11:40-11:45, Paper ThCT13.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3039'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              UPTor: Unified 3D Human Pose Dynamics and Trajectory Prediction for Human-Robot Interaction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424692" title="Click to go to the Author Index">
             Nilavadi, Nisarga
            </a>
           </td>
           <td class="r">
            University of Technology Nuremberg
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#190205" title="Click to go to the Author Index">
             Rudenko, Andrey
            </a>
           </td>
           <td class="r">
            Robert Bosch GmbH
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#167078" title="Click to go to the Author Index">
             Linder, Timm
            </a>
           </td>
           <td class="r">
            Robert Bosch GmbH
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3039" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#human_detection_and_tracking" title="Click to go to the Keyword Index">
               Human Detection and Tracking
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#human_factors_and_human_in_the_loop" title="Click to go to the Keyword Index">
               Human Factors and Human-in-the-Loop
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#datasets_for_human_motion" title="Click to go to the Keyword Index">
               Datasets for Human Motion
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We introduce a unified approach to forecast the dynamics of human keypoints along with the motion trajectory based on a short sequence of input poses. While many studies address either full-body pose prediction or motion trajectory prediction, only a few attempt to merge them. We propose a motion transformation technique to simultaneously predict full-body pose and trajectory key-points in a global coordinate frame. We utilize an off-the-shelf 3D human pose estimation module, a graph attention network to encode the skeleton structure, and a compact, non-autoregressive transformer suitable for real-time motion prediction for human-robot interaction and human-aware navigation. We introduce a human navigation dataset "DARKO" with specific focus on navigational activities that are relevant for human-aware mobile robot navigation. We perform extensive evaluation on Human3.6M, CMU-Mocap, and our DARKO dataset. In comparison to prior work, we show that our approach is compact, real-time, and accurate in predicting human navigation motion across all datasets. Result animations, our dataset, and code will be available at https://nisarganc.github.io/UPTor-page/
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct14">
             <b>
              ThCT14
             </b>
            </a>
           </td>
           <td class="r">
            402
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thct14" title="Click to go to the Program at a Glance">
             <b>
              Scene Reconstruction Using Radiance Fields
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct14_01">
             11:15-11:20, Paper ThCT14.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('111'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Category-Level Neural Field for Reconstruction of Partially Observed Objects in Indoor Environment
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#341944" title="Click to go to the Author Index">
             Lee, Taekbeom
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#243474" title="Click to go to the Author Index">
             Jang, Youngseok
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#103663" title="Click to go to the Author Index">
             Kim, H. Jin
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab111" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Neural implicit representation has attracting at- tention in 3D reconstruction through various success cases. For further applications such as scene understanding or editing, sev- eral works have shown progress towards object-compositional reconstruction. Despite their superior performance in observed regions, their performance is still limited in reconstructing ob- jects that are partially observed. To better treat this problem, we introduce a category-level neural fields which learns meaningful common 3D information among objects belonging to the same category present in the scene. Our key idea is to subcategorize objects based on their observed shape for better training of category-level model. Then we take advantage of the neural field to conduct the challenging task of registering partially observed objects by selecting and aligning against representa- tive objects selected by ray-based uncertainty. Experiments on both simulation and real-world dataset demonstrate that our method improve reconstruction of unobserved part for several categories.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct14_02">
             11:20-11:25, Paper ThCT14.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('292'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              PlanarNeRF: Online Learning of Planar Primitives with Neural Radiance Fields
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#246551" title="Click to go to the Author Index">
             Chen, Zheng
            </a>
           </td>
           <td class="r">
            Indiana University Bloomington
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#296337" title="Click to go to the Author Index">
             Yan, Qingan
            </a>
           </td>
           <td class="r">
            Goertek US
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#209821" title="Click to go to the Author Index">
             Zhan, Huangying
            </a>
           </td>
           <td class="r">
            The University of Adelaide
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#191083" title="Click to go to the Author Index">
             Cai, Changjiang
            </a>
           </td>
           <td class="r">
            Stevens Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#392022" title="Click to go to the Author Index">
             Xu, Xiangyu
            </a>
           </td>
           <td class="r">
            OPPO
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#415480" title="Click to go to the Author Index">
             Huang, Yuzhong
            </a>
           </td>
           <td class="r">
            University of Southern California
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#324835" title="Click to go to the Author Index">
             Wang, Weihan
            </a>
           </td>
           <td class="r">
            Stevens Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#311469" title="Click to go to the Author Index">
             Feng, Ziyue
            </a>
           </td>
           <td class="r">
            Clemson University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#168295" title="Click to go to the Author Index">
             Xu, Yi
            </a>
           </td>
           <td class="r">
            OPPO US Research Center
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#141677" title="Click to go to the Author Index">
             Liu, Lantao
            </a>
           </td>
           <td class="r">
            Indiana University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab292" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#recognition" title="Click to go to the Keyword Index">
               Recognition
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Identifying spatially complete planar primitives from visual data is a crucial task in computer vision. Prior methods are largely restricted to either 2D segment recovery or simplifying 3D structures, even with extensive plane annotations. We present PlanarNeRF, a novel framework capable of detecting dense 3D planes through online learning. Drawing upon the neural field representation, PlanarNeRF brings three major contributions. First, it enhances 3D plane detection with concurrent appearance and geometry knowledge. Second, a lightweight plane fitting module is used to estimate plane parameters. Third, a novel global memory bank structure with an update mechanism is introduced, ensuring consistent cross-frame correspondence. The flexible architecture of PlanarNeRF allows it to function in both 2D-supervised and self-supervised solutions, in each of which it can effectively learn from sparse training signals, significantly improving training efficiency. Through extensive experiments, we demonstrate the effectiveness of PlanarNeRF in various real-world scenarios and remarkable improvement in 3D plane detection over existing works.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct14_03">
             11:25-11:30, Paper ThCT14.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3599'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              FreeDriveRF: Monocular RGB Dynamic NeRF without Poses for Autonomous Driving Via Point-Level Dynamic-Static Decoupling
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423392" title="Click to go to the Author Index">
             Wen, Yue
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#350774" title="Click to go to the Author Index">
             Song, Liang
            </a>
           </td>
           <td class="r">
            Dimension
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423510" title="Click to go to the Author Index">
             Liu, Yijia
            </a>
           </td>
           <td class="r">
            China University of Mining and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423649" title="Click to go to the Author Index">
             Zhu, Siting
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#237597" title="Click to go to the Author Index">
             Miao, Yanzi
            </a>
           </td>
           <td class="r">
            China University of Mining and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#256999" title="Click to go to the Author Index">
             Han, Lijun
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#103003" title="Click to go to the Author Index">
             Wang, Hesheng
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3599" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_transportation" title="Click to go to the Keyword Index">
               Computer Vision for Transportation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Dynamic scene reconstruction for autonomous driving enables vehicles to perceive and interpret complex scene changes more precisely. Dynamic Neural Radiance Fields (NeRFs) have recently shown promising capability in scene modeling. However, many existing methods rely heavily on accurate poses inputs and multi-sensor data, leading to increased system complexity. To address this, we propose FreeDriveRF, which reconstructs dynamic driving scenes using only sequential RGB images without requiring poses inputs. We innovatively decouple dynamic and static parts at the early sampling level, avoiding image blurring and artifacts. To overcome the challenges posed by object motion and occlusion in monocular camera, we introduce a warped ray-guided dynamic object rendering consistency loss, utilizing optical flow to better constrain the dynamic modeling process. Additionally, we incorporate estimated dynamic flow to constrain the pose optimization process, improving the stability and accuracy of unbounded scene reconstruction. Extensive experiments conducted on the KITTI and Waymo datasets demonstrate the superior performance of our method in dynamic scene modeling for autonomous driving.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct14_04">
             11:30-11:35, Paper ThCT14.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3862'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              LLGS: Unsupervised Gaussian Splatting for Image Enhancement and Reconstruction in Pure Dark Environment
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425927" title="Click to go to the Author Index">
             Wang, Haoran
            </a>
           </td>
           <td class="r">
            The University of Sussex
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#415967" title="Click to go to the Author Index">
             Huang, Jingwei
            </a>
           </td>
           <td class="r">
            University of Electronic Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#205108" title="Click to go to the Author Index">
             Yang, Lu
            </a>
           </td>
           <td class="r">
            University of Electronic Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#312315" title="Click to go to the Author Index">
             Deng, Tianchen
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425894" title="Click to go to the Author Index">
             Zhang, Gaojing
            </a>
           </td>
           <td class="r">
            University of Sussex
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#346050" title="Click to go to the Author Index">
             Li, Mingrui
            </a>
           </td>
           <td class="r">
            Dalian University of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3862" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             3D Gaussian Splatting has shown remarkable capabilities in novel view rendering tasks and exhibits significant potential for multi-view optimization. However, the original 3D Gaussian Splatting lacks color representation for inputs in lowlight environments. Simply using enhanced images as inputs would lead to issues with multi-view consistency, and current single-view enhancement systems rely on pre-trained data, lacking scene generalization. These problems limit the application of 3D Gaussian Splatting in low-light conditions in the field of robotics, including high-fidelity modeling and feature matching. To address these challenges, we propose an unsupervised multiview stereoscopic system based on Gaussian Splatting, called Low-Light Gaussian Splatting (LLGS). This system aims to enhance images in low-light environments while reconstructing the scene. Our method introduces a decomposable Gaussian representation called M-Color, which separately characterizes color information for targeted enhancement. Furthermore, we propose an unsupervised optimization method with zeroknowledge priors, using direction-based enhancement to ensure multi-view consistency. Experiments conducted on real-world datasets demonstrate that our system outperforms state-of-theart methods in both low-light enhancement and 3D Gaussian Splatting.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct14_05">
             11:35-11:40, Paper ThCT14.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4073'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Hash-GS: Anchor-Based 3D Gaussian Splatting with Multi-Resolution Hash Encoding for Efficient Scene Reconstruction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#387226" title="Click to go to the Author Index">
             Xie, Yijia
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426301" title="Click to go to the Author Index">
             Lin, Yuhang
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#325876" title="Click to go to the Author Index">
             Li, Laijian
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#334693" title="Click to go to the Author Index">
             Liu, Lina
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416278" title="Click to go to the Author Index">
             Wei, Xiaobin
            </a>
           </td>
           <td class="r">
            Wasu Media&amp;Network Co..Ltd
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#122966" title="Click to go to the Author Index">
             Liu, Yong
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#246892" title="Click to go to the Author Index">
             Lv, Jiajun
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4073" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Realistic 3D object and scene reconstruction is pivotal in advancing fields such as world model simulation and embodied intelligence. In this paper, we introduce Hash-GS, a storage-efficient method for large-scale scene reconstruction using anchor-based 3D Gaussian Splatting (3DGS). The vanilla 3DGS struggles with high memory demands due to the large number of primitives, especially in complex or extensive scenes. Hash-GS addresses these challenges with a compact representation by leveraging high-dimensional features to parameterize primitive properties, stored in compact hash tables, which reduces memory usage while preserving rendering quality. It also incorporates adaptive anchor management to efficiently control the number of anchors and neural Gaussians. Additionally, we introduce an analytic 3D smoothing filter to mitigate aliasing and support Level-of-Detail for optimized rendering across varying intrinsic parameters. Experimental results on several datasets demonstrate that Hash-GS improves storage efficiency while maintaining competitive rendering performance, especially in large-scale scenes.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct14_06">
             11:40-11:45, Paper ThCT14.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4266'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Elite-EvGS: Learning Event-Based 3D Gaussian Splatting by Distilling Event-To-Video Priors
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421884" title="Click to go to the Author Index">
             Zhang, Zixin
            </a>
           </td>
           <td class="r">
            HKUST-GZ
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421811" title="Click to go to the Author Index">
             Chen, Kanghao
            </a>
           </td>
           <td class="r">
            Hong Kong University of Science and Technology (Guangzhou)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#256235" title="Click to go to the Author Index">
             Wang, Lin
            </a>
           </td>
           <td class="r">
            Nanyang Technological University (NTU)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4266" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Event cameras are bio-inspired sensors that output asynchronous and sparse event streams, instead of fixed frames. Benefiting from their distinct advantages, such as high dynamic range and high temporal resolution, event cameras have been applied to address 3D reconstruction, important for robotic mapping. Recently, neural rendering techniques, such as 3D Gaussian splatting (3DGS), have been shown successful in 3D reconstruction. However, it still remains under-explored how to develop an effective event-based 3DGS pipeline. In particular, as 3DGS typically depends on high-quality initialization and dense multiview constraints, a potential problem appears for the 3DGS optimization with events given its inherent sparse property. To this end, we propose a novel event-based 3DGS framework, named textbf{Elite-EvGS}. Our key idea is to distill the prior knowledge from the off-the-shelf event-to-video (E2V) models to effectively reconstruct 3D scenes from events in a coarse-to-fine optimization manner. Specifically, to address the complexity of 3DGS initialization from events, we introduce a novel textit{warm-up initialization strategy} that optimizes a coarse 3DGS from the frames generated by E2V models and then incorporates events to refine the details. Then, we propose a textit{progressive event supervision strategy} that employs the window-slicing operation to progressively reduce the number of events used for supervision. This subtly relives the temporal randomness of the event frames, benefiting the optimization of local textural and global structural details. Experiments on the benchmark datasets demonstrate that Elite-EvGS can reconstruct 3D scenes with better textural and structural details. Meanwhile, our method yields plausible performance on the captured real-world data, including diverse challenging conditions, such as fast motion and low light scenes. For demo and more results, please check our project page: https://vlislab22.github.io/elite-evgs/
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct15">
             <b>
              ThCT15
             </b>
            </a>
           </td>
           <td class="r">
            403
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thct15" title="Click to go to the Program at a Glance">
             <b>
              Continuum Robots 2
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#151228" title="Click to go to the Author Index">
             Krishnan, Girish
            </a>
           </td>
           <td class="r">
            University of Illinois Urbana Champaign
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct15_01">
             11:15-11:20, Paper ThCT15.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('48'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Hysteresis Compensation of Flexible Continuum Manipulator Using RGBD Sensing and Temporal Convolutional Network
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#353715" title="Click to go to the Author Index">
             Park, Junhyun
            </a>
           </td>
           <td class="r">
            DGIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#353720" title="Click to go to the Author Index">
             Jang, Seonghyeok
            </a>
           </td>
           <td class="r">
            DGIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#374268" title="Click to go to the Author Index">
             Park, Hyojae
            </a>
           </td>
           <td class="r">
            Daegu Gyeongbuk Institute of Science and Technology (DGIST)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#385976" title="Click to go to the Author Index">
             Bae, Seongjun
            </a>
           </td>
           <td class="r">
            DGIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#145094" title="Click to go to the Author Index">
             Hwang, Minho
            </a>
           </td>
           <td class="r">
            Daegu Gyeongbuk Instituute of Science and Technology (DGIST)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab48" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#tendon_wire_mechanism" title="Click to go to the Keyword Index">
               Tendon/Wire Mechanism
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#machine_learning_for_robot_control" title="Click to go to the Keyword Index">
               Machine Learning for Robot Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#modeling__control__and_learning_for_soft_robots" title="Click to go to the Keyword Index">
               Modeling, Control, and Learning for Soft Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Flexible continuum manipulators are valued for minimally invasive surgery, offering access to confined spaces through nonlinear paths. However, cable-driven manipulators face control difficulties due to hysteresis from cabling effects such as friction, elongation, and coupling. These effects are difficult to model due to nonlinearity and the difficulties become even more evident when dealing with long and coupled, multi-segmented manipulator. This paper proposes a data-driven approach based on Deep Neural Networks (DNN) to capture these nonlinear and previous states-dependent characteristics of cable actuation. We collect physical joint configurations according to command joint configurations using RGBD sensing and 7 fiducial markers to model the hysteresis of the proposed manipulator. Result on a study comparing the estimation performance of four DNN models show that the Temporal Convolution Network (TCN) demonstrates the highest predictive capability. Leveraging trained TCNs, we build a control algorithm to compensate for hysteresis. Tracking tests in task space using unseen trajectories show that the proposed control algorithm reduces the average position and orientation error by 61.39% (from 13.7mm to 5.29 mm) and 64.04% (from 31.17° to 11.21°), respectively. This result implies that the proposed calibrated controller effectively reaches the desired configurations by estimating the hysteresis of the manipulator. Applying this method in real surgical scenarios has the potential to enhance control precision and improve surgical performance.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct15_03">
             11:25-11:30, Paper ThCT15.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('892'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Command Filtered Cartesian Impedance Control for Tendon Driven Continuum Manipulators with Actuator Fault Compensation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419083" title="Click to go to the Author Index">
             Zheng, Xianjie
            </a>
           </td>
           <td class="r">
            Nanjing University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#382689" title="Click to go to the Author Index">
             Yu, Zhaobao
            </a>
           </td>
           <td class="r">
            Nanjing University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419126" title="Click to go to the Author Index">
             Ding, Meng
            </a>
           </td>
           <td class="r">
            Nanjing University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#382709" title="Click to go to the Author Index">
             Liu, Liaoxue
            </a>
           </td>
           <td class="r">
            Nanjing University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#288770" title="Click to go to the Author Index">
             Guo, Jian
            </a>
           </td>
           <td class="r">
            Nanjing Univ. of Sci. &amp; Tech
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#382714" title="Click to go to the Author Index">
             Guo, Yu
            </a>
           </td>
           <td class="r">
            Nanjing University of Science and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab892" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#modeling__control__and_learning_for_soft_robots" title="Click to go to the Keyword Index">
               Modeling, Control, and Learning for Soft Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#compliance_and_impedance_control" title="Click to go to the Keyword Index">
               Compliance and Impedance Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Continuum robots are well-suited for constrained environments due to their superior flexibility and structural compliance. However, relying solely on passive compliance may lead to damage to both the robot and the surrounding environment. This work proposes a finite-time Cartesian impedance control scheme for tendon-driven continuum manipulators (TDCMs), where a second-order low-pass filter is used to adjust the reference trajectory according to the external robot tip force. The controller is designed using the command filtered backstepping method, and the finite-time stability is established by the designed Lyapunov function. In TDCM systems, the tendons operate antagonistically, and actuators often fail to quickly reach the desired tendon tension, leading to partial failures. To address this, we propose an actuator fault compensation algorithm to enhance system performance and reliability. We conducted trajectory tracking experiments on a multi-segment TDCM prototype, the results demonstrate that the designed Cartesian impedance controller achieves effective compliance control effect and high position control accuracy.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct15_04">
             11:30-11:35, Paper ThCT15.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1429'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Synergistic Framework for Learning Shape Estimation and Shape-Aware Whole-Body Control Policy for Continuum Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#244242" title="Click to go to the Author Index">
             Kasaei, Mohammadreza
            </a>
           </td>
           <td class="r">
            University of Edinburgh
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#190490" title="Click to go to the Author Index">
             Alambeigi, Farshid
            </a>
           </td>
           <td class="r">
            University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#179020" title="Click to go to the Author Index">
             Khadem, Mohsen
            </a>
           </td>
           <td class="r">
            University of Edinburgh
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1429" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#modeling__control__and_learning_for_soft_robots" title="Click to go to the Keyword Index">
               Modeling, Control, and Learning for Soft Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#machine_learning_for_robot_control" title="Click to go to the Keyword Index">
               Machine Learning for Robot Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#soft_robot_applications" title="Click to go to the Keyword Index">
               Soft Robot Applications
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper, we present a novel synergistic framework for learning shape estimation and a shape-aware whole-body control policy for continuum robots. Our approach leverages the interaction between two Augmented Neural Ordinary Differential Equations (ANODEs) - the Shape-NODE and Control-NODE - to achieve continuous shape estimation and shape-aware control. The Shape-NODE integrates prior knowledge from Cosserat rod theory, allowing it to adapt and account for model mismatches, while the Control-NODE uses this shape information to optimize a whole-body control policy, trained in a Model Predictive Control (MPC) fashion. This unified framework effectively overcomes limitations of existing data-driven methods, such as poor shape awareness and challenges in capturing complex nonlinear dynamics. Extensive evaluations in both simulation and real-world environments demonstrate the framework’s robust performance in shape estimation, trajectory tracking, and obstacle avoidance. The proposed method consistently outperforms state-of-the-art end-to-end, Neural-ODE, and Recurrent Neural Network (RNN) models, particularly in terms of tracking accuracy and generalization capabilities.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct15_05">
             11:35-11:40, Paper ThCT15.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1701'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              On the Benefits of Hysteresis in Tendon Driven Continuum Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#172364" title="Click to go to the Author Index">
             Hanley, David
            </a>
           </td>
           <td class="r">
            University of Edinburgh
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#190490" title="Click to go to the Author Index">
             Alambeigi, Farshid
            </a>
           </td>
           <td class="r">
            University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#179020" title="Click to go to the Author Index">
             Khadem, Mohsen
            </a>
           </td>
           <td class="r">
            University of Edinburgh
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1701" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#soft_robot_materials_and_design" title="Click to go to the Keyword Index">
               Soft Robot Materials and Design
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#modeling__control__and_learning_for_soft_robots" title="Click to go to the Keyword Index">
               Modeling, Control, and Learning for Soft Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#surgical_robotics__steerable_catheters_needles" title="Click to go to the Keyword Index">
               Surgical Robotics: Steerable Catheters/Needles
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Hysteresis in the tendons driving continuum robots is frequently regarded as a nuisance and a problem that is best avoided. Some prior work seeks to ameliorate the effects of hysteresis through the selection of materials. Others propose models of hysteresis to compensate for their effects. In this work, we present an empirically validated model of hysteresis in tendon-driven continuum robots. We demonstrate that hysteresis contributes to the stability of these robots by mitigating undesirable tensions in robot's backbone. As a result, a model-based approach to hysteresis can be used not just for compensation of a nuisance, but to enhance the utility of continuum robots in safety critical applications such as medical robots.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct15_06">
             11:40-11:45, Paper ThCT15.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2652'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Automating Tension Calibration for Tendon-Driven Continuum Robots: A Low-Cost Approach towards Consistent Teleoperation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422202" title="Click to go to the Author Index">
             Lee, Kyum
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#351162" title="Click to go to the Author Index">
             Shentu, Chengnan
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#320375" title="Click to go to the Author Index">
             Pogue, Chloe
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#113447" title="Click to go to the Author Index">
             Burgner-Kahrs, Jessica
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2652" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#modeling__control__and_learning_for_soft_robots" title="Click to go to the Keyword Index">
               Modeling, Control, and Learning for Soft Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#soft_robot_applications" title="Click to go to the Keyword Index">
               Soft Robot Applications
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present a low-cost method to automate tension calibration for tendon-driven continuum robots (TDCRs), particularly those lacking tension sensing. Our method utilizes Hall effect sensors to localize the robot’s tip with respect to the one-dimensional trajectory it follows under individual tendon actuation. We propose two workflows for robots with and without a static model, making the method generalizable to other tendon-driven soft robots. We demonstrate our method’s ability to repeatably tension the tendons through associated tendon displacements. The calibration approach’s measured repeatability (±0.03 mm) is also benchmarked against manual calibration on a TDCR prototype, and its accuracy in achieving target tensions is assessed ((0.06±0.20) N). We further investigate how tension calibration impacts open-loop tracking accuracy, confirming the effectiveness of our method to enhance motion consistency in open-loop control and teleoperation.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct15_07">
             11:40-11:45, Paper ThCT15.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4253'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Neural Network-Based Framework for Fast and Smooth Posture Reconstruction of a Soft Continuum Arm
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#287312" title="Click to go to the Author Index">
             Wang, Tixian
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#309747" title="Click to go to the Author Index">
             Chang, Heng-Sheng
            </a>
           </td>
           <td class="r">
            University of Illinois Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#246746" title="Click to go to the Author Index">
             Kim, Seung Hyun
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424418" title="Click to go to the Author Index">
             Guo, Jiamiao
            </a>
           </td>
           <td class="r">
            University of Illinois Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#376484" title="Click to go to the Author Index">
             Akcal, M. Ugur
            </a>
           </td>
           <td class="r">
            University of Illinois Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#315543" title="Click to go to the Author Index">
             Walt, Benjamin
            </a>
           </td>
           <td class="r">
            University of Illinois Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422386" title="Click to go to the Author Index">
             Biskup, Darren
            </a>
           </td>
           <td class="r">
            Columbia University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#179083" title="Click to go to the Author Index">
             Halder, Udit
            </a>
           </td>
           <td class="r">
            University of South Florida
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#151228" title="Click to go to the Author Index">
             Krishnan, Girish
            </a>
           </td>
           <td class="r">
            University of Illinois Urbana Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#133090" title="Click to go to the Author Index">
             Chowdhary, Girish
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#227619" title="Click to go to the Author Index">
             Gazzola, Mattia
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#309751" title="Click to go to the Author Index">
             Mehta, Prashant
            </a>
           </td>
           <td class="r">
            University of Illinois Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4253" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#soft_robot_applications" title="Click to go to the Keyword Index">
               Soft Robot Applications
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#modeling__control__and_learning_for_soft_robots" title="Click to go to the Keyword Index">
               Modeling, Control, and Learning for Soft Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#software_hardware_integration_for_robot_systems" title="Click to go to the Keyword Index">
               Software-Hardware Integration for Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             A neural network-based framework is developed and experimentally demonstrated for the problem of estimating the shape of a soft continuum arm (SCA) from noisy measurements of the pose at a finite number of locations along the length of the arm. The neural network takes as input these measurements and produces as output a finite-dimensional approximation of the strain, which is further used to reconstruct the infinite-dimensional smooth posture. This problem is important for various soft robotic applications. It is challenging due to the flexible aspects that lead to the infinite-dimensional reconstruction problem for the continuous posture and strains. Because of this, past solutions to this problem are computationally intensive. The proposed fast smooth reconstruction method is shown to be five orders of magnitude faster while having comparable accuracy. The framework is evaluated on two testbeds: a simulated octopus muscular arm and a physical BR2 pneumatic soft manipulator.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct16">
             <b>
              ThCT16
             </b>
            </a>
           </td>
           <td class="r">
            404
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thct16" title="Click to go to the Program at a Glance">
             <b>
              Grasping 4
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#107599" title="Click to go to the Author Index">
             Chakraborty, Nilanjan
            </a>
           </td>
           <td class="r">
            Stony Brook University
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#102257" title="Click to go to the Author Index">
             Harada, Kensuke
            </a>
           </td>
           <td class="r">
            Osaka University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct16_01">
             11:15-11:20, Paper ThCT16.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1278'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              GraspSAM: When Segment Anything Model Meets Grasp Detection
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#313355" title="Click to go to the Author Index">
             Noh, Sangjun
            </a>
           </td>
           <td class="r">
            Gwangju Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#270250" title="Click to go to the Author Index">
             Kim, Jong-Won
            </a>
           </td>
           <td class="r">
            GIST(Gwangju Institute of Science and Technology)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#388604" title="Click to go to the Author Index">
             Nam, Dongwoo
            </a>
           </td>
           <td class="r">
            Gwangju Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#270223" title="Click to go to the Author Index">
             Back, Seunghyeok
            </a>
           </td>
           <td class="r">
            Korea Institute of Machinery &amp; Materials
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#313204" title="Click to go to the Author Index">
             Kang, Raeyoung
            </a>
           </td>
           <td class="r">
            Gwangju Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#100825" title="Click to go to the Author Index">
             Lee, Kyoobin
            </a>
           </td>
           <td class="r">
            Gwangju Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1278" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#grasping" title="Click to go to the Keyword Index">
               Grasping
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#transfer_learning" title="Click to go to the Keyword Index">
               Transfer Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Grasp detection requires flexibility to handle objects of various shapes without relying on prior object knowledge, while also offering intuitive, user-guided control. In this paper, we introduce GraspSAM, an innovative extension of the Segment Anything Model (SAM) designed for prompt-driven and category-agnostic grasp detection. Unlike previous methods, which are often limited by small-scale training data, GraspSAM leverages SAM’s large-scale training and prompt-based segmentation capabilities to efficiently support both target-object and category-agnostic grasping. By utilizing adapters, learnable token embeddings, and a lightweight modified decoder, GraspSAM requires minimal fine-tuning to integrate object segmentation and grasp prediction into a unified framework. Our model achieves state-of-the-art (SOTA) performance across multiple datasets, including Jacquard, Grasp-Anything, and Grasp-Anything++. Extensive experiments demonstrate GraspSAM’s flexibility in handling different types of prompts (such as points, boxes, and language), highlighting its robustness and effectiveness in real-world robotic applications. Robot demonstrations, additional results, and code can be found at https://gistailab.github.io/GraspSAM/.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct16_02">
             11:20-11:25, Paper ThCT16.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1324'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Dexterous Ungrasping Manipulation in Three Dimensions
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#302845" title="Click to go to the Author Index">
             Kang, Taewoong
            </a>
           </td>
           <td class="r">
            Pusan National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#302865" title="Click to go to the Author Index">
             Kim, Joonyoung
            </a>
           </td>
           <td class="r">
            Pusan National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#413930" title="Click to go to the Author Index">
             Oh, Seung Hwa
            </a>
           </td>
           <td class="r">
            Pusan National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419830" title="Click to go to the Author Index">
             Lim, WooSung
            </a>
           </td>
           <td class="r">
            Pusan National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419831" title="Click to go to the Author Index">
             Lee, Junwoo
            </a>
           </td>
           <td class="r">
            Pusan National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#133360" title="Click to go to the Author Index">
             Yi, Seung-Joon
            </a>
           </td>
           <td class="r">
            Pusan National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#150059" title="Click to go to the Author Index">
             Seo, Jungwon
            </a>
           </td>
           <td class="r">
            Pusan National University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1324" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#dexterous_manipulation" title="Click to go to the Keyword Index">
               Dexterous Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#grasping" title="Click to go to the Keyword Index">
               Grasping
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#assembly" title="Click to go to the Keyword Index">
               Assembly
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This study focuses on the robotic capability of ungrasping, or releasing, an object in a grasp from the gripper to the robot’s environment. The presented technique enables the delicate release of a grasped object using non-static contacts, allowing for rolling and/or sliding. This dexterous manipulation capability is particularly relevant when ungrasping thin or slender objects, as will be demonstrated with real examples. We initially discuss the establishment of three-dimensional stability during ungrasping manipulation, ensuring robustness. Subsequently, we present a planning and control solution for three-dimensional ungrasping, building upon our previous planar version. A series of experiments across various test scenarios, ranging from precision placement to puzzle tiling, showcase the viability and effectiveness of our approach.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct16_03">
             11:25-11:30, Paper ThCT16.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1523'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              RTAGrasp: Learning Task-Oriented Grasping from Human Videos Via Retrieval, Transfer, and Alignment
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417543" title="Click to go to the Author Index">
             Dong, Wenlong
            </a>
           </td>
           <td class="r">
            Southern University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#334035" title="Click to go to the Author Index">
             Huang, Dehao
            </a>
           </td>
           <td class="r">
            Southern University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#414675" title="Click to go to the Author Index">
             Liu, Jiangshan
            </a>
           </td>
           <td class="r">
            Southern University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#257410" title="Click to go to the Author Index">
             Tang, Chao
            </a>
           </td>
           <td class="r">
            Southern University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#100155" title="Click to go to the Author Index">
             Zhang, Hong
            </a>
           </td>
           <td class="r">
            SUSTech
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1523" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#grasping" title="Click to go to the Keyword Index">
               Grasping
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Task-oriented grasping (TOG) is crucial for robots to accomplish manipulation tasks, requiring the determination of TOG positions and directions. Existing methods either rely on costly manual TOG annotations or only extract coarse grasping positions or regions from human demonstrations, limiting their practicality in real-world applications. To address these limitations, we introduce RTAGrasp, a Retrieval, Transfer, and Alignment framework inspired by human grasping strategies. Specifically, our approach first effortlessly constructs a robot memory from human grasping demonstration videos, extracting both TOG position and direction constraints. Then, given a task instruction and a visual observation of the target object, RTAGrasp retrieves the most similar human grasping experience from its memory and leverages semantic matching capabilities of vision foundation models to transfer the TOG constraints to the target object in a training-free manner. Finally, RTAGrasp aligns the transferred TOG constraints with the robot's action for execution. Evaluations on the public TOG benchmark, TaskGrasp dataset, show the competitive performance of RTAGrasp on both seen and unseen object categories compared to existing baseline methods. Real-world experiments further validate its effectiveness on a robotic arm. Our code, appendix, and video are available at https://sites.google.com/view/rtagrasp/home.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct16_04">
             11:30-11:35, Paper ThCT16.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2103'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              You Only Estimate Once: Unified, One-Stage, Real-Time Category-Level Articulated Object 6D Pose Estimation for Robotic Grasping
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#379167" title="Click to go to the Author Index">
             Huang, Jingshun
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#296392" title="Click to go to the Author Index">
             Lin, Haitao
            </a>
           </td>
           <td class="r">
            Tencent
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#374143" title="Click to go to the Author Index">
             Wang, Tianyu
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#308158" title="Click to go to the Author Index">
             Fu, Yanwei
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#270573" title="Click to go to the Author Index">
             Jiang, Yu-Gang
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#256022" title="Click to go to the Author Index">
             Xue, Xiangyang
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2103" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper addresses the problem of category-level pose estimation for articulated objects in robotic manipulation tasks. Recent works have shown promising results in estimating part pose and size at the category level. However, these approaches primarily follow a complex multi-stage pipeline that first segments part instances in the point cloud and then estimates the Normalized Part Coordinate Space (NPCS) representation for 6D poses. These approaches suffer from high computational costs and low performance in real-time robotic tasks. To address these limitations, we propose YOEO, a single-stage method that simultaneously outputs instance segmentation and NPCS representations in an end-to-end manner. We use a unified network to generate point-wise semantic labels and centroid offsets, allowing points from the same part instance to vote for the same centroid. We further utilize a clustering algorithm to distinguish points based on their estimated centroid distances. Finally, we first separate the NPCS region of each instance. Then, we align the separated regions with the real point cloud to recover the final pose and size. Experimental results on the GAPart dataset demonstrate the pose estimation capabilities of our proposed single-shot method. We also deploy our synthetically-trained model in a real-world setting, providing real-time visual feedback at 200Hz, enabling a physical Kinova robot to interact with unseen articulated objects. This showcases the utility and effectiveness of our proposed method.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct16_05">
             11:35-11:40, Paper ThCT16.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3776'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Point Cloud Decomposition for Task-Oriented Grasping
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#339569" title="Click to go to the Author Index">
             Phi, Khiem
            </a>
           </td>
           <td class="r">
            Stony Brook University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#269340" title="Click to go to the Author Index">
             Patankar, Aditya
            </a>
           </td>
           <td class="r">
            Stony Brook University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#312636" title="Click to go to the Author Index">
             Mahalingam, Dasharadhan
            </a>
           </td>
           <td class="r">
            Stony Brook University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#107599" title="Click to go to the Author Index">
             Chakraborty, Nilanjan
            </a>
           </td>
           <td class="r">
            Stony Brook University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#339004" title="Click to go to the Author Index">
             Ramakrishnan, Iv
            </a>
           </td>
           <td class="r">
            Stony Brook University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3776" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#grasping" title="Click to go to the Keyword Index">
               Grasping
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Accurate localization of graspable regions within a single object point cloud is critical to enable task-based robot grasps. State-of-the-art task-based robot grasp synthesis methods fits over-approximated 3D bounding boxes that fails to isolate graspable regions even if they exist. While deep learning or geometrical shape decomposition methods can offer improved approximations, they lack guarantees for the graspability of segmented regions, require prior knowledge of the object, and/or demand large annotated datasets for fine-tuning. In this paper, we overcome these limitations to introduce ITSI. ITSI is a complete, task-oriented grasp synthesis approach that functions independently of object-specific knowledge. ITSI (Iterative Slicing) effectively segments multiple graspable regions that conform to the constraints of robot grippers thereby enabling compatibility with any object a robot seeks to grasp and any robot gripper size. Our extensive real-world and simulation experiments on diverse object datasets demonstrates how ITSI dramatically increases the number of discoverable robot grasps by up to 44% when compared to the state-of-the-art. We also expand ITSI's capabilities beyond task-based robot grasp synthesis to highlight its performance in human affordance segmentation outperforming fully supervised deep learning methods by 1%.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct16_06">
             11:40-11:45, Paper ThCT16.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4044'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Adaptive Grasping of Moving Objects in Dense Clutter Via Global-To-Local Detection and Static-To-Dynamic Planning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#310228" title="Click to go to the Author Index">
             Chen, Hao
            </a>
           </td>
           <td class="r">
            Osaka University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#232912" title="Click to go to the Author Index">
             Kiyokawa, Takuya
            </a>
           </td>
           <td class="r">
            Osaka University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#109858" title="Click to go to the Author Index">
             Wan, Weiwei
            </a>
           </td>
           <td class="r">
            Osaka University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#102257" title="Click to go to the Author Index">
             Harada, Kensuke
            </a>
           </td>
           <td class="r">
            Osaka University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4044" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#grasping" title="Click to go to the Keyword Index">
               Grasping
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#dexterous_manipulation" title="Click to go to the Keyword Index">
               Dexterous Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#planning_under_uncertainty" title="Click to go to the Keyword Index">
               Planning under Uncertainty
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robotic grasping is facing a variety of real-world uncertainties caused by non-static object states, unknown object properties, and cluttered object arrangements. The difficulty of grasping increases with the presence of more uncertainties, where commonly used learning-based approaches struggle to perform stably across varying conditions. In this study, we extend the idea of using similarity matching to tackle the challenge of grasping novel objects that are simultaneously in motion and densely cluttered where multiple uncertainties coexist with a single in-hand camera. We achieve this difficult task by shifting visual detection from global to local states and operating grasp planning from static to dynamic states. We propose several methods and algorithms to optimize planning efficiency and accuracy. Our system is adaptive to different object types, arrangements and movement speeds without additional training, as proved by our real-world experiments.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct17">
             <b>
              ThCT17
             </b>
            </a>
           </td>
           <td class="r">
            405
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thct17" title="Click to go to the Program at a Glance">
             <b>
              Localization 6
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#104304" title="Click to go to the Author Index">
             Scherer, Sebastian
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#150020" title="Click to go to the Author Index">
             Costante, Gabriele
            </a>
           </td>
           <td class="r">
            University of Perugia
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct17_01">
             11:15-11:20, Paper ThCT17.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1142'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              UASTHN: Uncertainty-Aware Deep Homography Estimation for UAV Satellite-Thermal Geo-Localization
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#268227" title="Click to go to the Author Index">
             Xiao, Jiuhong
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#142369" title="Click to go to the Author Index">
             Loianno, Giuseppe
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1142" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Geo-localization is an essential component of Unmanned Aerial Vehicle (UAV) navigation systems to ensure precise absolute self-localization in outdoor environments. To address the challenges of GPS signal interruptions or low illumination, Thermal Geo-localization (TG) employs aerial thermal imagery to align with reference satellite maps to accurately determine the UAV's location. However, existing TG methods lack uncertainty measurement in their outputs, compromising system robustness in the presence of textureless or corrupted thermal images, self-similar or outdated satellite maps, geometric noises, or thermal images exceeding satellite maps. To overcome these limitations, this paper presents UASTHN, a novel approach for Uncertainty Estimation (UE) in Deep Homography Estimation (DHE) tasks for TG applications. Specifically, we introduce a novel Crop-based Test-Time Augmentation (CropTTA) strategy, which leverages the homography consensus of cropped image views to effectively measure data uncertainty. This approach is complemented by Deep Ensembles (DE) employed for model uncertainty, offering comparable performance with improved efficiency and seamless integration with any DHE model. Extensive experiments across multiple DHE models demonstrate the effectiveness and efficiency of CropTTA in TG applications. Analysis of detected failure cases underscores the improved reliability of CropTTA under challenging conditions. Finally, we demonstrate the capability of combining CropTTA and DE for a comprehensive assessment of both data and model uncertainty. Our research provides profound insights into the broader intersection of localization and uncertainty estimation. The code and models are publicly available.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct17_02">
             11:20-11:25, Paper ThCT17.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3794'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Enhancing Feature Tracking Reliability for Visual Navigation Using Real-Time Safety Filter
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#269633" title="Click to go to the Author Index">
             Kim, Dabin
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#257191" title="Click to go to the Author Index">
             Jang, Inkyu
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#290011" title="Click to go to the Author Index">
             Han, Youngsoo
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#338500" title="Click to go to the Author Index">
             Hwang, Sunwoo
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#103663" title="Click to go to the Author Index">
             Kim, H. Jin
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3794" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#sensor_based_control" title="Click to go to the Keyword Index">
               Sensor-based Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reactive_and_sensor_based_planning" title="Click to go to the Keyword Index">
               Reactive and Sensor-Based Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#view_planning_for_slam" title="Click to go to the Keyword Index">
               View Planning for SLAM
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Vision sensors are extensively used for localizing a robot's pose, particularly in environments where global localization tools such as GPS or motion capture systems are unavailable. In many visual navigation systems, localization is achieved by detecting and tracking visual features or landmarks, which provide information about the sensor's relative pose. For reliable feature tracking and accurate pose estimation, it is crucial to maintain visibility of a sufficient number of features. This requirement can sometimes conflict with the robot's overall task objective. In this paper, we approach it as a constrained control problem. By leveraging the invariance properties of visibility constraints within the robot's kinematic model, we propose a real-time safety filter based on quadratic programming. This filter takes a reference velocity command as input and produces a modified velocity that minimally deviates from the reference while ensuring the information score from the currently visible features remains above a user-specified threshold. Numerical simulations demonstrate that the proposed safety filter preserves the invariance condition and ensures the visibility of more features than the required minimum. We also validated its real-world performance by integrating it into a visual simultaneous localization and mapping (SLAM) algorithm, where it maintained high estimation quality in challenging environments, outperforming a simple tracking controller.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct17_03">
             11:25-11:30, Paper ThCT17.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3864'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SuperLoc: The Key to Robust LiDAR-Inertial Localization Lies in Predicting Alignment Risks
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#226965" title="Click to go to the Author Index">
             Zhao, Shibo
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#379448" title="Click to go to the Author Index">
             Zhu, Honghao
            </a>
           </td>
           <td class="r">
            CMU
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425955" title="Click to go to the Author Index">
             Gao, Yuanjun
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#350838" title="Click to go to the Author Index">
             Kim, Beomsoo
            </a>
           </td>
           <td class="r">
            Hanyang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#278068" title="Click to go to the Author Index">
             Qiu, Yuheng
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#111288" title="Click to go to the Author Index">
             Johnson, Aaron M.
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104304" title="Click to go to the Author Index">
             Scherer, Sebastian
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3864" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Map-based LiDAR localization, while widely used in autonomous systems, faces significant challenges in degraded environments due to the lack of distinct geometric features. This paper introduces SuperLoc, a robust LiDAR localization package that addresses key limitations in existing methods. SuperLoc features a novel predictive alignment risk assessment technique, enabling early detection and mitigation of potential failures before optimization. This approach significantly improves performance in challenging scenarios such as corridors, tunnels, and caves. Unlike existing degeneracy mitigation algorithms that rely on post-optimization analysis and heuristic thresholds, SuperLoc evaluates the localizability of raw sensor measurements. Experimental results demonstrate significant performance improvements over state-of-the-art methods across various degraded environments. Our approach achieves a 49.7% increase in accuracy and exhibits the highest robustness. To facilitate further research, we release our implementation along with datasets from eight challenging scenarios.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct17_04">
             11:30-11:35, Paper ThCT17.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4235'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Active Illumination for Visual Ego-Motion Estimation in the Dark
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#172497" title="Click to go to the Author Index">
             Crocetti, Francesco
            </a>
           </td>
           <td class="r">
            University of Perugia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#275357" title="Click to go to the Author Index">
             Dionigi, Alberto
            </a>
           </td>
           <td class="r">
            University of Perugia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#307850" title="Click to go to the Author Index">
             Brilli, Raffaele
            </a>
           </td>
           <td class="r">
            University of Perugia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#150020" title="Click to go to the Author Index">
             Costante, Gabriele
            </a>
           </td>
           <td class="r">
            University of Perugia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#119696" title="Click to go to the Author Index">
             Valigi, Paolo
            </a>
           </td>
           <td class="r">
            Universita' Di Perugia
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4235" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#perception_action_coupling" title="Click to go to the Keyword Index">
               Perception-Action Coupling
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Visual Odometry (VO) and Visual SLAM (V-SLAM) systems often struggle in low-light and dark environments due to the lack of robust visual features. In this paper, we propose a novel active illumination framework to enhance the performance of VO and V-SLAM algorithms in these challenging conditions. The developed approach dynamically controls a moving light source to illuminate highly textured areas, thereby improving feature extraction and tracking. Specifically, a detector block, which incorporates a deep learning-based enhancing network, identifies regions with relevant features. Then, a pan-tilt controller is responsible for guiding the light beam toward these areas, so that to provide information-rich images to the ego-motion estimation algorithm. Experimental results on a real robotic platform demonstrate the effectiveness of the proposed method, showing a reduction in the pose estimation error up to 75% with respect to a traditional fixed lighting technique.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct17_05">
             11:35-11:40, Paper ThCT17.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4793'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Intensity Triangle Descriptor Constructed from High-Resolution Spinning LiDAR Intensity Image for Loop Closure Detection
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#334760" title="Click to go to the Author Index">
             Zhang, Yanfeng
            </a>
           </td>
           <td class="r">
            Institute of Automation, Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#336633" title="Click to go to the Author Index">
             Tian, Yunong
            </a>
           </td>
           <td class="r">
            Institute of Automation, Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#165588" title="Click to go to the Author Index">
             Yang, Guodong
            </a>
           </td>
           <td class="r">
            Institute of Automation, Chinese Academy of Sciences; Beijing Zh
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#336632" title="Click to go to the Author Index">
             Li, Zhishuo
            </a>
           </td>
           <td class="r">
            Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#318514" title="Click to go to the Author Index">
             Luo, Mingrui
            </a>
           </td>
           <td class="r">
            Institute of Automation, Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#165628" title="Click to go to the Author Index">
             Li, En
            </a>
           </td>
           <td class="r">
            Institute of Automation, Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#188605" title="Click to go to the Author Index">
             Jing, Fengshui
            </a>
           </td>
           <td class="r">
            Institute of Automation, CAS
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4793" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#recognition" title="Click to go to the Keyword Index">
               Recognition
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             LiDAR-based loop closure detection is a crucial part of realizing robust SLAM algorithms for intelligent vehicles with LiDAR sensors. Existing methods often reduce the keypoint dimension to encode the global descriptor, which sacrifices the freedom of loop detection and correction. Based on the 6-DOF rigid transformation property of spatial triangles, we propose an algorithm for extracting and describing 3D keypoints from high-resolution spinning LiDAR intensity images to encode triangle descriptors, termed intensity triangle descriptor (ITD). In comparison to the direct extraction of keypoints from the point cloud, the use of image-derived feature points provides additional photometric texture information and better handles uneven spatial density of the point cloud, which is advantageous in unstructured and geometrically degraded scenes. To enhance the stability of keypoints, the spatial positions of multi-frame image feature points are registered to a keyframe by an odometer for voxel downsampling and non-maximum suppression, with the objective of reducing unstable feature points. For high discrimination, the neighbor image patches of each vertex (keypoint) are aggregated to estimate a Gaussian mixture model (GMM) as the keypoint signature. An efficient two-stage loop closure detection method is then proposed for ITD, consisting of candidate retrieval based on triangle side lengths and vertex GMMs, followed by geometric verification of matched descriptor pairs. The effectiveness of the proposed method is evaluated on the STheReO, FusionPortable, and our self-collected datasets.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct17_06">
             11:40-11:45, Paper ThCT17.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('5037'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              IBTC: An Image-Assisting Binary and Triangle Combined Descriptor for Place Recognition by Fusing LiDAR and Camera Measurements
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#338792" title="Click to go to the Author Index">
             Zou, Zuhao
            </a>
           </td>
           <td class="r">
            HongKong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#288528" title="Click to go to the Author Index">
             Zheng, Chunran
            </a>
           </td>
           <td class="r">
            The University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#296567" title="Click to go to the Author Index">
             Yuan, Chongjian
            </a>
           </td>
           <td class="r">
            The University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#352741" title="Click to go to the Author Index">
             Zhou, Shunbo
            </a>
           </td>
           <td class="r">
            Huawei
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#218816" title="Click to go to the Author Index">
             Xue, Kaiwen
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong, Shenzhen
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#204381" title="Click to go to the Author Index">
             Zhang, Fu
            </a>
           </td>
           <td class="r">
            University of Hong Kong
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab5037" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this work, we introduce a novel multimodal descriptor, the image-assisting binary, and triangle combined (iBTC) descriptor, which fuses LiDAR (Light Detection and Ranging) and camera measurements for 3D place recognition. The inherent invariance of a triangle to rigid transformations inspires us to design triangle-based descriptors. We first extract distinct 3D key points from both LiDAR and camera measurements and organize them into triplets to form triangles. By utilizing the lengths of the sides of these triangles, we can create triangle descriptors, enabling the rapid retrieval of similar triangles from a database. By encoding the geometric and visual details at the triangle vertices into binary descriptors, we augment the triangle descriptors with richer local information. This enrichment process empowers our descriptors to reject mismatched triangle pairs. Consequently, the remaining matched triangle pairs yield accurate loop closure place indices and relative poses.
             <p>
              In our experiments, we conduct a thorough comparison of our proposed method with several SOTA methods across public and self-collected datasets. The results demonstrate that our method exhibits superior performance in place recognition and overcomes the limitations associated with unimodal methods like BTC, RING++, ORB-DBoW2, and NetVLAD. Additionally, we performed a time cost benchmark experiment and the result indicates that our method’s time consumption is reasonable, compared with baseline methods. A demonstration video is available on https://www.youtube.com/watch?v=fe1Q0eR2fWk.
             </p>
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct18">
             <b>
              ThCT18
             </b>
            </a>
           </td>
           <td class="r">
            406
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thct18" title="Click to go to the Program at a Glance">
             <b>
              Planning under Uncertainty 2
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#154676" title="Click to go to the Author Index">
             Gammell, Jonathan
            </a>
           </td>
           <td class="r">
            Queen's University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct18_01">
             11:15-11:20, Paper ThCT18.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('270'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Belief Roadmaps with Uncertain Landmark Evanescence
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#382344" title="Click to go to the Author Index">
             Fuentes, Erick
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#217509" title="Click to go to the Author Index">
             Strader, Jared
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#333068" title="Click to go to the Author Index">
             Fahnestock, Ethan
            </a>
           </td>
           <td class="r">
            MIT
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101682" title="Click to go to the Author Index">
             Roy, Nicholas
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab270" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#planning_under_uncertainty" title="Click to go to the Keyword Index">
               Planning under Uncertainty
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We would like a robot to navigate to a goal location while minimizing state uncertainty. To aid the robot in this endeavor, maps provide a prior belief over the location of objects and regions of interest. To localize itself within the map, a robot identifies mapped landmarks using its sensors. However, as the time between map creation and robot deployment increases, portions of the map can become stale, and landmarks, once believed to be permanent, may disappear. We refer to the propensity of a landmark to disappear as landmark evanescence. Reasoning about landmark evanescence during path planning, and the associated impact on localization accuracy, requires analyzing the presence or absence of each landmark, leading to an exponential number of possible outcomes of a given motion plan. To address this complexity, we develop BRULE, an extension of the Belief Roadmap. During planning, we replace the belief over future robot poses with a Gaussian mixture which is able to capture the effects of landmark evanescence. Furthermore, we show that belief updates can be made efficient, and that maintaining a random subset of mixture components is sufficient to find high quality solutions. We demonstrate performance in simulated and real-world experiments. Software is available at https://bit.ly/BRULE.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct18_02">
             11:20-11:25, Paper ThCT18.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('363'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Safe and Efficient Path Planning under Uncertainty Via Deep Collision Probability Fields
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#404802" title="Click to go to the Author Index">
             Herrmann, Felix
            </a>
           </td>
           <td class="r">
            Technische Universität Darmstadt
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#404801" title="Click to go to the Author Index">
             Zach, Sebastian Bernhard
            </a>
           </td>
           <td class="r">
            Technische Universität Darmstadt
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#180432" title="Click to go to the Author Index">
             Banfi, Jacopo
            </a>
           </td>
           <td class="r">
            Amazon
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104326" title="Click to go to the Author Index">
             Peters, Jan
            </a>
           </td>
           <td class="r">
            Technische Universität Darmstadt
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#168538" title="Click to go to the Author Index">
             Chalvatzaki, Georgia
            </a>
           </td>
           <td class="r">
            Technische Universität Darmstadt
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#183345" title="Click to go to the Author Index">
             Tateo, Davide
            </a>
           </td>
           <td class="r">
            Technische Universität Darmstadt
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab363" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#planning_under_uncertainty" title="Click to go to the Keyword Index">
               Planning under Uncertainty
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Estimating collision probabilities between robots and environmental obstacles or other moving agents is crucial to ensure safety during path planning. This is an important building block of modern planning algorithms in many application scenarios such as autonomous driving, where noisy sensors perceive obstacles. While many approaches exist, they either provide too conservative estimates of the collision probabilities or are computationally intensive due to their sampling-based nature. To deal with these issues, we introduce Deep Collision Probability Fields, a neural-based approach for computing collision probabilities of arbitrary objects with arbitrary unimodal uncertainty distributions.
             <p>
              Our approach relegates the computationally intensive estimation of collision probabilities via sampling at the training step, allowing for
              <p>
               fast neural network inference of the constraints during planning. In extensive experiments, we show that Deep Collision Probability Fields can produce reasonably accurate collision probabilities (up to 10^{-3}) for planning and that our approach can be easily plugged into standard path planning approaches to plan safe paths on 2-D maps containing uncertain static and dynamic obstacles. Additional material,
               <p>
                code, and videos are available at https://sites.google.com/view/ral-dcpf.
               </p>
              </p>
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct18_03">
             11:25-11:30, Paper ThCT18.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('558'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Safe POMDP Online Planning among Dynamic Agents Via Adaptive Conformal Prediction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#189133" title="Click to go to the Author Index">
             Sheng, Shili
            </a>
           </td>
           <td class="r">
            University of Virginia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#250827" title="Click to go to the Author Index">
             Yu, Pian
            </a>
           </td>
           <td class="r">
            University College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#172087" title="Click to go to the Author Index">
             Parker, David
            </a>
           </td>
           <td class="r">
            University of Oxford
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#205799" title="Click to go to the Author Index">
             Kwiatkowska, Marta
            </a>
           </td>
           <td class="r">
            University of Oxford
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#192462" title="Click to go to the Author Index">
             Feng, Lu
            </a>
           </td>
           <td class="r">
            University of Virginia
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab558" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#formal_methods_in_robotics_and_automation" title="Click to go to the Keyword Index">
               Formal Methods in Robotics and Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#planning_under_uncertainty" title="Click to go to the Keyword Index">
               Planning under Uncertainty
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#collision_avoidance" title="Click to go to the Keyword Index">
               Collision Avoidance
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Online planning for partially observable Markov decision processes (POMDPs) provides efficient techniques for robot decision-making under uncertainty. However, existing methods fall short of preventing safety violations in dynamic environments. This work presents a novel safe POMDP online planning approach that maximizes expected returns while providing probabilistic safety guarantees amidst environments populated by multiple dynamic agents. Our approach utilizes data-driven trajectory prediction models of dynamic agents and applies Adaptive Conformal Prediction (ACP) to quantify the uncertainties in these predictions. Leveraging the obtained ACP-based trajectory predictions, our approach constructs safety shields on-the-fly to prevent unsafe actions within POMDP online planning. Through experimental evaluation in various dynamic environments using real-world pedestrian trajectory data, the proposed approach has been shown to effectively maintain probabilistic safety guarantees while accommodating up to hundreds of dynamic agents.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct18_04">
             11:30-11:35, Paper ThCT18.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2586'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Rao-Blackwellized POMDP Planning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419015" title="Click to go to the Author Index">
             Lee, Jiho
            </a>
           </td>
           <td class="r">
            University of Colorado Boulder
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#111113" title="Click to go to the Author Index">
             Ahmed, Nisar
            </a>
           </td>
           <td class="r">
            University of Colorado Boulder
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#164029" title="Click to go to the Author Index">
             Wray, Kyle
            </a>
           </td>
           <td class="r">
            N/a
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#191745" title="Click to go to the Author Index">
             Sunberg, Zachary
            </a>
           </td>
           <td class="r">
            University of Colorado
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2586" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#planning_under_uncertainty" title="Click to go to the Keyword Index">
               Planning under Uncertainty
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#probabilistic_inference" title="Click to go to the Keyword Index">
               Probabilistic Inference
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Partially Observable Markov Decision Processes (POMDPs) provide a structured framework for decision-making under uncertainty, but their application requires efficient belief updates. Sequential Importance Resampling Particle Filters (SIRPF), also known as Bootstrap Particle Filters, are commonly used as belief updaters in large approximate POMDP solvers, but they face challenges such as particle deprivation and high computational costs as the system's state dimension grows. To address these issues, this study introduces Rao-Blackwellized POMDP (RB-POMDP) approximate solvers and outlines generic methods to apply Rao-Blackwellization in both belief updates and online planning. We compare the performance of SIRPF and Rao-Blackwellized Particle Filters (RBPF) in a simulated localization problem where an agent navigates toward a target in a GPS-denied environment using POMCPOW and RB-POMCPOW planners. Our results not only confirm that RBPFs maintain efficient belief approximations over time with fewer particles, but, more surprisingly, RBPFs combined with quadrature-based integration improves planning quality significantly compared to SIRPF-based planning under the same computational limits.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct18_05">
             11:35-11:40, Paper ThCT18.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2982'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Nearest-Neighbourless Asymptotically Optimal Motion Planning with Fully Connected Informed Trees (FCIT*)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424852" title="Click to go to the Author Index">
             Wilson, Tyler S.
            </a>
           </td>
           <td class="r">
            Queen's University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#193169" title="Click to go to the Author Index">
             Thomason, Wil
            </a>
           </td>
           <td class="r">
            The AI Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#180041" title="Click to go to the Author Index">
             Kingston, Zachary
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#102678" title="Click to go to the Author Index">
             Kavraki, Lydia
            </a>
           </td>
           <td class="r">
            Rice University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#154676" title="Click to go to the Author Index">
             Gammell, Jonathan
            </a>
           </td>
           <td class="r">
            Queen's University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2982" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#manipulation_planning" title="Click to go to the Keyword Index">
               Manipulation Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#constrained_motion_planning" title="Click to go to the Keyword Index">
               Constrained Motion Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Improving the performance of motion planning algorithms for high-degree-of-freedom robots usually requires reducing the cost or frequency of computationally expensive operations. Traditionally, and especially for asymptotically optimal sampling-based motion planners, the most expensive operations are local motion validation and querying the nearest neighbours of a configuration. Recent advances have significantly reduced the cost of motion validation by using single instruction/multiple data (SIMD) parallelism to improve solution times for satisficing motion planning problems. These advances have not yet been applied to asymptotically optimal motion planning. This paper presents Fully Connected Informed Trees (FCIT*), the first fully connected, informed, anytime almost-surely asymptotically optimal (ASAO) algorithm. FCIT* exploits the radically reduced cost of edge evaluation via SIMD parallelism to build and search fully connected graphs. This removes the need for nearest-neighbours structures, which are a dominant cost for many sampling-based motion planners, and allows it to find initial solutions faster than state-of-the-art ASAO (VAMP, OMPL) and satisficing (OMPL) algorithms on the MotionBenchMaker dataset while converging towards optimal plans in an anytime manner.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct18_06">
             11:40-11:45, Paper ThCT18.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4004'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Efficient Path Planning in Complex Environments with Trust Region Continuous Belief Tree Search
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#355363" title="Click to go to the Author Index">
             Nunez, Andre Julio
            </a>
           </td>
           <td class="r">
            University of Technology Sydney
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#217476" title="Click to go to the Author Index">
             Kong, Felix Honglim
            </a>
           </td>
           <td class="r">
            The University of Technology Sydney
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#230898" title="Click to go to the Author Index">
             González-Cantos, Alberto
            </a>
           </td>
           <td class="r">
            Navantia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104600" title="Click to go to the Author Index">
             Fitch, Robert
            </a>
           </td>
           <td class="r">
            University of Technology Sydney
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4004" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#constrained_motion_planning" title="Click to go to the Keyword Index">
               Constrained Motion Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#marine_robotics" title="Click to go to the Keyword Index">
               Marine Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Real-world applications of path planning must contend with complicated constraint and objective functions imposed by the surrounding operational and regulatory environment. Traditional methods such as PRM* and RRT* have asymptotic guarantees, but often struggle in practice with complex black-box objective/constraint functions, especially in compute-limited situations. Continuous Belief Tree Search (CBTS) addresses these limitations by maintaining local estimates of the objective function in order to sample new nodes from continuous space, often giving high-quality solutions more quickly. However, CBTS requires careful tuning of a control duration parameter, which introduces a tradeoff between compute time and path cost/feasibility. In environments with complex costs and constraints, there may be no single control duration that gives good paths in short compute time. This paper proposes Trust Region CBTS (TR-CBTS), an extension of CBTS with an adaptive control duration parameter inspired by trust region methods. TR-CBTS adjusts control duration based on information from recently sampled candidate nodes, allowing longer control duration where possible to speed up compute time, and shortening control duration when precise navigation in environments with complex, unknown constraint and objective functions. We show TR-CBTS outperforms existing comparable planners for a realistic robotic path planning application in autonomous ship routing.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct19">
             <b>
              ThCT19
             </b>
            </a>
           </td>
           <td class="r">
            407
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thct19" title="Click to go to the Program at a Glance">
             <b>
              Active Perception
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#133879" title="Click to go to the Author Index">
             Bezzo, Nicola
            </a>
           </td>
           <td class="r">
            University of Virginia
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#192998" title="Click to go to the Author Index">
             Lopez, Brett
            </a>
           </td>
           <td class="r">
            University of California, Los Angeles
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct19_01">
             11:15-11:20, Paper ThCT19.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1639'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              PRIMER: Perception-Aware Robust Learning-Based Multiagent Trajectory Planner
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#339128" title="Click to go to the Author Index">
             Kondo, Kota
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#378318" title="Click to go to the Author Index">
             Tewari, Claudius Taroon
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#205572" title="Click to go to the Author Index">
             Tagliabue, Andrea
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#238224" title="Click to go to the Author Index">
             Tordesillas Torres, Jesus
            </a>
           </td>
           <td class="r">
            ICAI School of Engineering, Comillas Pontifical University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#241322" title="Click to go to the Author Index">
             Lusk, Parker C.
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#354142" title="Click to go to the Author Index">
             Peterson, Mason B.
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104610" title="Click to go to the Author Index">
             How, Jonathan
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1639" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#path_planning_for_multiple_mobile_robots_or_agents" title="Click to go to the Keyword Index">
               Path Planning for Multiple Mobile Robots or Agents
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In decentralized multiagent trajectory planners, agents need to communicate and exchange their positions to generate collision-free trajectories. However, due to localization errors/uncertainties, trajectory deconfliction can fail even if trajectories are perfectly shared between agents. To address this issue, we first present PARM and PARM*, perception-aware, decentralized, asynchronous multiagent trajectory planners that enable a team of agents to navigate uncertain environments while deconflicting trajectories and avoiding obstacles using perception information. PARM* differs from PARM as it is less conservative, using more computation to find closer-to-optimal solutions. While these methods achieve state-of-the-art performance, they suffer from high computational costs as they need to solve large optimization problems onboard, making it difficult for agents to replan at high rates. To overcome this challenge, we present our second key contribution, PRIMER, a learning-based planner trained with imitation learning (IL) using PARM* as the expert demonstrator. PRIMER leverages the low computational requirements at deployment of neural networks and achieves a computation speed up to 5614 times faster than optimization-based approaches.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct19_02">
             11:20-11:25, Paper ThCT19.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2570'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              HGS-Planner: Hierarchical Planning Framework for Active Scene Reconstruction Using 3D Gaussian Splatting
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422062" title="Click to go to the Author Index">
             Xu, Zijun
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#302454" title="Click to go to the Author Index">
             Jin, Rui
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#367344" title="Click to go to the Author Index">
             Wu, Ke
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423861" title="Click to go to the Author Index">
             Zhao, Yi
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#290428" title="Click to go to the Author Index">
             Zhang, Zhiwei
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#339746" title="Click to go to the Author Index">
             Zhao, Jieru
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#200893" title="Click to go to the Author Index">
             Gao, Fei
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#322500" title="Click to go to the Author Index">
             Gan, Zhongxue
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#217534" title="Click to go to the Author Index">
             Ding, Wenchao
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2570" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#view_planning_for_slam" title="Click to go to the Keyword Index">
               View Planning for SLAM
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In complex missions such as search and rescue, robots must make intelligent decisions in unknown environments, relying on their ability to perceive and understand their surroundings. High-quality and real-time reconstruction enhances situational awareness and is crucial for intelligent robotics. Traditional methods often struggle with poor scene representation or are too slow for real-time use. Inspired by the efficacy of 3D Gaussian Splatting (3DGS), we propose a hierarchical planning framework for fast and high-fidelity active reconstruction. Our method evaluates completion and quality gain to adaptively guide reconstruction, integrating global and local planning for efficiency. Experiments in simulated and real-world environments show our approach outperforms existing real-time methods.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct19_03">
             11:25-11:30, Paper ThCT19.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3320'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              An Active Perception Game for Robust Information Gathering
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#395947" title="Click to go to the Author Index">
             He, Siming
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#283123" title="Click to go to the Author Index">
             Tao, Yuezhan
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#239282" title="Click to go to the Author Index">
             Spasojevic, Igor
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104342" title="Click to go to the Author Index">
             Kumar, Vijay
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#148833" title="Click to go to the Author Index">
             Chaudhari, Pratik
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3320" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#probability_and_statistical_methods" title="Click to go to the Keyword Index">
               Probability and Statistical Methods
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Active perception approaches select future viewpoints by using some estimate of the information gain. An inaccurate estimate can be detrimental in critical situations, e.g., locating a person in distress. However the true information gained can only be calculated post hoc, i.e., after the observation is realized. We present an approach to estimate the discrepancy between the estimated information gain (which is the expectation over putative future observations while neglecting correlations among them) and the true information gain. The key idea is to analyze the mathematical relationship between active perception and the estimation error of the information gain in a game-theoretic setting. Using this, we develop an online estimation approach that achieves sub-linear regret (in the number of time-steps) for the estimation of the true information gain and reduces the sub-optimality of active perception systems. We demonstrate our approach for active perception using a comprehensive set of experiments on: (a) different types of environments, including a quadrotor in a photorealistic simulation, real-world robotic data, and real-world experiments with ground robots exploring indoor and outdoor scenes; (b) different types of robotic perception data; and (c) different map representations. On average, our approach reduces information gain estimation errors by 42%, increases the information gain by 7%, PSNR by 5%, and semantic accuracy (measured as the number of objects that are localized correctly) by 6%. In real-world experiments with a Jackal ground robot, our approach demonstrated complex trajectories to explore occluded regions.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct19_04">
             11:30-11:35, Paper ThCT19.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3561'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Take Your Best Shot: Sampling-Based Planning for Autonomous Photography
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#278671" title="Click to go to the Author Index">
             Gao, Shijie
            </a>
           </td>
           <td class="r">
            University of Virginia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#321794" title="Click to go to the Author Index">
             Bramblett, Lauren
            </a>
           </td>
           <td class="r">
            University of Virginia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#133879" title="Click to go to the Author Index">
             Bezzo, Nicola
            </a>
           </td>
           <td class="r">
            University of Virginia
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3561" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#planning_under_uncertainty" title="Click to go to the Keyword Index">
               Planning under Uncertainty
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reactive_and_sensor_based_planning" title="Click to go to the Keyword Index">
               Reactive and Sensor-Based Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Autonomous mobile robots (AMRs) equipped with high-quality cameras are revolutionizing the field of autonomous photography by delivering efficient and cost-effective methods for capturing dynamic visual content. As AMRs are deployed in increasingly diverse environments, the challenge of consistently producing high-quality photographic content remains. Traditional approaches often involve AMRs following a predetermined path while capturing data-intensive imagery, which can be suboptimal, especially in environments with limited connectivity or physical obstructions. These drawbacks necessitate intelligent decision-making to pinpoint optimal vantage points for image capture. Inspired by Next Best View studies, we propose a novel autonomous photography framework that enhances image quality and minimizes the number of photos needed. This framework incorporates a proposed evaluation metric that leverages ray-tracing and Gaussian process interpolation, enabling the assessment of potential visual information from the target in partially known environments. A derivative-free optimization (DFO) method is then proposed to sample candidate views and identify the optimal viewpoint. The effectiveness of our approach is demonstrated by comparing it with existing methods and further validated through simulations and experiments with various vehicles.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct19_05">
             11:35-11:40, Paper ThCT19.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4866'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              An Addendum to NeBula: Toward Extending Team CoSTAR’s Solution to Larger Scale Environments (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#220348" title="Click to go to the Author Index">
             Morrell, Benjamin
            </a>
           </td>
           <td class="r">
            Jet Propulsion Laboratory, California Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#166212" title="Click to go to the Author Index">
             Otsu, Kyohei
            </a>
           </td>
           <td class="r">
            California Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#114852" title="Click to go to the Author Index">
             Agha-mohammadi, Ali-akbar
            </a>
           </td>
           <td class="r">
            NASA-JPL, Caltech
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#221454" title="Click to go to the Author Index">
             Fan, David D
            </a>
           </td>
           <td class="r">
            NASA Jet Propulsion Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#118338" title="Click to go to the Author Index">
             Kim, Sung-Kyun
            </a>
           </td>
           <td class="r">
            NASA Jet Propulsion Laboratory, Caltech
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#278868" title="Click to go to the Author Index">
             Ginting, Muhammad Fadhil
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#268122" title="Click to go to the Author Index">
             Lei, Xianmei
            </a>
           </td>
           <td class="r">
            NASA JPL
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#130394" title="Click to go to the Author Index">
             Edlund, Jeffrey
            </a>
           </td>
           <td class="r">
            Jet Propulsion Lab
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#239484" title="Click to go to the Author Index">
             Fakoorian, Seyed Abolfazl
            </a>
           </td>
           <td class="r">
            Cleveland State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#245028" title="Click to go to the Author Index">
             Bouman, Amanda
            </a>
           </td>
           <td class="r">
            Caltech
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#283109" title="Click to go to the Author Index">
             Chavez, Fernando
            </a>
           </td>
           <td class="r">
            Jet Propulsion Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#296104" title="Click to go to the Author Index">
             Kim, Taeyeon
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#238218" title="Click to go to the Author Index">
             Correa, Gustavo J.
            </a>
           </td>
           <td class="r">
            University of California Riverside
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#207038" title="Click to go to the Author Index">
             Saboia Da Silva, Maira
            </a>
           </td>
           <td class="r">
            NASA Jet Propulsion Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#159272" title="Click to go to the Author Index">
             Santamaria-Navarro, Angel
            </a>
           </td>
           <td class="r">
            Universitat Politècnica De Catalunya
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#192998" title="Click to go to the Author Index">
             Lopez, Brett
            </a>
           </td>
           <td class="r">
            University of California, Los Angeles
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#317781" title="Click to go to the Author Index">
             Kim, Boseong
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology (KAIST)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#251515" title="Click to go to the Author Index">
             Jung, Chanyoung
            </a>
           </td>
           <td class="r">
            KAIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#321938" title="Click to go to the Author Index">
             Sobue, Mamoru
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#267588" title="Click to go to the Author Index">
             Peltzer, Oriana
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#317800" title="Click to go to the Author Index">
             Ott, Joshua
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#330629" title="Click to go to the Author Index">
             Trybula, Robert
            </a>
           </td>
           <td class="r">
            University of Southern California
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#278801" title="Click to go to the Author Index">
             Touma, Thomas
            </a>
           </td>
           <td class="r">
            Caltech
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#238704" title="Click to go to the Author Index">
             Kaufmann, Marcel
            </a>
           </td>
           <td class="r">
            Polytechnique Montreal
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#164568" title="Click to go to the Author Index">
             Vaquero, Tiago
            </a>
           </td>
           <td class="r">
            JPL, Caltech
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#243676" title="Click to go to the Author Index">
             Pailevanian, Torkom
            </a>
           </td>
           <td class="r">
            Jet Propulsion Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#268943" title="Click to go to the Author Index">
             Palieri, Matteo
            </a>
           </td>
           <td class="r">
            NASA Jet Propulsion Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#247014" title="Click to go to the Author Index">
             Chang, Yun
            </a>
           </td>
           <td class="r">
            MIT
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#289263" title="Click to go to the Author Index">
             Reinke, Andrzej
            </a>
           </td>
           <td class="r">
            University of Bonn
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#256947" title="Click to go to the Author Index">
             Spieler, Patrick
            </a>
           </td>
           <td class="r">
            JPL
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#277587" title="Click to go to the Author Index">
             Clark, Lillian
            </a>
           </td>
           <td class="r">
            University of Southern California
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#332210" title="Click to go to the Author Index">
             Archanian, Avak
            </a>
           </td>
           <td class="r">
            Jet Propulsion Laboratory, California Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#277924" title="Click to go to the Author Index">
             Chen, Kenny
            </a>
           </td>
           <td class="r">
            University of California, Los Angeles
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#332209" title="Click to go to the Author Index">
             Melikyan, Hovhannes
            </a>
           </td>
           <td class="r">
            Jet Propulsion Laboratory, California Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#277311" title="Click to go to the Author Index">
             Dixit, Anushri
            </a>
           </td>
           <td class="r">
            University of California, Los Angeles
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#315651" title="Click to go to the Author Index">
             Delecki, Harrison
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#245024" title="Click to go to the Author Index">
             Pastor, Daniel
            </a>
           </td>
           <td class="r">
            Caltech
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#115674" title="Click to go to the Author Index">
             Ridge, Barry
            </a>
           </td>
           <td class="r">
            NASA Jet Propulsion Laboratory, California Institute of Technolo
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#256838" title="Click to go to the Author Index">
             Marchal, Nicolas Paul
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#430114" title="Click to go to the Author Index">
             Uribe, Jose
            </a>
           </td>
           <td class="r">
            Jet Propulsion Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#170846" title="Click to go to the Author Index">
             Kochenderfer, Mykel
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#189409" title="Click to go to the Author Index">
             Beltrame, Giovanni
            </a>
           </td>
           <td class="r">
            Ecole Polytechnique De Montreal
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#105882" title="Click to go to the Author Index">
             Nikolakopoulos, George
            </a>
           </td>
           <td class="r">
            Luleå University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104593" title="Click to go to the Author Index">
             Shim, David Hyunchul
            </a>
           </td>
           <td class="r">
            KAIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#119426" title="Click to go to the Author Index">
             Carlone, Luca
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#100193" title="Click to go to the Author Index">
             Burdick, Joel
            </a>
           </td>
           <td class="r">
            California Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4866" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#field_robots" title="Click to go to the Keyword Index">
               Field Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#software_hardware_integration_for_robot_systems" title="Click to go to the Keyword Index">
               Software-Hardware Integration for Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This article presents an appendix to the original NeBula autonomy solution developed by the Team Collaborative SubTerranean Autonomous Robots (CoSTAR), participating in the DARPA Subterranean Challenge. Specifically, this article presents extensions to NeBula’s hardware, software, and algorithmic components that focus on increasing the range and scale of the exploration environment. From the algorithmic perspective, we discuss the following extensions to the original NeBula framework: 1) large-scale geometric and semantic environment mapping; 2) an adaptive positioning system; 3) probabilistic traversability analysis and local planning; 4) large-scale partially observable Markov decision process (POMDP)-based global motion planning and exploration behavior; 5) large-scale networking and decentralized reasoning; 6) communication-aware mission planning; and 7) multimodal ground–aerial exploration solutions. We demonstrate the application and deployment of the presented systems and solutions in various large-scale underground environments, including limestone mine exploration scenarios as well as deployment in the DARPA Subterranean challenge.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct19_06">
             11:40-11:45, Paper ThCT19.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('5033'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              InstanceVO: Self-Supervised Semantic Visual Odometry by Using Metric Learning to Incorporate Geometrical Priors in Instance Objects
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#335779" title="Click to go to the Author Index">
             Xie, Yuanyan
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#409698" title="Click to go to the Author Index">
             Yang, Junzhe
            </a>
           </td>
           <td class="r">
            University of Science and Technology Beijing
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#272420" title="Click to go to the Author Index">
             Zhou, Huaidong
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#134089" title="Click to go to the Author Index">
             Sun, Fuchun
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab5033" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#autonomous_agents" title="Click to go to the Keyword Index">
               Autonomous Agents
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Visual odometry is one of the key technologies for unmanned ground vehicles. To improve the robustness of the systems and enable intelligent tasks, researchers introduced learning-based recognition modules into visual odometry systems, but didn't realize tight coupling between visual odometry systems and recognition modules. This paper proposes a self-supervised semantic visual odometry method, which can complete the tasks of ego-motion estimation, depth prediction, and instance segmentation with a shared encoder. The potential dynamic regions are removed and the image reconstruction loss is rectified by instance detection results. Moreover, the instance-guided triplet loss and cross-task self-attention modules are devised to learn the geometrical relationships among pixels that are implied in instance object priors. The proposed method is validated on KITTI and ComplexUrban datasets. The experimental results show that our method has superiority to baseline models in both pose estimation and depth prediction. We also discuss the efficacy of evaluation metrics for pose estimation, and consider the accumulation errors of trajectories.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct20">
             <b>
              ThCT20
             </b>
            </a>
           </td>
           <td class="r">
            408
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thct20" title="Click to go to the Program at a Glance">
             <b>
              In-Hand Manipulation
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#107845" title="Click to go to the Author Index">
             Mason, Matthew T.
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#124170" title="Click to go to the Author Index">
             Iba, Soshi
            </a>
           </td>
           <td class="r">
            Honda Research Institute USA
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct20_01">
             11:15-11:20, Paper ThCT20.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('565'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              GET-Zero: Graph Embodiment Transformer for Zero-Shot Embodiment Generalization
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#285770" title="Click to go to the Author Index">
             Patel, Austin
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#193621" title="Click to go to the Author Index">
             Song, Shuran
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab565" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#transfer_learning" title="Click to go to the Keyword Index">
               Transfer Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#dexterous_manipulation" title="Click to go to the Keyword Index">
               Dexterous Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multifingered_hands" title="Click to go to the Keyword Index">
               Multifingered Hands
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper introduces GET-Zero, a model architecture and training procedure for learning an embodiment-aware control policy that can immediately adapt to new hardware changes without retraining. To do so, we present Graph Embodiment Transformer (GET), a transformer model that leverages the embodiment graph connectivity as a learned structural bias in the attention mechanism. We use behavior cloning to distill demonstration data from embodiment-specific expert policies into an embodiment-aware GET model that conditions on the hardware configuration of the robot to make control decisions. We conduct a case study on a dexterous in-hand object rotation task using different configurations of a four-fingered robot hand with joints removed and with link length extensions. Using the GET model along with a self-modeling loss enables GET-Zero to zero-shot generalize to unseen variation in graph structure and link length, yielding a 20% improvement over baseline methods. All code and qualitative video results are on our project website https://get-zero-paper.github.io.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct20_02">
             11:20-11:25, Paper ThCT20.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('792'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Proprioceptive Object Shape and Size Extraction Via In-Hand-Manipulation with a Variable Friction Robot Gripper
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#387278" title="Click to go to the Author Index">
             Bodnar, Igor
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#134555" title="Click to go to the Author Index">
             Spiers, Adam
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab792" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#in_hand_manipulation" title="Click to go to the Keyword Index">
               In-Hand Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#grippers_and_other_end_effectors" title="Click to go to the Keyword Index">
               Grippers and Other End-Effectors
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#force_and_tactile_sensing" title="Click to go to the Keyword Index">
               Force and Tactile Sensing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robotic manipulation tasks commonly rely on computer vision or tactile sensing to extract the physical characteristics of an object. However, this additional sensing capability adds complexity and financial cost to a robotic system. Our work investigates the inexpensive alternative of feature extraction via proprioceptive sensing. Our goal is to determine whether proprioceptive data combined with in-hand-manipulation provides sufficient information to enable geometric reconstruction of object profiles. We use a newly designed 3-DOF robotic gripper with variable-friction finger surfaces to perform model-free in-hand-anipulation on a set of test objects comprised of two dimensional convex prisms. We have devised a manipulation sequence based on the rotation and sliding of test objects to allow side-counting with the successful measurement of shapes and sizes with average angle and size errors of 1.64% and 6.76% respectively. In addition, we have outlined potential research directions aimed at resolving inherent limitations of proprioceptive approaches and making our algorithm generalisable to any arbitrary shape.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct20_03">
             11:25-11:30, Paper ThCT20.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3210'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Diffusion-Informed Probabilistic Contact Search for Multi-Finger Manipulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#371882" title="Click to go to the Author Index">
             Kumar, Abhinav
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#261356" title="Click to go to the Author Index">
             Power, Thomas
            </a>
           </td>
           <td class="r">
            Robotics Institute, University of Michigan
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#290206" title="Click to go to the Author Index">
             Yang, Fan
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#172462" title="Click to go to the Author Index">
             Aguilera, Sergio
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#124170" title="Click to go to the Author Index">
             Iba, Soshi
            </a>
           </td>
           <td class="r">
            Honda Research Institute USA
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#172554" title="Click to go to the Author Index">
             Soltani Zarrin, Rana
            </a>
           </td>
           <td class="r">
            Honda Research Institute - USA
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#113264" title="Click to go to the Author Index">
             Berenson, Dmitry
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3210" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#dexterous_manipulation" title="Click to go to the Keyword Index">
               Dexterous Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#manipulation_planning" title="Click to go to the Keyword Index">
               Manipulation Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Planning contact-rich interactions for multi-finger manipulation is challenging due to the high-dimensionality and hybrid nature of dynamics. Recent advances in data-driven methods have shown promise, but are sensitive to the quality of training data. Combining learning with classical methods like trajectory optimization and search adds additional structure to the problem and domain knowledge in the form of constraints, which can lead to outperforming the data on which models are trained. We present Diffusion-Informed Probabilistic Contact Search (DIPS), which uses an A* search to plan a sequence of contact modes informed by a diffusion model. We train the diffusion model on a dataset of demonstrations consisting of contact modes and trajectories generated by a trajectory optimizer given those modes. In addition, we use a particle filter-inspired method to reason about variability in diffusion sampling arising from model error, estimating likelihoods of trajectories using a learned discriminator. We show that our method outperforms ablations that do not reason about variability and can plan contact sequences that outperform those found in training data across multiple tasks. We evaluate on simulated tabletop card sliding and screwdriver turning tasks, as well as the screwdriver task in hardware to show that our combined learning and planning approach transfers to the real world.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct20_04">
             11:30-11:35, Paper ThCT20.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3522'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Variable-Friction In-Hand Manipulation for Arbitrary Objects Via Diffusion-Based Imitation Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424875" title="Click to go to the Author Index">
             Yan, Qiyang
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#268642" title="Click to go to the Author Index">
             Ding, Zihan
            </a>
           </td>
           <td class="r">
            Princeton University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#295640" title="Click to go to the Author Index">
             Zhou, Xin
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#134555" title="Click to go to the Author Index">
             Spiers, Adam
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3522" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#in_hand_manipulation" title="Click to go to the Keyword Index">
               In-Hand Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#machine_learning_for_robot_control" title="Click to go to the Keyword Index">
               Machine Learning for Robot Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Dexterous in-hand manipulation (IHM) for arbitrary objects is challenging due to the rich and subtle contact process. Variable-friction manipulation is an alternative approach to dexterity, previously demonstrating robust and versatile 2D IHM capabilities with only two single-joint fingers. However, the hard-coded manipulation methods for variable friction hands are restricted to regular polygon objects and limited target poses, as well as requiring the policy to be tailored for each object. This paper proposes an end-to-end learning-based manipulation method to achieve arbitrary object manipulation for any target pose on real hardware, with minimal engineering efforts and data collection. The method features a diffusion policy-based imitation learning method with co-training from simulation and a small amount of real-world data. With the proposed framework, arbitrary objects including polygons and non-polygons can be precisely manipulated to reach arbitrary goal poses within 2 hours of training on an A100 GPU and only 1 hour of real-world data collection. The precision is higher than previous customized object-specific policies, achieving an average success rate of 71.3% with average pose error being 2.676 mm and 1.902°. Code and videos can be found at: https://sites.google.com/view/vf-ihm-il/home.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct20_05">
             11:35-11:40, Paper ThCT20.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3668'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              From Simple to Complex Skills: The Case of In-Hand Object Reorientation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#294264" title="Click to go to the Author Index">
             Qi, Haozhi
            </a>
           </td>
           <td class="r">
            UC Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#233716" title="Click to go to the Author Index">
             Yi, Brent
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#256986" title="Click to go to the Author Index">
             Lambeta, Mike Maroje
            </a>
           </td>
           <td class="r">
            Facebook
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#115293" title="Click to go to the Author Index">
             Ma, Yi
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#155792" title="Click to go to the Author Index">
             Calandra, Roberto
            </a>
           </td>
           <td class="r">
            TU Dresden
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#205586" title="Click to go to the Author Index">
             Malik, Jitendra
            </a>
           </td>
           <td class="r">
            UC Berkeley
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3668" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#in_hand_manipulation" title="Click to go to the Keyword Index">
               In-Hand Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#dexterous_manipulation" title="Click to go to the Keyword Index">
               Dexterous Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Learning policies in simulation and transferring them to the real world has become a promising approach in dexterous manipulation. However, bridging the sim-to-real gap for each new task requires substantial human effort, such as careful reward engineering, hyperparameter tuning, and system identification. In this work, we present a system that leverages low-level skills to address these challenges for more complex tasks. Specifically, we introduce a hierarchical policy for in-hand object reorientation based on previously acquired rotation skills. This hierarchical policy learns to select which low-level skill to execute based on feedback from both the environment and the low-level skill policies themselves. Compared to learning from scratch, the hierarchical policy is more robust to out-of-distribution changes and transfers easily from simulation to real-world environments. Additionally, we propose a generalizable object pose estimator that uses proprioceptive information, low-level skill predictions, and control errors as inputs to estimate the object's pose over time. We demonstrate that our system can reorient objects, including symmetrical and textureless ones, to a desired pose.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct20_06">
             11:40-11:45, Paper ThCT20.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4080'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              DROP: Dextereous Reorientation Via Online Planning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#220442" title="Click to go to the Author Index">
             Li, Albert H.
            </a>
           </td>
           <td class="r">
            California Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#210078" title="Click to go to the Author Index">
             Culbertson, Preston
            </a>
           </td>
           <td class="r">
            Cornell University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#205664" title="Click to go to the Author Index">
             Kurtz, Vincent
            </a>
           </td>
           <td class="r">
            California Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#116276" title="Click to go to the Author Index">
             Ames, Aaron
            </a>
           </td>
           <td class="r">
            California Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4080" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#in_hand_manipulation" title="Click to go to the Keyword Index">
               In-Hand Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#dexterous_manipulation" title="Click to go to the Keyword Index">
               Dexterous Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#manipulation_planning" title="Click to go to the Keyword Index">
               Manipulation Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Achieving human-like dexterity is a longstanding challenge in robotics, in part due to the complexity of planning and control for contact-rich systems. In reinforcement learning (RL), one popular approach has been to use massively-parallelized, domain-randomized simulations to learn a policy offline over a vast array of contact conditions, allowing robust sim-to-real transfer. Inspired by recent advances in real-time parallel simulation, this work considers instead the viability of online planning methods for contact-rich manipulation by studying the well-known in-hand cube reorientation task. We propose a simple architecture that employs a sampling-based predictive controller and vision-based pose estimator to search for contact-rich control actions online. We conduct thorough experiments to assess the real-world performance of our method, architectural design choices, and key factors for robustness, demonstrating that our simple sampled-based approach achieves performance comparable to prior RL-based works. Supplemental material: https://caltech-amber.github.io/drop.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct21">
             <b>
              ThCT21
             </b>
            </a>
           </td>
           <td class="r">
            410
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thct21" title="Click to go to the Program at a Glance">
             <b>
              Safety and Control in HRI
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#121880" title="Click to go to the Author Index">
             He, Hongsheng
            </a>
           </td>
           <td class="r">
            The University of Alabama
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#127992" title="Click to go to the Author Index">
             Kim, Wansoo
            </a>
           </td>
           <td class="r">
            Hanyang University ERICA
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct21_01">
             11:15-11:20, Paper ThCT21.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1405'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Uncertainty-Aware Probabilistic 3D Human Motion Forecasting Via Invertible Networks
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#418334" title="Click to go to the Author Index">
             Ma, Yue
            </a>
           </td>
           <td class="r">
            Beihang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#418667" title="Click to go to the Author Index">
             Zhou, Kanglei
            </a>
           </td>
           <td class="r">
            Beihang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#418659" title="Click to go to the Author Index">
             Yu, Fuyang
            </a>
           </td>
           <td class="r">
            Beihang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419055" title="Click to go to the Author Index">
             Li, Frederick W. B.
            </a>
           </td>
           <td class="r">
            University of Durham
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#363907" title="Click to go to the Author Index">
             Xiaohui, Liang
            </a>
           </td>
           <td class="r">
            State Key Laboratory of Virtual Reality Technology and Systems,
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1405" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#human_and_humanoid_motion_analysis_and_synthesis" title="Click to go to the Keyword Index">
               Human and Humanoid Motion Analysis and Synthesis
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#modeling_and_simulating_humans" title="Click to go to the Keyword Index">
               Modeling and Simulating Humans
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#safety_in_hri" title="Click to go to the Keyword Index">
               Safety in HRI
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             3D human motion forecasting aims to enable autonomous applications. Estimating uncertainty for each prediction (i.e., confidence based on probability density or quantile) is essential for safety-critical contexts like human-robot collaboration to minimize risks. However, existing diverse motion forecasting approaches struggle with uncertainty quantification due to implicit probabilistic representations hindering uncertainty modeling. We propose ProbHMI, which introduces invertible networks to parameterize poses in a disentangled latent space, enabling probabilistic dynamics modeling. A forecasting module then explicitly predicts future latent distributions, allowing effective uncertainty quantification. Evaluated on benchmarks, ProbHMI achieves strong performance for both deterministic and diverse prediction while validating uncertainty calibration, critical for risk-aware decision making.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct21_02">
             11:20-11:25, Paper ThCT21.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3072'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              MonLog: MONotonic-Constrained LOGistic Regressions for Automated Safety Curve Design
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#391334" title="Click to go to the Author Index">
             Melone, Alessandro
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#221953" title="Click to go to the Author Index">
             Kirschner, Robin Jeanne
            </a>
           </td>
           <td class="r">
            TU Munich, Institute for Robotics and Systems Intelligence
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#397971" title="Click to go to the Author Index">
             Müller, Dirk
            </a>
           </td>
           <td class="r">
            Department of Orthopaedics and Sports Orthopaedics, Klinikum Rec
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#292159" title="Click to go to the Author Index">
             Swikir, Abdalla
            </a>
           </td>
           <td class="r">
            Mohamed Bin Zayed University of Artificial Intelligence
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#108317" title="Click to go to the Author Index">
             Haddadin, Sami
            </a>
           </td>
           <td class="r">
            Mohamed Bin Zayed University of Artificial Intelligence
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3072" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#safety_in_hri" title="Click to go to the Keyword Index">
               Safety in HRI
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#physical_human_robot_interaction" title="Click to go to the Keyword Index">
               Physical Human-Robot Interaction
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#human_centered_robotics" title="Click to go to the Keyword Index">
               Human-Centered Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The increasing integration of robots in close human environments necessitates robust safety measures that can adapt to evolving tasks and conditions. Current standards rely on task-specific safety evaluations that are often inflexible, requiring repeated assessments whenever task parameters change. This work proposes MonLog, a data-driven, probabilistic method to automatically derive safety curves (SCs) from recent injury protection data sets. By leveraging non-linear modeling techniques, our approach addresses the limitations of conventional linear SCs, which often result in overly conservative speed restrictions. We present a comprehensive test routine to validate our method, highlighting improvements in both compliance with safety constraints and operational efficiency. Our findings demonstrate that the proposed approach not only enhances safety but also optimizes robotic performance, making it suitable for a wide range of applications.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct21_03">
             11:25-11:30, Paper ThCT21.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3300'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Passivity Filters for Bilateral Teleoperation with Variable Impedance Control
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#352436" title="Click to go to the Author Index">
             Alyousef Almasalmah, Fadi
            </a>
           </td>
           <td class="r">
            University of Strasbourg
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#266472" title="Click to go to the Author Index">
             Poignonec, Thibault
            </a>
           </td>
           <td class="r">
            University of Strasbourg, Icube Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#251306" title="Click to go to the Author Index">
             Omran, Hassan
            </a>
           </td>
           <td class="r">
            ICube Laboratory, University of Strasbourg, Strasbourg
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#270439" title="Click to go to the Author Index">
             Liu, Chao
            </a>
           </td>
           <td class="r">
            LIRMM
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#105396" title="Click to go to the Author Index">
             Bayle, Bernard
            </a>
           </td>
           <td class="r">
            University of Strasbourg
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3300" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#telerobotics_and_teleoperation" title="Click to go to the Keyword Index">
               Telerobotics and Teleoperation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#compliance_and_impedance_control" title="Click to go to the Keyword Index">
               Compliance and Impedance Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#safety_in_hri" title="Click to go to the Keyword Index">
               Safety in HRI
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In robotic teleoperation, it is crucial to be able to dynamically adjust interactions with the environment. Drawing inspiration from human behavior during interactions, Variable Impedance Control (VIC) has been widely adopted to enhance robotic flexibility and adaptability. However, maintaining the passivity of such control systems remains a critical safety concern. This paper introduces an optimization-based framework for passive variable impedance control in bilateral teleoperation, combining the advantages of Passivity Filters (PFs), Time-Domain Passivity (TDP) control, and Passive-Set-Position-Modulation (PSPM). The method solves an optimization problem aimed at dissipating the energy that could lead to a lack of passivity. The proposed method is assessed through experiments, illustrating its ability to keep the teleoperation system passive and safe under a variable impedance profile.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct21_04">
             11:30-11:35, Paper ThCT21.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3347'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Robots That Learn to Safely Influence Via Prediction-Informed Reach-Avoid Dynamic Games
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#220203" title="Click to go to the Author Index">
             Pandya, Ravi
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#171352" title="Click to go to the Author Index">
             Liu, Changliu
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#191008" title="Click to go to the Author Index">
             Bajcsy, Andrea
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3347" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#safety_in_hri" title="Click to go to the Keyword Index">
               Safety in HRI
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#robot_safety" title="Click to go to the Keyword Index">
               Robot Safety
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robots can influence people to accomplish their tasks more efficiently: autonomous cars can inch forward at an intersection to pass through, and tabletop manipulators can go for an object on the table first. However, a robot's ability to influence can also compromise the physical safety of nearby people if naively executed. In this work, we pose and solve a novel robust reach-avoid dynamic game which enables robots to be maximally influential, but only when a safety backup control exists. On the human side, we model the human's behavior as goal-driven but conditioned on the robot's plan, enabling us to capture influence. On the robot side, we solve the dynamic game in the joint physical and belief space, enabling the robot to reason about how its uncertainty in human behavior will evolve over time. We instantiate our method, called SLIDE (Safely Leveraging Influence in Dynamic Environments), in a high-dimensional (39-D) simulated human-robot collaborative manipulation task solved via offline game-theoretic reinforcement learning. We compare our approach to a robust baseline that treats the human as a worst-case adversary, a safety controller that does not explicitly reason about influence, and an energy-function-based safety shield. We find that SLIDE consistently enables the robot to leverage the influence it has on the human when it is safe to do so, ultimately allowing the robot to be less conservative while still ensuring a high safety rate during task execution.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct21_05">
             11:35-11:40, Paper ThCT21.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3836'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Multi-Layered Safety of Redundant Robot Manipulators Via Task-Oriented Planning and Control
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#316774" title="Click to go to the Author Index">
             Jia, Xinyu
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#279144" title="Click to go to the Author Index">
             Wang, Wenxin
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#317226" title="Click to go to the Author Index">
             Yang, Jun
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#181302" title="Click to go to the Author Index">
             Pan, Yongping
            </a>
           </td>
           <td class="r">
            Peng Cheng Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#107709" title="Click to go to the Author Index">
             Yu, Haoyong
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3836" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#safety_in_hri" title="Click to go to the Keyword Index">
               Safety in HRI
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#collision_avoidance" title="Click to go to the Keyword Index">
               Collision Avoidance
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_control" title="Click to go to the Keyword Index">
               Motion Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Ensuring safety is crucial to promote the application of robot manipulators in open workspaces. Factors such as sensor errors or unpredictable collisions make the environment full of uncertainties. In this work, we investigate these potential safety challenges on redundant robot manipulators, and propose a task-oriented planning and control framework to achieve multi-layered safety while maintaining efficient task execution. Our approach consists of two main parts: a task-oriented trajectory planner based on multiple-shooting model predictive control (MPC) method, and a torque controller that allows safe and efficient collision reaction using only proprioceptive data. Through extensive simulations and real-hardware experiments, we demonstrate that the proposed framework can effectively handle uncertain static or dynamic obstacles, and perform disturbance resistance in manipulation tasks when unforeseen contacts occur.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct21_06">
             11:40-11:45, Paper ThCT21.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('5021'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Multi-Task Energy-Aware Impedance Controller for Enhanced Safety in Physical Human-Robot Interaction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#372970" title="Click to go to the Author Index">
             Choi, SeungMin
            </a>
           </td>
           <td class="r">
            Hanyang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#360450" title="Click to go to the Author Index">
             Ha, Seongmin
            </a>
           </td>
           <td class="r">
            Hanyang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#127992" title="Click to go to the Author Index">
             Kim, Wansoo
            </a>
           </td>
           <td class="r">
            Hanyang University ERICA
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab5021" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#safety_in_hri" title="Click to go to the Keyword Index">
               Safety in HRI
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#physical_human_robot_interaction" title="Click to go to the Keyword Index">
               Physical Human-Robot Interaction
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In physical human-robot interaction (pHRI), ensuring human safety in all tasks conducted by the robot is crucial. Traditional compliance control strategies, such as admittance and impedance control, often lead to unpredictable robot behavior due to incidents like contact loss or unexpected external forces, which can cause significant harm to humans.
             <p>
              To overcome these limitations, this study introduces a multi-task energy-aware impedance controller for kinematically redundant robots. This controller extends the energy-aware impedance control strategy, which ensures the passivity and safety of a single task using a virtual global energy tank, to kinematically redundant robots performing multiple tasks. The proposed controller effectively regulates the power flow of all tasks performed by the robot through a single global energy tank, ensuring the safety and passivity of the tasks.
              <p>
               Experimental results in a shared environment, where external forces are simultaneously applied to the end-effector and the third joint of the Franka Emika Panda, showed that the robot's energy and power, as well as the power of all tasks, consistently remained within predefined thresholds. Additionally, when comparing the proposed controllers with controller that do not consider null space projection in the power regulation stage and controller that do not regulate the robot's power, our approach effectively managed the robot's energy and power and the power of all tasks, ensuring passivity and enhanced safety.
              </p>
             </p>
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct22">
             <b>
              ThCT22
             </b>
            </a>
           </td>
           <td class="r">
            411
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thct22" title="Click to go to the Program at a Glance">
             <b>
              Learning for Manipulation
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct22_01">
             11:15-11:20, Paper ThCT22.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1790'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Parameter-Efficient Tuning Framework for Language-Guided Object Grounding and Robot Grasping
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#335053" title="Click to go to the Author Index">
             Yu, Houjian
            </a>
           </td>
           <td class="r">
            University of Minnesota, Twin Cities
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#315217" title="Click to go to the Author Index">
             Li, Mingen
            </a>
           </td>
           <td class="r">
            University of Minnesota Twin Cities
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#337704" title="Click to go to the Author Index">
             Rezazadeh, Alireza
            </a>
           </td>
           <td class="r">
            University of Minnesota
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#251702" title="Click to go to the Author Index">
             Yang, Yang
            </a>
           </td>
           <td class="r">
            Meta
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#110852" title="Click to go to the Author Index">
             Choi, Changhyun
            </a>
           </td>
           <td class="r">
            University of Minnesota, Twin Cities
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1790" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The language-guided robot grasping task requires a robot agent to integrate multimodal information from both visual and linguistic inputs to predict actions for target-driven grasping. While recent approaches utilizing Multimodal Large Language Models (MLLMs) have shown promising results, their extensive computation and data demands limit the feasibility of local deployment and customization. To address this, we propose a novel CLIP-based multimodal parameter-efficient tuning (PET) framework designed for three language-guided object grounding and grasping tasks: (1) Referring Expression Segmentation (RES), (2) Referring Grasp Synthesis (RGS), and (3) Referring Grasp Affordance (RGA). Our approach introduces two key innovations: a bi-directional vision-language adapter that aligns multimodal inputs for pixel-level language understanding and a depth fusion branch that incorporates geometric cues to facilitate robot grasping predictions. Experiment results demonstrate superior performance in the RES object grounding task compared with existing CLIP-based full-model tuning or PET approaches. In the RGS and RGA tasks, our model not only effectively interprets object attributes based on simple language descriptions but also shows strong potential for comprehending complex spatial reasoning scenarios, such as multiple identical objects present in the workspace.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct22_02">
             11:20-11:25, Paper ThCT22.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1819'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Cascaded Diffusion Models for Neural Motion Planning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#242690" title="Click to go to the Author Index">
             Sharma, Mohit
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#257018" title="Click to go to the Author Index">
             Fishman, Adam
            </a>
           </td>
           <td class="r">
            OpenAI
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#151441" title="Click to go to the Author Index">
             Kumar, Vikash
            </a>
           </td>
           <td class="r">
            Meta AI
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#171475" title="Click to go to the Author Index">
             Paxton, Chris
            </a>
           </td>
           <td class="r">
            Meta AI
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#124019" title="Click to go to the Author Index">
             Kroemer, Oliver
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1819" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#ai_based_methods" title="Click to go to the Keyword Index">
               AI-Based Methods
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robots in the real world need to perceive and move to goals in complex environments without collisions. Avoiding collisions is especially difficult when relying on sensor perception and when goals are among clutter. Diffusion policies and other generative models have shown strong performance in solving textit{local} planning problems, but often struggle at avoiding all of the subtle constraint violations that characterize truly challenging global motion planning problems. In this work, we propose an approach for learning global motion planning using diffusion policies, allowing the robot to generate full trajectories through complex scenes and reasoning about multiple obstacles along the path. Our approach uses cascaded hierarchical models which unify global prediction and local refinement together with online plan repair to ensure the trajectories are collision free. Our method outperforms (approx 5%) a wide variety of baselines on challenging tasks in multiple domains including navigation and manipulation.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct22_03">
             11:25-11:30, Paper ThCT22.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2753'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Reinforcement Learning with Lie Group Orientations for Robotics
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#413469" title="Click to go to the Author Index">
             Schuck, Martin
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#285394" title="Click to go to the Author Index">
             Bruedigam, Jan
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#107646" title="Click to go to the Author Index">
             Hirche, Sandra
            </a>
           </td>
           <td class="r">
            Technische Universität München
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#124265" title="Click to go to the Author Index">
             Schoellig, Angela P.
            </a>
           </td>
           <td class="r">
            TU Munich
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2753" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#grasping" title="Click to go to the Keyword Index">
               Grasping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Handling orientations of robots and objects is a crucial aspect of many applications. Yet, ever so often, there is a lack of mathematical correctness when dealing with orientations, especially in learning pipelines involving, for example, artificial neural networks. In this paper, we investigate reinforcement learning with orientations and propose a simple modification of the network's input and output that adheres to the Lie group structure of orientations. As a result, we obtain a practically efficient implementation that is directly usable with existing learning libraries and achieves significantly better performance than other common orientation representations. We briefly introduce Lie theory specifically for orientations in robotics to motivate and outline our approach. Subsequently, a thorough empirical evaluation of different combinations of orientation representations for states and actions demonstrates the superior performance of our proposed approach in different scenarios, including: direct orientation control, end effector orientation control, and pick-and-place tasks.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct22_04">
             11:30-11:35, Paper ThCT22.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4812'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              DexTouch: Learning to Seek and Manipulate Objects with Tactile Dexterity
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#296185" title="Click to go to the Author Index">
             Lee, Kang-Won
            </a>
           </td>
           <td class="r">
            Dongguk University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#293758" title="Click to go to the Author Index">
             Qin, Yuzhe
            </a>
           </td>
           <td class="r">
            UC San Diego
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#280419" title="Click to go to the Author Index">
             Wang, Xiaolong
            </a>
           </td>
           <td class="r">
            UC San Diego
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#144046" title="Click to go to the Author Index">
             Lim, Soo-Chul
            </a>
           </td>
           <td class="r">
            Dongguk University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4812" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#dexterous_manipulation" title="Click to go to the Keyword Index">
               Dexterous Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The sense of touch is an essential ability for skillfully performing a variety of tasks, providing the capacity to search and manipulate objects without relying on visual information. In this paper, we introduce a multi-finger robot system designed to manipulate objects using the sense of touch, without relying on vision. For tasks that mimic daily life, the robot uses its sense of touch to manipulate randomly placed objects in dark. The objective of this study is to enable robots to perform manipulation without vision by using tactile sensation to compensate for the information gap caused by the absence of vision, given the presence of prior information. Training the policy through reinforcement learning in simulation and transferring the trained policy to the real environment,we demonstrate that manipulationwithout visual input can be applied to robots without vision. In addition, the experiments showcase the importance of tactile sensing in tasks performed without vision. Our project page is available at https://lee-kangwon.github.io/dextouch/
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct22_05">
             11:35-11:40, Paper ThCT22.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1540'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Catch It! Learning to Catch in Flight with Mobile Dexterous Hands
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420940" title="Click to go to the Author Index">
             Zhang, Yuanhang
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#352171" title="Click to go to the Author Index">
             Liang, Tianhai
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#316530" title="Click to go to the Author Index">
             Chen, Zhenyang
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#344963" title="Click to go to the Author Index">
             Ze, Yanjie
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#220412" title="Click to go to the Author Index">
             Xu, Huazhe
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1540" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#mobile_manipulation" title="Click to go to the Keyword Index">
               Mobile Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Catching objects in flight (i.e., thrown objects) is a common daily skill for humans, yet it presents a significant challenge for robots. This task requires a robot with agile and accurate motion, a large spatial workspace, and the ability to interact with diverse objects. In this paper, we build a mobile manipulator composed of a mobile base, a 6-DoF arm, and a 12-DoF dexterous hand to tackle such a challenging task. We propose a two-stage reinforcement learning framework to efficiently train a whole-body-control catching policy for this high-DoF system in simulation. The objects' throwing configurations, shapes, and sizes are randomized during training to enhance policy adaptivity to various trajectories and object characteristics in flight. The results show that our trained policy catches diverse objects with randomly thrown trajectories, at a high success rate of about 80% in simulation, with a significant improvement over the baselines. The policy trained in simulation can be directly deployed in the real world with onboard sensing and computation, which achieves catching sandbags in various shapes, randomly thrown by humans. Our project page is available at href{https://mobile-dex-catch.github.io/}{https://mobile-d ex-catch.github.io}
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct23">
             <b>
              ThCT23
             </b>
            </a>
           </td>
           <td class="r">
            412
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thct23" title="Click to go to the Program at a Glance">
             <b>
              Legged Robots
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#111288" title="Click to go to the Author Index">
             Johnson, Aaron M.
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#203949" title="Click to go to the Author Index">
             Zhao, Ding
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct23_01">
             11:15-11:20, Paper ThCT23.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1542'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Adaptive Complexity Model Predictive Control
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#239292" title="Click to go to the Author Index">
             Norby, Joseph
            </a>
           </td>
           <td class="r">
            Apptronik
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#335477" title="Click to go to the Author Index">
             Tajbakhsh, Ardalan
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#314998" title="Click to go to the Author Index">
             Yang, Yanhao
            </a>
           </td>
           <td class="r">
            Oregon State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#111288" title="Click to go to the Author Index">
             Johnson, Aaron M.
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1542" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#underactuated_robots" title="Click to go to the Keyword Index">
               Underactuated Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#dynamics" title="Click to go to the Keyword Index">
               Dynamics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This work introduces a formulation of model predictive control (MPC) which adaptively reasons about the complexity of the model while maintaining feasibility and stability guarantees. Existing approaches often handle computational complexity by shortening prediction horizons or simplifying models, both of which can result in instability. Inspired by related approaches in behavioral economics, motion planning, and biomechanics, our method solves MPC problems with a simple model for dynamics and constraints over regions of the horizon where such a model is feasible and a complex model where it is not. The approach leverages an interleaving of planning and execution to iteratively identify these regions, which can be safely simplified if they satisfy an exact template/anchor relationship. We show that this method does not compromise the stability and feasibility properties of the system, and measure performance in simulation experiments on a quadrupedal robot executing agile behaviors over terrains of interest. We find that this adaptive method enables more agile motion (55% increase in top speed) and expands the range of executable tasks compared to fixed-complexity implementations.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct23_02">
             11:20-11:25, Paper ThCT23.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2776'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Benchmarking Different QP Formulations and Solvers for Dynamic Quadrupedal Walking
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420069" title="Click to go to the Author Index">
             Stark, Franek
            </a>
           </td>
           <td class="r">
            Robotics Innovation Center, DFKI GmbH
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416422" title="Click to go to the Author Index">
             Middelberg, Jakob
            </a>
           </td>
           <td class="r">
            German Research Center for Artificial Intelligence
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#136476" title="Click to go to the Author Index">
             Mronga, Dennis
            </a>
           </td>
           <td class="r">
            University of Bremen, German Research Center for Artificial Inte
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#311333" title="Click to go to the Author Index">
             Vyas, Shubham
            </a>
           </td>
           <td class="r">
            Robotics Innovation Center, DFKI GmbH
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#109056" title="Click to go to the Author Index">
             Kirchner, Frank
            </a>
           </td>
           <td class="r">
            University of Bremen
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2776" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#performance_evaluation_and_benchmarking" title="Click to go to the Keyword Index">
               Performance Evaluation and Benchmarking
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#whole_body_motion_planning_and_control" title="Click to go to the Keyword Index">
               Whole-Body Motion Planning and Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Quadratic Programs (QPs) are widely used in the control of walking robots, especially in Model Predictive Control (MPC) and Whole-Body Control (WBC). In both cases, the controller design requires the formulation of a QP and the selection of a suitable QP solver, both requiring considerable time and expertise. While computational performance benchmarks exist for QP solvers, studies comparing optimal combinations of computational hardware (HW), QP formulation, and solver performance are lacking. In this work, we compare dense and sparse QP formulations, and multiple solving methods on different HW architectures, focusing on their computational efficiency in dynamic walking of four-legged robots using MPC. We introduce the Solve Frequency per Watt (SFPW) as a performance measure to enable a cross-hardware comparison of the efficiency of QP solvers. We also benchmark different QP solvers for WBC that we use for trajectory stabilization in quadrupedal walking. As a result, this paper recommends a starting point for practitioners on the selection of QP formulations and solvers for different HW architectures in walking robots and indicates which problems should be devoted the greater technical effort.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct23_03">
             11:25-11:30, Paper ThCT23.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2881'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Indoor and Outdoor Multi-Terrain Stair-Climbing Robot Design
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420027" title="Click to go to the Author Index">
             Chen, Wei-Ting
            </a>
           </td>
           <td class="r">
            National Taiwan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426425" title="Click to go to the Author Index">
             Tsui, En-Chieh
            </a>
           </td>
           <td class="r">
            National Taiwan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#144825" title="Click to go to the Author Index">
             Yu, Wei-Shun
            </a>
           </td>
           <td class="r">
            National Taiwan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#123193" title="Click to go to the Author Index">
             Lin, Pei-Chun
            </a>
           </td>
           <td class="r">
            National Taiwan University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2881" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#wheeled_robots" title="Click to go to the Keyword Index">
               Wheeled Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mechanism_design" title="Click to go to the Keyword Index">
               Mechanism Design
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper introduces an Autonomous Mobile Robot (IOMT) designed for indoor and outdoor multi-terrain environments. The robot features a four-wheel independent drive and steering system (4WID-4WIS), allowing it to maintain high maneuverability on smooth surfaces. Additionally, based on reducing the control complexity, the IOMT addresses the challenges associated with stair climbing by providing stable pitch control, which effectively reduces the impact of stairs on the robot’s posture like pitch angle. The design also incorporates a special mechanism which reducing energy consumption through its worm gear system with self-locking characteristics, and combining steering with shock absorption to simplify both the mechanism complexity. This paper not only proposes a stair climbing strategy for the IOMT configuration but also explores the impact of various design parameters on the robot’s pitch angle, ultimately validating the feasibility and development potential of the design for multi-terrain mobility.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct23_04">
             11:30-11:35, Paper ThCT23.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3829'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              WaLTER: A Wheel and Leg Tumbling Expedition Robot
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#311730" title="Click to go to the Author Index">
             Jay, David
            </a>
           </td>
           <td class="r">
            FAMU-FSU College of Engineering
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#276767" title="Click to go to the Author Index">
             Hackett, Jacob
            </a>
           </td>
           <td class="r">
            Florida State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#139538" title="Click to go to the Author Index">
             Bosscher, Paul
            </a>
           </td>
           <td class="r">
            Harris Corporation
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#148199" title="Click to go to the Author Index">
             Hubicki, Christian
            </a>
           </td>
           <td class="r">
            Florida State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104389" title="Click to go to the Author Index">
             Clark, Jonathan
            </a>
           </td>
           <td class="r">
            Florida State University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3829" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#wheeled_robots" title="Click to go to the Keyword Index">
               Wheeled Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#field_robots" title="Click to go to the Keyword Index">
               Field Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             For effective operation in challenging outdoor environments, mobile unmanned robots face stiff and competing demands including payload capacity, driving speed, range, as well as the ability to traverse rough terrain. To address these issues we introduce the hybrid wheel-leg quadrupedal robot WaLTER. WaLTER utilizes a unique combination of continuously rotating distal leg joints, actuated wheels, and a roll body DOF to efficiently drive on flat ground and effectively tumble over stairs and difficult, broken terrain. We developed intuitive teleoperation scheme and a employed deep reinforcement learning as proof of concept control techniques for the novel morphology. To test its capabilities, we constructed a multi-body simulation in MuJoCo and a 2.1-kg physical prototype for experimentation on traversability and energy economy. Our testing demonstrated the ability to traverse rougher terrain negotiation relative to larger-wheeled counterparts and reliable stair-climbing while maintaining a 4km range on a 24.4 Wh battery (COT: 1.21).
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct23_05">
             11:35-11:40, Paper ThCT23.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3868'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Deformable Multibody Modeling for Model Predictive Control in Legged Locomotion with Embodied Compliance
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#276287" title="Click to go to the Author Index">
             Ye, Keran
            </a>
           </td>
           <td class="r">
            University of California, Riverside
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#151110" title="Click to go to the Author Index">
             Karydis, Konstantinos
            </a>
           </td>
           <td class="r">
            University of California, Riverside
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3868" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#dynamics" title="Click to go to the Keyword Index">
               Dynamics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#compliant_joints_and_mechanisms" title="Click to go to the Keyword Index">
               Compliant Joints and Mechanisms
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The paper presents a method to stabilize dynamic gait for a legged robot with embodied compliance. Our approach introduces a unified description for rigid and compliant bodies to approximate their deformation and a formulation for deformable multibody systems. We develop the centroidal composite predictive deformed inertia (CCPDI) tensor of a deformable multibody system and show how to integrate it with the standard-of-practice model predictive controller (MPC). Simulation shows that the resultant control framework can stabilize trot stepping on a quadrupedal robot with both rigid and compliant spines under the same MPC configurations. Compared to standard MPC, the developed CCPDI-enabled MPC distributes the ground reactive forces closer to the heuristics for body balance, and it is thus more likely to stabilize the gaits of the compliant robot. A parametric study shows that our method preserves some robustness within a suitable envelope of key parameter values.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct23_06">
             11:40-11:45, Paper ThCT23.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4578'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Learning Multi-Agent Loco-Manipulation for Long-Horizon Quadrupedal Pushing
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426055" title="Click to go to the Author Index">
             Feng, Yuming
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426048" title="Click to go to the Author Index">
             Hong, Chuye
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#220764" title="Click to go to the Author Index">
             Niu, Yaru
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#339118" title="Click to go to the Author Index">
             Liu, Shiqi
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#220165" title="Click to go to the Author Index">
             Yang, Yuxiang
            </a>
           </td>
           <td class="r">
            Google Deepmind
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#203949" title="Click to go to the Author Index">
             Zhao, Ding
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4578" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Recently, quadrupedal locomotion has achieved significant success, but their manipulation capabilities, particularly in handling large objects, remain limited, restricting their usefulness in demanding real-world applications such as search and rescue, construction, industrial automation, and room organization. This paper tackles the task of obstacle-aware, long-horizon pushing by multiple quadrupedal robots. We propose a hierarchical multi-agent reinforcement learning framework with three levels of control. The high-level controller integrates an RRT planner and a centralized adaptive policy to generate subgoals, while the mid-level controller uses a decentralized goal-conditioned policy to guide the robots toward these sub-goals. A pre-trained low-level locomotion policy executes the movement commands. We evaluate our method against several baselines in simulation, demonstrating significant improvements over baseline approaches, with 36.0% higher success rates and 24.5% reduction in completion time than the best baseline. Our framework successfully enables long-horizon, obstacle-aware manipulation tasks like Push-Cuboid and Push-T on Go1 robots in the real world. The videos and code of this work can be found at: https://collaborative-mapush.github.io/.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt1">
             <b>
              ThDT1
             </b>
            </a>
           </td>
           <td class="r">
            302
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thdt1" title="Click to go to the Program at a Glance">
             <b>
              Model Predictive Control
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#101887" title="Click to go to the Author Index">
             Lin, Ming C.
            </a>
           </td>
           <td class="r">
            University of Maryland at College Park
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt1_01">
             15:15-15:20, Paper ThDT1.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('915'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Time-Correlated Model Predictive Path Integral: Smooth Action Generation for Sampling-Based Control
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#286546" title="Click to go to the Author Index">
             Lee, Minhyeong
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104207" title="Click to go to the Author Index">
             Lee, Dongjun
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab915" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#integrated_planning_and_control" title="Click to go to the Keyword Index">
               Integrated Planning and Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper, we introduce time-correlated model predictive path integral (TC-MPPI), a novel approach to mitigate action noise in sampling-based control methods. Unlike conventional smoothing techniques that rely on post-processing or additional state variables, TC-MPPI directly incorporates temporal correlation of actions into stochastic optimal control, effectively enforcing quadratic costs on action derivatives. This reformulation enables us to generate smooth action sequences without extra modifications, using a time-correlated and conditional Gaussian sampling distribution. We demonstrate the effectiveness of our approach through simulations on various robotic platforms, including a pendulum, cart-pole, 2D bicopter, 3D quadcopter, and autonomous vehicle. Simulation videos are available at https://youtu.be/nWfJ2MAV2JI.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt1_02">
             15:20-15:25, Paper ThDT1.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3614'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Gradient-Based Trajectory Optimization with Parallelized Differentiable Traffic Simulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#339540" title="Click to go to the Author Index">
             Son, Sanghyun
            </a>
           </td>
           <td class="r">
            University of Maryland
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#277405" title="Click to go to the Author Index">
             Zheng, Laura
            </a>
           </td>
           <td class="r">
            University of Maryland, College Park
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#134056" title="Click to go to the Author Index">
             Clipp, Brian
            </a>
           </td>
           <td class="r">
            Kitware Inc
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422021" title="Click to go to the Author Index">
             Greenwell, Connor
            </a>
           </td>
           <td class="r">
            Kitware Inc
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424881" title="Click to go to the Author Index">
             Philip, Sujin
            </a>
           </td>
           <td class="r">
            Kitware Inc
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101887" title="Click to go to the Author Index">
             Lin, Ming C.
            </a>
           </td>
           <td class="r">
            University of Maryland at College Park
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3614" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#simulation_and_animation" title="Click to go to the Keyword Index">
               Simulation and Animation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present a parallelized differentiable traffic simulator based on the Intelligent Driver Model (IDM), a car-following framework that incorporates driver behavior as key variables. Our vehicle simulator efficiently models vehicle motion, generating trajectories that can be supervised to fit real-world data. By leveraging its differentiable nature, IDM parameters are optimized using gradient-based methods. With the capability to simulate up to 2 million vehicles in real time, the system is scalable for large-scale trajectory optimization. We show that we can use the simulator to filter noise in the input trajectories (trajectory filtering), reconstruct dense trajectories from sparse ones (trajectory reconstruction), and predict future trajectories (trajectory prediction), with all generated trajectories adhering to physical laws. We validate our simulator and algorithm on several datasets including NGSIM and Waymo Open Dataset. The code is publicly available at: https://github.com/SonSang/diffidm.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt1_03">
             15:25-15:30, Paper ThDT1.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3649'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Swept Volume-Aware Trajectory Planning and MPC Tracking for Multi-Axle Swerve-Drive AMRs
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425802" title="Click to go to the Author Index">
             Hu, Tianxin
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#185927" title="Click to go to the Author Index">
             Yuan, Shenghai
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#296723" title="Click to go to the Author Index">
             Bai, Ruofei
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#353500" title="Click to go to the Author Index">
             Xu, Xinhang
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421949" title="Click to go to the Author Index">
             Liao, Yuwen
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#338549" title="Click to go to the Author Index">
             Liu, Fen
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#115410" title="Click to go to the Author Index">
             Xie, Lihua
            </a>
           </td>
           <td class="r">
            NanyangTechnological University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3649" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#integrated_planning_and_control" title="Click to go to the Keyword Index">
               Integrated Planning and Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computational_geometry" title="Click to go to the Keyword Index">
               Computational Geometry
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Multi-axle autonomous mobile robots (AMRs) are set to revolutionize the future of robotics in logistics. As the backbone of next-generation solutions, these robots face a critical challenge: managing and minimizing swept volume during turns while maintaining precise control. Traditional systems designed for standard vehicles often struggle with the complex dynamics of multi-axle configurations, leading to inefficiency and increased safety risk in confined spaces. Our innovative framework overcomes these limitations by combining swept volume minimization with Signed Distance Field (SDF) path planning and model predictive control (MPC) for independent wheel steering. This approach not only plans paths with an awareness of the swept volume, but actively minimizes it in real-time, allowing each axle to follow a precise trajectory while significantly reducing the space the vehicle occupies. By predicting future states and adjusting the turning radius of each wheel, our method enhances both maneuverability and safety, even in the most constrained environments. Unlike previous works, our solution goes beyond basic path calculation and tracking, offering real-time path optimization with minimal swept volume and efficient individual axle control. To our knowledge, this is the first comprehensive approach to tackle these challenges, delivering life-saving improvements in control, efficiency, and safety for multi-axle AMRs. Furthermore, we will open-source our work to foster collaboration and enable others to advance safer and more efficient autonomous systems.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt1_04">
             15:30-15:35, Paper ThDT1.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3962'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Efficient Trajectory Generation Based on Traversable Planes in 3D Complex Architectural Spaces
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#337145" title="Click to go to the Author Index">
             Zhang, Mengke
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#397022" title="Click to go to the Author Index">
             Tian, Zhihao
            </a>
           </td>
           <td class="r">
            Nanjing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#443921" title="Click to go to the Author Index">
             Xia, Yaoguang
            </a>
           </td>
           <td class="r">
            China Tobacco Zhejiang Industrial Co., Ltd
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#213352" title="Click to go to the Author Index">
             Xu, Chao
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#200893" title="Click to go to the Author Index">
             Gao, Fei
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#178926" title="Click to go to the Author Index">
             Cao, Yanjun
            </a>
           </td>
           <td class="r">
            Zhejiang University, Huzhou Institute of Zhejiang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3962" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#field_robots" title="Click to go to the Keyword Index">
               Field Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#nonholonomic_motion_planning" title="Click to go to the Keyword Index">
               Nonholonomic Motion Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             With the increasing integration of robots into human life, their role in architectural spaces where people spend most of their time has become more prominent. While motion capabilities and accurate localization for automated robots have rapidly developed, the challenge remains to generate efficient, smooth, comprehensive, and high-quality trajectories in these areas. In this paper, we propose a novel efficient planner for ground robots to autonomously navigate in large complex multi-layered architectural spaces. Considering that traversable regions typically include ground, slopes, and stairs, which are planar or nearly planar structures, we simplify the problem to navigation within and between complex intersecting planes. We first extract traversable planes from 3D point clouds through segmenting, merging, classifying, and connecting to build a plane-graph, which is lightweight but fully represents the traversable regions. We then build a trajectory optimization based on motion state trajectory and fully consider special constraints when crossing multi-layer planes to maximize the robot's maneuverability. We conduct experiments in simulated environments and test on a CubeTrack robot in real-world scenarios, validating the method's effectiveness and practicality.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt1_05">
             15:35-15:40, Paper ThDT1.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4091'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Model Predictive Control with Visibility Graphs for Humanoid Path Planning and Tracking against Adversarial Opponents
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#390635" title="Click to go to the Author Index">
             Hou, Ruochen
            </a>
           </td>
           <td class="r">
            UCLA
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#237912" title="Click to go to the Author Index">
             Fernandez, Gabriel Ikaika
            </a>
           </td>
           <td class="r">
            University of California Los Angeles
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#355101" title="Click to go to the Author Index">
             Zhu, Mingzhang
            </a>
           </td>
           <td class="r">
            University of California, Los Angeles
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#106488" title="Click to go to the Author Index">
             Hong, Dennis
            </a>
           </td>
           <td class="r">
            UCLA
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4091" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#collision_avoidance" title="Click to go to the Keyword Index">
               Collision Avoidance
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper we detail the methods used for obstacle avoidance, path planning, and trajectory tracking that helped us win the adult-sized, autonomous humanoid soccer league in RoboCup 2024. Our team was undefeated for all seated matches and scored 45 goals over 6 games, winning the championship game 6 to 1. During the competition, a major challenge for collision avoidance was the measurement noise coming from bipedal locomotion and a limited field of view (FOV). Furthermore, obstacles would sporadically jump in and out of our planned trajectory. At times our estimator would place our robot inside a hard constraint. Any planner in this competition must also be be computationally efficient enough to re-plan and react in real time. This motivated our approach to trajectory generation and tracking. In many scenarios long-term and short-term planning is needed. To efficiently find a long-term general path that avoids all obstacles we developed DAVG (Dynamic Augmented Visibility Graphs). DAVG focuses on essential path planning by setting certain regions to be active based on obstacles and the desired goal pose. By augmenting the states in the graph, turning angles are considered, which is crucial for a large soccer playing robot as turning may be more costly. A trajectory is formed by linearly interpolating between discrete points generated by DAVG. A modified version of model predictive control (MPC) is used to then track this trajectory called cf-MPC (Collision-Free MPC). This ensures short-term planning. Without having to switch formulations cf-MPC takes into account the robot dynamics and collision free constraints. Without a hard switch the control input can smoothly transition in cases where the noise places our robot inside a constraint boundary. The nonlinear formulation runs at approximately 120 Hz, while the quadratic version achieves around 400 Hz.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt1_06">
             15:40-15:45, Paper ThDT1.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4203'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Learning Time-Optimal Online Replanning for Distributed Model Predictive Contouring Control of Quadrotors
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424504" title="Click to go to the Author Index">
             Guan, Xin
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#354526" title="Click to go to the Author Index">
             Zhao, Fangguo
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425908" title="Click to go to the Author Index">
             Tian, Shunxin
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#230382" title="Click to go to the Author Index">
             Li, Shuo
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4203" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__mechanics_and_control" title="Click to go to the Keyword Index">
               Aerial Systems: Mechanics and Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Achieving time-optimal flight in real time for multi-drone systems presents significant challenges, particularly in scenarios requiring rapid responses or aggressive maneuvers. This paper introduces a novel framework that bridges the gap between time-optimal polynomial trajectory generation and optimal control, facilitating efficient online replanning (100 Hz onboard) for multiple quadrotors. Specifically, the proposed method leverages a neural network to learn optimal time allocations for polynomial trajectories, which are then integrated with Model Predictive Contouring Control to fully exploit the dynamics of quadrotors. We further extend this approach to multi-drone systems, enabling collaborative high-speed flight with reciprocal collision avoidance. We benchmark the time-optimal performance and computational efficiency of our method in a drone racing scenario and demonstrate its effectiveness in agile cooperative flight within more constrained simulation and real-world environments. The results demonstrate that the proposed method achieves agile waypoint traverse at a speed of up to 19 m/s in simulation and up to 9 m/s in two-drone real-world scenario.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt1_07">
             15:45-15:50, Paper ThDT1.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4785'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Predictive Control with Indirect Adaptive Laws for Payload Transportation by Quadrupedal Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#293644" title="Click to go to the Author Index">
             Amanzadeh, Leila
            </a>
           </td>
           <td class="r">
            Virginia Tech University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#358527" title="Click to go to the Author Index">
             Chunawala, Taizoon Aliasgar
            </a>
           </td>
           <td class="r">
            Virginia Polytechnic Institute and State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#278482" title="Click to go to the Author Index">
             Fawcett, Randall
            </a>
           </td>
           <td class="r">
            Virginia Polytechnic Institute and State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#136444" title="Click to go to the Author Index">
             Leonessa, Alexander
            </a>
           </td>
           <td class="r">
            Virginia Tech
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#134267" title="Click to go to the Author Index">
             Akbari Hamed, Kaveh
            </a>
           </td>
           <td class="r">
            Virginia Tech
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4785" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_control" title="Click to go to the Keyword Index">
               Motion Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multi_contact_whole_body_motion_planning_and_control" title="Click to go to the Keyword Index">
               Multi-Contact Whole-Body Motion Planning and Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper formally develops a novel hierarchical planning and control framework for robust payload transportation by quadrupedal robots, integrating a model predictive control (MPC) algorithm with a gradient-descent-based adaptive updating law. At the framework's high level, an indirect adaptive law estimates the unknown parameters of the reduced-order (template) locomotion model under varying payloads. These estimated parameters feed into an MPC algorithm for real-time trajectory planning, incorporating a convex stability criterion within the MPC constraints to ensure the stability of the template model's estimation error. The optimal reduced-order trajectories generated by the high-level adaptive MPC (AMPC) are then passed to a low-level nonlinear whole-body controller (WBC) for tracking. Extensive numerical investigations validate the framework's capabilities, showcasing the robot's proficiency in transporting unmodeled, unknown static payloads up to 109% in experiments on flat terrains and 91% on rough experimental terrains. The robot also successfully manages dynamic payloads with 73% of its mass on rough terrains. Performance comparisons with a normal MPC and an L1-MPC indicate a significant improvement. Furthermore, comprehensive hardware experiments conducted in indoor and outdoor environments confirm the method’s efficacy on rough terrains despite uncertainties such as payload variations, push disturbances, and obstacles.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt2">
             <b>
              ThDT2
             </b>
            </a>
           </td>
           <td class="r">
            301
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thdt2" title="Click to go to the Program at a Glance">
             <b>
              Learning-Based SLAM 3
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#151104" title="Click to go to the Author Index">
             Leutenegger, Stefan
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt2_01">
             15:15-15:20, Paper ThDT2.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('618'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              M3DSS: A Multi-Platform, Multi-Sensor, and Multi-Scenario Dataset for SLAM System
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#393690" title="Click to go to the Author Index">
             Huang, Shulei
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#393697" title="Click to go to the Author Index">
             Zhang, Haotian
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#393711" title="Click to go to the Author Index">
             Xu, Kang
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#393712" title="Click to go to the Author Index">
             Lv, Xianwei
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#362841" title="Click to go to the Author Index">
             Ma, Xiaoguang
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab618" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#data_sets_for_slam" title="Click to go to the Keyword Index">
               Data Sets for SLAM
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#visual_inertial_slam" title="Click to go to the Keyword Index">
               Visual-Inertial SLAM
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper proposed M3DSS, a multi-platform, multi-sensor, and multi-scenario dataset for Simultaneous Localization and Mapping (SLAM) systems. Fifty-five sequences were collected from multiple platforms, including a handheld equipment, an unmanned ground vehicle, a quadruped robot, a car, and an unmanned aerial vehicle. Sensors used in M3DSS included two pairs of stereo event cameras with resolutions of 640×480 and 346×260, one infrared camera, four RGB cameras, two visual-inertial sensors, four mechanical and one solid-state LiDARs, three inertial measurement units, two global navigation satellite and inertial navigation systems with real-time kinematic signals. 21 various sensors were used on 5 different platforms under various challenging scenarios, including extreme illumination, aggressive motion, low-texture, high-speed driving scenarios, etc. To the best of our knowledge, M3DSS offered the richest event-based sensory information for SLAM up to date. We comprehensively evaluated state-of-the-art SLAM approaches and identified their limitations on M3DSS. Details could be found at https://neufs-ma.github.io/M3DSS.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt2_02">
             15:20-15:25, Paper ThDT2.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1083'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Uncertainty-Aware Visual-Inertial SLAM with Volumetric Occupancy Mapping
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#255404" title="Click to go to the Author Index">
             Jung, Jaehyung
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#309262" title="Click to go to the Author Index">
             Boche, Simon
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#313096" title="Click to go to the Author Index">
             Barbas Laina, Sebastián
            </a>
           </td>
           <td class="r">
            TU Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#151104" title="Click to go to the Author Index">
             Leutenegger, Stefan
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1083" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#visual_inertial_slam" title="Click to go to the Keyword Index">
               Visual-Inertial SLAM
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We propose visual-inertial simultaneous localization and mapping that tightly couples sparse reprojection errors, inertial measurement unit pre-integrals, and relative pose factors with dense volumetric occupancy mapping. Hereby depth predictions from a deep neural network are fused in a fully probabilistic manner. Specifically, our method is rigorously uncertainty-aware: first, we use depth and uncertainty predictions from a deep network not only from the robot's stereo rig, but we further probabilistically fuse motion stereo that provides depth information across a range of baselines, therefore drastically increasing mapping accuracy. Next, predicted and fused depth uncertainty propagates not only into occupancy probabilities but also into alignment factors between generated dense submaps that enter the probabilistic nonlinear least squares estimator. This submap representation offers globally consistent geometry at scale. Our method is thoroughly evaluated in two benchmark datasets, resulting in localization and mapping accuracy that exceeds the state of the art, while simultaneously offering volumetric occupancy directly usable for downstream robotic planning and control in real-time.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt2_03">
             15:25-15:30, Paper ThDT2.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3893'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Real-Time 3D Reconstruction Via Camera-LIDAR (2D) Fusion for Mobile Robots: A Gaussian Splatting Approach
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#344266" title="Click to go to the Author Index">
             Sandula, Ajay Kumar
            </a>
           </td>
           <td class="r">
            Indian Institute of Science, Bengaluru
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423348" title="Click to go to the Author Index">
             Damodaran, Shriram
            </a>
           </td>
           <td class="r">
            National Institute of Technology, Jalandhar, India
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424260" title="Click to go to the Author Index">
             Nagaraj, Suhas
            </a>
           </td>
           <td class="r">
            University of Maryland, College Park
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#118356" title="Click to go to the Author Index">
             Ghose, Debasish
            </a>
           </td>
           <td class="r">
            Indian Institute of Science
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#307862" title="Click to go to the Author Index">
             Biswas, Pradipta
            </a>
           </td>
           <td class="r">
            Indian Institute of Science
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3893" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#visual_inertial_slam" title="Click to go to the Keyword Index">
               Visual-Inertial SLAM
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#sensor_fusion" title="Click to go to the Keyword Index">
               Sensor Fusion
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present a novel 3D reconstruction-based SLAM (Simultaneous Localization and Mapping) approach for robots that leverage multimodal sensory input data, including a camera and a 2D lidar. By integrating these inputs with the gaussian splatting technique, our method significantly enhances performance over traditional SLAM approaches. Traditional SLAM techniques often struggle with the limitations of monocular vision and fail to accurately map and locate objects in dynamic and cluttered environments. Purely relying on camera to localize the robot and map creation is challenging in the presence of dynamic obstacles in the scene. To address this, we proposed a multimodal sensor fusion based 3D reconstruction. Our approach employs lidar-based localization to achieve precise positioning of both the camera and the robot, while utilizing the gaussian splatting technique for robust environmental mapping and 3D reconstruction. This approach is robust to dynamic obstacles in the scene. We have conducted extensive experiments in various real-world and simulated environments, demonstrating that our method not only outperforms traditional monocular SLAM approaches but also achieves higher accuracy in terms of localization and constructed map. Our results demonstrate substantial improvements in 3D reconstruction for mobile robots, achieving reduced computational load, higher FPS and enhanced scaling accuracy
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt2_04">
             15:30-15:35, Paper ThDT2.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3941'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              DVN-SLAM: Dynamic Visual Neural SLAM Based on Local-Global Encoding
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#300087" title="Click to go to the Author Index">
             Wu, Wenhua
            </a>
           </td>
           <td class="r">
            Shang Hai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#234155" title="Click to go to the Author Index">
             Wang, Guangming
            </a>
           </td>
           <td class="r">
            University of Cambridge
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422711" title="Click to go to the Author Index">
             Deng, Ting
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#396401" title="Click to go to the Author Index">
             Aegidius, Sebastian
            </a>
           </td>
           <td class="r">
            University College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#375672" title="Click to go to the Author Index">
             Shanks, Stuart
            </a>
           </td>
           <td class="r">
            University College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#177063" title="Click to go to the Author Index">
             Modugno, Valerio
            </a>
           </td>
           <td class="r">
            University College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#147896" title="Click to go to the Author Index">
             Kanoulas, Dimitrios
            </a>
           </td>
           <td class="r">
            University College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#103003" title="Click to go to the Author Index">
             Wang, Hesheng
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3941" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Recent research on Simultaneous Localization and Mapping (SLAM) based on implicit representation has shown promising results in indoor environments. However, some challenges remain: the limited scene representation capability of implicit encoding, the uncertainty in the rendering process from implicit representations, and the disruption of consistency by dynamic objects. To address these challenges, we propose a dynamic visual SLAM system based on local-global fusion neural implicit representation, named DVN-SLAM. To improve the scene representation capability, we introduce a local-global fusion neural implicit representation that enables the construction of an implicit map while considering both global structure and local details. To tackle uncertainties arising from the rendering process, we design an information concentration loss for optimization, aiming to concentrate scene information on object surfaces. The proposed DVN-SLAM achieves competitive performance in localization and mapping across multiple datasets. More importantly, DVN-SLAM demonstrates robustness without semantic and optical flow prior in dynamic scenes, which sets it apart from other NeRF-based methods.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt2_05">
             15:35-15:40, Paper ThDT2.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4172'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Dy3DGS-SLAM: Monocular 3DGS-SLAM System for Dynamic Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#346050" title="Click to go to the Author Index">
             Li, Mingrui
            </a>
           </td>
           <td class="r">
            Dalian University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#411073" title="Click to go to the Author Index">
             Zhou, Yiming
            </a>
           </td>
           <td class="r">
            Saarland University of Applied Science
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#367769" title="Click to go to the Author Index">
             Zhou, Hongxing
            </a>
           </td>
           <td class="r">
            Beijing University of Chemical Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#290762" title="Click to go to the Author Index">
             Hu, Xinggang
            </a>
           </td>
           <td class="r">
            Dalian University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425424" title="Click to go to the Author Index">
             Roemer, Florian
            </a>
           </td>
           <td class="r">
            Fraunhofer IZFP
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#360071" title="Click to go to the Author Index">
             Wang, Hongyu
            </a>
           </td>
           <td class="r">
            Dalian University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425412" title="Click to go to the Author Index">
             Osman, Ahmad
            </a>
           </td>
           <td class="r">
            Htw Saar
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4172" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The current SLAM methods based on NeRF or 3DGS have shown impressive results in reconstructing ideal static 3D scenes. However, they perform poorly in tracking and reconstruction when facing more challenging dynamic environments, such as real-world scenes involving dynamic elements. Although some NeRF-based SLAM methods have attempted to address these dynamic challenges, they rely on RGB-D inputs, and there is a lack of methods that work with pure RGB input. To address these challenges, we introduce Dy3DGS-SLAM, the first 3DGS-SLAM method for dynamic scenes using monocular RGB input. For tracking, our method first acquires dynamic object masks through an optical flow estimation system, then combines them with a monocular depth estimation system to obtain merged masks and recover scale. This allows us to remove dynamic objects from non-predefined scenes, enabling dense frame-to-frame mapping. For rendering, we prune the Gaussians generated by pixels with dynamic masks, while applying a scale regularizer to avoid Gaussian artifacts. We impose additional photometric, geometric, and uncertainty losses on the proxy depth to improve rendering accuracy. Experimental results show that our method achieves state-of-the-art (SOTA) tracking and rendering results in dynamic environments, while also being competitive with or outperforming RGB-D methods.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt2_06">
             15:40-15:45, Paper ThDT2.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4799'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SGBA: Semantic Gaussian Mixture Model-Based LiDAR Bundle Adjustment
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#337824" title="Click to go to the Author Index">
             Ji, Xingyu
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#185927" title="Click to go to the Author Index">
             Yuan, Shenghai
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#375293" title="Click to go to the Author Index">
             Li, Jianping
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#276650" title="Click to go to the Author Index">
             Yin, Pengyu
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#336709" title="Click to go to the Author Index">
             Cao, Haozhi
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#115410" title="Click to go to the Author Index">
             Xie, Lihua
            </a>
           </td>
           <td class="r">
            NanyangTechnological University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4799" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             LiDAR bundle adjustment (BA) is an effective approach to reduce the drifts in pose estimation from the front-end. Existing works on LiDAR BA usually rely on predefined geometric features for landmark representation. This reliance restricts generalizability, as the system
             <p>
              will inevitably deteriorate in environments where these specific features are absent. To address this issue, we propose SGBA, a LiDAR BA
              <p>
               scheme that models the environment as a semantic Gaussian mixture model
               <p>
                (GMM) without predefined feature types. This approach encodes both geometric and semantic information, offering a comprehensive and general representation adaptable to various environments. Additionally,
                <p>
                 to limit computational complexity while ensuring generalizability, we propose an adaptive semantic selection framework that selects the most informative semantic clusters for optimization by evaluating the condition number of the cost function. Lastly, we introduce a probabilistic feature association scheme that considers the entire probability density of assignments, which can manage uncertainties in measurement and initial pose estimation. We have conducted various experiments and the results demonstrate that SGBA can achieve accurate and robust pose refinement even in challenging scenarios with low-quality initial pose estimation and limited geometric features. We plan to open source the work for the benefit of the community @ https://github.com/Ji1Xinyu/SGBA.
                </p>
               </p>
              </p>
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt2_07">
             15:45-15:50, Paper ThDT2.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4997'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              GeoRecon: Geometric Coherence for Online 3D Scene Reconstruction from Monocular Video
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#410904" title="Click to go to the Author Index">
             Wang, Yanmei
            </a>
           </td>
           <td class="r">
            Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#414104" title="Click to go to the Author Index">
             Chu, Fupeng
            </a>
           </td>
           <td class="r">
            Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#350621" title="Click to go to the Author Index">
             Han, Zhi
            </a>
           </td>
           <td class="r">
            Shenyang Institute of Automation, Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#130098" title="Click to go to the Author Index">
             Tang, Yandong
            </a>
           </td>
           <td class="r">
            Shenyang Institute of Automation, CAS
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4997" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#cognitive_modeling" title="Click to go to the Keyword Index">
               Cognitive Modeling
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Online 3D scene reconstruction from monocular video aims to incrementally recover 3D mesh from monocular RGB videos.It enables robots to accomplish tasks involving interactions with the environment.Due to the high memory consumption of 3D data,almost all existing methods adopt the coarse-to-fine architecture,in which the voxel is progressively sparsified and split across levels.However,these
             <p>
              methods overlook alignment between different levels,resulting in poor geometric properties of reconstructed scene.Furthermore,the whole framework relies on voxel features for supervision, lacking effective supervision of the image geometric features extracted by the feature extraction network.These geometric features are essential for further 3D scene reconstruction. To tackle the above problems,we propose GeoRecon,which achieves geometric coherent reconstruction through keyframe 2D representation self-regression and cross-level 3D voxel fea- ture alignment.Specifically,for 2D image space,to alleviate the lack of supervision in 2D feature extraction,an image recon- struction self-supervision regression constraint is introduced on the input 2D keyframes to ensure that the extracted features can learn accurate geometric features and further voxel features. For 3D voxel features space,to achieve consistent alignment between different levels,the high-level voxel features are used to constrain low-level voxel features,and achieve alignment from coarse (i.e.,low-level)voxel features to fine (i.e.,high-level)voxel features.With the design of these two components,the proposed method effectively reconstructs the geometric structures of the scene.The experimental results demonstrate the effectiveness of the proposed method.
             </p>
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt3">
             <b>
              ThDT3
             </b>
            </a>
           </td>
           <td class="r">
            303
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thdt3" title="Click to go to the Program at a Glance">
             <b>
              Space Robotics 2
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#103428" title="Click to go to the Author Index">
             Vidal-Calleja, Teresa A.
            </a>
           </td>
           <td class="r">
            University of Technology Sydney
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt3_01">
             15:15-15:20, Paper ThDT3.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('711'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              AstroLoc2: Fast Sequential Depth-Enhanced Localization for Free-Flying Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#233386" title="Click to go to the Author Index">
             Soussan, Ryan
            </a>
           </td>
           <td class="r">
            Aerodyne Industries
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#250846" title="Click to go to the Author Index">
             Moreira, Marina
            </a>
           </td>
           <td class="r">
            Instituto Superior Técnico, Lisbon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#137355" title="Click to go to the Author Index">
             Coltin, Brian
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#106900" title="Click to go to the Author Index">
             Smith, Trey
            </a>
           </td>
           <td class="r">
            NASA Ames Research Center
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab711" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#space_robotics_and_automation" title="Click to go to the Keyword Index">
               Space Robotics and Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present AstroLoc2, a monocular and time-of-flight (ToF) visual-inertial graph-based localizer used by the Astrobee free-flying robots on the International Space Station (ISS). AstroLoc2 sequentially performs odometry and absolute localization in a single process to decouple map noise from velocity and IMU bias estimation and run efficiently on resource constrained platforms. It improves monocular visual-inertial odometry robustness by adding ToF correspondence factors and uses adaptive map-matching to increase image registration reliability in dynamic environments while preserving fast matching in static ones. We evaluate the performance of AstroLoc2 on a public dataset of 10 ISS activities and show that it improves localization accuracy by 16% and success rates by 5.5% while maintaining a faster runtime than leading methods. AstroLoc2 has enabled the Astrobee robots to perform higher precision maneuvers in changing environments on the ISS. It can be configured for other limited computation platforms and we release the source code to the public.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt3_02">
             15:20-15:25, Paper ThDT3.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('871'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Mixing Data-Driven and Geometric Models for Satellite Docking Port State Estimation Using an RGB or Event Camera
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#218805" title="Click to go to the Author Index">
             Le Gentil, Cedric
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#338425" title="Click to go to the Author Index">
             Naylor, Jack
            </a>
           </td>
           <td class="r">
            University of Sydney
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#253022" title="Click to go to the Author Index">
             Munasinghe, Nuwan
            </a>
           </td>
           <td class="r">
            University of Technology Sydney (UTS)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#269409" title="Click to go to the Author Index">
             Mehami, Jasprabhjit
            </a>
           </td>
           <td class="r">
            University of Technology Sydney
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#323386" title="Click to go to the Author Index">
             Dai, Benny
            </a>
           </td>
           <td class="r">
            University of Technology Sydney
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419640" title="Click to go to the Author Index">
             Asavkin, Mikhail
            </a>
           </td>
           <td class="r">
            ANT61
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#141009" title="Click to go to the Author Index">
             Dansereau, Donald
            </a>
           </td>
           <td class="r">
            University of Sydney
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#103428" title="Click to go to the Author Index">
             Vidal-Calleja, Teresa A.
            </a>
           </td>
           <td class="r">
            University of Technology Sydney
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab871" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#space_robotics_and_automation" title="Click to go to the Keyword Index">
               Space Robotics and Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#visual_tracking" title="Click to go to the Keyword Index">
               Visual Tracking
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In-orbit automated servicing is a promising path towards lowering the cost of satellite operations and reducing the amount of orbital debris. For this purpose, we present a pipeline for automated satellite docking port detection and state estimation using monocular vision data from standard RGB sensing or an event camera. Rather than taking snapshots of the environment, an event camera has independent pixels that asynchronously respond to light changes, offering advantages such as high dynamic range, low power consumption and latency. This work focuses on satellite-agnostic operations (only a geometric knowledge of the actual port is required) using the recently released Lockheed Martin Mission Augmentation Port (LM-MAP) as the target. By leveraging shallow data-driven techniques to preprocess the incoming data to highlight the LM-MAP's reflective navigational aids and then using basic geometric models for state estimation, we present a lightweight and data-efficient pipeline that can be used independently with either RGB or event cameras. We demonstrate the soundness of the pipeline and perform a quantitative comparison of the two modalities based on data collected with a photometrically accurate test bench that includes a robotic arm to simulate the target satellite's uncontrolled motion. The data has been made publicly available: https://uts-ri.github.io/rgb_event_docking_port/
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt3_03">
             15:25-15:30, Paper ThDT3.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2078'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Visual Servo System for Robotic On-Orbit Servicing Based on 3D Perception of Non-Cooperative Satellite
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#371865" title="Click to go to the Author Index">
             Zhao, Panpan
            </a>
           </td>
           <td class="r">
            Shandong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#307761" title="Click to go to the Author Index">
             Jin, Li
            </a>
           </td>
           <td class="r">
            Shandong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#288434" title="Click to go to the Author Index">
             Chen, Yeheng
            </a>
           </td>
           <td class="r">
            Zhejiang Lab
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#328930" title="Click to go to the Author Index">
             Li, Jiachen
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#377286" title="Click to go to the Author Index">
             Song, Xiuqiang
            </a>
           </td>
           <td class="r">
            Shandong University, China; Engineering Research Center of Digit
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#313653" title="Click to go to the Author Index">
             Chen, Wenxuan
            </a>
           </td>
           <td class="r">
            Zhejiang Lab
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#147211" title="Click to go to the Author Index">
             Li, Nan
            </a>
           </td>
           <td class="r">
            Technology and Engineering Center for Space Utilization, Chinese
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#196151" title="Click to go to the Author Index">
             Du, Wenjuan
            </a>
           </td>
           <td class="r">
            Zhejiang Lab
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#373459" title="Click to go to the Author Index">
             Ma, Ke
            </a>
           </td>
           <td class="r">
            Zhejiang Lab
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#373458" title="Click to go to the Author Index">
             Wang, Xiaokun
            </a>
           </td>
           <td class="r">
            Zhejianglab
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#289172" title="Click to go to the Author Index">
             Li, Yuehua
            </a>
           </td>
           <td class="r">
            Zhejiang Lab
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#377031" title="Click to go to the Author Index">
             Xiangxu, Xiangxu
            </a>
           </td>
           <td class="r">
            Shandong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#313498" title="Click to go to the Author Index">
             Qin, Xueying
            </a>
           </td>
           <td class="r">
            Shandong University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2078" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#space_robotics_and_automation" title="Click to go to the Keyword Index">
               Space Robotics and Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#visual_servoing" title="Click to go to the Keyword Index">
               Visual Servoing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The 3D perception of satellites, including both their shape and pose, is a key foundation for robotic on-orbit servicing. However, the demanding space environment—such as intense and dim illumination—presents significant challenges. Previous non-cooperative methods focus on specific geometric features like solar panel brackets or docking rings, overlooking the satellite's overall shape and increasing the risk of collisions during grasping. Additionally, satellites are often weakly textured, limiting the accuracy of 3D perception. To address these issues, we propose, for the first time, a 3D perception-based visual servo system of non-cooperative satellites. This system combines reconstruction and tracking to enhance shape perception and pose estimation accuracy in orbital conditions. Specifically, we employ an alternating iterative strategy to simultaneously reconstruct and track the satellite and introduce a novel constraint to fuse different cues under extreme conditions. Further, we develop a simulation environment platform, a dual-arm microgravity grasping system, and an online monitoring module to enhance system capabilities for on-orbit servicing. Synthetic and real-world datasets from the simulation environment are also created for experimental validation. Results show that each module of our system achieves state-of-the-art performance.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt3_04">
             15:30-15:35, Paper ThDT3.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2243'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Control Strategy for an Orbital Manipulator Equipped with an External Actuator at the End-Effector
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421472" title="Click to go to the Author Index">
             Sena, Francesco
            </a>
           </td>
           <td class="r">
            German Aerospace Center (DLR)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#224062" title="Click to go to the Author Index">
             Mishra, Hrishik
            </a>
           </td>
           <td class="r">
            German Aerospace Center (DLR)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#312778" title="Click to go to the Author Index">
             Vijayan, Ria
            </a>
           </td>
           <td class="r">
            German Aerospace Center (DLR)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#172127" title="Click to go to the Author Index">
             De Stefano, Marco
            </a>
           </td>
           <td class="r">
            German Aerospace Center (DLR)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2243" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#space_robotics_and_automation" title="Click to go to the Keyword Index">
               Space Robotics and Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_control" title="Click to go to the Keyword Index">
               Motion Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#dynamics" title="Click to go to the Keyword Index">
               Dynamics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper exploits the robotic capabilities of an orbital manipulator equipped with an actuation module at its end-effector to perform close-proximity robotic operations. The proposed control strategy enables repositioning the system’s center-of-mass by reconfiguring the manipulator configuration and using the end-effector-mounted thrusting mechanism to achieve displacement. The key advantage of the proposed method is that the plume impingement due to thruster firing of the servicer satellite in close-proximity operations towards the client is mitigated. This is achieved by regulating the internal motion of the manipulator such that the thrust firing does not occur near the space asset. The effectiveness of the controller is verified through a multibody dynamic simulation of an orbital manipulator.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt3_05">
             15:35-15:40, Paper ThDT3.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3674'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Robotic Space Simulator: Controls Implementation for Auxiliary Axes and Zero-G Dynamics
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#410798" title="Click to go to the Author Index">
             Hilburn, Eddie
            </a>
           </td>
           <td class="r">
            Texas A&amp;M University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#237118" title="Click to go to the Author Index">
             Pettinger, Adam
            </a>
           </td>
           <td class="r">
            Texas A&amp;M University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#412847" title="Click to go to the Author Index">
             Wilkinson, Emily
            </a>
           </td>
           <td class="r">
            Texas A&amp;M University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425766" title="Click to go to the Author Index">
             Lansdowne, Ian
            </a>
           </td>
           <td class="r">
            Texas A&amp;M University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#337083" title="Click to go to the Author Index">
             Ambrose, Robert
            </a>
           </td>
           <td class="r">
            Texas A&amp;M University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3674" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#space_robotics_and_automation" title="Click to go to the Keyword Index">
               Space Robotics and Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#force_control" title="Click to go to the Keyword Index">
               Force Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#parallel_robots" title="Click to go to the Keyword Index">
               Parallel Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The Robotic Space Simulator was developed as a physical simulation for in-space manipulation tasks. It incorporates external inputs to its dynamics simulation via force/torque sensors mounted to the 2 6-DoF Stewart platforms which compose its primary structure. Each platform is augmented with an additional degree of freedom in the form of an auxiliary axis - one in translation and one in rotation. Previous work has not effectively included the additional workspace provided by these auxiliary axes. Additionally, it limited the use of external force/torque inputs to the case of platform translation only because the external forces/torques due to platform motion and gravitational force were not removed from the sensor inputs prior to inclusion in the dynamic simulation. In this work, we address each of these limitations. We develop and test two methods of auxiliary axis control: Cartesian Workspace and Joint Cost-Function, and find that both methods are an improvement over the existing system. Additionally we develop and test a method for calculating the mass properties of hardware mounted to the force/torque sensors and a dynamics compensation method for this hardware. Using this technique we are able to effectively compensate for gravitational force in different platform orientations, and achieve zero-g behavior of the system.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt3_06">
             15:40-15:45, Paper ThDT3.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4880'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Dynamics, Simulation &amp; Control of Orbital Modules for On-Orbit Assembly
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#224062" title="Click to go to the Author Index">
             Mishra, Hrishik
            </a>
           </td>
           <td class="r">
            German Aerospace Center (DLR)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#409194" title="Click to go to the Author Index">
             Vicariotto, Tommaso
            </a>
           </td>
           <td class="r">
            Politecnico Di Milano
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#172127" title="Click to go to the Author Index">
             De Stefano, Marco
            </a>
           </td>
           <td class="r">
            German Aerospace Center (DLR)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4880" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#space_robotics_and_automation" title="Click to go to the Keyword Index">
               Space Robotics and Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_control" title="Click to go to the Keyword Index">
               Motion Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In the context of in-orbit assembly, modular building blocks offer the advantage of distributed launches. After the orbit injection, the overall motion control requires the individual modules to approach each other while regulating their relative shape and total formation. This kind of formation control has already been addressed for rigid body modules. However, in practical cases, each module might be a multibody (with rotors) system. To address the control problem for such a fleet of fixed-inertia multibody modules, we propose a novel dynamics formulation that is inertia-decoupled, singularity-free, and invariant of their absolute poses. We extend the passive decomposition theory for deriving new representative systems corresponding to the total momentum (locked) and relative shape variations. We exploit the dynamics to design two distinct control laws with complementary mission benefits to regulate the locked and relative motions. We also leverage the proposed formulation to design a Hardware-in-the-Loop (HIL) framework, in which the facility reproduced the relative motions while total momentum was propagated in software. Furthermore, the proposed HIL framework and the motion control are experimentally validated.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt3_07">
             15:45-15:50, Paper ThDT3.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4917'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Int-Ball2: On-Orbit Demonstration of Autonomous Intravehicular Flight and Docking for Image Capturing and Recharging
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#148000" title="Click to go to the Author Index">
             Hirano, Daichi
            </a>
           </td>
           <td class="r">
            Japan Aerospace Exploration Agency
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#207702" title="Click to go to the Author Index">
             Mitani, Shinji
            </a>
           </td>
           <td class="r">
            JAXA
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#100878" title="Click to go to the Author Index">
             Watanabe, Keisuke
            </a>
           </td>
           <td class="r">
            Japan Aerospace Exploration Agency
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#337864" title="Click to go to the Author Index">
             Nishishita, Taisei
            </a>
           </td>
           <td class="r">
            Japan Aerospace Exploration Agency
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#389587" title="Click to go to the Author Index">
             Yamamoto, Tatsuya
            </a>
           </td>
           <td class="r">
            Japan Aerospace Exploration Agency (JAXA)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#217132" title="Click to go to the Author Index">
             Yamaguchi, Seiko Piotr
            </a>
           </td>
           <td class="r">
            Japan Aerospace Exploration Agency (JAXA)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4917" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#space_robotics_and_automation" title="Click to go to the Keyword Index">
               Space Robotics and Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__mechanics_and_control" title="Click to go to the Keyword Index">
               Aerial Systems: Mechanics and Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_control" title="Click to go to the Keyword Index">
               Motion Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This article presents the system architecture and the orbital demonstration results of the Int-Ball2, a free-flying camera robot developed by the Japan Aerospace Exploration Agency (JAXA). The purpose of the Int-Ball2 project is to assist astronauts and reduce their workload in the International Space Station (ISS). This robot is an upgrade from the first Int-Ball, enhancing the propulsion subsystem for greater maneuverability and adding a new docking station (DS) for autonomous battery recharging. This study performed comprehensive ground tests for autonomous maneuvering and docking, employing a combination of a fully software-based simulator,a hardware-in-the-loop (HIL) simulator, and a planar air-bearing facility. After a successful launch to the ISS, the Int-Ball2 demonstrated its ability to work in microgravity without relying on astronaut support. The results obtained from ground and orbital tests underscored the effectiveness of our system design and ground verification approach. Further, we present key technologies essential for the Int-Ball2's successful implementation on board the ISS. We expect the insights from this project to be invaluable to future missions involving free-flying robots in microgravity.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt4">
             <b>
              ThDT4
             </b>
            </a>
           </td>
           <td class="r">
            304
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thdt4" title="Click to go to the Program at a Glance">
             <b>
              Bioinspiration and Biomimetics 2
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#101240" title="Click to go to the Author Index">
             Hasegawa, Yasuhisa
            </a>
           </td>
           <td class="r">
            Nagoya University
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#149224" title="Click to go to the Author Index">
             Ozkan-Aydin, Yasemin
            </a>
           </td>
           <td class="r">
            University of Notre Dame
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt4_01">
             15:15-15:20, Paper ThDT4.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Harnessing Flagella Dynamics for Enhanced Robot Locomotion at Low Reynolds Number
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#354986" title="Click to go to the Author Index">
             Chikere, Nnamdi
            </a>
           </td>
           <td class="r">
            University of Notre Dame
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#149224" title="Click to go to the Author Index">
             Ozkan-Aydin, Yasemin
            </a>
           </td>
           <td class="r">
            University of Notre Dame
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#biologically_inspired_robots" title="Click to go to the Keyword Index">
               Biologically-Inspired Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#biomimetics" title="Click to go to the Keyword Index">
               Biomimetics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#soft_robot_applications" title="Click to go to the Keyword Index">
               Soft Robot Applications
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Navigating environments with low Reynolds numbers (Re), where viscous forces dominate, presents unique challenges, such as the need for non-reciprocal motion dynamics. Microorganisms like algae and bacteria, with their specialized structures such as asymmetrical and flexible cilia and flagella, inspire efficient propulsion in such media. However, the mechanism for enhancing the propulsion speed of these microorganisms remains not fully understood. This study introduces a quadriflagellated, algae-inspired, cable-driven robot that mirrors these biological locomotion mechanisms. A single DC motor actuates four multi-segmented flagella, modulating their stiffness throughout the propulsion cycle. We focus on enhancing propulsion speed, hypothesizing that strategic flexibility alterations in flagella—increased during the backward stroke and decreased during the forward stroke—significantly improve propulsion speed. Our experimental results confirm this, showing a marked improvement in propulsion speed, achieving a rate of 0.7+-0.11 cm/cycle. Additionally, we explore the impact of flagella length and number on propulsion, providing valuable insights for biomedical and microfluidic research applications.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt4_02">
             15:20-15:25, Paper ThDT4.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2250'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Development of Multi-Joint Biohybrid Soft Robot by Using Skeletal Muscle Tissue
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#184847" title="Click to go to the Author Index">
             Kim, Eunhye
            </a>
           </td>
           <td class="r">
            Nagoya University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#123774" title="Click to go to the Author Index">
             Takeuchi, Masaru
            </a>
           </td>
           <td class="r">
            Nagoya University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101240" title="Click to go to the Author Index">
             Hasegawa, Yasuhisa
            </a>
           </td>
           <td class="r">
            Nagoya University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#321011" title="Click to go to the Author Index">
             Fukuda, Toshio
            </a>
           </td>
           <td class="r">
            Nagoya University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2250" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#biological_cell_manipulation" title="Click to go to the Keyword Index">
               Biological Cell Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#micro_nano_robots" title="Click to go to the Keyword Index">
               Micro/Nano Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#soft_sensors_and_actuators" title="Click to go to the Keyword Index">
               Soft Sensors and Actuators
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Various forms of biohybrid robots have been developed; however, creating robots with multiple degrees of freedom remains a challenging task. In this paper, we developed a multi-joint biohybrid robot by using skeletal muscle tissue. To achieve this, we first developed a modular bio-actuator actuated by skeletal muscle tissues. The objective of this study was to enhance the contraction force of the actuator and establish optimal experimental conditions for creating high-performance robots. By applying continuous electrical stimulation for five days during culture of bio-actuator, we were able to increase the contraction force by more than threefold. Additionally, we determined the appropriate electric field based on the electrode distance, which enabled us to establish an optimal experimental setup. We also confirmed that connecting the actuators in series can significantly increase the moving distance. Connecting two actuators in series resulted in a total movement distance equivalent to the sum of the distances of each actuator. This finding suggests the potential to create robots with a larger operational workspace. Using these actuators, we first constructed a manipulator with a rotational joint. This research is expected to contribute not only to the development of various robots utilizing bio-actuators but also to advancements in biology technology.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt4_03">
             15:25-15:30, Paper ThDT4.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2601'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Novel Underwater Robot with Carangiform Locomotion Achieved Via Single Degree of Actuation and Magnetically Transmitted Traveling Wave
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#321018" title="Click to go to the Author Index">
             Manduca, Gianluca
            </a>
           </td>
           <td class="r">
            Scuola Superiore Sant'Anna
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#382092" title="Click to go to the Author Index">
             Luca, Padovani
            </a>
           </td>
           <td class="r">
            Sapienza
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#179669" title="Click to go to the Author Index">
             Santaera, Gaspare
            </a>
           </td>
           <td class="r">
            Sant'Anna School of Advanced Studies
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#396874" title="Click to go to the Author Index">
             Graziani, Giorgio
            </a>
           </td>
           <td class="r">
            Sapienza University, Rome
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101839" title="Click to go to the Author Index">
             Dario, Paolo
            </a>
           </td>
           <td class="r">
            Scuola Superiore Sant'Anna
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#191962" title="Click to go to the Author Index">
             Romano, Donato
            </a>
           </td>
           <td class="r">
            Scuola Superiore Sant’Anna
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101894" title="Click to go to the Author Index">
             Stefanini, Cesare
            </a>
           </td>
           <td class="r">
            Scuola Superiore Sant'Anna
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2601" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#biologically_inspired_robots" title="Click to go to the Keyword Index">
               Biologically-Inspired Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#marine_robotics" title="Click to go to the Keyword Index">
               Marine Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mechanism_design" title="Click to go to the Keyword Index">
               Mechanism Design
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The phenomenon of the “traveling wave,” commonly observed in various organisms, involves a wave that propagates along the body, serving as a locomotion mechanism. Particularly, in aquatic environments, organisms such as fish and cetaceans utilize traveling waves to propel themselves through water, minimizing fluid drag and maximizing movement efficiency. Inspired by nature, robotics has extensively explored replicating such locomotion strategies. This work presents a fish robot with an innovative magnetic transmission system. The mechanism transforms the unidirectional rotation of a single motor into an oscillatory, phase-shifted movement across the modules of the kinematic chain, generating a traveling wave along the body. The robot’s design and functionality are detailed, highlighting advancements in bio-inspired robotics for underwater applications, such as efficient and non-invasive monitoring and exploration of marine ecosystems. The fish robot achieved a swimming speed of approximately 2 body lengths per second (BL/s) with a tail-beat frequency of 3.24 Hz and a minimum Cost of Transport (CoT) of 5.33 J/(kg·m). Biomimetic robotics can play a key role in sustainable aquafarming, biodiversity conservation, and animal-robot interaction research, offering the potential to minimize ecosystem disruption and advance marine science.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt4_04">
             15:30-15:35, Paper ThDT4.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2629'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              AquaMILR: Mechanical Intelligence Simplifies Control of Undulatory Robots in Cluttered Fluid Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#274915" title="Click to go to the Author Index">
             Wang, Tianyu
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#410532" title="Click to go to the Author Index">
             Mankame, Nishanth
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#379438" title="Click to go to the Author Index">
             Fernandez, Matthew
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#367987" title="Click to go to the Author Index">
             Kojouharov, Velin
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#142549" title="Click to go to the Author Index">
             Goldman, Daniel
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2629" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#biologically_inspired_robots" title="Click to go to the Keyword Index">
               Biologically-Inspired Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#redundant_robots" title="Click to go to the Keyword Index">
               Redundant Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#search_and_rescue_robots" title="Click to go to the Keyword Index">
               Search and Rescue Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             While undulatory swimming of elongate limbless robots has been extensively studied in open hydrodynamic environments, less research has been focused on limbless locomotion in complex, cluttered aquatic environments. Motivated by the concept of mechanical intelligence, where controls for obstacle navigation can be offloaded to passive body mechanics in terrestrial limbless locomotion, we hypothesize that principles of mechanical intelligence can be extended to cluttered hydrodynamic regimes. To test this, we developed an untethered limbless robot capable of undulatory swimming on water surfaces, utilizing a bilateral cable-driven mechanism inspired by organismal muscle actuation morphology to achieve programmable anisotropic body compliance. We demonstrated through robophysical experiments that, similar to terrestrial locomotion, an appropriate level of body compliance can facilitate emergent swim through complex hydrodynamic environments under pure open-loop control. Moreover, we found that swimming performance depends on undulation frequency, with effective locomotion achieved only within a specific frequency range. This contrasts with highly damped terrestrial regimes, where inertial effects can often be neglected. Further, to enhance performance and address the challenges posed by nondeterministic obstacle distributions, we incorporated computational intelligence by developing a real-time body compliance tuning controller based on cable tension feedback. This controller improves the robot's robustness and overall speed in heterogeneous hydrodynamic environments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt4_05">
             15:35-15:40, Paper ThDT4.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3415'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Ambient Flow Perception of Freely Swimming Robotic Fish Using an Artificial Lateral Line System
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425271" title="Click to go to the Author Index">
             Dai, Hongru
            </a>
           </td>
           <td class="r">
            Shanghaitech University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#352055" title="Click to go to the Author Index">
             Lin, Xiaozhu
            </a>
           </td>
           <td class="r">
            ShanghaiTech University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425270" title="Click to go to the Author Index">
             Chao, Kaitian
            </a>
           </td>
           <td class="r">
            ShanghaiTech University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#352058" title="Click to go to the Author Index">
             Wang, Yang
            </a>
           </td>
           <td class="r">
            Shanghaitech University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3415" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#biologically_inspired_robots" title="Click to go to the Keyword Index">
               Biologically-Inspired Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#bioinspired_robot_learning" title="Click to go to the Keyword Index">
               Bioinspired Robot Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#marine_robotics" title="Click to go to the Keyword Index">
               Marine Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robotic fish hold significant promise as efficient underwater systems, yet their inability to accurately perceive ambient flow hinders their deployment in real-world scenarios. Inspired by the natural lateral line system(LLS), a flowresponsive organ in fish that plays a crucial role in behaviors such as rheotaxis, this paper introduces the first Artificial Lateral Line System (ALLS)-based ambient flow classifier for robotic fish that allows robotic fish to perceive flow fields while swimming freely. To be specific, using just 5 pressure sensors and 3.5 minutes of swimming data, we trained a Long Short-Term Memory (LSTM) network, achieving a classification accuracy of 81.25% across 8 flow speed categories, ranging from 0.08 m/s to 0.18 m/s. A key innovation of this work is the formulation of ambient flow perception as a classification task, which not only enables the robotic fish to extract meaningful information but also enhances the robustness and generalizability of the perception framework. Extensive experiments further identify critical factors such as affecting the effectiveness of the ambient flow classifier, offering valuable insights for future development.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt4_06">
             15:40-15:45, Paper ThDT4.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3684'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Leader-Follower Formation Enabled by Pressure Sensing in Free-Swimming Undulatory Robotic Fish
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#336469" title="Click to go to the Author Index">
             Panta, Kundan
            </a>
           </td>
           <td class="r">
            The Pennsylvania State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#291017" title="Click to go to the Author Index">
             Deng, Hankun
            </a>
           </td>
           <td class="r">
            Penn State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#415653" title="Click to go to the Author Index">
             DeLattre, Micah
            </a>
           </td>
           <td class="r">
            Penn State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#143624" title="Click to go to the Author Index">
             Cheng, Bo
            </a>
           </td>
           <td class="r">
            Pennsylvania State University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3684" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#biologically_inspired_robots" title="Click to go to the Keyword Index">
               Biologically-Inspired Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#marine_robotics" title="Click to go to the Keyword Index">
               Marine Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Fish use their lateral lines to sense flows and pressure gradients, enabling them to detect nearby objects and organisms. Towards replicating this capability, we demonstrated successful leader-follower formation swimming using flow pressure sensing in our undulatory robotic fish (µBot/MUBot). The follower µBot is equipped at its head with bilateral pressure sensors to detect signals excited by both its own and the leader's movements. First, using experiments with static formations between an undulating leader and a stationary follower, we determined the formation that resulted in strong pressure variations measured by the follower. This formation was then selected as the desired formation in free swimming for obtaining an expert policy. Next, a long short-term memory neural network was used as the control policy that maps the pressure signals along with the robot motor commands and the Euler angles (measured by the onboard IMU) to the steering command. The policy was trained to imitate the expert policy using behavior cloning and Dataset Aggregation (DAgger). The results show that with merely two bilateral pressure sensors and less than one hour of training data, the follower effectively tracked the leader within distances of up to 200 mm (= 1 body length) while swimming at speeds of 155 mm/s (= 0.8 body lengths/s). This work highlights the potential of fish-inspired robots to effectively navigate fluid environments and achieve formation swimming through the use of flow pressure feedback.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt4_07">
             15:45-15:50, Paper ThDT4.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4272'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Analysis of Kinematics and Propulsion of a Self-Sensing Multi-DoF Undulating Soft Robotic Fish
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#239605" title="Click to go to the Author Index">
             Park, Myungsun
            </a>
           </td>
           <td class="r">
            University of California San Diego
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#383383" title="Click to go to the Author Index">
             Cervera Torralba, Jacobo
            </a>
           </td>
           <td class="r">
            University of California, San Diego
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#270634" title="Click to go to the Author Index">
             Adibnazari, Iman
            </a>
           </td>
           <td class="r">
            University of California, San Deigo
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#451028" title="Click to go to the Author Index">
             Pawlak, Geno
            </a>
           </td>
           <td class="r">
            UC San Diego
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#130599" title="Click to go to the Author Index">
             Tolley, Michael T.
            </a>
           </td>
           <td class="r">
            University of California, San Diego
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4272" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#biologically_inspired_robots" title="Click to go to the Keyword Index">
               Biologically-Inspired Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#soft_robot_applications" title="Click to go to the Keyword Index">
               Soft Robot Applications
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#marine_robotics" title="Click to go to the Keyword Index">
               Marine Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper we explore kinematics ranging from anguilliform to thunniform achieved in a self-sensing multi-degree-of-freedom soft robotic fish and analyze the effect of them on the swimming. First, we examine the characteristics of the bending actuators of the robotic fish. Then, we express the kinematics of the fish as a propagating wave parameterized by three bending amplitudes and a wavelength, which are determined by the flow rates and phase shift of the pumps. We capture various motion patterns generated by different actuator inputs and directly measure the thrust generated by each pattern. We observe that the robotic swimmer can reproduce two different modes of propulsion, that are embodied by two distinct morphological patterns in nature: anguilliform and thunniform. When neither of modes are activated, propulsion is zero or even negative. Finally, we estimate the stationary swimming speed by towing the undulating fish, which satisfies the slip condition (with the speed of the body wave matching the swimming velocity). The analysis of a wide range of kinematic patterns in this study, including two extreme cases of anguilliform and thunniform modes, will provide insights for comprehensive understanding the mechanics of efficient swimming.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt5">
             <b>
              ThDT5
             </b>
            </a>
           </td>
           <td class="r">
            305
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thdt5" title="Click to go to the Program at a Glance">
             <b>
              Model Predictive Control for Legged Robots 2
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#131965" title="Click to go to the Author Index">
             Wensing, Patrick M.
            </a>
           </td>
           <td class="r">
            University of Notre Dame
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#138298" title="Click to go to the Author Index">
             Park, Hae-Won
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt5_01">
             15:15-15:20, Paper ThDT5.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('203'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Model Predictive Parkour Control of a Monoped Hopper in Dynamically Changing Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#404316" title="Click to go to the Author Index">
             Albracht, Maximilian
            </a>
           </td>
           <td class="r">
            German Aerospace Center (DLR)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#198353" title="Click to go to the Author Index">
             Kumar, Shivesh
            </a>
           </td>
           <td class="r">
            DFKI GmbH
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#311333" title="Click to go to the Author Index">
             Vyas, Shubham
            </a>
           </td>
           <td class="r">
            Robotics Innovation Center, DFKI GmbH
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#109056" title="Click to go to the Author Index">
             Kirchner, Frank
            </a>
           </td>
           <td class="r">
            University of Bremen
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab203" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#underactuated_robots" title="Click to go to the Keyword Index">
               Underactuated Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             A great advantage of legged robots is their ability to operate on particularly difficult and obstructed terrain, which demands dynamic, robust, and precise movements. The study of obstacle courses provides invaluable insights into the challenges legged robots face, offering a controlled environment to assess and enhance their capabilities. Traversing it with a one-legged hopper introduces intricate challenges, such as planning over contacts and dealing with flight phases, which necessitates a sophisticated controller. A novel model predictive parkour controller is introduced, that finds an optimal path through a real-time changing obstacle course with mixed integer motion planning. The execution of this optimized path is then achieved through a state machine employing a PD control scheme with feedforward torques, ensuring robust and accurate performance.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt5_02">
             15:20-15:25, Paper ThDT5.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('952'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Humanoid Walking Stabilization Via Model Predictive Control with Step Adjustment Based on the 3D Divergent Component of Motion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#318912" title="Click to go to the Author Index">
             Park, Gyeongjae
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#255236" title="Click to go to the Author Index">
             Kim, Myeong-Ju
            </a>
           </td>
           <td class="r">
            Hyundai Motor Company
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#380528" title="Click to go to the Author Index">
             Lee, Kwanwoo
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101200" title="Click to go to the Author Index">
             Park, Jaeheung
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab952" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#humanoid_and_bipedal_locomotion" title="Click to go to the Keyword Index">
               Humanoid and Bipedal Locomotion
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#body_balancing" title="Click to go to the Keyword Index">
               Body Balancing
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper, as an approach to stabilize humanoid walking where the height of CoM varies, a Novel Model Predictive Control framework based on three dimensional Divergent Component of Motion (3D-DCM) is proposed. To ensure the feasible utilization of contact forces for maintaining humanoid balance, constraints on the control inputs, Virtual Repellent Point (VRP) and footstep adjustment, and their correlation are analytically formulated into a quadratic form, resulting a Quadratically Constrained Quadratic Programming. Additionally, to enable the humanoid robot to withstand disturbances over a broader range of strides or safely navigates various terrains without encountering knee stretch, the distance between the CoM and the foot is constrained in the 3D-CoM trajectory planner. The effectiveness of the proposed method is validated through simulations and real-robot experiments in scenarios involving external disturbances and step down motions.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt5_03">
             15:25-15:30, Paper ThDT5.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2278'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              MPC-QP-Based Control Framework for Compliant Behavior of Humanoid Robots in Physical Collaboration with Humans
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423994" title="Click to go to the Author Index">
             Kumbhar, Shubham
            </a>
           </td>
           <td class="r">
            University of Delaware
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#102240" title="Click to go to the Author Index">
             Artemiadis, Panagiotis
            </a>
           </td>
           <td class="r">
            University of Delaware
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2278" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present a control framework specifically for physical human-humanoid collaboration involving the transportation and manipulation of heavy objects. Using this framework, the humanoid can exhibit desired levels of compliance with the object to be co-transported. This desired compliance is achieved through an admittance model. A Model Predictive Control (MPC) problem, based on a novel Interaction Linear Inverted Pendulum (I-LIP) model, generates footstep patterns that facilitate this desired compliant behavior while keeping the robot stable. Subsequently, we have an object-informed low-level quadratic program (QP) that sends control input to realize the high-level plans on the robot. The stiffness parameters of the I-LIP are modulated in real time for better compliance tracking performance of the robot. We verify all the results through simulation on the humanoid platform, the Digit, showing the prowess of the framework in collaboratively transporting heavy objects with a human.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt5_04">
             15:30-15:35, Paper ThDT5.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2966'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Real-Time Whole-Body Control of Legged Robots with Model-Predictive Path Integral Control
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#395028" title="Click to go to the Author Index">
             Alvarez Padilla, Juan Rodolfo
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#346357" title="Click to go to the Author Index">
             Zhang, John
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424575" title="Click to go to the Author Index">
             Kwok, Sofia
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104857" title="Click to go to the Author Index">
             Dolan, John M.
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#382264" title="Click to go to the Author Index">
             Manchester, Zachary
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2966" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multi_contact_whole_body_motion_planning_and_control" title="Click to go to the Keyword Index">
               Multi-Contact Whole-Body Motion Planning and Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_control" title="Click to go to the Keyword Index">
               Motion Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a system for enabling real-time synthesis of whole-body locomotion and manipulation policies for real-world legged robots. Motivated by recent advancements in robot simulation, we leverage the efficient parallelization capabilities of the MuJoCo simulator on a multi-core CPU to achieve fast sampling over the robot state and action trajectories. Our results show surprisingly effective real-world locomotion and manipulation capabilities with a very simple control strategy. We demonstrate our approach on several hardware and simulation experiments: robust locomotion over flat and uneven terrains, climbing over a box whose height is comparable to the robot, and pushing a box to a goal position. To our knowledge, this is the first successful deployment of whole-body sampling-based MPC on real-world legged robot hardware.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt5_05">
             15:35-15:40, Paper ThDT5.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3460'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Wallbounce: Push Wall to Navigate with Contact-Implicit MPC
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#310587" title="Click to go to the Author Index">
             Liu, Xiaohan
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#322941" title="Click to go to the Author Index">
             Dai, Cunxi
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#346357" title="Click to go to the Author Index">
             Zhang, John
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#346358" title="Click to go to the Author Index">
             Bishop, Arun
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#205543" title="Click to go to the Author Index">
             Manchester, Zachary
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#100085" title="Click to go to the Author Index">
             Hollis, Ralph
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3460" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#multi_contact_whole_body_motion_planning_and_control" title="Click to go to the Keyword Index">
               Multi-Contact Whole-Body Motion Planning and Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#body_balancing" title="Click to go to the Keyword Index">
               Body Balancing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this work, we introduce a framework that enables highly maneuverable locomotion using non-periodic contacts. This task is challenging for traditional optimization and planning methods to handle due to difficulties in specifying contact mode sequences in real-time. To address this, we use a bi-level contact-implicit planner and hybrid model predictive controller to draft and execute a motion plan. We investigate how this method allows us to plan arm contact events on the shmoobot, a smaller ballbot, which uses an inverse mouse-ball drive to achieve dynamic balancing with a low number of actuators. Through multiple experiments we show how the arms allow for acceleration, deceleration and dynamic obstacle avoidance that are not achievable with the mouse-ball drive alone. This demonstrates how a holistic approach to locomotion can increase the control authority of unique robot morpohologies without additional hardware by leveraging robot arms that are typically used only for manipulation. Project website: https://cmushmoobot.github.io/Wallbounce
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt5_06">
             15:40-15:45, Paper ThDT5.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4163'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Reduced-Order Model Guided Contact-Implicit Model Predictive Control for Humanoid Locomotion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#372429" title="Click to go to the Author Index">
             Esteban, Sergio
            </a>
           </td>
           <td class="r">
            California Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#205664" title="Click to go to the Author Index">
             Kurtz, Vincent
            </a>
           </td>
           <td class="r">
            California Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#381172" title="Click to go to the Author Index">
             Ghansah, Adrian
            </a>
           </td>
           <td class="r">
            California Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#134049" title="Click to go to the Author Index">
             Ames, Aaron
            </a>
           </td>
           <td class="r">
            Caltech
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4163" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#multi_contact_whole_body_motion_planning_and_control" title="Click to go to the Keyword Index">
               Multi-Contact Whole-Body Motion Planning and Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#whole_body_motion_planning_and_control" title="Click to go to the Keyword Index">
               Whole-Body Motion Planning and Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#humanoid_and_bipedal_locomotion" title="Click to go to the Keyword Index">
               Humanoid and Bipedal Locomotion
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Humanoid robots have great potential for real-world applications due to their ability to operate in environments built for humans, but their deployment is hindered by the challenge of controlling their underlying high-dimensional nonlinear hybrid dynamics. While reduced-order models like the Hybrid Linear Inverted Pendulum (HLIP) are simple and computationally efficient, they lose whole-body expressiveness. Meanwhile, recent advances in Contact-Implicit Model Predictive Control (CI-MPC) enable robots to plan through multiple hybrid contact modes, but remain vulnerable to local minima and require significant tuning. We propose a control framework that combines the strengths of HLIP and CI-MPC. The reduced-order model generates a nominal gait, while CI-MPC manages the whole-body dynamics and modifies the contact schedule as needed. We demonstrate the effectiveness of this approach in simulation with a novel 24 degree-of-freedom humanoid robot: Achilles. Our proposed framework achieves rough terrain walking, disturbance recovery, robustness under model and state uncertainty, and allows the robot to interact with obstacles in the environment, all while running online in real-time at 50 Hz.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt5_08">
             15:45-15:50, Paper ThDT5.8
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4891'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              CAFE-MPC: A Cascaded-Fidelity Model Predictive Control Framework with Tuning-Free Whole-Body Control
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#275637" title="Click to go to the Author Index">
             Li, He
            </a>
           </td>
           <td class="r">
            University of Notre Dame
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#131965" title="Click to go to the Author Index">
             Wensing, Patrick M.
            </a>
           </td>
           <td class="r">
            University of Notre Dame
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4891" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#humanoid_and_bipedal_locomotion" title="Click to go to the Keyword Index">
               Humanoid and Bipedal Locomotion
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#whole_body_control" title="Click to go to the Keyword Index">
               Whole-Body Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This work introduces an optimization-based locomotion control framework for on-the-fly synthesis of complex dynamic maneuvers. At the core of the proposed framework is a cascaded-fidelity model predictive controller (CAFE-MPC). CAFE-MPC strategically relaxes the planning problem along the prediction horizon (i.e., with descending model fidelity, increasingly coarse time steps, and relaxed constraints) for computational and performance gains. This problem is numerically solved with an efficient customized multiple-shooting iLQR (MS-iLQR) solver. The action-value function from CAFE-MPC is then used as the basis for a new value-function-based whole-body control (VWBC) technique that avoids additional tuning for the WBC. We show that CAFE-MPC if configured appropriately, advances the performance of whole-body MPC without necessarily increasing computational cost. Further, we show the superior performance of the proposed VWBC over the Ricatti feedback controller in terms of constraint handling. The proposed framework enables accomplishing for the first time gymnastic-style running barrel roll on the MIT Mini Cheetah.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt6">
             <b>
              ThDT6
             </b>
            </a>
           </td>
           <td class="r">
            307
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thdt6" title="Click to go to the Program at a Glance">
             <b>
              Perception for Manipulation 3
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#146275" title="Click to go to the Author Index">
             Wachs, Juan
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#101828" title="Click to go to the Author Index">
             Ogata, Tetsuya
            </a>
           </td>
           <td class="r">
            Waseda University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt6_01">
             15:15-15:20, Paper ThDT6.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('245'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Accurate Robotic Pushing Manipulation through Online Model Estimation under Uncertain Object Properties
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#379801" title="Click to go to the Author Index">
             Lee, Yongseok
            </a>
           </td>
           <td class="r">
            Pohang University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#103228" title="Click to go to the Author Index">
             Kim, Keehoon
            </a>
           </td>
           <td class="r">
            POSTECH, Pohang University of Science and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab245" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#model_learning_for_control" title="Click to go to the Keyword Index">
               Model Learning for Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#manipulation_planning" title="Click to go to the Keyword Index">
               Manipulation Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robotic pushing is a fundamental non-prehensile manipulation skill essential for handling objects that are difficult to grasp. This letter proposes a highly accurate robotic pushing framework that utilizes an online estimated model to push objects along a given nominal trajectory, despite uncertain object properties such as friction coefficients, mass distribution, and the position of the center of friction (CoF). The core concept involves estimating an optimal pushing motion model capable of representing observed local motions. A generalized form of the conventional analytical model, coupled with a moving-window Unscented Kalman Filter (UKF), serves as the online estimated model. It captures the local behavior of the pushed objects and is integrated with a model predictive control-based pushing strategy to achieve precise pushing performance. In experiments, the proposed robotic pushing framework demonstrated superior accuracy in tracking the given nominal trajectory compared to the conventional analytical model and data-driven model approaches, even when the motion model was perturbed. Additionally, the practicality of the proposed framework was showcased through a demonstration involving an autonomous robot collecting dishes, illustrating its applicability in various real-world applications.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt6_02">
             15:20-15:25, Paper ThDT6.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('513'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Exploring the Domain-Invariant Flow Representation in Vision-Based Tactile Sensors for Omni-Hardness Perception
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417566" title="Click to go to the Author Index">
             Yang, Xuewen
            </a>
           </td>
           <td class="r">
            Ocean University of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417485" title="Click to go to the Author Index">
             Wang, Nan
            </a>
           </td>
           <td class="r">
            Ocean University of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417571" title="Click to go to the Author Index">
             Gu, Jiayang
            </a>
           </td>
           <td class="r">
            Ocean University of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417572" title="Click to go to the Author Index">
             Zhang, Yugang
            </a>
           </td>
           <td class="r">
            Ocean University of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417574" title="Click to go to the Author Index">
             Wang, Guoyu
            </a>
           </td>
           <td class="r">
            Ocean University of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#112790" title="Click to go to the Author Index">
             Song, Aiguo
            </a>
           </td>
           <td class="r">
            Southeast University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab513" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#force_and_tactile_sensing" title="Click to go to the Keyword Index">
               Force and Tactile Sensing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Vision-based tactile sensors have recently gained prominence due to their superior resolution and ability to capture multi-dimensional contact information. However, even when sensors share the same sensing principle, variations in production factors can lead to differences in the color patterns of tactile signals. Unlike common vision tasks, vision-based tactile perception depends on tracking light variation in colorful signals, making it more susceptible to lighting conditions and thus more prone to domain gaps. In this paper, we propose an Omni-hardness perception framework that enables adaptation across various vision-based tactile sensors. Firstly, in-depth analyses of the factors influencing the generalization of hardness perception are presented. Furthermore, the light balance module and the force scale module are coupled to regulate network learning of generalized representations. Experimental results across multiple sensors demonstrate the transferability of learned representations. Additionally, downstream tasks in natural object perception, tumor detection, and grasping stability prediction, are proposed to evaluate the potential applications. The framework’s performance shows promise for advancing general tactile sensing and embodied tactile perception.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt6_03">
             15:25-15:30, Paper ThDT6.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1964'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Focused Blind Switching Manipulation Based on Constrained and Regional Touch States of Multi-Fingered Hand Using Deep Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#184706" title="Click to go to the Author Index">
             Funabashi, Satoshi
            </a>
           </td>
           <td class="r">
            Waseda University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#310749" title="Click to go to the Author Index">
             Hiramoto, Atsumu
            </a>
           </td>
           <td class="r">
            Waseda University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#217536" title="Click to go to the Author Index">
             Chiba, Naoya
            </a>
           </td>
           <td class="r">
            Osaka University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#108304" title="Click to go to the Author Index">
             Schmitz, Alexander
            </a>
           </td>
           <td class="r">
            Waseda University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#275238" title="Click to go to the Author Index">
             Kulkarni, Shardul
            </a>
           </td>
           <td class="r">
            Waseda University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101828" title="Click to go to the Author Index">
             Ogata, Tetsuya
            </a>
           </td>
           <td class="r">
            Waseda University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1964" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#force_and_tactile_sensing" title="Click to go to the Keyword Index">
               Force and Tactile Sensing
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multifingered_hands" title="Click to go to the Keyword Index">
               Multifingered Hands
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             To achieve a desired grasping posture (including object position and orientation), multi-finger motions need to be conducted according to the the current touch state. Specifically, when subtle changes happen during correcting the object state, not only proprioception but also tactile information from the entire hand can be beneficial. However, switching motions with high-DOFs of multiple fingers and abundant tactile information is still challenging. In this study, we propose a loss function with constraints of touch states and an attention mechanism for focusing on important modalities depending on the touch states. The policy model is AE-LSTM which consists of Autoencoder (AE) which compresses abundant tactile information and Long Short-Term Memory (LSTM) which switches the motion depending on the touch states. Motion for cap-opening was chosen as a target task which consists of subtasks of sliding an object and opening its cap. As a result, the proposed method achieved the best success rates with a variety of objects for real time cap-opening manipulation. Furthermore, we could confirm that the proposed model acquired the features of each subtask and attention on specific modalities.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt6_04">
             15:30-15:35, Paper ThDT6.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3035'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Magnetic-Actuated Vision-Based Whisker Array for Contact Perception and Grasping
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#313740" title="Click to go to the Author Index">
             Hu, Zhixian
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#146275" title="Click to go to the Author Index">
             Wachs, Juan
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#184771" title="Click to go to the Author Index">
             She, Yu
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3035" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#grippers_and_other_end_effectors" title="Click to go to the Keyword Index">
               Grippers and Other End-Effectors
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#force_and_tactile_sensing" title="Click to go to the Keyword Index">
               Force and Tactile Sensing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Tactile sensing and the manipulation of delicate objects are critical challenges in robotics. This study presents a vision-based magnetic-actuated whisker array sensor that integrates these functions. The sensor features eight whiskers arranged circularly, supported by an elastomer membrane and actuated by electromagnets and permanent magnets. A camera tracks whisker movements, enabling high-resolution tactile feedback. The sensor's performance was evaluated through object classification and grasping experiments. In the classification experiment, the sensor approached objects from four directions and accurately identified five distinct objects with a classification accuracy of 99.17% using a Multi-Layer Perceptron model. In the grasping experiment, the sensor tested configurations of eight, four, and two whiskers, achieving the highest success rate of 87% with eight whiskers. These results highlight the sensor's potential for precise tactile sensing and reliable manipulation.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt6_05">
             15:35-15:40, Paper ThDT6.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3843'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              GAPartManip: A Large-Scale Part-Centric Dataset for Material-Agnostic Articulated Object Manipulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#403558" title="Click to go to the Author Index">
             Cui, Wenbo
            </a>
           </td>
           <td class="r">
            Institute of Automation, Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#378747" title="Click to go to the Author Index">
             Zhao, Chengyang
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#306414" title="Click to go to the Author Index">
             Wei, Songlin
            </a>
           </td>
           <td class="r">
            Soochow University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#331922" title="Click to go to the Author Index">
             Zhang, Jiazhao
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#341712" title="Click to go to the Author Index">
             Geng, Haoran
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#267019" title="Click to go to the Author Index">
             Chen, Yaran
            </a>
           </td>
           <td class="r">
            Institute of Automation, Chinese Academy of Sciense
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#267037" title="Click to go to the Author Index">
             Li, Haoran
            </a>
           </td>
           <td class="r">
            Institute of Automation, Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#155911" title="Click to go to the Author Index">
             Wang, He
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3843" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Effectively manipulating articulated objects in household scenarios is a crucial step toward achieving general embodied artificial intelligence. Mainstream research in 3D vision has primarily focused on manipulation through depth perception and pose detection. However, in real-world environments, these methods often face challenges due to imperfect depth perception, such as with transparent lids and reflective handles. Moreover, they generally lack the diversity in part-based interactions required for flexible and adaptable manipulation. To address these challenges, we introduced a large-scale part-centric dataset for articulated object manipulation that features both photo-realistic material randomizations and detailed annotations of part-oriented, scene-level actionable interaction poses. We evaluated the effectiveness of our dataset by integrating it with several state-of-the-art methods for depth estimation and interaction pose prediction. Additionally, we proposed a novel modular framework that delivers superior and robust performance for generalizable articulated object manipulation. Our extensive experiments demonstrate that our dataset significantly improves the performance of depth perception and actionable interaction pose prediction in both simulation and real-world scenarios. More information and demos can be found at: https://pku-epic.github.io/GAPartManip/.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt6_06">
             15:40-15:45, Paper ThDT6.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4140'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              High-Precision Object Pose Estimation Using Visual-Tactile Information for Dynamic Interactions in Robotic Grasping
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423352" title="Click to go to the Author Index">
             Peng, Zicai
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#377622" title="Click to go to the Author Index">
             Cui, Te
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#301096" title="Click to go to the Author Index">
             Chen, Guangyan
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#397219" title="Click to go to the Author Index">
             Lu, Haoyang
            </a>
           </td>
           <td class="r">
            Beijing Institute of Techonology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#128868" title="Click to go to the Author Index">
             Yang, Yi
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#205136" title="Click to go to the Author Index">
             Yue, Yufeng
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4140" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#force_and_tactile_sensing" title="Click to go to the Keyword Index">
               Force and Tactile Sensing
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#manipulation_planning" title="Click to go to the Keyword Index">
               Manipulation Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#grasping" title="Click to go to the Keyword Index">
               Grasping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In various robotic applications, understanding accurate object poses for robots is essential for high-precision tasks such as factory assembly or daily insertions. Tactile sensing, which compensates for visual information, offers rich texture-based or force-based data for object pose estimation. However, previous methods for pose estimation typically overlook dynamic situations, such as slippage of grasped objects or movement of contacted objects during interactions with the environment, thus increasing the complexity of pose estimation. To address these challenges, we propose an efficient method that utilizes visual and tactile sensing to estimate object poses through particle filtering. We leverage visual information to track the pose of the contacted object in real-time and estimate the pose changes of the grasped object using displacement data obtained from tactile sensors. Our experimental evaluation on 13 objects with diverse geometric shapes demonstrated the ability to estimate high-precision poses, which revealed the robot's powerful ability to cope with dynamic scenes for compelled motion of objects, proving our framework's adaptability in practical scenarios with uncertainty.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt6_07">
             15:45-15:50, Paper ThDT6.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4826'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Object-Aware Impedance Control for Human-Robot Collaborative Task with Online Object Parameter Estimation (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#148185" title="Click to go to the Author Index">
             Park, Jinseong
            </a>
           </td>
           <td class="r">
            Korea Institute of Machinery and Materials
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#219090" title="Click to go to the Author Index">
             Shin, Young-Sik
            </a>
           </td>
           <td class="r">
            KIMM
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#160839" title="Click to go to the Author Index">
             Kim, Sanghyun
            </a>
           </td>
           <td class="r">
            Kyung Hee University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4826" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#physical_human_robot_interaction" title="Click to go to the Keyword Index">
               Physical Human-Robot Interaction
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#compliance_and_impedance_control" title="Click to go to the Keyword Index">
               Compliance and Impedance Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Physical human-robot interactions (pHRIs) can improve robot autonomy and reduce physical demands on humans. In this paper, we consider a collaborative task with a considerably long object and no prior knowledge of the object's parameters. An integrated control framework with an online object parameter estimator and a Cartesian object-aware impedance controller is proposed to realize complicated scenarios. During the transportation task, the object parameters are estimated online while a robot and human keep lifting an object. The perturbation motion is incorporated into the null space of the desired trajectory to enhance the estimator precision. An object-aware impedance controller is designed by incorporating the real-time estimation results to effectively transmit the intended human motion to the robot through the object. Experimental demonstrations of collaborative tasks, including object transportation and assembly, are implemented to show the effectiveness of our proposed method. The proposed controller was also compared to a conventional impedance controller through subjective testing and found to be more sensitive, requiring less human effort.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt7">
             <b>
              ThDT7
             </b>
            </a>
           </td>
           <td class="r">
            309
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thdt7" title="Click to go to the Program at a Glance">
             <b>
              Navigation Planning
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#180315" title="Click to go to the Author Index">
             Andersson, Olov
            </a>
           </td>
           <td class="r">
            KTH Royal Institute of Technology
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt7_01">
             15:15-15:20, Paper ThDT7.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1305'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SARO: Space-Aware Robot System for Terrain Crossing Via Vision-Language Model
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#414070" title="Click to go to the Author Index">
             Zhu, Shaoting
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#392559" title="Click to go to the Author Index">
             Li, Derun
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416173" title="Click to go to the Author Index">
             Mou, Linzhan
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#122966" title="Click to go to the Author Index">
             Liu, Yong
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#392856" title="Click to go to the Author Index">
             Xu, Ningyi
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#207381" title="Click to go to the Author Index">
             Zhao, Hang
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1305" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#autonomous_agents" title="Click to go to the Keyword Index">
               Autonomous Agents
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The application of vision-language models (VLMs) has achieved impressive success in various robotics tasks. However, there are few explorations for foundation models used in quadruped robot navigation through terrains in 3D environments. We introduce SARO (Space-Aware Robot System for Terrain Crossing), an innovative system composed of a high-level reasoning module, a closed-loop sub-task execution module, and a low-level control policy. It enables the robot to navigate across 3D terrains and reach the goal position. For high-level reasoning and execution, we propose a novel algorithmic system taking advantage of a VLM, with a design of task decomposition and a closed-loop sub-task execution mechanism. For low-level locomotion control, we utilize the Probability Annealing Selection (PAS) method to effectively train a control policy by reinforcement learning. Numerous experiments show that our whole system can accurately and robustly navigate across several 3D terrains, and its generalization ability ensures the applications in diverse indoor and outdoor scenarios and terrains. Appendix and Videos can be found in project page: https://saro-vlm.github.io/
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt7_02">
             15:20-15:25, Paper ThDT7.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1651'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Lab2Car: A Versatile Wrapper for Deploying Experimental Planners in Complex Real-World Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421735" title="Click to go to the Author Index">
             Heim, Marc
            </a>
           </td>
           <td class="r">
            Motional AD
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#149123" title="Click to go to the Author Index">
             Suárez-Ruiz, Francisco
            </a>
           </td>
           <td class="r">
            Motional Inc
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421670" title="Click to go to the Author Index">
             Bhuiyan, Ishraq
            </a>
           </td>
           <td class="r">
            Motional
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#211223" title="Click to go to the Author Index">
             Brito, Bruno
            </a>
           </td>
           <td class="r">
            TU Delft
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#337170" title="Click to go to the Author Index">
             Tomov, Momchil
            </a>
           </td>
           <td class="r">
            Motional
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1651" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#autonomous_agents" title="Click to go to the Keyword Index">
               Autonomous Agents
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#machine_learning_for_robot_control" title="Click to go to the Keyword Index">
               Machine Learning for Robot Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Human-level autonomous driving is an ever-elusive goal, with planning and decision making -- the cognitive functions that determine driving behavior -- posing the greatest challenge. Despite a proliferation of promising approaches, progress is stifled by the difficulty of deploying experimental planners in naturalistic settings. In this work, we propose Lab2Car, an optimization-based wrapper that can take a trajectory sketch from an arbitrary motion planner and convert it to a safe, comfortable, dynamically feasible trajectory that the car can follow. This allows motion planners that do not provide such guarantees to be safely tested and optimized in real-world environments. We demonstrate the versatility of Lab2Car by using it to deploy a machine learning (ML) planner and a classical planner on self-driving cars in Las Vegas. The resulting systems handle challenging scenarios, such as cut-ins, overtaking, and yielding, in complex urban environments like casino pick-up/drop-off areas. Our work paves the way for quickly deploying and evaluating candidate motion planners in realistic settings, ensuring rapid iteration and accelerating progress towards human-level autonomy.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt7_03">
             15:25-15:30, Paper ThDT7.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2387'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              One Map to Find Them All: Real-Time Open-Vocabulary Mapping for Zero-Shot Multi-Object Navigation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#354439" title="Click to go to the Author Index">
             Busch, Finn Lukas
            </a>
           </td>
           <td class="r">
            KTH Royal Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#182647" title="Click to go to the Author Index">
             Homberger, Timon
            </a>
           </td>
           <td class="r">
            KTH Royal Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422070" title="Click to go to the Author Index">
             Ortega Peimbert, Jesús Gerardo
            </a>
           </td>
           <td class="r">
            KTH Royal Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#283805" title="Click to go to the Author Index">
             Yang, Quantao
            </a>
           </td>
           <td class="r">
            KTH Royal Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#180315" title="Click to go to the Author Index">
             Andersson, Olov
            </a>
           </td>
           <td class="r">
            KTH Royal Institute
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2387" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#autonomous_agents" title="Click to go to the Keyword Index">
               Autonomous Agents
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The capability to efficiently search for objects in complex environments is fundamental for many real-world robot applications. Recent advances in open-vocabulary vision models have resulted in semantically-informed object navigation methods that allow a robot to search for an arbitrary object without prior training. However, these zero-shot methods have so far treated the environment as unknown for each consecutive query. In this paper we introduce a new benchmark for zero-shot multi-object navigation, allowing the robot to leverage information gathered from previous searches to more efficiently find new objects. To address this problem we build a reusable open-vocabulary feature map tailored for real-time object search. We further propose a probabilistic-semantic map update that mitigates common sources of errors in semantic feature extraction and leverage this semantic uncertainty for informed multi-object exploration. We evaluate our method on a set of object navigation tasks in both simulation as well as with a real robot, running in real-time on a Jetson Orin AGX. We demonstrate that it outperforms existing state-of-the-art approaches both on single and multi-object navigation tasks. Additional videos, code and the multi-object navigation benchmark will be available on https://finnbsch.github.io/OneMap.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt7_04">
             15:30-15:35, Paper ThDT7.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2533'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Exploring Adversarial Obstacle Attacks in Search-Based Path Planning for Autonomous Mobile Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419430" title="Click to go to the Author Index">
             Szvoren, Adrian
            </a>
           </td>
           <td class="r">
            University College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#365173" title="Click to go to the Author Index">
             Liu, Jianwei
            </a>
           </td>
           <td class="r">
            University College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#147896" title="Click to go to the Author Index">
             Kanoulas, Dimitrios
            </a>
           </td>
           <td class="r">
            University College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420144" title="Click to go to the Author Index">
             Tuptuk, Nilufer
            </a>
           </td>
           <td class="r">
            University College London
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2533" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#autonomous_agents" title="Click to go to the Keyword Index">
               Autonomous Agents
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#constrained_motion_planning" title="Click to go to the Keyword Index">
               Constrained Motion Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#performance_evaluation_and_benchmarking" title="Click to go to the Keyword Index">
               Performance Evaluation and Benchmarking
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Path planning algorithms, such as the search-based A*, are a critical component of autonomous mobile robotics, enabling robots to navigate from a starting point to a destination efficiently and safely. We investigated the resilience of the A* algorithm in the face of potential adversarial interventions known as obstacle attacks. The adversary’s goal is to delay the robot’s timely arrival at its destination by introducing obstacles along its original path. We developed malicious software to execute the attacks and conducted experiments to assess their impact, both in simulation using TurtleBot in Gazebo and in real-world deployment with the Unitree Go1 robot. In simulation, the attacks resulted in an average delay of 36%, with the most significant delays occurring in scenarios where the robot was forced to take substantially longer alternative paths. In real-world experiments, the delays were even more pronounced, with all attacks successfully rerouting the robot and causing measurable disruptions. These results highlight that the algorithm’s robustness is not solely an attribute of its design but is significantly influenced by the operational environment. For example, in constrained environments like tunnels, the delays were maximized due to the limited availability of alternative routes.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt7_05">
             15:35-15:40, Paper ThDT7.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2994'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Topological Mapping for Traversability-Aware Long-Range Navigation in Off-Road Terrain
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#291994" title="Click to go to the Author Index">
             Tremblay, Jean-François
            </a>
           </td>
           <td class="r">
            McGill University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#384397" title="Click to go to the Author Index">
             Alhosh, Julie
            </a>
           </td>
           <td class="r">
            McGill University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#284112" title="Click to go to the Author Index">
             Petit, Louis
            </a>
           </td>
           <td class="r">
            Université De Sherbrooke
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#298541" title="Click to go to the Author Index">
             Lotfi, Faraz
            </a>
           </td>
           <td class="r">
            McGill University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424784" title="Click to go to the Author Index">
             Landauro, Lara
            </a>
           </td>
           <td class="r">
            McGill University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#102603" title="Click to go to the Author Index">
             Meger, David Paul
            </a>
           </td>
           <td class="r">
            McGill University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2994" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#field_robots" title="Click to go to the Keyword Index">
               Field Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#integrated_planning_and_learning" title="Click to go to the Keyword Index">
               Integrated Planning and Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Autonomous robots navigating in off-road terrain like forests open new opportunities for automation. While off-road navigation has been studied, existing work often relies on clearly delineated pathways. We present a method allowing for long-range planning, exploration and low-level control in unknown off-trail forest terrain, using vision and GPS only. We represent outdoor terrain with a topological map, which is a set of panoramic snapshots connected with edges containing traversability information. A novel traversability analysis method is demonstrated, predicting the existence of a safe path towards a target in an image. Navigating between nodes is done using goal-conditioned behavior cloning, leveraging the power of a pretrained vision transformer. An exploration planner is presented, efficiently covering an unknown off-road area with unknown traversability using a frontiers-based approach. The approach is successfully deployed to autonomously explore two 400 m² forest sites unseen during training, in difficult conditions for navigation.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt7_06">
             15:40-15:45, Paper ThDT7.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4773'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              GPU-Enabled Parallel Trajectory Optimization Framework for Safe Motion Planning of Autonomous Vehicles
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#267798" title="Click to go to the Author Index">
             Lee, Yeongseok
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#184156" title="Click to go to the Author Index">
             Choi, Keun Ha
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#126125" title="Click to go to the Author Index">
             Kim, Kyung-Soo
            </a>
           </td>
           <td class="r">
            KAIST(Korea Advanced Institute of Science and Technology)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4773" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#autonomous_vehicle_navigation" title="Click to go to the Keyword Index">
               Autonomous Vehicle Navigation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#integrated_planning_and_control" title="Click to go to the Keyword Index">
               Integrated Planning and Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a GPU-enabled parallel trajectory optimization framework for model predictive control (MPC) in complex urban environments. It fuses the advantages of sampling-based MPC, which can cope with nonconvex costmaps through random sampling of trajectories, with the advantages of gradient-based MPC, which can generate smooth trajectories. In addition, we leverage a generalized safety-embedded MPC problem definition with a discrete barrier state (DBaS). The proposed framework has three steps: 1) a costmap builder to generate the barrier function map, 2) a seed trajectory generator to choose randomly generated trajectories to send to the optimizers, and 3) a batch trajectory optimizer to optimize each of the seed trajectories and select the best trajectory. Experiments with real-time simulations compare the effectiveness of the proposed framework, sampling-based MPC, and gradient-based MPC, which optimizes a single trajectory. The experiments also compare the application of two different control sequence sampling schemes to the proposed framework. The results show that the proposed framework performs gradient-based optimization but can plan a better trajectory even in complex environments by providing various initial guesses. We also show that the proposed framework can perform more accurate control actions than sampling-based MPC.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt7_07">
             15:45-15:50, Paper ThDT7.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4830'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Real-Time Spatio-Temporal Trajectory Planner for Autonomous Vehicles with Semantic Graph Optimization
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#406681" title="Click to go to the Author Index">
             He, Shan
            </a>
           </td>
           <td class="r">
            Beihang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#307353" title="Click to go to the Author Index">
             Ma, Yalong
            </a>
           </td>
           <td class="r">
            Beihang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#407150" title="Click to go to the Author Index">
             Song, Tao
            </a>
           </td>
           <td class="r">
            Beihang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#407149" title="Click to go to the Author Index">
             Jiang, Yongzhi
            </a>
           </td>
           <td class="r">
            Beihang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#307333" title="Click to go to the Author Index">
             Wu, Xinkai
            </a>
           </td>
           <td class="r">
            Beihang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4830" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#autonomous_vehicle_navigation" title="Click to go to the Keyword Index">
               Autonomous Vehicle Navigation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#intelligent_transportation_systems" title="Click to go to the Keyword Index">
               Intelligent Transportation Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Planning a safe and feasible trajectory for autonomous vehicles in real-time by fully utilizing perceptual information in complex urban environments is challenging. In this paper, we propose a spatio-temporal trajectory planning method based on graph optimization. It efficiently extracts the multi-modal information of the perception module by constructing a semantic spatio-temporal map through separation processing of static and dynamic obstacles, and then quickly generates feasible trajectories via sparse graph optimization based on a semantic spatio-temporal hypergraph. Extensive experiments have proven that the proposed method can effectively handle complex urban public road scenarios and perform in real time. We will also release our codes to accommodate benchmarking for the research community
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt8">
             <b>
              ThDT8
             </b>
            </a>
           </td>
           <td class="r">
            311
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thdt8" title="Click to go to the Program at a Glance">
             <b>
              Collision Avoidance 1
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#160744" title="Click to go to the Author Index">
             Hereid, Ayonga
            </a>
           </td>
           <td class="r">
            Ohio State University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt8_01">
             15:15-15:20, Paper ThDT8.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('78'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Sailing through Point Clouds: Safe Navigation Using Point Cloud Based Control Barrier Functions
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#243874" title="Click to go to the Author Index">
             Dai, Bolun
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#223910" title="Click to go to the Author Index">
             Khorrambakht, Rooholla
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#111353" title="Click to go to the Author Index">
             Krishnamurthy, Prashanth
            </a>
           </td>
           <td class="r">
            New York University Tandon School of Engineering
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#108037" title="Click to go to the Author Index">
             Khorrami, Farshad
            </a>
           </td>
           <td class="r">
            New York University Tandon School of Engineering
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab78" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#robot_safety" title="Click to go to the Keyword Index">
               Robot Safety
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#collision_avoidance" title="Click to go to the Keyword Index">
               Collision Avoidance
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The capability to navigate safely in an unstructured environment is crucial when deploying robotic systems in real-world scenarios. Recently, control barrier function (CBF) based approaches have been highly effective in synthesizing safety-critical controllers. In this work, we propose a novel CBF-based local planner comprised of two components: Vessel and Mariner. The Vessel is a novel scaling factor based CBF formulation that synthesizes CBFs using only point cloud data. The Mariner is a CBF-based preview control framework that is used to mitigate getting stuck in spurious equilibria during navigation. To demonstrate the efficacy of our proposed approach, we first compare the proposed point cloud based CBF formulation with other point cloud based CBF formulations. Then, we demonstrate the performance of our proposed approach and its integration with global planners using experimental studies on the Unitree B1 and Unitree Go2 quadruped robots in various environments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt8_02">
             15:20-15:25, Paper ThDT8.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('783'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Parallel-Constraint Model Predictive Control: Exploiting Parallel Computation for Improving Safety
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419143" title="Click to go to the Author Index">
             Fontanari, Elias
            </a>
           </td>
           <td class="r">
            University of Trento
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#340353" title="Click to go to the Author Index">
             Lunardi, Gianni
            </a>
           </td>
           <td class="r">
            University of Trento
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#154562" title="Click to go to the Author Index">
             Saveriano, Matteo
            </a>
           </td>
           <td class="r">
            University of Trento
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#147547" title="Click to go to the Author Index">
             Del Prete, Andrea
            </a>
           </td>
           <td class="r">
            University of Trento
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab783" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#robot_safety" title="Click to go to the Keyword Index">
               Robot Safety
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_control" title="Click to go to the Keyword Index">
               Motion Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Ensuring constraint satisfaction is a key requirement for safety-critical systems, which include most robotic platforms. For example, constraints can be used for modeling joint position/velocity/torque limits and collision avoidance. Constrained systems are often controlled using Model Predictive Control, because of its ability to naturally handle constraints relying on numerical optimization. However, ensuring constraint satisfaction is challenging for nonlinear systems/constraints. A well-known tool to make controllers safe is the so-called control-invariant set (a.k.a. safe set). In our previous work we have shown that safety can be improved by letting the safe set constraint recede along the horizon. In this paper we push that idea further. We suggest to exploit parallel computation for solving several MPC problems at the same time. Each problem instantiate the safe set constraint at a different time step along the horizon. Finally, the controller can select the best solution according to some user-defined criteria. We validated this idea through extensive simulations with a 3-joint robotic arm, showing that significant improvements can be achieved, even using as little as 4 computational cores.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt8_03">
             15:25-15:30, Paper ThDT8.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1543'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Dual-AEB: Synergizing Rule-Based and Multimodal Large Language Models for Effective Emergency Braking
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#414497" title="Click to go to the Author Index">
             Zhang, Wei
            </a>
           </td>
           <td class="r">
            Harbin Institute of Techonolgy
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#339767" title="Click to go to the Author Index">
             Li, Pengfei
            </a>
           </td>
           <td class="r">
            Institute for AI Industry Research (AIR), Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#415197" title="Click to go to the Author Index">
             Wang, Junli
            </a>
           </td>
           <td class="r">
            Institute of Automation, Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#418073" title="Click to go to the Author Index">
             Sun, Bingchuan
            </a>
           </td>
           <td class="r">
            Lenovo
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419346" title="Click to go to the Author Index">
             Jin, Qihao
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#418113" title="Click to go to the Author Index">
             Bao, Guangjun
            </a>
           </td>
           <td class="r">
            Lenovo
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#418069" title="Click to go to the Author Index">
             Yu, Yang
            </a>
           </td>
           <td class="r">
            Lenovo
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#217534" title="Click to go to the Author Index">
             Ding, Wenchao
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419375" title="Click to go to the Author Index">
             Li, Peng
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421352" title="Click to go to the Author Index">
             Chen, Yilun
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1543" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#autonomous_agents" title="Click to go to the Keyword Index">
               Autonomous Agents
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Automatic Emergency Braking (AEB) systems are a crucial component in ensuring the safety of passengers in autonomous vehicles. Conventional AEB systems primarily rely on closed-set perception modules to recognize traffic conditions and assess collision risks. To enhance the adaptability of AEB systems in open scenarios, we propose Dual-AEB, a system combines an advanced multimodal large language model (MLLM) for comprehensive scene understanding and a conventional rule-based rapid AEB to ensure quick response times. To the best of our knowledge, Dual-AEB is the first method to incorporate MLLMs within AEB systems. Through extensive experimentation, we have validated the effectiveness of our method.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt8_04">
             15:30-15:35, Paper ThDT8.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1724'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Estimating Control Barriers from Offline Data
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#313097" title="Click to go to the Author Index">
             Yu, Hongzhan
            </a>
           </td>
           <td class="r">
            University of California San Diego
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419841" title="Click to go to the Author Index">
             Farrell, Seth
            </a>
           </td>
           <td class="r">
            University of California San Diego
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419842" title="Click to go to the Author Index">
             Yoshimitsu, Ryo
            </a>
           </td>
           <td class="r">
            IHI Corporation
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#284967" title="Click to go to the Author Index">
             Qin, Zhizhen
            </a>
           </td>
           <td class="r">
            University of California, San Diego
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#310792" title="Click to go to the Author Index">
             Christensen, Henrik
            </a>
           </td>
           <td class="r">
            University of California, San Diego
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#290969" title="Click to go to the Author Index">
             Gao, Sicun
            </a>
           </td>
           <td class="r">
            UCSD
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1724" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#ai_based_methods" title="Click to go to the Keyword Index">
               AI-Based Methods
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#robot_safety" title="Click to go to the Keyword Index">
               Robot Safety
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#collision_avoidance" title="Click to go to the Keyword Index">
               Collision Avoidance
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Learning-based methods for constructing control barrier functions (CBFs) are gaining popularity, for enforcing safety in practical robot control under complex dynamics and uncertainty that are hard to model. A major limitation of existing methods is their reliance on extensive sampling over the state space, making it hard to construct CBFs on real robots. In this work we introduce methods for learning neural CBFs through a fixed, sparsely-labeled dataset collected prior to training either the CBFs or the controllers. We propose novel annotation techniques based on out-of-distribution analysis to effectively propagate the information from the limited labeled data to the unlabeled data. We evaluate the proposed algorithm on real-world platforms. With limited amount of offline data, the proposed methods can achieve state-of-the-art performance for dynamic obstacle avoidance, with statistically safer and less conservative maneuvers compared to existing methods.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt8_05">
             15:35-15:40, Paper ThDT8.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3005'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Real-Time Safe Bipedal Robot Navigation Using Linear Discrete Control Barrier Functions
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#340788" title="Click to go to the Author Index">
             Peng, Chengyang
            </a>
           </td>
           <td class="r">
            The Ohio State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#180261" title="Click to go to the Author Index">
             Paredes, Victor
            </a>
           </td>
           <td class="r">
            The Ohio State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#238332" title="Click to go to the Author Index">
             Castillo, Guillermo A.
            </a>
           </td>
           <td class="r">
            The Ohio State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#160744" title="Click to go to the Author Index">
             Hereid, Ayonga
            </a>
           </td>
           <td class="r">
            Ohio State University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3005" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#humanoid_and_bipedal_locomotion" title="Click to go to the Keyword Index">
               Humanoid and Bipedal Locomotion
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#integrated_planning_and_control" title="Click to go to the Keyword Index">
               Integrated Planning and Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#collision_avoidance" title="Click to go to the Keyword Index">
               Collision Avoidance
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Safe navigation in real-time is an essential task for humanoid robots in real-world deployment. Since humanoid robots are inherently underactuated thanks to unilateral ground contacts, a path is considered safe if it is obstacle-free and respects the robot's physical limitations and underlying dynamics. Existing approaches often decouple path planning from gait control due to the significant computational challenge caused by the full-order robot dynamics. In this work, we develop a unified, safe path and gait planning framework that can be evaluated online in real-time, allowing the robot to navigate clustered environments while sustaining stable locomotion. Our approach uses the popular Linear Inverted Pendulum (LIP) model as a template model to represent walking dynamics. It incorporates heading angles in the model to evaluate kinematic constraints essential for physically feasible gaits properly. In addition, we leverage discrete control barrier functions (DCBF) for obstacle avoidance, ensuring that the subsequent foot placement provides a safe navigation path within clustered environments. To guarantee real-time computation, we use a novel approximation of the DCBF to produce linear DCBF constraints. We validate our proposed approach in simulation using a Digit robot in randomly generated environments. The results demonstrate that the proposed approach can generate safe gaits for a non-trivial humanoid robot to navigate in a clustered environment in real-time.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt8_06">
             15:40-15:45, Paper ThDT8.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4392'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              FuzzRisk: Online Collision Risk Estimation for Autonomous Vehicles Based on Depth-Aware Object Detection Via Fuzzy Inference
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#297343" title="Click to go to the Author Index">
             Liao, Brian Hsuan-Cheng
            </a>
           </td>
           <td class="r">
            DENSO AUTOMOTIVE Deutschland GmbH
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420521" title="Click to go to the Author Index">
             Xu, Yingjie
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#130911" title="Click to go to the Author Index">
             Cheng, Chih-Hong
            </a>
           </td>
           <td class="r">
            Chalmers University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#103128" title="Click to go to the Author Index">
             Esen, Hasan
            </a>
           </td>
           <td class="r">
            DENSO AUTOMOTIVE Deutschland GmbH
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#105608" title="Click to go to the Author Index">
             Knoll, Alois
            </a>
           </td>
           <td class="r">
            Tech. Univ. Muenchen TUM
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4392" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#robot_safety" title="Click to go to the Keyword Index">
               Robot Safety
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#intelligent_transportation_systems" title="Click to go to the Keyword Index">
               Intelligent Transportation Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a novel monitoring framework that infers the level of collision risk for autonomous vehicles (AVs) based on their object detection performance. The framework takes two sets of predictions from different algorithms and associates their inconsistencies with the collision risk via fuzzy inference. The first set of predictions is obtained by retrieving safety-critical 2.5D objects from a depth map, and the second set comes from the ordinary AV's 3D object detector. We experimentally validate that, based on Intersection-over-Union (IoU) and a depth discrepancy measure, the inconsistencies between the two sets of predictions strongly correlate to the error of the 3D object detector against ground truths. This correlation allows us to construct a fuzzy inference system and map the inconsistency measures to an AV collision risk indicator. In particular, we optimize the fuzzy inference system towards an existing offline metric that matches AV collision rates well. Lastly, we validate our monitor's capability to produce relevant risk estimates with the large-scale nuScenes dataset and demonstrate that it can safeguard an AV in closed-loop simulations.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt8_07">
             15:45-15:50, Paper ThDT8.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4463'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Adaptive Deadlock Avoidance for Decentralized Multi-Agent Systems Via CBF-Inspired Risk Measurement
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#376188" title="Click to go to the Author Index">
             Zhang, Yanze
            </a>
           </td>
           <td class="r">
            University of Illinois Chicago
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#244910" title="Click to go to the Author Index">
             Lyu, Yiwei
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#381694" title="Click to go to the Author Index">
             Jo, Siwon
            </a>
           </td>
           <td class="r">
            University of North Carolina at Charlotte
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#326194" title="Click to go to the Author Index">
             Yang, Yupeng
            </a>
           </td>
           <td class="r">
            University of North Carolina at Charlotte
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#185040" title="Click to go to the Author Index">
             Luo, Wenhao
            </a>
           </td>
           <td class="r">
            University of Illinois Chicago
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4463" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#autonomous_agents" title="Click to go to the Keyword Index">
               Autonomous Agents
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#agent_based_systems" title="Click to go to the Keyword Index">
               Agent-Based Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Decentralized safe control plays an important role in multi-agent systems given the scalability and robustness without reliance on a central authority. However, without an explicit global coordinator, the decentralized control methods are often prone to deadlock --- a state where the system reaches equilibrium, causing the robots to stall. In this paper, we propose a generalized decentralized framework that unifies the Control Lyapunov Function (CLF) and Control Barrier Function (CBF) to facilitate efficient task execution and ensure deadlock-free trajectories for the multi-agent systems. As the agents approach the deadlock-related undesirable equilibrium, the framework can detect the equilibrium and drive agents away before that happens. This is achieved by a secondary deadlock resolution design with an auxiliary CBF to prevent the multi-agent systems from converging to the undesirable equilibrium. To avoid dominating effects due to the deadlock resolution over the original task-related controllers, a deadlock indicator function using CBF-inspired risk measurement is proposed and encoded in the unified framework for the agents to adaptively determine when to activate the deadlock resolution. This allows the agents to follow their original control tasks and seamlessly unlock or deactivate deadlock resolution as necessary, effectively improving task efficiency. We demonstrate the effectiveness of the proposed method through theoretical analysis, numerical simulations, and real-world experiments.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt9">
             <b>
              ThDT9
             </b>
            </a>
           </td>
           <td class="r">
            312
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thdt9" title="Click to go to the Program at a Glance">
             <b>
              Task and Motion Planning 3
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#116028" title="Click to go to the Author Index">
             Park, Shinkyu
            </a>
           </td>
           <td class="r">
            KAUST
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt9_01">
             15:15-15:20, Paper ThDT9.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('532'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SEAL: A Sample-Efficient Adjustment-Learning Method for Table Tennis Robot Serve
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#406215" title="Click to go to the Author Index">
             Guo, Qitong
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#309843" title="Click to go to the Author Index">
             Shi, Xiaohang
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#364620" title="Click to go to the Author Index">
             Murakami, Kenichi
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#335710" title="Click to go to the Author Index">
             Jia, Ruoyu
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#109216" title="Click to go to the Author Index">
             Yamakawa, Yuji
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab532" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#machine_learning_for_robot_control" title="Click to go to the Keyword Index">
               Machine Learning for Robot Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#task_and_motion_planning" title="Click to go to the Keyword Index">
               Task and Motion Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#manipulation_planning" title="Click to go to the Keyword Index">
               Manipulation Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Table tennis robots have significantly advanced in performance owing to the rapid progress in deep learning and reinforcement learning technologies. However, these advancements often require a large number of training samples. Besides, research focused on the robot serve task remains relatively limited. In response to these problems, this paper proposes a sample-efficient adjustment-learning (SEAL) method for the serve task inspired by human experience in table tennis, which can inherently augment the available training samples without the need for additional sample collection. The adjustment learning does not require complex network structures but demonstrates superior performances. The models trained by adjustment learning have good generalization and robustness, that can adapt to different serve styles and reduce system transfer errors very efficiently. In addition, the random interpolation method during dataset generation stage is introduced, and the effectiveness of simultaneous learning in both joint space and Cartesian space is also demonstrated. For specific serve task, an accuracy of less than 30 mm to any designated position at the first shot is achieved.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt9_02">
             15:20-15:25, Paper ThDT9.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1317'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Inference Based Multi-Object Reactive Search in a Partially Known Environment with Temporal Logic Specifications
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421113" title="Click to go to the Author Index">
             Kang, Yaohui
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#330115" title="Click to go to the Author Index">
             Chen, Ziyang
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#352483" title="Click to go to the Author Index">
             Xia, Yanjie
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#206291" title="Click to go to the Author Index">
             Kan, Zhen
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1317" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#task_and_motion_planning" title="Click to go to the Keyword Index">
               Task and Motion Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#formal_methods_in_robotics_and_automation" title="Click to go to the Keyword Index">
               Formal Methods in Robotics and Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Efficiently searching for multiple objects in a partially known environment, where only the names and locations of landmarks are available, presents significant challenges. Existing search algorithms in the literature fail to fully utilize prior knowledge to improve search efficiency, and exhibit significantly diminished efficiency when extended to multi- object search. To address these limitations, we propose an inference-based multi-object reactive search framework. This framework utilizes the COMET inference model to reason about co-occurrence values between the target objects and known landmarks, thereby enhancing search efficiency. These co-occurrence values are integrated into a reactive temporal logic motion planning strategy, which allows the robot search for multiple objects with temporal logic constraints specified by LTL and adapt dynamically if the inferred reasoning differs from the actual object arrangement encountered during the search. Extensive simulations were conducted to evaluate the feasibility and efficiency of the proposed motion planning algorithm. Results demonstrate that the integration of commonsense reasoning with reactive temporal logic planning significantly improves multi-object search efficiency. Project website: https://sites.google.com/view/imors.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt9_03">
             15:25-15:30, Paper ThDT9.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1726'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Planning with Adaptive World Models for Autonomous Driving
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#216306" title="Click to go to the Author Index">
             Vasudevan, Arun Balajee
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#405380" title="Click to go to the Author Index">
             Peri, Neehar
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#147934" title="Click to go to the Author Index">
             Schneider, Jeff
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#134179" title="Click to go to the Author Index">
             Ramanan, Deva
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1726" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#task_and_motion_planning" title="Click to go to the Keyword Index">
               Task and Motion Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#behavior_based_systems" title="Click to go to the Keyword Index">
               Behavior-Based Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#robust_adaptive_control" title="Click to go to the Keyword Index">
               Robust/Adaptive Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Motion planning is crucial for safe navigation in complex urban environments. Historically, motion planners (MPs) have been evaluated with procedurally-generated simulators like CARLA. However, such synthetic benchmarks do not capture real-world multi-agent interactions. nuPlan, a recently released MP benchmark, addresses this limitation by augmenting real-world driving logs with closed-loop simulation logic, effectively turning the fixed dataset into a reactive simulator. We analyze the characteristics of nuPlan's recorded logs and find that each city has its own unique driving behaviors, suggesting that robust planners must adapt to different environments. We learn to model such unique behaviors with BehaviorNet, a graph convolutional neural network (GCNN) that predicts reactive agent behaviors using features derived from recently-observed agent histories; intuitively, some aggressive agents may tailgate lead vehicles, while others may not. To model such phenomena, BehaviorNet predicts the parameters of an agent's motion controller rather than directly predicting its spacetime trajectory (as most forecasters do). Finally, we present AdaptiveDriver, a model-predictive control (MPC) based planner that unrolls different world models conditioned on BehaviorNet's predictions. Our extensive experiments demonstrate that AdaptiveDriver achieves state-of-the-art results on the nuPlan closed-loop planning benchmark, improving over prior work by 2% on Test-14 Hard R-CLS, and generalizes even when evaluated on never-before-seen cities.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt9_04">
             15:30-15:35, Paper ThDT9.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2194'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Subassembly to Full Assembly: Effective Assembly Sequence Planning through Graph-Based Reinforcement Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416705" title="Click to go to the Author Index">
             Shu, Chang
            </a>
           </td>
           <td class="r">
            KAUST
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#243238" title="Click to go to the Author Index">
             Kim, Anton
            </a>
           </td>
           <td class="r">
            KAUST
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#116028" title="Click to go to the Author Index">
             Park, Shinkyu
            </a>
           </td>
           <td class="r">
            KAUST
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2194" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#task_and_motion_planning" title="Click to go to the Keyword Index">
               Task and Motion Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#assembly" title="Click to go to the Keyword Index">
               Assembly
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#manipulation_planning" title="Click to go to the Keyword Index">
               Manipulation Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper proposes an assembly sequence planning framework, named Subassembly to Assembly (S2A). The framework is designed to enable a robotic manipulator to assemble multiple parts in a prespecified structure by leveraging object manipulation actions. The primary technical challenge lies in the exponentially increasing complexity of identifying a feasible assembly sequence as the number of parts grows. To address this, we introduce a graph-based reinforcement learning approach, where a graph attention network is trained using a delayed reward assignment strategy. In this strategy, rewards are assigned only when an assembly action contributes to the successful completion of the assembly task. We validate the framework's performance through physics-based simulations, comparing it against various baselines to emphasize the significance of the proposed reward assignment approach. Additionally, we demonstrate the feasibility of deploying our framework in a real-world robotic assembly scenario.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt9_05">
             15:35-15:40, Paper ThDT9.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2329'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Fuel-Optimal Operational Speed Planning for Autonomous Trucking on Highways
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#287152" title="Click to go to the Author Index">
             Li, Wei
            </a>
           </td>
           <td class="r">
            Inceptio
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423912" title="Click to go to the Author Index">
             Wu, Bin
            </a>
           </td>
           <td class="r">
            Inceptio
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#368544" title="Click to go to the Author Index">
             Xiang, Jiahao
            </a>
           </td>
           <td class="r">
            Tongji University, Inceptio Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#286555" title="Click to go to the Author Index">
             Ren, Jiaping
            </a>
           </td>
           <td class="r">
            Inceptio Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#377588" title="Click to go to the Author Index">
             Wu, Yi
            </a>
           </td>
           <td class="r">
            Nanjing University of Posts and Telecommunications
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#143098" title="Click to go to the Author Index">
             Yang, Ruigang
            </a>
           </td>
           <td class="r">
            University of Kentucky
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2329" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#task_and_motion_planning" title="Click to go to the Keyword Index">
               Task and Motion Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#logistics" title="Click to go to the Keyword Index">
               Logistics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#planning__scheduling_and_coordination" title="Click to go to the Keyword Index">
               Planning, Scheduling and Coordination
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The rapid advancement of autonomous driving technology, particularly in autonomous trucking on highways, shows great value for enhancing efficiency and reducing costs in the logistics industry. In this work, we define the full-trip speed planning problem for autonomous trucks under delivery time and fuel consumption constraints, referred to as the Operational Speed Planning (OSP) problem. To support and accelerate research on the OSP problem, we have developed a comprehensive dataset using a fleet of over 400 trucks. The dataset contains rich, diverse information covering more than 22 million kilometers of real-world highway driving data. In addition to this static dataset, we have developed a closed-loop simulator that allows for the interactive evaluation of OSP solutions, enabling researchers to test speed planning strategies in a realistic environment. Furthermore, we provide an OSP baseline method based on dynamic programming to optimize speed planning, balancing the delivery time requirements and fuel consumption. Our extensive experiments demonstrate both the accuracy of the simulation and the effectiveness of the OSP baseline in planning optimal speeds, proving its capability to meet time constraints while improving fuel efficiency. The dataset, simulator, and baseline will be made publicly available to foster further research and innovation in this area.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt9_06">
             15:40-15:45, Paper ThDT9.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3323'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Verifiably Following Complex Robot Instructions with Foundation Models
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#354573" title="Click to go to the Author Index">
             Quartey, Benedict
            </a>
           </td>
           <td class="r">
            Brown University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#204944" title="Click to go to the Author Index">
             Rosen, Eric
            </a>
           </td>
           <td class="r">
            The AI Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#136380" title="Click to go to the Author Index">
             Tellex, Stefanie
            </a>
           </td>
           <td class="r">
            Brown
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#106428" title="Click to go to the Author Index">
             Konidaris, George
            </a>
           </td>
           <td class="r">
            Brown University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3323" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#task_and_motion_planning" title="Click to go to the Keyword Index">
               Task and Motion Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mobile_manipulation" title="Click to go to the Keyword Index">
               Mobile Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             When instructing robots, users want to flexibly express constraints, refer to arbitrary landmarks, and verify robot behavior, while robots must disambiguate instructions into specifications and ground instruction referents in the real world. To address this problem, we propose Language Instruction grounding for Motion Planning (LIMP), an approach that enables robots to verifiably follow complex, open-ended instructions in real-world environments without prebuilt semantic maps. LIMP constructs a symbolic instruction representation that reveals the robot’s alignment with an instructor’s intended motives and affords the synthesis of correct-by-construction robot behaviors. We conduct a large-scale evaluation of LIMP on 150 instructions across five real-world environments, demonstrating its versatility and ease of deployment in diverse, unstructured domains. LIMP performs comparably to state-of-the-art baselines on standard open-vocabulary tasks and additionally achieves a 79% success rate on complex spatiotemporal instructions, significantly outperforming baselines that only reach 38%.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt9_07">
             15:45-15:50, Paper ThDT9.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4861'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Hierarchical Approach for Joint Task Allocation and Path Planning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#308416" title="Click to go to the Author Index">
             Ho, Florence
            </a>
           </td>
           <td class="r">
            NEC Corporation, National Institute of Advanced Industrial Scien
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#276396" title="Click to go to the Author Index">
             Higa, Ryota
            </a>
           </td>
           <td class="r">
            NEC Corporation, National Institute of Advanced Industrial Scien
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#231274" title="Click to go to the Author Index">
             Kato, Takuro
            </a>
           </td>
           <td class="r">
            National Institute of Advanced Industrial Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#224797" title="Click to go to the Author Index">
             Nakadai, Shinji
            </a>
           </td>
           <td class="r">
            NEC Corporation
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4861" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#task_planning" title="Click to go to the Keyword Index">
               Task Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#path_planning_for_multiple_mobile_robots_or_agents" title="Click to go to the Keyword Index">
               Path Planning for Multiple Mobile Robots or Agents
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper addresses the joint task allocation and path planning problem, whereby a fleet of vehicles must be optimally assigned to service multiple given tasks while their planned paths must be collision-free. Such a problem composed of two tightly coupled optimization problems has a high complexity with the number of tasks and the number of vehicles, thus optimal solvers do not scale to large size instances. Therefore, we propose a novel method to solve this problem, HTAPPS, which introduces a hierarchical resolution framework. Our proposed approach decomposes a given instance into three levels of abstractions and associated amount of details that progressively filter the search space. This allows us to reduce the computational effort required when performing task allocation and multi-agent path planning jointly. We perform simulations on automated warehouse scenarios, and compare our approach to baseline solvers. The obtained results show that our proposed approach is able to solve large size instances within a limited time.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt10">
             <b>
              ThDT10
             </b>
            </a>
           </td>
           <td class="r">
            313
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thdt10" title="Click to go to the Program at a Glance">
             <b>
              Multi-Robot Systems 6
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#101179" title="Click to go to the Author Index">
             Tsiotras, Panagiotis
            </a>
           </td>
           <td class="r">
            Georgia Tech
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt10_08">
             15:45-15:50, Paper ThDT10.8
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('37'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Multi S-Graphs: An Efficient Distributed Semantic-Relational Collaborative SLAM
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#326120" title="Click to go to the Author Index">
             Fernandez-Cortizas, Miguel
            </a>
           </td>
           <td class="r">
            Universidad Politécnica De Madrid
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#226120" title="Click to go to the Author Index">
             Bavle, Hriday
            </a>
           </td>
           <td class="r">
            University of Luxembourg
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#326126" title="Click to go to the Author Index">
             Perez Saura, David
            </a>
           </td>
           <td class="r">
            Computer Vision and Aerial Robotics Group (CVAR), Universidad Po
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#158182" title="Click to go to the Author Index">
             Sanchez-Lopez, Jose Luis
            </a>
           </td>
           <td class="r">
            University of Luxembourg
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104938" title="Click to go to the Author Index">
             Campoy, Pascual
            </a>
           </td>
           <td class="r">
            Computer Vision &amp; Aerial Rootics Group. Universidad Politécnica
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#115622" title="Click to go to the Author Index">
             Voos, Holger
            </a>
           </td>
           <td class="r">
            University of Luxembourg
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab37" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_slam" title="Click to go to the Keyword Index">
               Multi-Robot SLAM
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Collaborative Simultaneous Localization and Mapping (CSLAM) is critical
             <p>
              to enable multiple robots to operate in complex environments. Most CSLAM techniques rely on raw sensor measurement or low-level features such as keyframe descriptors, which can lead to wrong loop closures due
              <p>
               to the lack of deep understanding of the environment. Moreover, the exchange of these measurements and low-level features among the robots requires the transmission of a significant amount of data, which limits
               <p>
                the scalability of the system. To overcome these limitations, we present Multi S-Graphs, a decentralized CSLAM system that utilizes high-level semantic-relational information embedded in the four-layered
                <p>
                 hierarchical and optimizable situational graphs for cooperative map generation and localization in structured environments while minimizing the information exchanged between the robots. To support this, we present a novel room-based descriptor which, along with its connected walls, is used to perform inter-robot loop closures, addressing the challenges of multi-robot kidnapped problem initialization. Multiple experiments in simulated and real environments validate the improvement in accuracy and robustness of the proposed approach while reducing the amount of data exchanged between robots compared to other state-of-the-art approaches.
                </p>
               </p>
              </p>
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt10_09">
             15:45-15:50, Paper ThDT10.9
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('945'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Language-Conditioned Offline RL for Multi-Robot Navigation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#253779" title="Click to go to the Author Index">
             Morad, Steven
            </a>
           </td>
           <td class="r">
            The University of Cambridge
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#185364" title="Click to go to the Author Index">
             Shankar, Ajay
            </a>
           </td>
           <td class="r">
            University of Cambridge, UK
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#267264" title="Click to go to the Author Index">
             Blumenkamp, Jan
            </a>
           </td>
           <td class="r">
            University of Cambrdige
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#124025" title="Click to go to the Author Index">
             Prorok, Amanda
            </a>
           </td>
           <td class="r">
            University of Cambridge
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab945" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#networked_robots" title="Click to go to the Keyword Index">
               Networked Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present a method for synthesizing navigation policies for multi-robot teams that interpret and follow natural language instructions. We condition these policies on embeddings from pretrained Large Language Models (LLMs), and train them via offline reinforcement learning with as little as 20 minutes of randomly-collected real-world data. Experiments on a team of five real robots show that these policies generalize well to unseen commands, indicating an understanding of the LLM latent space. Our method requires no simulators or environment models, and produces low-latency control policies that can be deployed directly to real robots without finetuning. We provide videos of our experiments at https://sites.google.com/view/llm-marl.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt10_10">
             15:45-15:50, Paper ThDT10.10
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2898'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Deep Reinforcement Learning for Coordinated Payload Transport in Biped-Wheeled Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#388298" title="Click to go to the Author Index">
             Mehta, Dhruv K
            </a>
           </td>
           <td class="r">
            Clemson University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#351323" title="Click to go to the Author Index">
             Joglekar, Ajinkya
            </a>
           </td>
           <td class="r">
            Clemson University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101679" title="Click to go to the Author Index">
             Krovi, Venkat
            </a>
           </td>
           <td class="r">
            Clemson University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2898" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#cooperating_robots" title="Click to go to the Keyword Index">
               Cooperating Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Coordinated payload transport via a fleet of modular wheeled mobile robots offers flexibility for handling larger loads in indoor and outdoor environments. Biped-wheeled robots have recently emerged as a viable architecture for an independent/stand-alone wheeled mobile robot. In this work, we explore the use of two biped-wheeled robots that can leverage their mobility and maneuvarability for enhanced spatial pose control and stabilization for various payload transport tasks. However, coordinated control of multiple articulated wheeled robots for path tracking of a payload presents significant (and potentially competing) challenges, including kinematic redundancy, stability concerns, relative motion between the payload and robots, and precise motion control to achieve effective coordination. To address these challenges, we propose a Deep Reinforcement Learning (DRL) framework to develop the motion-plans for the system. In particular, this approach generates the ego robot's body twist and the follower robot's relative twist with respect to the ego robot. By formulating the action space of the follower robot as a relative twist, our approach facilitates pairwise interactions between robots. Furthermore, we use only relative pose information and the errors as states for the DRL controller, thereby making it agnostic to initial conditions and avoiding explicit dependency on absolute pose. We validate our approach through simulations conducted in Isaac Sim and on hardware using Diablo biped-wheeled robots with zero-shot transfer, demonstrating effective payload path tracking across varying parameters.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt10_11">
             15:45-15:50, Paper ThDT10.11
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3267'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Reinforcement Learning within the Classical Robotics Stack: A Case Study in Robot Soccer
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#372680" title="Click to go to the Author Index">
             Labiosa, Adam
            </a>
           </td>
           <td class="r">
            University of Wisconsin-Madison
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424820" title="Click to go to the Author Index">
             Wang, Zhihan
            </a>
           </td>
           <td class="r">
            The University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424840" title="Click to go to the Author Index">
             Agarwal, Siddhant
            </a>
           </td>
           <td class="r">
            The University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424839" title="Click to go to the Author Index">
             Cong, William
            </a>
           </td>
           <td class="r">
            University of Wisconsin-Madison
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#327281" title="Click to go to the Author Index">
             Hemkumar, Geethika
            </a>
           </td>
           <td class="r">
            The University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424829" title="Click to go to the Author Index">
             Harish, Abhinav Narayan
            </a>
           </td>
           <td class="r">
            University of Wisconsin Madison
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#372806" title="Click to go to the Author Index">
             Hong, Benjamin
            </a>
           </td>
           <td class="r">
            University of Wisconsin - Madison
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#205385" title="Click to go to the Author Index">
             Kelle, Josh
            </a>
           </td>
           <td class="r">
            University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#373288" title="Click to go to the Author Index">
             Li, Chen
            </a>
           </td>
           <td class="r">
            UW-Madison
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424835" title="Click to go to the Author Index">
             Li, Yuhao
            </a>
           </td>
           <td class="r">
            University of Wisconsin–Madison
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424858" title="Click to go to the Author Index">
             Shao, Zisen
            </a>
           </td>
           <td class="r">
            University of Wisconsin–Madison
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#106083" title="Click to go to the Author Index">
             Stone, Peter
            </a>
           </td>
           <td class="r">
            University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#205386" title="Click to go to the Author Index">
             Hanna, Josiah
            </a>
           </td>
           <td class="r">
            University of Wisconsin -- Madison
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3267" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#machine_learning_for_robot_control" title="Click to go to the Keyword Index">
               Machine Learning for Robot Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robot decision-making in partially observable, real-time, dynamic, and multi-agent environments remains a difficult and unsolved challenge. Model-free reinforcement learning (RL) is a promising approach to learning decision-making in such domains, however, end-to-end RL in complex environments is often intractable. To address this challenge in the RoboCup Standard Platform League (SPL) domain, we developed a novel architecture integrating RL within a classical robotics stack, while employing a multi-fidelity sim2real approach and decomposing behavior into learned sub-behaviors with heuristic selection. Our architecture led to victory in the 2024 RoboCup SPL Challenge Shield Division. In this work, we fully describe our system's architecture and empirically analyze key design decisions that contributed to its success. Our approach demonstrates how RL-based behaviors can be integrated into complete robot behavior architectures.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt10_12">
             15:45-15:50, Paper ThDT10.12
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3319'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Residual Descent Differential Dynamic Game (RD3G) -- a Fast Newton Solver for Constrained General Sum Games
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#313193" title="Click to go to the Author Index">
             Zhang, Zhiyuan
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101179" title="Click to go to the Author Index">
             Tsiotras, Panagiotis
            </a>
           </td>
           <td class="r">
            Georgia Tech
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3319" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#path_planning_for_multiple_mobile_robots_or_agents" title="Click to go to the Keyword Index">
               Path Planning for Multiple Mobile Robots or Agents
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#integrated_planning_and_control" title="Click to go to the Keyword Index">
               Integrated Planning and Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present Residual Descent Differential Dynamic Game (RD3G), a Newton-based solver for constrained multi- agent game-control problems. The proposed solver seeks a local Nash equilibrium for games where agents are coupled through their rewards and state constraints. By maintaining a dynamic set of active constraints, combined with a barrier function on satisfied constraints and a backtracking line search, the proposed method is able to satisfy state constraints while keeping the dimension of the Newton descent direction problem to a minimum. We compare the proposed method against state- of-the-art techniques and showcase the computational benefits of the RD3G algorithm on several example problems. The RD3G is up to 4X faster and has 2X higher convergence rate than existing approaches in higher dimensional games.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt10_13">
             15:45-15:50, Paper ThDT10.13
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3335'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              MARLadona - towards Cooperative Team Play Using Multi-Agent Reinforcement Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424201" title="Click to go to the Author Index">
             Li, Zichong
            </a>
           </td>
           <td class="r">
            ANYbotics
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#310688" title="Click to go to the Author Index">
             Bjelonic, Filip
            </a>
           </td>
           <td class="r">
            ETH Zürich, Switzerland
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#236286" title="Click to go to the Author Index">
             Klemm, Victor
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#114045" title="Click to go to the Author Index">
             Hutter, Marco
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3335" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#cooperating_robots" title="Click to go to the Keyword Index">
               Cooperating Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robot soccer, in its full complexity, poses an unsolved research challenge. Current solutions heavily rely on engineered heuristic strategies, which lack robustness and adaptability. Deep reinforcement learning has gained significant traction in various complex robotics tasks such as locomotion, manipulation, and competitive games (e.g., AlphaZero, OpenAI Five), making it a promising solution to the robot soccer problem. This paper introduces MARLadona. A decentralized multi-agent reinforcement learning (MARL) training pipeline capable of producing agents with sophisticated team play behavior, bridging the shortcomings of heuristic methods. Furthermore, we created an open-source multi-agent soccer environment. Utilizing our MARL framework and a modified a global entity encoder (GEE) as our core architecture, our approach achieves a 66.8 % win rate against HELIOS agent, which employs a state-of-the-art heuristic strategy. In addition, we provided an in-depth analysis of the policy behavior and interpreted the agent’s intention using the critic network.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt10_14">
             15:45-15:50, Paper ThDT10.14
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4538'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Multi-Agent Inverse Q-Learning from Demonstrations
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426862" title="Click to go to the Author Index">
             Haynam, Nathaniel
            </a>
           </td>
           <td class="r">
            UC Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426863" title="Click to go to the Author Index">
             Khoja, Adam
            </a>
           </td>
           <td class="r">
            UC Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426864" title="Click to go to the Author Index">
             Kumar, Dhruv
            </a>
           </td>
           <td class="r">
            UC Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#341264" title="Click to go to the Author Index">
             Myers, Vivek
            </a>
           </td>
           <td class="r">
            UC Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#246919" title="Click to go to the Author Index">
             Bıyık, Erdem
            </a>
           </td>
           <td class="r">
            University of Southern California
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4538" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             When reward functions are hand-designed, deep reinforcement learning algorithms often suffer from reward misspecification, causing them to learn suboptimal policies. In the single-agent case, Inverse Reinforcement Learning (IRL) techniques attempt to address this issue by inferring the reward function from expert demonstrations. However, in multi-agent problems, misalignment between the learned and true objectives is exacerbated due to increased environment non-stationarity and variance that scale with multiple agents. As such, in multi-agent general-sum games, multi-agent IRL algorithms have difficulty balancing cooperative and competitive objectives. To address these issues, we propose Multi-Agent Marginal Q-Learning from Demonstrations (MAMQL), a novel sample-efficient framework for multi-agent IRL. For each agent, MAMQL learns a critic marginalized over the other agents' policies, allowing for a well-motivated use of Boltzmann policies in the multi-agent context. We identify a connection between optimal marginalized critics and single-agent soft-Q IRL, allowing us to apply a direct, simple optimization criterion from the single-agent domain. Across our experiments on three different simulated domains, MAMQL significantly outperforms previous multi-agent methods in average reward, sample efficiency, and reward recovery by often more than 2-5x. We make our code available at https://sites.google.com/view/mamql .
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt11">
             <b>
              ThDT11
             </b>
            </a>
           </td>
           <td class="r">
            314
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thdt11" title="Click to go to the Program at a Glance">
             <b>
              Robot Vision 2
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#256235" title="Click to go to the Author Index">
             Wang, Lin
            </a>
           </td>
           <td class="r">
            Nanyang Technological University (NTU)
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt11_01">
             15:15-15:20, Paper ThDT11.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('547'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              LoGS: Visual Localization for Mobile Robots with Gaussian Splatting
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417896" title="Click to go to the Author Index">
             Cheng, Yuzhou
            </a>
           </td>
           <td class="r">
            University College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#212673" title="Click to go to the Author Index">
             Jiao, Jianhao
            </a>
           </td>
           <td class="r">
            University College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#156231" title="Click to go to the Author Index">
             Wang, Yue
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#147896" title="Click to go to the Author Index">
             Kanoulas, Dimitrios
            </a>
           </td>
           <td class="r">
            University College London
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab547" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Visual localization involves estimating a query im-age’s 6-DoF (degrees of freedom) camera pose, which is a funda-mental component in various computer vision and robotic tasks. This paper presents LoGS, a vision-based localization pipeline utilizing the 3D Gaussian Splatting (GS) technique as scene representation. This novel representation allows high-quality novel view synthesis. During the mapping phase, structure-from-motion (SfM) is applied first, followed by the generation of a GS map. During localization, the initial position is obtained through image retrieval, local feature matching coupled with a PnP solver, and then a high-precision pose is achieved through the analysis-by-synthesis manner on the GS map. Experimental results on four large-scale datasets demonstrate the proposed approach’s SoTA accuracy in estimating camera poses and robustness under challenging few-shot conditions.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt11_02">
             15:20-15:25, Paper ThDT11.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1084'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Unified Human Localization and Trajectory Prediction with Monocular Vision
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#367409" title="Click to go to the Author Index">
             Luan, Po-Chien
            </a>
           </td>
           <td class="r">
            EPFL
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417440" title="Click to go to the Author Index">
             Gao, Yang
            </a>
           </td>
           <td class="r">
            EPFL
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417441" title="Click to go to the Author Index">
             Demonsant, Céline
            </a>
           </td>
           <td class="r">
            EPFL
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#205644" title="Click to go to the Author Index">
             Alahi, Alexandre
            </a>
           </td>
           <td class="r">
            EPFL
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1084" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#intelligent_transportation_systems" title="Click to go to the Keyword Index">
               Intelligent Transportation Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_transportation" title="Click to go to the Keyword Index">
               Computer Vision for Transportation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Conventional human trajectory prediction models rely on clean curated data, requiring specialized equipment or manual labeling, which is often impractical for robotic applications. The existing predictors tend to overfit to clean observation affecting their robustness when used with noisy inputs. In this work, we propose MonoTransmotion (MT), a Transformer-based framework that uses only a monocular camera to jointly solve localization and prediction tasks. Our framework has two main modules: Bird’s Eye View (BEV) localization and trajectory prediction. The BEV localization module estimates the position of a person using 2D human poses, enhanced by a novel directional loss for smoother sequential localizations. The trajectory prediction module predicts future motion from these estimates. We show that by jointly training both tasks with our unified framework, our method is more robust in real-world scenarios made of noisy inputs. We validate our MT network on both curated and non-curated datasets. On the curated dataset, MT achieves around 12% improvement over baseline models on BEV localization and trajectory prediction. On real-world non-curated dataset, experimental results indicate that MT maintains similar performance levels, highlighting its robustness and generalization capability.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt11_03">
             15:25-15:30, Paper ThDT11.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1852'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              HGSLoc: 3DGS-Based Heuristic Camera Pose Refinement
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#418638" title="Click to go to the Author Index">
             Niu, Zhongyan
            </a>
           </td>
           <td class="r">
            National University of Defense Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#362032" title="Click to go to the Author Index">
             Tan, Zhen
            </a>
           </td>
           <td class="r">
            National University of Defense Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422351" title="Click to go to the Author Index">
             Zhang, Jinpu
            </a>
           </td>
           <td class="r">
            National University of Defense Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422304" title="Click to go to the Author Index">
             Yang, Xueliang
            </a>
           </td>
           <td class="r">
            National University of Defense Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#332310" title="Click to go to the Author Index">
             Hu, Dewen
            </a>
           </td>
           <td class="r">
            National University of Defense Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1852" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Visual localization refers to the process of determining camera poses and orientation within a known scene representation. This task is often complicated by factors such as changes in illumination and variations in viewing angles. In this paper, we propose HGSLoc, a novel lightweight plug-and-play pose optimization framework, which integrates 3D reconstruction with a heuristic refinement strategy to achieve higher pose estimation accuracy. Specifically, we introduce an explicit geometric map for 3D representation and high-fidelity rendering, allowing the generation of high-quality synthesized views to support accurate visual localization. Our method demonstrates higher localization accuracy compared to NeRF-based neural rendering localization approaches. We introduce a heuristic refinement strategy, its efficient optimization capability can quickly locate the target node, while we set the step-level optimization step to enhance the pose accuracy in the scenarios with small errors. With carefully designed heuristic functions, it offers efficient optimization capabilities, enabling rapid error reduction in rough localization estimations. Our method mitigates the dependence on complex neural network models while demonstrating improved robustness against noise and higher localization accuracy in challenging environments, as compared to neural network joint optimization strategies. The optimization framework proposed in this paper introduces novel approaches to visual localization by integrating the advantages of 3D reconstruction and the heuristic refinement strategy, which demonstrates strong performance across multiple benchmark datasets, including 7Scenes and Deep Blending dataset. The implementation of our method has been released at https://github.com/anchang699/HGSLoc.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt11_04">
             15:30-15:35, Paper ThDT11.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3439'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Depth Estimation Based on 3D Gaussian Splatting Siamese Defocus
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#379578" title="Click to go to the Author Index">
             Zhang, Jinchang
            </a>
           </td>
           <td class="r">
            University of Georgia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#383476" title="Click to go to the Author Index">
             Xu, Ningning
            </a>
           </td>
           <td class="r">
            University of Georgia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#138554" title="Click to go to the Author Index">
             Zhang, Hao
            </a>
           </td>
           <td class="r">
            University of Massachusetts Amherst
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#235923" title="Click to go to the Author Index">
             Lu, Guoyu
            </a>
           </td>
           <td class="r">
            University of Georgia
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3439" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#range_sensing" title="Click to go to the Keyword Index">
               Range Sensing
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Depth estimation is a fundamental task in 3D geometry. While stereo depth estimation can be achieved through triangulation methods, it is not as straightforward for monocular methods, which require the integration of global and local information. The Depth from Defocus (DFD) method utilizes camera lens models and parameters to recover depth information from blurred images and has been proven to perform well. However, these methods rely on All-In-Focus (AIF) images for depth estimation, which is nearly impossible to obtain in real-world applications. To address this issue, we propose a self-supervised framework based on 3D Gaussian splatting and Siamese networks. By learning the blur levels at different focal distances of the same scene in the focal stack, the framework predicts the defocus map and Circle of Confusion (CoC) from a single defocused image, using the defocus map as input to DepthNet for monocular depth estimation. The 3D Gaussian splatting model renders defocused images using the predicted CoC, and the differences between these and the real defocused images provide additional supervision signals for the Siamese Defocus self-supervised network. This framework has been validated on both artificially synthesized and real blurred datasets. Subsequent quantitative and visualization experiments demonstrate that our proposed framework is highly effective as a DFD method.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt11_05">
             15:35-15:40, Paper ThDT11.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4934'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              GSFusion: Online RGB-D Mapping Where Gaussian Splatting Meets TSDF Fusion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#288835" title="Click to go to the Author Index">
             Wei, Jiaxin
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#151104" title="Click to go to the Author Index">
             Leutenegger, Stefan
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4934" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Traditional volumetric fusion algorithms preserve the spatial structure of 3D scenes, which is beneficial for many tasks in computer vision and robotics. However, they often lack realism in terms of visualization. Emerging 3D Gaussian splatting bridges this gap, but existing Gaussian-based reconstruction methods often suffer from artifacts and inconsistencies with the underlying 3D structure, and struggle with real-time optimization, unable to provide users with immediate feedback in high quality. One of the bottlenecks arises from the massive amount of Gaussian parameters that need to be updated during optimization. Instead of using 3D Gaussian as a standalone map representation, we incorporate it into a volumetric mapping system to take advantage of geometric information and propose to use a quadtree data structure on images to drastically reduce the number of splats initialized. In this way, we simultaneously generate a compact 3D Gaussian map with fewer artifacts and a volumetric map on the fly. Our method, GSFusion, significantly enhances computational efficiency without sacrificing rendering quality, as demonstrated on both synthetic and real datasets. Code is available at https://github.com/goldoak/GSFusion.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt11_06">
             15:40-15:45, Paper ThDT11.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4948'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              San Francisco World: Leveraging Structural Regularities of Slope for 3-DoF Visual Compass
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#390048" title="Click to go to the Author Index">
             Ham, Jungil
            </a>
           </td>
           <td class="r">
            Gwangju Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#402408" title="Click to go to the Author Index">
             Kim, Minji
            </a>
           </td>
           <td class="r">
            Gwangju Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#334476" title="Click to go to the Author Index">
             Kang, Suyoung
            </a>
           </td>
           <td class="r">
            University of Massachusetts Amherst
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#188483" title="Click to go to the Author Index">
             Joo, Kyungdon
            </a>
           </td>
           <td class="r">
            UNIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#210132" title="Click to go to the Author Index">
             Li, Haoang
            </a>
           </td>
           <td class="r">
            Hong Kong University of Science and Technology (Guangzhou)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#177811" title="Click to go to the Author Index">
             Kim, Pyojin
            </a>
           </td>
           <td class="r">
            Gwangju Institute of Science and Technology (GIST)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4948" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We propose the San Francisco world (SFW) model, a novel structural model inspired by San Francisco's hilly terrain, enabling 3D inter-floor navigation in urban areas rather than being limited to 2D intra-floor navigation of various robotics platforms. Our SFW consists of a single vertical dominant direction (VDD), two horizontal dominant directions (HDDs), and four sloping dominant directions (SDDs) sharing a common inclination angle. Although SFW is a more general model than the Manhattan world (MW), it is a more compact model than the mixture of Manhattan world (MMW). Leveraging the structural regularities of SFW, such as uniform inclination angle and geometric patterns of the four SDDs, we design an efficient and robust DD/vanishing point estimation method by aggregating sloping line normals on the Gaussian sphere. We further utilize the structural patterns of SFW for the 3-DoF visual compass, the rotational motion tracking from a single line and plane, which corresponds to the theoretical minimal sampling for 3-DoF rotation estimation. Our method demonstrates enhanced adaptability in more challenging inter-floor scenes in urban areas and the highest rotational tracking accuracy compared to state-of-the-art methods. We release the first dataset of sequential RGB-D images captured in San Francisco world (SFW) and open source codes at: https://SanFranciscoWorld.github.io/.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt11_07">
             15:45-15:50, Paper ThDT11.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4988'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Monocular 360 Depth Estimation Via Spherical Fully-Connected CRFs
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#363942" title="Click to go to the Author Index">
             Cao, Zidong
            </a>
           </td>
           <td class="r">
            HKUST
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#256235" title="Click to go to the Author Index">
             Wang, Lin
            </a>
           </td>
           <td class="r">
            Nanyang Technological University (NTU)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4988" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#omnidirectional_vision" title="Click to go to the Keyword Index">
               Omnidirectional Vision
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Monocular 360 depth estimation poses significant challenges due to the inherent distortion of the equirectangular projection (ERP). This distortion separates adjacent spherical points after their projection onto the ERP plane, especially in the polar regions, resulting in insufficient spherical relationships. To address this issue, recent methods calculate spherical neighbors within the tangent domain. However, since the tangent patch and the sphere share only one common point, spherical relationships are established only among neighbors around this common point. In this paper, we propose Spherical Fully-Connected CRFs (SF-CRFs). We start by evenly partitioning an ERP image into regular windows, where windows at the equator have broader spherical neighbors than those at the poles. To enhance spherical relationships, our SF-CRFs feature two key components. Firstly, to include sufficient spherical neighbors, we introduce a Spherical Window Transform (SWT) module. This module replicates the equator window’s spherical relationships across all other windows, leveraging the rotational invariance of the sphere. Remarkably, the transformation process is efficient, transforming all windows in a 512x1024 ERP image in just 0.038 seconds on a CPU. Secondly, we introduce a Planar-Spherical Interaction (PSI) module to calculate the SF-CRFs, which facilitates the relationships between regular and transformed windows. By integrating SF-CRFs blocks into a decoder, we propose CRF360D, a novel 360 depth estimation framework that achieves state-of-the-art performance across diverse datasets. Our CRF360D is compatible with different perspective image-trained backbones, serving as the encoder.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt12">
             <b>
              ThDT12
             </b>
            </a>
           </td>
           <td class="r">
            315
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thdt12" title="Click to go to the Program at a Glance">
             <b>
              Motion Control 1
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#424144" title="Click to go to the Author Index">
             Zhang, Cheng
            </a>
           </td>
           <td class="r">
            Texas A&amp;M University
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#168176" title="Click to go to the Author Index">
             Roncone, Alessandro
            </a>
           </td>
           <td class="r">
            University of Colorado Boulder
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt12_01">
             15:15-15:20, Paper ThDT12.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('153'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Bidirectional Energy Flow Modulation for Passive Admittance Control
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#196687" title="Click to go to the Author Index">
             Lee, Donghyeon
            </a>
           </td>
           <td class="r">
            Pohang University of Science and Technology(POSTECH)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#237548" title="Click to go to the Author Index">
             Ko, Dongwoo
            </a>
           </td>
           <td class="r">
            POSTECH
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#149446" title="Click to go to the Author Index">
             Kim, Min Jun
            </a>
           </td>
           <td class="r">
            KAIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#100095" title="Click to go to the Author Index">
             Chung, Wan Kyun
            </a>
           </td>
           <td class="r">
            POSTECH
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab153" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#compliance_and_impedance_control" title="Click to go to the Keyword Index">
               Compliance and Impedance Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#force_control" title="Click to go to the Keyword Index">
               Force Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#physical_human_robot_interaction" title="Click to go to the Keyword Index">
               Physical Human-Robot Interaction
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#passivity_based_control" title="Click to go to the Keyword Index">
               Passivity-based Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Admittance control is a control scheme to enable physical interactions of a robot, but it easily induces instability when the robot contacts a rigid surface. In this study, passivity analysis was conducted on a robotic system with admittance control. The results showed that coupled stability with the environment can be ensured when the velocity error between the proxy and the real robot is eliminated. Thus, an adaptive structure modification method is proposed to suppress the possible source of instability. In addition, the energy tank method is combined with the proposed method to ensure system passivity. As a proof of concept, three robot experiments were performed, and the results of the proposed method were compared with those of the conventional admittance control and impedance control (with friction compensation). The comparison showed that the proposed method could make the system passive while it realized the desired dynamics during the interaction.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt12_02">
             15:20-15:25, Paper ThDT12.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('354'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Minimum-Jerk Approach to Handle Singularities in Virtual Fixtures
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#340524" title="Click to go to the Author Index">
             Braglia, Giovanni
            </a>
           </td>
           <td class="r">
            University of Modena and Reggio Emilia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#107089" title="Click to go to the Author Index">
             Calinon, Sylvain
            </a>
           </td>
           <td class="r">
            Idiap Research Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#112181" title="Click to go to the Author Index">
             Biagiotti, Luigi
            </a>
           </td>
           <td class="r">
            University of Modena and Reggio Emilia
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab354" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Implementing virtual fixtures in guiding tasks constrains the movement of the robot's end effector to specific curves within its workspace. However, incorporating guiding frameworks may encounter discontinuities when optimizing the reference target position to the nearest point relative to the current robot position. This article aims to give a geometric interpretation of such discontinuities, with specific reference to the commonly adopted Gauss-Newton algorithm. The effect of such discontinuities, defined as Euclidean Distance Singularities, is experimentally proved.We then propose a solution that is based on a linear quadratic tracking problem with minimum jerk command, then compare and validate the performances of the proposed framework in two different human-robot interaction scenarios.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt12_03">
             15:25-15:30, Paper ThDT12.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1261'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Continuous Wrist Control on the Hannes Prosthesis: A Vision-Based Shared Autonomy Framework
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#325269" title="Click to go to the Author Index">
             Vasile, Federico
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#216562" title="Click to go to the Author Index">
             Maiettini, Elisa
            </a>
           </td>
           <td class="r">
            Humanoid Sensing and Perception, Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#180351" title="Click to go to the Author Index">
             Pasquale, Giulia
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#232991" title="Click to go to the Author Index">
             Boccardo, Nicolò
            </a>
           </td>
           <td class="r">
            IIT - Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#111056" title="Click to go to the Author Index">
             Natale, Lorenzo
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1261" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#sensor_based_control" title="Click to go to the Keyword Index">
               Sensor-based Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#prosthetics_and_exoskeletons" title="Click to go to the Keyword Index">
               Prosthetics and Exoskeletons
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Most control techniques for prosthetic grasping focus on dexterous fingers control, but overlook the wrist motion. This forces the user to perform compensatory movements with the elbow, shoulder and hip to adapt the wrist for grasping. We propose a computer vision-based system that leverages the collaboration between the user and an automatic system in a shared autonomy framework, to perform continuous control of the wrist degrees of freedom in a prosthetic arm, promoting a more natural approach-to-grasp motion. Our pipeline allows to seamlessly control the prosthetic wrist to follow the target object and finally orient it for grasping according to the user intent. We assess the effectiveness of each system component through quantitative analysis and finally deploy our method on the Hannes prosthetic arm. Code and videos: https://hsp-iit.github.io/hannes-wrist-control
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt12_04">
             15:30-15:35, Paper ThDT12.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3874'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Integrating Learning-Based Manipulation and Physics-Based Locomotion for Whole-Body Badminton Robot Control
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#384252" title="Click to go to the Author Index">
             Wang, Haochen
            </a>
           </td>
           <td class="r">
            Shandong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423531" title="Click to go to the Author Index">
             Zhiwei, Shi
            </a>
           </td>
           <td class="r">
            Shandong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423522" title="Click to go to the Author Index">
             Zhu, Chengxi
            </a>
           </td>
           <td class="r">
            Shandong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425941" title="Click to go to the Author Index">
             Qiao, Yafei
            </a>
           </td>
           <td class="r">
            Shandong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424144" title="Click to go to the Author Index">
             Zhang, Cheng
            </a>
           </td>
           <td class="r">
            Texas A&amp;M University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425972" title="Click to go to the Author Index">
             Yang, Fan
            </a>
           </td>
           <td class="r">
            Deepcode Robotics Co., Ltd
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425190" title="Click to go to the Author Index">
             Ren, Pengjie
            </a>
           </td>
           <td class="r">
            Shandong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#418405" title="Click to go to the Author Index">
             Lu, Lan
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423238" title="Click to go to the Author Index">
             Xuan, Dong
            </a>
           </td>
           <td class="r">
            Shandong University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3874" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#product_design__development_and_prototyping" title="Click to go to the Keyword Index">
               Product Design, Development and Prototyping
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#autonomous_agents" title="Click to go to the Keyword Index">
               Autonomous Agents
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Learning-based methods, such as imitation learning (IL) and reinforcement learning (RL), can produce excel control policies over challenging agile robot tasks, such as sports robot. However, no existing work has harmonized learning-based policy with model-based methods to reduce training complexity and ensure the safety and stability for agile badminton robot control. In this paper, we introduce Hamlet, a novel hybrid control system for agile badminton robots. Specifically, we propose a model-based strategy for chassis locomotion which provides a base for arm policy. We introduce a physics-informed “IL+RL” training framework for learning-based arm policy. In this train framework, a model-based strategy with privileged information is used to guide arm policy training during both IL and RL phases. In addition, we train the critic model during IL phase to alleviate the performance drop issue when transitioning from IL to RL. We present results on our self-engineered badminton robot, achieving 94.5% success rate against the serving machine and 90.7% success rate against human players. Our system can be easily generalized to other agile mobile manipulation tasks e.g., agile catching, table tennis. A video demonstrating our system can be viewed at https://youtu.be/8-ixKAD18Mk.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt12_05">
             15:35-15:40, Paper ThDT12.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4478'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Leveraging Symmetry to Accelerate Learning of Trajectory Tracking Controllers for Free-Flying Robotic Systems
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#208740" title="Click to go to the Author Index">
             Welde, Jake
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#333534" title="Click to go to the Author Index">
             Rao, Nishanth Arun
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#247349" title="Click to go to the Author Index">
             Kunapuli, Pratik
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#207182" title="Click to go to the Author Index">
             Jayaraman, Dinesh
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104342" title="Click to go to the Author Index">
             Kumar, Vijay
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4478" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#dynamics" title="Click to go to the Keyword Index">
               Dynamics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__mechanics_and_control" title="Click to go to the Keyword Index">
               Aerial Systems: Mechanics and Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Tracking controllers enable robotic systems to accurately follow planned reference trajectories. In particular, reinforcement learning (RL) has shown promise in the synthesis of controllers for systems with complex dynamics and modest online compute budgets. However, the poor sample efficiency of RL and the challenges of reward design make training slow and sometimes unstable, especially for high-dimensional systems. In this work, we leverage the inherent Lie group symmetries of robotic systems with a floating base to mitigate these challenges when learning tracking controllers. We model a general tracking problem as a Markov decision process (MDP) that captures the evolution of both the physical and reference states. Next, we prove that symmetry in the underlying dynamics and running costs leads to an MDP homomorphism, a mapping that allows a policy trained on a lower-dimensional "quotient" MDP to be lifted to an optimal tracking controller for the original system. We compare this symmetry-informed approach to an unstructured baseline, using Proximal Policy Optimization (PPO) to learn tracking controllers for three systems: the Particle (a forced point mass), the Astrobee (a fully-actuated space robot), and the Quadrotor (an underactuated system). Results show that a symmetry-aware approach both accelerates training and reduces tracking error at convergence.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt12_06">
             15:40-15:45, Paper ThDT12.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4961'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Quadratic Programming-Based Reference Spreading Control for Dual-Arm Robotic Manipulation with Planned Simultaneous Impacts
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#307517" title="Click to go to the Author Index">
             van Steen, Jari J.
            </a>
           </td>
           <td class="r">
            Eindhoven University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#362611" title="Click to go to the Author Index">
             van den Brandt, Gijs
            </a>
           </td>
           <td class="r">
            Eindhoven University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#110160" title="Click to go to the Author Index">
             van de Wouw, Nathan
            </a>
           </td>
           <td class="r">
            Eindhoven University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#117613" title="Click to go to the Author Index">
             Kober, Jens
            </a>
           </td>
           <td class="r">
            TU Delft
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#112679" title="Click to go to the Author Index">
             Saccon, Alessandro
            </a>
           </td>
           <td class="r">
            Eindhoven University of Technology - TU/e
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4961" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#impact_aware_manipulation" title="Click to go to the Keyword Index">
               Impact-aware manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_control_of_manipulators" title="Click to go to the Keyword Index">
               Motion Control of Manipulators
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#dual_arm_manipulation" title="Click to go to the Keyword Index">
               Dual Arm Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             With the aim of further enabling the exploitation of intentional impacts in robotic manipulation, a control framework is presented that directly tackles the challenges posed by tracking control of robotic manipulators that are tasked to perform nominally simultaneous impacts. This framework is an extension of the reference spreading (RS) control framework, in which overlapping anteand post-impact references that are consistent with impact dynamics are defined. In this work, such a reference is constructed starting from a teleoperation-based approach. By using the corresponding ante- and post-impact control modes in the scope of a quadratic programming control approach, peaking of the velocity error and control inputs due to impacts is avoided while maintaining high tracking performance. With the inclusion of a novel interim mode, we aim to also avoid input peaks and steps when uncertainty in the environment causes a series of unplanned single impacts to occur rather than the planned simultaneous impact. This work in particular presents for the first time an experimental evaluation of RS control on a robotic setup, showcasing its robustness against uncertainty in the environment compared to three baseline control approaches.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt12_07">
             15:45-15:50, Paper ThDT12.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('5038'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              HARMONIOUS - Human-Like Reactive Motion Control and Multimodal Perception for Humanoid Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#284753" title="Click to go to the Author Index">
             Rozlivek, Jakub
            </a>
           </td>
           <td class="r">
            Czech Technical University in Prague, Faculty of Electrical Engi
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#168176" title="Click to go to the Author Index">
             Roncone, Alessandro
            </a>
           </td>
           <td class="r">
            University of Colorado Boulder
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#134046" title="Click to go to the Author Index">
             Pattacini, Ugo
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#114104" title="Click to go to the Author Index">
             Hoffmann, Matej
            </a>
           </td>
           <td class="r">
            Czech Technical University in Prague, Faculty of Electrical Engi
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab5038" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#humanoid_robots" title="Click to go to the Keyword Index">
               Humanoid Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#physical_human_robot_interaction" title="Click to go to the Keyword Index">
               Physical Human-Robot Interaction
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#collision_avoidance" title="Click to go to the Keyword Index">
               Collision Avoidance
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#biologically_inspired_robots" title="Click to go to the Keyword Index">
               Biologically-Inspired Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             For safe and effective operation of humanoid robots in human-populated environments, the problem of commanding a large number of Degrees of Freedom (DoF) while simultaneously considering dynamic obstacles and human proximity has still not been solved. We present a new reactive motion controller that commands two arms of a humanoid robot and three torso joints (17 DoF in total). We formulate a quadratic program that seeks joint velocity commands respecting multiple constraints while minimizing the magnitude of the velocities. We introduce a new unified treatment of obstacles that dynamically maps visual and proximity (pre-collision) and tactile (post-collision) obstacles as additional constraints to the motion controller, in a distributed fashion over the surface of the upper body of the iCub robot (with 2000 pressure-sensitive receptors). This results in a bio-inspired controller that: (i) gives rise to a robot with whole-body visuo-tactile awareness, resembling peripersonal space representations, and (ii) produces human-like minimum jerk movement profiles. The controller was extensively experimentally validated, including a physical human-robot interaction scenario.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt13">
             <b>
              ThDT13
             </b>
            </a>
           </td>
           <td class="r">
            316
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thdt13" title="Click to go to the Program at a Glance">
             <b>
              Resiliency and Security 1
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt13_01">
             15:15-15:20, Paper ThDT13.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('365'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              FedDet: Data Poisoning Attack Detection for Federated Skeleton-Based Action Recognition
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#373977" title="Click to go to the Author Index">
             Kim, Min Hyuk
            </a>
           </td>
           <td class="r">
            Chonnam National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416522" title="Click to go to the Author Index">
             Lee, Eungi
            </a>
           </td>
           <td class="r">
            Chonnam National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#361903" title="Click to go to the Author Index">
             Yoo, Seok Bong
            </a>
           </td>
           <td class="r">
            Chonnam National University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab365" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#recognition" title="Click to go to the Keyword Index">
               Recognition
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Skeleton-based action recognition (SAR) models often centralize skeleton data, increasing significant privacy concerns. To address this, decentralized training models for SAR have been advanced, particularly using federated learning (FL), a research area of considerable value with wide-ranging applications, including human-robot interaction, camera-enabled devices, and security surveillance. However, FL-based SAR faces the challenge of substantial accuracy degradation due to data poisoning attacks; thus, it requires the identification of malicious clients. This paper introduces a novel approach for detecting data poisoning attacks in federated SAR, called FedDet. The method involves creating prototypes of perspective transform and exchanging these matrices between the clients and server to identify the malicious client. Additionally, a prototype-guided attack detector is developed, incorporating spatiotemporal matching to analyze the correlation between prototype skeleton data. Experimental results on FL frameworks and SAR models demonstrate that the proposed approach outperforms existing models. Our code is publicly available at https://github.com/alsgur0720/federated-detection.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt13_02">
             15:20-15:25, Paper ThDT13.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('573'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              ROS2WASM: Bringing the Robot Operating System to the Web
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#191075" title="Click to go to the Author Index">
             Fischer, Tobias
            </a>
           </td>
           <td class="r">
            Queensland University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417932" title="Click to go to the Author Index">
             Paredes, Isabel
            </a>
           </td>
           <td class="r">
            QuantStack, RWTH Aachen
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416789" title="Click to go to the Author Index">
             Batchelor, Michael
            </a>
           </td>
           <td class="r">
            Queensland University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417006" title="Click to go to the Author Index">
             Beier, Thorsten
            </a>
           </td>
           <td class="r">
            QuantStack
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#268749" title="Click to go to the Author Index">
             Haviland, Jesse
            </a>
           </td>
           <td class="r">
            Queensland University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#167320" title="Click to go to the Author Index">
             Traversaro, Silvio
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#182705" title="Click to go to the Author Index">
             Vollprecht, Wolf Kristian
            </a>
           </td>
           <td class="r">
            QuantStack
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#253619" title="Click to go to the Author Index">
             Schmitz, Markus
            </a>
           </td>
           <td class="r">
            RWTH Aachen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#106848" title="Click to go to the Author Index">
             Milford, Michael J
            </a>
           </td>
           <td class="r">
            Queensland University of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab573" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#software_tools_for_robot_programming" title="Click to go to the Keyword Index">
               Software Tools for Robot Programming
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#software_tools_for_benchmarking_and_reproducibility" title="Click to go to the Keyword Index">
               Software Tools for Benchmarking and Reproducibility
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#engineering_for_robotic_systems" title="Click to go to the Keyword Index">
               Engineering for Robotic Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The Robot Operating System (ROS) has become the de facto standard middleware in robotics, widely adopted across domains ranging from education to industrial applications. The RoboStack distribution, a conda-based packaging system for ROS, has extended ROS's accessibility by facilitating installation across all major operating systems and architectures, integrating seamlessly with scientific tools such as PyTorch and Open3D. This paper presents ROS2WASM, a novel integration of RoboStack with WebAssembly, enabling the execution of ROS 2 and its associated software directly within web browsers, without requiring local installations. ROS2WASM significantly enhances the reproducibility and shareability of research, lowers barriers to robotics education, and leverages WebAssembly's robust security framework to protect against malicious code. We detail our methodology for cross-compiling ROS 2 packages into WebAssembly, the development of a specialized middleware for ROS 2 communication within browsers, and the implementation of www.ros2wasm.dev, a web platform enabling users to interact with ROS 2 environments. Additionally, we extend support to the Robotics Toolbox for Python and adapt its Swift simulator for browser compatibility. Our work paves the way for unprecedented accessibility in robotics, offering scalable, secure, and reproducible environments that have the potential to transform educational and research paradigms.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt13_03">
             15:25-15:30, Paper ThDT13.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('619'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Prepared for the Worst: Resilience Analysis of the ICP Algorithm Via Learning-Based Worst-Case Adversarial Attacks
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#394315" title="Click to go to the Author Index">
             Zhang, Ziyu
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#236390" title="Click to go to the Author Index">
             Laconte, Johann
            </a>
           </td>
           <td class="r">
            French National Research Institute for Agriculture, Food and The
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#295983" title="Click to go to the Author Index">
             Lisus, Daniil
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#115139" title="Click to go to the Author Index">
             Barfoot, Timothy
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab619" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#robot_safety" title="Click to go to the Keyword Index">
               Robot Safety
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a novel method for assessing the resilience of the ICP algorithm via learning-based, worst-case attacks on lidar point clouds. For safety-critical applications such as autonomous navigation, ensuring the resilience of algorithms before deployments is crucial. The ICP algorithm is the standard for lidar-based localization, but its accuracy can be greatly affected by corrupted measurements from various sources, including occlusions, adverse weather, or mechanical sensor issues. Unfortunately, the complex and iterative nature of ICP makes assessing its resilience to corruption challenging. While there have been efforts to create challenging datasets and develop simulations to evaluate the resilience of ICP, our method focuses on finding the maximum possible ICP error that can arise from corrupted measurements at a location. We demonstrate that our perturbation-based adversarial attacks can be used pre-deployment to identify locations on a map where ICP is particularly vulnerable to corruptions in the measurements. With such information, autonomous robots can take safer paths when deployed, to mitigate against their measurements being corrupted. The proposed attack outperforms baselines more than 88% of the time across a wide range of scenarios.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt13_04">
             15:30-15:35, Paper ThDT13.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1262'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SLAMSpoof: Practical LiDAR Spoofing Attacks on Localization Systems Guided by Scan Matching Vulnerability Analysis
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419347" title="Click to go to the Author Index">
             Nagata, Rokuto
            </a>
           </td>
           <td class="r">
            Keio University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#168862" title="Click to go to the Author Index">
             Koide, Kenji
            </a>
           </td>
           <td class="r">
            National Institute of Advanced Industrial Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420884" title="Click to go to the Author Index">
             Hayakawa, Yuki
            </a>
           </td>
           <td class="r">
            Keio University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420775" title="Click to go to the Author Index">
             Suzuki, Ryo
            </a>
           </td>
           <td class="r">
            Keio University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420900" title="Click to go to the Author Index">
             Ikeda, Kazuma
            </a>
           </td>
           <td class="r">
            Keio University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420365" title="Click to go to the Author Index">
             Sako, Ozora
            </a>
           </td>
           <td class="r">
            Keio University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#339580" title="Click to go to the Author Index">
             Chen, Qi Alfred
            </a>
           </td>
           <td class="r">
            University of California, Irvine
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#354329" title="Click to go to the Author Index">
             Sato, Takami
            </a>
           </td>
           <td class="r">
            University of California, Irvine
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#249880" title="Click to go to the Author Index">
             Yoshioka, Kentaro
            </a>
           </td>
           <td class="r">
            Keio University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1262" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#intelligent_transportation_systems" title="Click to go to the Keyword Index">
               Intelligent Transportation Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Accurate localization is essential for enabling modern full self-driving services. These services heavily rely on map-based traffic information to reduce uncertainties in recognizing lane shapes, traffic light locations, and traffic signs. Achieving this level of reliance on map information requires centimeter-level localization accuracy, which is currently only achievable with LiDAR sensors. However, LiDAR is known to be vulnerable to spoofing attacks that emit malicious lasers against LiDAR to overwrite its measurements. Once localization is compromised, the attack could lead the victim off roads or make them ignore traffic lights. Motivated by these serious safety implications, we design SLAMSpoof, the first practical LiDAR spoofing attack on localization systems for self-driving to assess the actual attack significance on autonomous vehicles. SLAMSpoof can effectively find the effective attack location based on our scan matching vulnerability score (SMVS), a point-wise metric representing the potential vulnerability to spoofing attacks. To evaluate the effectiveness of the attack, we conduct real-world experiments on ground vehicles and confirm its high capability in real-world scenarios, inducing position errors of ≥4.2 meters (more than typical lane width) for all 3 popular LiDAR-based localization algorithms. We finally discuss the potential countermeasures of this attack. Code is available at https://github.com/Keio-CSG/slamspoof
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt13_05">
             15:35-15:40, Paper ThDT13.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1838'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Gradient-Based Adversarial Attacks on Deep LiDAR Odometry
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#266220" title="Click to go to the Author Index">
             Song, Zhenbo
            </a>
           </td>
           <td class="r">
            Nanjing University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#393353" title="Click to go to the Author Index">
             Chen, Xuanzhu
            </a>
           </td>
           <td class="r">
            Nanjing University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#392983" title="Click to go to the Author Index">
             Zhang, Zhenyuan
            </a>
           </td>
           <td class="r">
            Nanjing University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#355049" title="Click to go to the Author Index">
             Zhang, Kaihao
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology, Shenzhen
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#195506" title="Click to go to the Author Index">
             Lu, Jianfeng
            </a>
           </td>
           <td class="r">
            Nanjing University of Science &amp; Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#395851" title="Click to go to the Author Index">
             Li, Weiqing
            </a>
           </td>
           <td class="r">
            Nanjing University of Sci.&amp;Tech
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1838" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#intelligent_transportation_systems" title="Click to go to the Keyword Index">
               Intelligent Transportation Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#robot_safety" title="Click to go to the Keyword Index">
               Robot Safety
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Adversarial attacks have been recently investigated in LiDAR perception problems for autonomous driving, where a small perturbation to the source inputs can result in incorrect predictions. However, most prior studies focus on attacks on single-frame perception modules, lacking explorations of attacks on consecutive-frame tasks, i.e. the LiDAR odometry. In this paper, we propose a gradient optimization-based adversarial attack towards deep LiDAR odometry networks. To generate point clouds consistent with real-world scenarios, we constrain adversarial points within the range of a small object, e.g. a traffic cone, and render new points to simulate real LiDAR measurements. By incorporating such adversarial points in consecutive frames, we demonstrate a significant decrease in pose estimation accuracy of current popular LiDAR odometry networks. In addition, we also evaluate traditional geometric odometry approaches and report their robustness over adversarial points. Extensive experiments on the KITTI and Waymo datasets illustrate the effectiveness of the proposed attack method and the vulnerability of deep LiDAR odometry methods against adversarial points.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt13_06">
             15:40-15:45, Paper ThDT13.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3595'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Enhancing 3D Robotic Vision Robustness by Minimizing Adversarial Mutual Information through a Curriculum Training Approach
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#378909" title="Click to go to the Author Index">
             Darabi, Nastaran
            </a>
           </td>
           <td class="r">
            University of Illinois Chicago
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425435" title="Click to go to the Author Index">
             Jayasuriya, Dinithi
            </a>
           </td>
           <td class="r">
            University of Illinois Chicago
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425459" title="Click to go to the Author Index">
             Naik, Devashri
            </a>
           </td>
           <td class="r">
            University of Illinois Chicago
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#355276" title="Click to go to the Author Index">
             Tulabandhula, Theja
            </a>
           </td>
           <td class="r">
            University of Illinois Chicago
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#355268" title="Click to go to the Author Index">
             Trivedi, Amit Ranjan
            </a>
           </td>
           <td class="r">
            University of Illinois at Chicago (UIC), Chicago, USA
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3595" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#robot_safety" title="Click to go to the Keyword Index">
               Robot Safety
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Adversarial attacks exploit vulnerabilities in a model's decision boundaries through small, carefully crafted perturbations that lead to significant mispredictions. In 3D vision, the high dimensionality and sparsity of data greatly expand the attack surface, making 3D vision particularly vulnerable for safety-critical robotics. To enhance 3D vision's adversarial robustness, we propose a training objective that simultaneously minimizes prediction loss and mutual information (MI) under adversarial perturbations to contain the upper bound of misprediction errors. This approach simplifies handling adversarial examples compared to conventional methods, which require explicit searching and training on adversarial samples. However, minimizing prediction loss conflicts with minimizing MI, leading to reduced robustness and catastrophic forgetting. To address this, we integrate curriculum advisors in the training setup that gradually introduce adversarial objectives to balance training and prevent models from being overwhelmed by difficult cases early in the process. The advisors also enhance robustness by encouraging training on diverse MI examples through entropy regularizers. We evaluated our method on ModelNet40 and KITTI using PointNet, DGCNN, SECOND, and PointTransformers, achieving 2--5% accuracy gains on ModelNet40 and a 5--10% mAP improvement in object detection. Our code is publicly available at https://github.com/nstrndrbi/Mine-N-Learn.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt14">
             <b>
              ThDT14
             </b>
            </a>
           </td>
           <td class="r">
            402
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thdt14" title="Click to go to the Program at a Glance">
             <b>
              End-Effectors
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#194194" title="Click to go to the Author Index">
             Hughes, Josie
            </a>
           </td>
           <td class="r">
            EPFL
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#100118" title="Click to go to the Author Index">
             Tadokoro, Satoshi
            </a>
           </td>
           <td class="r">
            Tohoku University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt14_01">
             15:15-15:20, Paper ThDT14.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('96'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              PaTS-Wheel: A Passively-Transformable Single-Part Wheel for Mobile Robot Navigation on Unstructured Terrain
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#359289" title="Click to go to the Author Index">
             Godden, Thomas
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#320715" title="Click to go to the Author Index">
             Mulvey, Barry William
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#385326" title="Click to go to the Author Index">
             Redgrave, Ellen
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#140137" title="Click to go to the Author Index">
             Nanayakkara, Thrishantha
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab96" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#compliant_joints_and_mechanisms" title="Click to go to the Keyword Index">
               Compliant Joints and Mechanisms
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#underactuated_robots" title="Click to go to the Keyword Index">
               Underactuated Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Most mobile robots use wheels that perform well on even and structured ground, like in factories and warehouses. However, they face challenges traversing unstructured terrain such as stepped obstacles. This paper presents the design and testing of the PaTS-Wheel: a Passively-Transformable Single-part Wheel that can transform to render hooks when presented with obstacles. The passive rendering of this useful morphological feature is guided purely by the geometry of the obstacle. The energy consumption and vibrational profile of the PaTS-Wheel on flat ground is comparable to a standard wheel of the same size. In addition, our novel wheel design (with a diameter of 120 mm) was tested traversing different terrains with stepped obstacles of incremental heights. The PaTS-Wheel achieved 100 % success rate at traversing stepped obstacles 83 mm high (~ 70 % its diameter), higher than the results obtained for an equivalent wheel at 30 mm (~ 25 % its diameter) and an equivalent wheg at 73 mm (~ 61% its diameter). This achieves the design objectives of combining the energy efficiency and ride smoothness of wheels with the obstacle traversal capabilities of legged robots, all without requiring any sensors, actuators, or controllers.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt14_02">
             15:20-15:25, Paper ThDT14.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('914'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Balloon Pin-Array Gripper: Two-Step Shape Adaptation Mechanism for Stable Grasping against Object Misalignment
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#321877" title="Click to go to the Author Index">
             Kemmotsu, Yuto
            </a>
           </td>
           <td class="r">
            Tohoku University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#102290" title="Click to go to the Author Index">
             Tadakuma, Kenjiro
            </a>
           </td>
           <td class="r">
            Osaka University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#254295" title="Click to go to the Author Index">
             Abe, Kazuki
            </a>
           </td>
           <td class="r">
            Osaka University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#192974" title="Click to go to the Author Index">
             Watanabe, Masahiro
            </a>
           </td>
           <td class="r">
            Osaka University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#100118" title="Click to go to the Author Index">
             Tadokoro, Satoshi
            </a>
           </td>
           <td class="r">
            Tohoku University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab914" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#compliant_joints_and_mechanisms" title="Click to go to the Keyword Index">
               Compliant Joints and Mechanisms
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#grasping" title="Click to go to the Keyword Index">
               Grasping
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#soft_robot_materials_and_design" title="Click to go to the Keyword Index">
               Soft Robot Materials and Design
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This study introduces a balloon pin-array gripper combining shape adaptability to various objects, stable holding by multipoint contact, and isotropic grasping performance. This is particularly useful when the shape or position of the objects cannot be accurately determined because of sensor limitations. This gripper has multiple pins whose tips are covered by flexible balloons. The gripper can adapt to the shapes of objects in two steps: axial sliding of the pins and radial inflation of the balloons. This study focuses on the effect of the layout of pins on grasping and proposes a simulation model to quantify the characteristics of each layout. Simulations showed that the concentric layout enables stable grasping by ensuring many pins contact the object, regardless of misalignment. Experiments using a prototype gripper demonstrated a trend consistent with the simulation results, proving the validity of the simulation model.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt14_03">
             15:25-15:30, Paper ThDT14.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1010'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Adaptive Perching and Grasping by Aerial Robot with Light-Weight and High Grip-Force Tendon-Driven Three-Fingered Hand Using Single Actuator
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420295" title="Click to go to the Author Index">
             Iida, Hisaaki
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#358537" title="Click to go to the Author Index">
             Sugihara, Junichiro
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#358672" title="Click to go to the Author Index">
             Sugihara, Kazuki
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#346949" title="Click to go to the Author Index">
             Kozuka, Haruki
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#298646" title="Click to go to the Author Index">
             Li, Jinjie
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#256864" title="Click to go to the Author Index">
             Nagato, Keisuke
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#164284" title="Click to go to the Author Index">
             Zhao, Moju
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1010" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multifingered_hands" title="Click to go to the Keyword Index">
               Multifingered Hands
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#tendon_wire_mechanism" title="Click to go to the Keyword Index">
               Tendon/Wire Mechanism
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Aerial robots, especially multirotor type, have been utilized in various scenarios such as inspection, surveillance, and logistics. The most critical issue for multirotor type is the limited flight time due to the large power consumption for hovering against gravity. Inspired by nature, various research focus on the perching and grasping ability by deploying a gripper on the multirotor to grasp arboreal environments for saving energy; however, most the mechanical design for gripper restricts the approach path, which significantly limits the performance of perching and grasping. Besides, it is also challenging to design a light gripper that also offers sufficiently large grip force to hang itself. Therefore, in this work, we develop a single-actuator hand for aerial robot that enables adaptive grasping of various objects, and thus can perch from various approach directions. First, we present the design of the lightweight three-fingered hand with a pair of special two-dimensional differential plates that enables the adaptive grasping with a single actuator. In addition, we develop a unique control method for the over-actuated aerial robot equipped with this hand to perform both adaptive pendulum-like perching and detachment. Finally, we demonstrate the feasibility of the prototype hand via load-bearing test and various object grasping tests, along with the inflight perching experiments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt14_04">
             15:30-15:35, Paper ThDT14.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2861'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              CAFEs: Cable-Driven Collaborative Floating End-Effectors for Agriculture Applications
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#222424" title="Click to go to the Author Index">
             Cheng, Hung Hon
            </a>
           </td>
           <td class="r">
            EPFL
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#194194" title="Click to go to the Author Index">
             Hughes, Josie
            </a>
           </td>
           <td class="r">
            EPFL
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2861" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#robotics_and_automation_in_agriculture_and_forestry" title="Click to go to the Keyword Index">
               Robotics and Automation in Agriculture and Forestry
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#tendon_wire_mechanism" title="Click to go to the Keyword Index">
               Tendon/Wire Mechanism
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#actuation_and_joint_mechanisms" title="Click to go to the Keyword Index">
               Actuation and Joint Mechanisms
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             CAFEs (Collaborative Agricultural Floating End-effectors) is a new robot design and control approach to automating large-scale agricultural tasks. Based upon a cable driven robot architecture, by sharing the same roller-driven cable set with modular robotic arms, a fast-switching clamping mechanism allows each CAFE to clamp onto or release from the moving cables, enabling both independent and synchronized movement across the workspace. The methods developed to enable this system include the mechanical design, precise position control and a dynamic model for the spring-mass liked system, ensuring accurate and stable movement of the robotic arms. The system's scalability is further explored by studying the tension and sag in the cables to maintain performance as more robotic arms are deployed. Experimental and simulation results demonstrate the system’s effectiveness in tasks including pick-and-place showing its potential to contribute to agricultural automation.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt14_05">
             15:35-15:40, Paper ThDT14.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4600'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Robotic Finger with a 4-Bar Linkage-Based Compact and Continuously Variable Active Transmission
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#385757" title="Click to go to the Author Index">
             Chung, Sungho
            </a>
           </td>
           <td class="r">
            Sogang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#385759" title="Click to go to the Author Index">
             Sohn, Eugene
            </a>
           </td>
           <td class="r">
            Sogang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#179932" title="Click to go to the Author Index">
             Jeong, Seokhwan
            </a>
           </td>
           <td class="r">
            Mechanical Eng., Sogang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4600" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#mechanism_design" title="Click to go to the Keyword Index">
               Mechanism Design
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#actuation_and_joint_mechanisms" title="Click to go to the Keyword Index">
               Actuation and Joint Mechanisms
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#compliant_joints_and_mechanisms" title="Click to go to the Keyword Index">
               Compliant Joints and Mechanisms
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a practical design implementation of 4-bar linkage-based compact and continuously variable active transmission (CCVAT) specifically tailored for the form factor of a robotic finger. The proposed CCVAT aims to solve the two major limitations of conventional linkage-based continuously variable transmission: increased inertia and complexities associated with miniaturization. To counter these limitations, our design incorporates a custom flexible shaft within the joint of the robotic finger, enhancing its adaptability and operational efficiency. In addition, we proposed a cascaded control architecture, combining a disturbance-observer-based low-level controller and a mid-level controller responsible for managing both the transmission ratio and flexion angle of the system. Finally, the feasibility of the prototype was evaluated by conducting several experiments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt14_06">
             15:40-15:45, Paper ThDT14.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4937'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Dexterous and Compliant (DexCo) Hand Based on Soft Hydraulic Actuation for Human Inspired Fine In-Hand Manipulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#208757" title="Click to go to the Author Index">
             Zhou, Jianshu
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#269820" title="Click to go to the Author Index">
             Junda, Huang
            </a>
           </td>
           <td class="r">
            Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#345968" title="Click to go to the Author Index">
             Dou, Qi
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#107568" title="Click to go to the Author Index">
             Abbeel, Pieter
            </a>
           </td>
           <td class="r">
            UC Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#100055" title="Click to go to the Author Index">
             Liu, Yunhui
            </a>
           </td>
           <td class="r">
            Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4937" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#dexterous_manipulation" title="Click to go to the Keyword Index">
               Dexterous Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#soft_robot_applications" title="Click to go to the Keyword Index">
               Soft Robot Applications
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multifingered_hands" title="Click to go to the Keyword Index">
               Multifingered Hands
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#grippers_and_other_end_effectors" title="Click to go to the Keyword Index">
               Grippers and Other End-Effectors
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Human beings possess a remarkable skill for fine in-hand manipulation, utilizing both intra-finger interactions (in-finger) and finger-environment interactions across a wide range of daily tasks. These tasks range from skilled activities like screwing light bulbs, picking and sorting pills, and in-hand rotation, to more complex tasks such as opening plastic bags, cluttered bin picking, and counting cards. Despite its prevalence in human activities, replicating these fine motor skills in robotics remains a substantial challenge. This study tackles the challenge of fine in-hand manipulation by introducing the dexterous and compliant (DexCo) hand system. The DexCo hand mimics human dexterity, replicating the intricate interaction between the thumb, index, and middle fingers, with a contractable palm. The key to maneuverable fine in-hand manipulation lies in its innovative soft hydraulic actuation, which strikes a balance between control complexity, dexterity, compliance, and motion accuracy within a compact structure, enhancing the overall performance of the system. The model of soft hydraulic actuation, based on hydrostatic force analysis, reveals the compliance of hand joints, whic
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt15">
             <b>
              ThDT15
             </b>
            </a>
           </td>
           <td class="r">
            403
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thdt15" title="Click to go to the Program at a Glance">
             <b>
              Robot Applications
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#122553" title="Click to go to the Author Index">
             Yim, Sehyuk
            </a>
           </td>
           <td class="r">
            KIST
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#231863" title="Click to go to the Author Index">
             Cramariuc, Andrei
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt15_01">
             15:15-15:20, Paper ThDT15.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('786'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Minimally Designed Audio-Animatronic Robot
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#273751" title="Click to go to the Author Index">
             Park, Kyu Min
            </a>
           </td>
           <td class="r">
            Sejong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#352374" title="Click to go to the Author Index">
             Cheon, Jeongah
            </a>
           </td>
           <td class="r">
            Korea Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#122553" title="Click to go to the Author Index">
             Yim, Sehyuk
            </a>
           </td>
           <td class="r">
            KIST
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab786" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#mechanism_design" title="Click to go to the Keyword Index">
               Mechanism Design
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#additive_manufacturing" title="Click to go to the Keyword Index">
               Additive Manufacturing
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#tendon_wire_mechanism" title="Click to go to the Keyword Index">
               Tendon/Wire Mechanism
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#audio_driven_motion_generation" title="Click to go to the Keyword Index">
               Audio-Driven Motion Generation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Animatronic robots that simulate lively and realistic motions of creatures can be excellent robotic platforms for social interaction with people. In particular, a robot head is a very important part to express various emotions and generate human-friendly and aesthetic impressions. This article presents Ray, a new type of audio-animatronic robot head. All mechanical structure of the robot is built in one step by 3D printing and has multiple layers expressing the overall shape of a human head and important features such as eyes, nose, mouth, and chin. This simple, lightweight structure and the separate tendon-based actuation system underneath allow for smooth, fast motions of the robot. We also develop an audio-driven motion generation module that automatically synthesizes natural and rhythmic motions of the head and mouth. The developed robot platform is used for various applications for example as a talking robot, robot singer, and robot MC. We expect this research opens up a new paradigm and application possibilities for minimally designed audio-animatronic robots.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt15_02">
             15:20-15:25, Paper ThDT15.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1211'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              High Speed Robotic Table Tennis Swinging Using Lightweight Hardware with Model Predictive Control
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#354609" title="Click to go to the Author Index">
             Nguyen, David
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420704" title="Click to go to the Author Index">
             Cancio, Kendrick
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#106208" title="Click to go to the Author Index">
             Kim, Sangbae
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1211" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#hardware_software_integration_in_robotics" title="Click to go to the Keyword Index">
               Hardware-Software Integration in Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#humanoid_robot_systems" title="Click to go to the Keyword Index">
               Humanoid Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present a robotic table tennis platform that achieves a variety of hit styles and ball-spins with high precision, power, and consistency. This is enabled by a custom lightweight, high-torque, low rotor inertia, five degree-of-freedom arm capable of high acceleration. To generate swing trajectories, we formulate an optimal control problem (OCP) that constrains the state of the paddle at the time of the strike. The terminal position is given by a predicted ball trajectory, and the terminal orientation and velocity of the paddle are chosen to match various possible styles of hits: loops (topspin), drives (flat), and chops (backspin). Finally, we construct a fixed-horizon model predictive controller (MPC) around this OCP to allow the hardware to quickly react to changes in the predicted ball trajectory. We validate on hardware that the system is capable of hitting balls with an average exit velocity of 11 m/s at an 88% success rate across the three swing types.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt15_03">
             15:25-15:30, Paper ThDT15.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1949'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Learning Quiet Walking for a Small Home Robot
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#359765" title="Click to go to the Author Index">
             Watanabe, Ryo
            </a>
           </td>
           <td class="r">
            SONY Group
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#219795" title="Click to go to the Author Index">
             Miki, Takahiro
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#206191" title="Click to go to the Author Index">
             Shi, Fan
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#305151" title="Click to go to the Author Index">
             Kadokawa, Yuki
            </a>
           </td>
           <td class="r">
            Nara Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#310688" title="Click to go to the Author Index">
             Bjelonic, Filip
            </a>
           </td>
           <td class="r">
            ETH Zürich, Switzerland
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#206398" title="Click to go to the Author Index">
             Kawaharazuka, Kento
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#231863" title="Click to go to the Author Index">
             Cramariuc, Andrei
            </a>
           </td>
           <td class="r">
            ETHZ
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#114045" title="Click to go to the Author Index">
             Hutter, Marco
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1949" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#domestic_robotics" title="Click to go to the Keyword Index">
               Domestic Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             As home robotics gains traction, robots are increasingly integrated into households, offering companionship and assistance. Quadruped robots, particularly those resembling dogs, have emerged as popular alternatives for traditional pets. However, user feedback highlights concerns about the noise these robots generate during walking at home, particularly the loud footstep impact sound. To address this issue, we propose a reinforcement learning (RL) based approach to minimize the foot contact velocity highly related to the footstep sound. Our framework incorporates three key elements: learning varying PD gains to actively dampen and stiffen each joint, utilizing foot contact sensors, and employing curriculum learning to gradually enforce penalties on foot contact velocity. Experiments demonstrate that our learned policy achieves superior quietness compared to a RL baseline and the carefully handcrafted Sony commercial controller baselines. Furthermore, the trade-off between robustness and quietness is shown. This research contributes to developing quieter and more user-friendly robotic companions in home environments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt15_04">
             15:30-15:35, Paper ThDT15.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2944'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Evaluating Human-Robot Skill Gaps in Electrical Circuit Inspection: A New Electronic Task Board for Benchmarking Manipulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#287054" title="Click to go to the Author Index">
             So, Peter
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#292159" title="Click to go to the Author Index">
             Swikir, Abdalla
            </a>
           </td>
           <td class="r">
            Mohamed Bin Zayed University of Artificial Intelligence
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#143489" title="Click to go to the Author Index">
             Abu-Dakka, Fares
            </a>
           </td>
           <td class="r">
            New York University Abu Dhabi
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#108317" title="Click to go to the Author Index">
             Haddadin, Sami
            </a>
           </td>
           <td class="r">
            Mohamed Bin Zayed University of Artificial Intelligence
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2944" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#performance_evaluation_and_benchmarking" title="Click to go to the Keyword Index">
               Performance Evaluation and Benchmarking
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#industrial_robots" title="Click to go to the Keyword Index">
               Industrial Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#software_tools_for_benchmarking_and_reproducibility" title="Click to go to the Keyword Index">
               Software Tools for Benchmarking and Reproducibility
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robot manipulation researchers reference human performance as a goal for their work, however, human data is seldom present in robotics benchmarks. We introduce a real-world benchmark targeting manipulation skills for performing electrical circuit inspection with a multimeter using an Internet-connected electronic task board. We present timing study results and an exemplary robot solution across six different tasks from the Robothon Grand Challenge at the automatica conference in 2023. Contributions from 16 robot teams were collected using task boards we manufactured and distributed as part of the 30-day international competition as an initial performance database. Our work systematically highlights the skill gap between the winning robot solution and the best human performance from a group of 30 subjects. Our goal is to chronicle progress over time in robot manipulation skills and provide a standardized, physical benchmark across the global community. Videos of the team submissions, the exemplary robot solution, as well as the project reproduction code are provided in the included repository.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt15_05">
             15:35-15:40, Paper ThDT15.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3245'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              RaccoonBot: An Autonomous Wire-Traversing Solar-Tracking Robot for Persistent Environmental Monitoring
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#399098" title="Click to go to the Author Index">
             Mendez-Flores, Efrain
            </a>
           </td>
           <td class="r">
            University of California, Irvine
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#399106" title="Click to go to the Author Index">
             Pourshahidi, Agaton
            </a>
           </td>
           <td class="r">
            University of California, Irvine
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101985" title="Click to go to the Author Index">
             Egerstedt, Magnus
            </a>
           </td>
           <td class="r">
            University of California, Irvine
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3245" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#hardware_software_integration_in_robotics" title="Click to go to the Keyword Index">
               Hardware-Software Integration in Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#environment_monitoring_and_management" title="Click to go to the Keyword Index">
               Environment Monitoring and Management
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#energy_and_environment_aware_automation" title="Click to go to the Keyword Index">
               Energy and Environment-Aware Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Environmental monitoring is used to characterize the health and relationship between organisms and their environments. In forest ecosystems, robots can serve as platforms to acquire such data, even in hard-to-reach places where wire-traversing platforms are particularly promising due to their efficient displacement. This paper presents the RaccoonBot, which is a novel autonomous wire-traversing robot for persistent environmental monitoring, featuring a fail-safe mechanical design with a self-locking mechanism in case of electrical shortage. The robot also features energy-aware mobility through a novel Solar tracking algorithm, that allows the robot to find a position on the wire to have direct contact with solar power to increase the energy harvested. Experimental results validate the electro-mechanical features of the RaccoonBot, showing that it is able to handle wire perturbations, different inclinations, and achieving energy autonomy.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt15_06">
             15:40-15:45, Paper ThDT15.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4694'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Fast and Accurate Relative Motion Tracking for Dual Industrial Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#301044" title="Click to go to the Author Index">
             He, Honglu
            </a>
           </td>
           <td class="r">
            Rensselaer Polytechnic Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#331022" title="Click to go to the Author Index">
             Lu, Chen-Lung
            </a>
           </td>
           <td class="r">
            Rensselaer Polytechnic Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#112758" title="Click to go to the Author Index">
             Saunders, Glenn
            </a>
           </td>
           <td class="r">
            Rensselaer Polytechnic Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#112580" title="Click to go to the Author Index">
             Wason, John
            </a>
           </td>
           <td class="r">
            Wason Technology, LLC
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#339121" title="Click to go to the Author Index">
             Yang, Pinghai
            </a>
           </td>
           <td class="r">
            GE Research
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#341418" title="Click to go to the Author Index">
             Schoonover, Jeffrey
            </a>
           </td>
           <td class="r">
            GE Research
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#404814" title="Click to go to the Author Index">
             Ajdelsztajn, Leo
            </a>
           </td>
           <td class="r">
            GE
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#227397" title="Click to go to the Author Index">
             Paternain, Santiago
            </a>
           </td>
           <td class="r">
            Rensselaer Polytechnic Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#121978" title="Click to go to the Author Index">
             Julius, Agung
            </a>
           </td>
           <td class="r">
            Rensselaer Polytechnic Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#107418" title="Click to go to the Author Index">
             Wen, John
            </a>
           </td>
           <td class="r">
            Rensselaer Polytechnic Institute
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4694" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#industrial_robots" title="Click to go to the Keyword Index">
               Industrial Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Industrial robotic applications such as spraying, welding, and additive manufacturing frequently require fast, accurate, and uniform motion along a 3D spatial curve. To increase process throughput, some manufacturers propose a dual-arm setup to overcome the speed limitation of a single robot. Industrial robot motion is programmed through waypoints connected by motion primitives (Cartesian linear and circular paths and linear joint paths at constant Cartesian speed). The actual robot motion is affected by the blending between these motion primitives and the pose of the robot (an outstretched/near-singularity pose tends to have larger path-tracking errors). Choosing the waypoints and the speed along each motion segment to achieve the performance requirement is challenging. At present, there is no automated solution, and laborious manual tuning by robot experts is needed to approach the desired performance. In this letter, we present a systematic three-step approach to designing and programming a dual-arm system to optimize system performance. The first step is to select the relative placement between the two robots based on the specified relative motion path. The second step is to select the relative waypoints and the motion primitives. The final step is to update the waypoints iteratively based on the actual measured relative motion. Waypoint iteration is first executed in simulation and then completed using the actual robots. For performance assessment, we use the mean path speed subject to the relative position and orientation constraints and the path speed uniformity constraint. We have demonstrated the effectiveness of this method on two systems, a physical testbed of two ABB robots and a simulation testbed of two FANUC robots, for two challenging test curves.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt16">
             <b>
              ThDT16
             </b>
            </a>
           </td>
           <td class="r">
            404
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thdt16" title="Click to go to the Program at a Glance">
             <b>
              Soft Robotics 2
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#221352" title="Click to go to the Author Index">
             Chin, Lillian
            </a>
           </td>
           <td class="r">
            UT Austin
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt16_01">
             15:15-15:20, Paper ThDT16.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('50'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Inflatable-Structure-Based Working-Channel Securing Mechanism for Soft Growing Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#334402" title="Click to go to the Author Index">
             Seo, Dongoh
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#295695" title="Click to go to the Author Index">
             Kim, Nam Gyun
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#102369" title="Click to go to the Author Index">
             Ryu, Jee-Hwan
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab50" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#soft_robot_materials_and_design" title="Click to go to the Keyword Index">
               Soft Robot Materials and Design
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#soft_robot_applications" title="Click to go to the Keyword Index">
               Soft Robot Applications
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#soft_sensors_and_actuators" title="Click to go to the Keyword Index">
               Soft Sensors and Actuators
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Soft growing robots are being used in various fields owing to their distinct advantages. However, their ability to manipulate tools in different applications is still challenging. In this paper, we propose an inflatable-structure-based working- channel securing mechanism for soft growing robots. The pro- posed mechanism provides a solution for securing a stable and accessible working channel with pressure equal to the atmospheric pressure, while maintaining the unique advantages of soft growing robots. The proposed soft growing robot can freely transfer materials and tools through its interior channel; therefore, it can adapt and replace equipment based on specific work requirements. This capability enhances the versatility and efficiency of the robot in various applications. Prototyping and experimental validation were conducted to show the performance and capabilities of the robot. The results of the experiments demonstrated that the soft growing robot effectively secured the working channel, enabling the transfer of materials and tools without interference from the inflation pressure. The accessibility of the secured channel was validated through slide-plate and pipe-pulling experiments. The demonstration of the growing mechanism confirmed the ability of the robot to secure a working channel during its growth, whereas the steering demonstration showcased its inherent steering function.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt16_02">
             15:20-15:25, Paper ThDT16.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1001'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Tendon Locking for Antagonistic Configuration and Stiffness-Control in Soft Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#383262" title="Click to go to the Author Index">
             Licher, Johann
            </a>
           </td>
           <td class="r">
            Leibniz University Hannover
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#243636" title="Click to go to the Author Index">
             Peters, Jan
            </a>
           </td>
           <td class="r">
            Leibniz Universität Hannover
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#109146" title="Click to go to the Author Index">
             Raatz, Annika
            </a>
           </td>
           <td class="r">
            Leibniz Universität Hannover
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#142097" title="Click to go to the Author Index">
             Wurdemann, Helge Arne
            </a>
           </td>
           <td class="r">
            University College London
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1001" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#soft_sensors_and_actuators" title="Click to go to the Keyword Index">
               Soft Sensors and Actuators
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#soft_robot_applications" title="Click to go to the Keyword Index">
               Soft Robot Applications
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#soft_robot_materials_and_design" title="Click to go to the Keyword Index">
               Soft Robot Materials and Design
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Some applications, such as surgical interventions, require that potential soft robots have the capability to alter their shape and enhance their force output on demand. This paper presents an antagonistic stiffening mechanism combining pneumatic actuation with tendon locking to achieve configuration- and stiffness control. Elongation of a soft pneumatic section, resulting from air actuation, is opposed by constraining the length of integrated tendons. These tendons can be locked in length by pneumatically activated levers at the base of each segment. Hence, tendon locking will not affect the configuration of other segments of a multi-segment manipulator. Our concept achieves a stiffness increase of up to 201.7% and a larger, more uniform radial workspace compared to the widely used pneumatic actuation concept while maintaining the low technical effort required for actuation. We also demonstrate how our actuation concept enables independent control of stiffness levels for individual segments of a multi-segment manipulator and their MR compatibility.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt16_03">
             15:25-15:30, Paper ThDT16.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1430'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Large-Expansion Bi-Layer Auxetics Create Compliant Cellular Motion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#221352" title="Click to go to the Author Index">
             Chin, Lillian
            </a>
           </td>
           <td class="r">
            UT Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#341605" title="Click to go to the Author Index">
             Xie, Gregory
            </a>
           </td>
           <td class="r">
            MIT
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#196842" title="Click to go to the Author Index">
             Lipton, Jeffrey
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101388" title="Click to go to the Author Index">
             Rus, Daniela
            </a>
           </td>
           <td class="r">
            MIT
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1430" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#actuation_and_joint_mechanisms" title="Click to go to the Keyword Index">
               Actuation and Joint Mechanisms
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#swarm_robotics" title="Click to go to the Keyword Index">
               Swarm Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#compliant_joints_and_mechanisms" title="Click to go to the Keyword Index">
               Compliant Joints and Mechanisms
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             There is significant interest in creating compliant modular robots that can change their volume. Inspired by how biological cells move, these systems can potentially combine the resilience of modular robotics with the increased environmental interactions of soft robotics. However, current versions have limited speed, expansion, and portability. In this paper, we address these concerns through AuxSwarm, a compliant system composed of auxetic-based robotic voxels. These voxels control their volume through a scissor-like bi-layer auxetic design, growing up to 1.57 times their original size in 0.2 seconds. This combination of speed and expansion is unique across modular soft robots, enabling dynamic locomotion capabilities. We characterize the voxels and demonstrate the versatility of this approach through case studies of 2D bending and 3D cube flipping. AuxSwarm provides a first step towards addressable voxel-based smart materials, while simultaneously addressing the robustness and actuation challenges faced by soft robots
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt16_04">
             15:30-15:35, Paper ThDT16.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1550'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              EViper-2D: A Thin Large-Area Soft Robotics Platform
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#313388" title="Click to go to the Author Index">
             Cheng, Hsin
            </a>
           </td>
           <td class="r">
            Princeton University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#418836" title="Click to go to the Author Index">
             Veilleux, Elias
            </a>
           </td>
           <td class="r">
            Princeton University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#279008" title="Click to go to the Author Index">
             Zheng, Zhiwu
            </a>
           </td>
           <td class="r">
            Princeton University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#313374" title="Click to go to the Author Index">
             Wagner, Sigurd
            </a>
           </td>
           <td class="r">
            Princeton University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#313370" title="Click to go to the Author Index">
             Verma, Naveen
            </a>
           </td>
           <td class="r">
            Princeton University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#313368" title="Click to go to the Author Index">
             Sturm, James
            </a>
           </td>
           <td class="r">
            Princeton University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#313381" title="Click to go to the Author Index">
             Chen, Minjie
            </a>
           </td>
           <td class="r">
            Princeton University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1550" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#modeling__control__and_learning_for_soft_robots" title="Click to go to the Keyword Index">
               Modeling, Control, and Learning for Soft Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents the key principles of eViper-2D -- a thin large-area soft robotics platform -- as a new development of the previous extendable Vibrating Intelligent Piezo-Electric Robot (eViper) platform. We first introduce the mechanical, electrical, and control framework of eViper-2D, and then develop systematic and scalable methods to study the impact of diverse actuation patterns on robotic motion dynamics and energy efficiency. By integrating power electronics, communication circuits, piezoelectric actuators, and batteries onboard, the eViper-2D platform enables rapid design iteration and quick evaluation of different control strategies for the multi-actuator soft robot. The platform supports data-driven modeling via automated data acquisition. We show that eViper-2D can provide rich insights into optimizing actuation patterns to achieve agile motion and minimal cost of transport (COT).
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt16_05">
             15:35-15:40, Paper ThDT16.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3110'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Bio-Inspired Soft Magnetic Swimming Robot for Flexible Motions
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#320064" title="Click to go to the Author Index">
             Li, Xiaosa
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#324708" title="Click to go to the Author Index">
             Lin, Zenan
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#318552" title="Click to go to the Author Index">
             Ding, Wenbo
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3110" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#soft_robot_materials_and_design" title="Click to go to the Keyword Index">
               Soft Robot Materials and Design
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#modeling__control__and_learning_for_soft_robots" title="Click to go to the Keyword Index">
               Modeling, Control, and Learning for Soft Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#software_hardware_integration_for_robot_systems" title="Click to go to the Keyword Index">
               Software-Hardware Integration for Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Bio-inspired soft robots have gained significant attention for their flexible design and adaptability to various environments, making them suitable for exploration and task execution in confined or hazardous areas. However, the deformation and motion of soft magnetic robots rely on both their structural design and magnetization, which complicates the guided movement and balance maintenance for aquatic environments. In this work, inspired by the flat and symmetrical body of rays, we design a soft magnetic fish-shaped robot capable of flexible motions and trajectory swimming on the water surface. This robot features the muscle made of magnetic elastomer, which connects with the acrylic skeleton and silicone film fins with a soft body. In the external magnetic field, the robot achieves hovering by flapping its fins, driven by the magnetically actuated deformation of its magnetic muscle. Besides, the robot's axial magnetization enables the rapid steering guided by a horizontal field. In experiments, the soft magnetic robot was tasked with performing a looping figure-eight trajectory movement on the water surface, guided by the field gradient generated by a dense planar electromagnetic coils' array. When moving, the onboard circuit board of the robot collected its inertial and temperature information, and sent these data to the host computer via Bluetooth in real-time for motion monitoring. Received data demonstrated that our robot performed the specified afloat swimming trajectory, exhibiting a good stability on its yaw angle during the continuous motion. The soft magnetic swimming robot shows its integrated functionalities in untethered actuation, on-robot sensing, and wireless communication, indicating a significant prospect on applications in inspection and cleaning within narrow pipelines and enclosed mechanical interior spaces.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt16_06">
             15:40-15:45, Paper ThDT16.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3826'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Magnetic Programming of Soft Materials Using Digitally Processed Laser Heating
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422191" title="Click to go to the Author Index">
             Kocabas, Fatih
            </a>
           </td>
           <td class="r">
            University of Wisconsin-Madison
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422196" title="Click to go to the Author Index">
             Oguztuzun, Ozan
            </a>
           </td>
           <td class="r">
            University of Wisconsin-Madison
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422197" title="Click to go to the Author Index">
             Zhou, Youyi
            </a>
           </td>
           <td class="r">
            University of Wisconsin-Madison
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#238795" title="Click to go to the Author Index">
             Alapan, Yunus
            </a>
           </td>
           <td class="r">
            University of Wisconsin-Madison
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3826" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#soft_robot_materials_and_design" title="Click to go to the Keyword Index">
               Soft Robot Materials and Design
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Spatial programming of magnetic soft materials holds immense potential for wide ranging applications in soft robotics, minimally invasive medicine, and haptic interfaces. Despite tremendous and rapid progress in encoding spatially resolved magnetization directions over soft structures, the currently available approaches employ sequential encoding, resulting in slow and tedious processes with limited throughput. In this paper, we present a rapid and parallel magnetic programming strategy based on digitally processed laser heating. Heating above the Curie temperature of the magnetic microparticles embedded within the soft material allows their facile magnetization in desired directions via small external magnetic fields. To achieve parallel and rapid magnetic programming, we developed an integrated digital laser processing and magnetic field generation system, facilitating generation of desired shapes and patterns at high-resolution. Performance of the pattern generation and magnetic soft material are experimentally evaluated. Employing the described magnetic programming framework, shape-morphing of magnetic soft structures with varying magnetic profiles are shown. The proposed approach establishes a rapid and facile encoding procedure with high-throughput magnetic programming potential.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt16_07">
             15:45-15:50, Paper ThDT16.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4850'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Proprioceptive State Estimation for Amphibious Tactile Sensing
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#286840" title="Click to go to the Author Index">
             Han, Xudong
            </a>
           </td>
           <td class="r">
            Southern University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#286518" title="Click to go to the Author Index">
             Guo, Ning
            </a>
           </td>
           <td class="r">
            Southern University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#379574" title="Click to go to the Author Index">
             Zhong, Shuqiao
            </a>
           </td>
           <td class="r">
            Southern University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#379581" title="Click to go to the Author Index">
             Zhou, Zhiyuan
            </a>
           </td>
           <td class="r">
            Southern University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#379584" title="Click to go to the Author Index">
             Lin, Jian
            </a>
           </td>
           <td class="r">
            Southern University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#204251" title="Click to go to the Author Index">
             Song, Chaoyang
            </a>
           </td>
           <td class="r">
            Southern University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#207989" title="Click to go to the Author Index">
             Wan, Fang
            </a>
           </td>
           <td class="r">
            Southern University of Science and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4850" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#modeling__control__and_learning_for_soft_robots" title="Click to go to the Keyword Index">
               Modeling, Control, and Learning for Soft Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_other_robotic_applications" title="Click to go to the Keyword Index">
               Computer Vision for Other Robotic Applications
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#grasping" title="Click to go to the Keyword Index">
               Grasping
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#proprioceptive_state_estimation" title="Click to go to the Keyword Index">
               Proprioceptive State Estimation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a novel vision-based proprioception approach for a soft robotic finger that can estimate and reconstruct tactile interactions in terrestrial and aquatic environments. The key to this system lies in the finger's unique metamaterial structure, which facilitates omni-directional passive adaptation during grasping, protecting delicate objects across diverse scenarios. A compact in-finger camera captures high-framerate images of the finger's deformation during contact, extracting crucial tactile data in real time. We present a volumetric discretized model of the soft finger and use the geometry constraints captured by the camera to find the optimal estimation of the deformed shape. The approach is benchmarked using a motion capture system with sparse markers and a haptic device with dense measurements. Both results show state-of-the-art accuracies, with a median error of 1.96 mm for overall body deformation, corresponding to 2.1% of the finger's length. More importantly, the state estimation is robust in both on-land and underwater environments, as we demonstrate its usage for underwater object shape sensing. This combination of passive adaptation and real-time tactile sensing paves the way for amphibious robotic grasping applications. All codes are shared on GitHub: https://github.com/ancorasir/PropSE.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt17">
             <b>
              ThDT17
             </b>
            </a>
           </td>
           <td class="r">
            405
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thdt17" title="Click to go to the Program at a Glance">
             <b>
              Planning with Contact
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#106519" title="Click to go to the Author Index">
             Lozano-Perez, Tomas
            </a>
           </td>
           <td class="r">
            MIT
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#114207" title="Click to go to the Author Index">
             Stueckler, Joerg
            </a>
           </td>
           <td class="r">
            University of Augsburg
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt17_01">
             15:15-15:20, Paper ThDT17.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('21'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Fast Contact-Implicit Model Predictive Control
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#268823" title="Click to go to the Author Index">
             Le Cleac'h, Simon
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#246462" title="Click to go to the Author Index">
             Howell, Taylor
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#178816" title="Click to go to the Author Index">
             Yang, Shuo
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#215358" title="Click to go to the Author Index">
             Lee, Chi-Yen
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#346357" title="Click to go to the Author Index">
             Zhang, John
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#346358" title="Click to go to the Author Index">
             Bishop, Arun
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#106233" title="Click to go to the Author Index">
             Schwager, Mac
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#205543" title="Click to go to the Author Index">
             Manchester, Zachary
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab21" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#model_predictive_control" title="Click to go to the Keyword Index">
               Model Predictive Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_control" title="Click to go to the Keyword Index">
               Motion Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present a general approach for controlling robotic systems that make and break contact with their environments. Contact-implicit model predictive control (CI-MPC) generalizes linear MPC to contact-rich settings by utilizing a bi-level planning formulation with lower-level contact dynamics formulated as time-varying linear complementarity problems (LCPs) computed using strategic Taylor approximations about a reference trajectory. These dynamics enable the upper-level planning problem to reason about contact timing and forces, and generate entirely new contact-mode sequences online. To achieve reliable and fast numerical convergence, we devise a structure-exploiting interior-point solver for these LCP contact dynamics and a custom trajectory optimizer for the tracking problem. We demonstrate real-time solution rates for CI-MPC and the ability to generate and track non-periodic behaviours in hardware experiments on a quadrupedal robot. We also show that the controller is robust to model mismatch and can respond to disturbances by discovering and exploiting new contact modes across a variety of robotic systems in simulation, including a pushbot, planar hopper, planar quadruped, and planar biped.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt17_02">
             15:20-15:25, Paper ThDT17.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('281'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Robo-GS: A Physics Consistent Spatial-Temporal Model for Robotic Arm with Hybrid Representation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#370104" title="Click to go to the Author Index">
             Lou, Haozhe
            </a>
           </td>
           <td class="r">
            University of Southern California
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#374886" title="Click to go to the Author Index">
             Liu, Yurong
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#415105" title="Click to go to the Author Index">
             Pan, Yike
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#319458" title="Click to go to the Author Index">
             Geng, Yiran
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#407065" title="Click to go to the Author Index">
             Chen, Jianteng
            </a>
           </td>
           <td class="r">
            Hong Kong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#414636" title="Click to go to the Author Index">
             Ma, Wenlong
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#414631" title="Click to go to the Author Index">
             Li, Chenglong
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#414632" title="Click to go to the Author Index">
             Wang, Lin
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#414635" title="Click to go to the Author Index">
             Feng, Hengzhen
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#269171" title="Click to go to the Author Index">
             Shi, Lu
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#338957" title="Click to go to the Author Index">
             Shi, Yongliang
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab281" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#simulation_and_animation" title="Click to go to the Keyword Index">
               Simulation and Animation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#methods_and_tools_for_robot_system_design" title="Click to go to the Keyword Index">
               Methods and Tools for Robot System Design
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#software_architecture_for_robotic_and_automation" title="Click to go to the Keyword Index">
               Software Architecture for Robotic and Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Real2Sim2Real plays a critical role in robotic arm control and reinforcement learning, yet bridging this gap remains a significant challenge due to the complex physical properties of robots and the objects they manipulate. Existing methods lack a comprehensive solution to accurately reconstruct real-world objects with spatial representations and their associated physics attributes. We propose a Real2Sim pipeline with a hybrid representation model that integrates mesh geometry, 3D Gaussian kernels, and physics attributes to enhance the digital asset representation of robotic arms. This hybrid representation is implemented through a Gaussian-Mesh-Pixel binding technique, which establishes an isomorphic mapping between mesh vertices and Gaussian models. This enables a fully differentiable rendering pipeline that can be optimized through numerical solvers, achieves high-fidelity rendering via Gaussian Splatting, and facilitates physically plausible simulation of the robotic arm's interaction with its environment using mesh-based methods. Given the digital assets, we propose a manipulable Real2Sim pipeline that standardizes coordinate systems and scales, ensuring the seamless integration of multiple components. In addition to reconstructing the robotic arm, the surrounding static background and objects can be holistically reconstructed, enabling seamless interactions between the robotic arm and its environment. We also provide datasets covering various robotic manipulation tasks and robotic arm mesh reconstructions. These datasets include real-world motion captured in digital assets, ensuring precise representation of mass and friction, which are crucial for robotic manipulation. Our model achieves state-of-the-art results in realistic rendering and mesh reconstruction quality for robotic applications.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt17_03">
             15:25-15:30, Paper ThDT17.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1951'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              One-Shot Manipulation Strategy Learning by Making Contact Analogies
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422678" title="Click to go to the Author Index">
             Liu, Yuyao
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#337529" title="Click to go to the Author Index">
             Mao, Jiayuan
            </a>
           </td>
           <td class="r">
            MIT
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#202511" title="Click to go to the Author Index">
             Tenenbaum, Joshua
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#106519" title="Click to go to the Author Index">
             Lozano-Perez, Tomas
            </a>
           </td>
           <td class="r">
            MIT
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#103153" title="Click to go to the Author Index">
             Kaelbling, Leslie
            </a>
           </td>
           <td class="r">
            MIT
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1951" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#integrated_planning_and_learning" title="Click to go to the Keyword Index">
               Integrated Planning and Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present a novel approach, MAGIC (manipulation analogies for generalizable intelligent contacts), for one-shot learning of manipulation strategies with fast and extensive generalization to novel objects. By leveraging a reference action trajectory, MAGIC effectively identifies similar contact points and sequences of actions on novel objects to replicate a demonstrated strategy, such as using different hooks to retrieve distant objects of different shapes and sizes. Our method is based on a two-stage contact-point matching process that combines global shape matching using pretrained neural features with local curvature analysis to ensure precise and physically plausible contact points. We experiment with three tasks including scooping, hanging, and hooking objects. MAGIC demonstrates superior performance over existing methods, achieving significant improvements in runtime speed and generalization to different object categories. Website: https://magic-2024.github.io/.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt17_04">
             15:30-15:35, Paper ThDT17.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2094'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Incremental Few-Shot Adaptation for Non-Prehensile Object Manipulation Using Parallelizable Physics Simulators
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#398774" title="Click to go to the Author Index">
             Baumeister, Fabian
            </a>
           </td>
           <td class="r">
            Max Planck Institute for Intelligent Systems
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#340840" title="Click to go to the Author Index">
             Mack, Lukas
            </a>
           </td>
           <td class="r">
            University of Augsburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#114207" title="Click to go to the Author Index">
             Stueckler, Joerg
            </a>
           </td>
           <td class="r">
            University of Augsburg
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2094" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#incremental_learning" title="Click to go to the Keyword Index">
               Incremental Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#integrated_planning_and_learning" title="Click to go to the Keyword Index">
               Integrated Planning and Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#learning_from_experience" title="Click to go to the Keyword Index">
               Learning from Experience
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Few-shot adaptation is an important capability for intelligent robots that perform tasks in open-world settings such as everyday environments or flexible production. In this paper, we propose a novel approach for non-prehensile manipulation which incrementally adapts a physics-based dynamics model for model-predictive control (MPC). The model prediction is aligned with a few examples of robot-object interactions collected with the MPC. This is achieved by using a parallelizable rigid-body physics simulation as dynamic world model and sampling-based optimization of the model parameters. In turn, the optimized dynamics model can be used for MPC using efficient sampling-based optimization. We evaluate our few-shot adaptation approach in object pushing experiments in simulation and with a real robot.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt17_05">
             15:35-15:40, Paper ThDT17.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2670'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Efficient Gradient-Based Inference for Manipulation Planning in Contact Factor Graphs
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#274086" title="Click to go to the Author Index">
             Lee, Jeongmin
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#395770" title="Click to go to the Author Index">
             Park, Sunkyung
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#237547" title="Click to go to the Author Index">
             Lee, Minji
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104207" title="Click to go to the Author Index">
             Lee, Dongjun
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2670" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#manipulation_planning" title="Click to go to the Keyword Index">
               Manipulation Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#contact_modeling" title="Click to go to the Keyword Index">
               Contact Modeling
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#dexterous_manipulation" title="Click to go to the Keyword Index">
               Dexterous Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a framework designed to tackle a range of planning problems arise in manipulation, which typically involve complex geometric-physical reasoning related to contact and dynamic constraints. We introduce the Contact Factor Graph (CFG) to graphically model these diverse factors, enabling us to perform inference on the graphs to approximate the distribution and sample appropriate solutions. We propose a novel approach that can incorporate various phenomena of contact manipulation as differentiable factors, and develop an efficient inference algorithm for CFG that leverages this differentiability along with the conditional probabilities arising from the structured nature of contact. Our results demonstrate the capability of our framework in generating viable samples and approximating posterior distributions for various manipulation scenarios.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt17_06">
             15:40-15:45, Paper ThDT17.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3199'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Polyhedral Collision Detection Via Vertex Enumeration
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#301552" title="Click to go to the Author Index">
             Cinar, Andrew
            </a>
           </td>
           <td class="r">
            Vanderbilt University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425217" title="Click to go to the Author Index">
             Zhao, Yue
            </a>
           </td>
           <td class="r">
            Vanderbilt University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#238029" title="Click to go to the Author Index">
             Laine, Forrest
            </a>
           </td>
           <td class="r">
            Vanderbilt University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3199" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#collision_avoidance" title="Click to go to the Keyword Index">
               Collision Avoidance
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#constrained_motion_planning" title="Click to go to the Keyword Index">
               Constrained Motion Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Collision detection is a critical functionality for robotics. The degree to which objects collide cannot be represented as a continuously differentiable function for any shapes other than spheres. This paper proposes a framework for handling collision detection between polyhedral shapes. We frame the signed distance between two polyhedral bodies as the optimal value of a convex optimization, and consider constraining the signed distance in a bilevel optimization problem. To avoid relying on specialized bilevel solvers, our method exploits the fact that the signed distance is the minimal point of a convex region related to the two bodies. Our method enumerates the values obtained at all extreme points of this region and lists them as constraints in the higher-level problem. We compare our formulation to existing methods in terms of accuracy and speed when solved using the same mixed complementarity problem solver. We demonstrate that our approach more reliably solves difficult collision detection problems with multiple obstacles than other methods, and is faster than existing methods in some cases.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt17_07">
             15:45-15:50, Paper ThDT17.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4859'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Flying Calligrapher: Contact-Aware Motion and Force Planning and Control for Aerial Manipulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#246468" title="Click to go to the Author Index">
             Guo, Xiaofeng
            </a>
           </td>
           <td class="r">
            Carnegie Mellon Univeristy
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#341903" title="Click to go to the Author Index">
             He, Guanqi
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#341398" title="Click to go to the Author Index">
             Xu, Jiahe
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#234248" title="Click to go to the Author Index">
             Mousaei, Mohammadreza
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#219469" title="Click to go to the Author Index">
             Geng, Junyi
            </a>
           </td>
           <td class="r">
            Pennsylvania State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104304" title="Click to go to the Author Index">
             Scherer, Sebastian
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#236627" title="Click to go to the Author Index">
             Shi, Guanya
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4859" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__mechanics_and_control" title="Click to go to the Keyword Index">
               Aerial Systems: Mechanics and Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#integrated_planning_and_control" title="Click to go to the Keyword Index">
               Integrated Planning and Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Aerial manipulation has gained interest in completing high-altitude tasks that are challenging for human workers, such as contact inspection and defect detection, etc. Previous research has focused on maintaining static contact points or forces. This letter addresses a more general and dynamic task: simultaneously tracking time-varying contact force in the surface normal direction and motion trajectories on tangential surfaces. We propose a pipeline that includes a contact-aware trajectory planner to generate dynamically feasible trajectories, and a hybrid motion-force controller to track such trajectories. We demonstrate the approach in an aerial calligraphy task using a novel sponge pen design as the end-effector, whose stroke width is positively related to the contact force. Additionally, we develop a touchscreen interface for flexible user input. Experiments show our method can effectively draw diverse letters, achieving an IoU of 0.59 and an end-effector position (force) tracking RMSE of 2.9 cm (0.7 N). Website: https://xiaofeng-guo.github.io/flying-calligrapher/.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt18">
             <b>
              ThDT18
             </b>
            </a>
           </td>
           <td class="r">
            406
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thdt18" title="Click to go to the Program at a Glance">
             <b>
              Imaging, Scanning, Localization
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#234039" title="Click to go to the Author Index">
             Huang, Baoru
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt18_01">
             15:15-15:20, Paper ThDT18.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('69'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Autonomous Robotic Ultrasound Approach for Fetoscope Tracking by Fusing Optical and 2D Ultrasound Data
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#307737" title="Click to go to the Author Index">
             Cai, Yuyu
            </a>
           </td>
           <td class="r">
            KU Leuven
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#305078" title="Click to go to the Author Index">
             Li, Ruixuan
            </a>
           </td>
           <td class="r">
            KU Leuven
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#288951" title="Click to go to the Author Index">
             Davoodi, Ayoob
            </a>
           </td>
           <td class="r">
            Katholieke Universiteit Leuven(KU Leuven)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#168180" title="Click to go to the Author Index">
             Ourak, Mouloud
            </a>
           </td>
           <td class="r">
            University of Leuven
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#184631" title="Click to go to the Author Index">
             Deprest, Jan
            </a>
           </td>
           <td class="r">
            University Hospital Leuven
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#108277" title="Click to go to the Author Index">
             Vander Poorten, Emmanuel B
            </a>
           </td>
           <td class="r">
            KU Leuven
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab69" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#sensor_fusion" title="Click to go to the Keyword Index">
               Sensor Fusion
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             2D ultrasound (US) guidance is an essential tool in fetoscopic laser photocoagulation (FLP) to treat twin-to-twin transfusion syndrome (TTTS). During the procedure, the sonographer and endoscopic surgeon manage different image modalities each with its own field of view. Tacit collaboration is needed between them to visualize the right information and ensure the smooth operation of the procedure. Robotic approaches could simplify this interaction but would require robust localization tools to cope with the complex fetoscopic motion patterns. This study proposes a method for robotic ultrasound (rUS) fetoscope tracking, fusing optical tracking system (OTS) and 2D US imaging. The Kalman filter is defined to guarantee robust online registration and enhance fetoscope tracking. Real-time detection of the fetoscope tip is achieved using the You Only Look Once (YOLO v7) algorithm. Additionally, a US image-based searching strategy is proposed for situations where the optical camera is obstructed. Hybrid position-force control is employed to manipulate the US probe safely against the pregnant abdomen. Validation on a silicone phantom demonstrates accurate tracking results with a mean error below 2.59 mm and tip visibility exceeding 90% is found in most experiments. The proposed system could potentially reduce surgeon workload and training costs for FLP surgery.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt18_02">
             15:20-15:25, Paper ThDT18.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('713'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Guiding the Last Centimeter: Novel Anatomy-Aware Probe Servoing for Standardized Imaging Plane Navigation in Robotic Lung Ultrasound (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#253512" title="Click to go to the Author Index">
             Ma, Xihan
            </a>
           </td>
           <td class="r">
            Worcester Polytechnic Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#418848" title="Click to go to the Author Index">
             Zeng, Mingjie
            </a>
           </td>
           <td class="r">
            Worcester Polytechnic Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#285874" title="Click to go to the Author Index">
             Hill, Jeffrey C.
            </a>
           </td>
           <td class="r">
            MCPHS University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#285993" title="Click to go to the Author Index">
             Hoffmann, Beatrice
            </a>
           </td>
           <td class="r">
            Beth Israel Deaconess Medical Center
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#284478" title="Click to go to the Author Index">
             Zhang, Ziming
            </a>
           </td>
           <td class="r">
            Worcester Polytechnic Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#192441" title="Click to go to the Author Index">
             Zhang, Haichong
            </a>
           </td>
           <td class="r">
            Worcester Polytechnic Institute
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab713" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#visual_servoing" title="Click to go to the Keyword Index">
               Visual Servoing
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Navigating the ultrasound (US) probe to the standardized imaging plane (SIP) for image acquisition is a critical but operator-dependent task in conventional freehand diagnostic US. Robotic US systems (RUSS) offer the potential to enhance imaging consistency by leveraging real-time US image feedback to optimize the probe pose, thereby reducing reliance on operator expertise. However, determining the proper approach to extracting generalizable features from the US images for probe pose adjustment remains challenging. In this work, we propose a SIP navigation framework for RUSS, exemplified in the context of robotic lung ultrasound (LUS). This framework facilitates automatic probe adjustment when in proximity to the SIP. This is achieved by explicitly extracting multiple anatomical features presented in real-time LUS images and performing non-patient-specific template matching to generate probe motion towards the SIP using image-based visual servoing (IBVS). The framework is further integrated with the active-sensing end-effector (A-SEE), a customized robot end-effector that leverages patient external body geometry to maintain optimal probe alignment with the contact surface, thus preserving US signal quality throughout the navigation. The proposed approach ensures procedural interpretability and inter-patient adaptability. Validation is conducted through anatomy-mimicking phantom and in-vivo evaluations involving five human subjects.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt18_03">
             15:25-15:30, Paper ThDT18.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2346'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Automatic Robotic-Assisted Diffuse Reflectance Spectroscopy Scanning System
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#378565" title="Click to go to the Author Index">
             Deng, Kaizhong
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#396620" title="Click to go to the Author Index">
             Peters, Christopher
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#117933" title="Click to go to the Author Index">
             Mylonas, George
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#123487" title="Click to go to the Author Index">
             Elson, Daniel
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2346" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_medical_robotics" title="Click to go to the Keyword Index">
               Computer Vision for Medical Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#visual_servoing" title="Click to go to the Keyword Index">
               Visual Servoing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Diffuse Reflectance Spectroscopy (DRS) is a well-established optical technique for tissue composition assessment which has been validated for tumour detection to ensure the complete removal of cancerous tissue. While point-wise assessment has many potential applications, incorporating automated large-area scanning would enable holistic tissue sampling with higher consistency. We propose a robotic system to facilitate autonomous DRS scanning with hybrid visual servoing control. A specially designed height compensation module enables precise contact condition control. The evaluation results show that the system can accurately execute the scanning command and acquire consistent DRS spectra with comparable results to the manual collection, which is the current gold standard protocol. Integrating the proposed system into surgery lays the groundwork for autonomous intra-operative DRS tissue assessment with high reliability and repeatability. This could reduce the need for manual scanning by the surgeon while ensuring complete tumor removal in clinical practice.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt18_04">
             15:30-15:35, Paper ThDT18.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2470'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Robust and Accurate Multi-View 2D/3D Image Registration with Differentiable X-Ray Rendering and Dual Cross-View Constraints
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424073" title="Click to go to the Author Index">
             Cui, Yuxin
            </a>
           </td>
           <td class="r">
            Shandong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#206118" title="Click to go to the Author Index">
             Min, Zhe
            </a>
           </td>
           <td class="r">
            University College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#225131" title="Click to go to the Author Index">
             Song, Rui
            </a>
           </td>
           <td class="r">
            Shandong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#163802" title="Click to go to the Author Index">
             Li, Yibin
            </a>
           </td>
           <td class="r">
            Shandong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#100160" title="Click to go to the Author Index">
             Meng, Max Q.-H.
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2470" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_medical_robotics" title="Click to go to the Keyword Index">
               Computer Vision for Medical Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robust and accurate 2D/3D registration, which aligns the preoperative model and the intraoperative image of the same anatomy, plays an important role in enabling successful interventional navigation. To alleviate the challenge of limited field of view associated with single intraoperative image, more than one intraoperative images can be leveraged and the multi-view 2D/3D registration is thus needed. In this paper, we propose a novel multi-view 2D/3D rigid registration approach which consists of two stages. In the first stage, the combined loss function consisting of the differences between the predicted and ground-truth poses, and dissimilarities (e.g., normalized crosscorrelation) between the simulated and observed intraoperative images. More importantly, the additional cross-view training loss terms are formulated for both pose and image loss, to explicitly consider the cross-view constraints. In the second stage, the test-time optimization is conducted to refine the estimated poses in the coarse stage. Our method leverages the mutual constraints of multi-frame view projection poses to enhance the robustness of the multi-view 2D/3D registration approach. The proposed framework achieves an mTRE of 0.79±2.17 mm on six datasets from DeepFluoro, further advancing beyond the state-of-the-art registration algorithms on this dataset.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt18_05">
             15:35-15:40, Paper ThDT18.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2584'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Robust Robotic Breast Ultrasound Scanning and Real-Time Lesion Localization
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#399918" title="Click to go to the Author Index">
             Cao, Zhiyan
            </a>
           </td>
           <td class="r">
            Huazhong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#199701" title="Click to go to the Author Index">
             Wang, Yiwei
            </a>
           </td>
           <td class="r">
            Huazhong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#140969" title="Click to go to the Author Index">
             Zhao, Huan
            </a>
           </td>
           <td class="r">
            Huazhong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#195115" title="Click to go to the Author Index">
             Ding, Han
            </a>
           </td>
           <td class="r">
            Huazhong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#193401" title="Click to go to the Author Index">
             Zhang, Shaohua
            </a>
           </td>
           <td class="r">
            Huazhong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2584" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_medical_robotics" title="Click to go to the Keyword Index">
               Computer Vision for Medical Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The inherent flexibility and real-time deformation of breast tissue pose significant challenges for achieving full coverage and accurate lesion localization in autonomous breast ultrasound scanning. This paper introduces a robust finite state machine-based framework that mimics the decision-making process of an experienced physician, dynamically transitioning between global breast scan and fine lesion scan. An autonomous radial and anti-radial global scan pattern ensures comprehensive breast coverage. To avoid lesion misidentification caused by soft tissue movement, a real-time lesion fine scan method is proposed for lesion detection and localization. Experimental results demonstrate that our system in full coverage tests achieves 7 identified lesions out of 7 existing lesions and maintains a robust localization accuracy of 3.23 mm across phantoms with varying stiffnesses.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt18_06">
             15:40-15:45, Paper ThDT18.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3284'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Hybrid Deep Reinforcement Learning for Radio Tracer Localisation in Robotic-Assisted Radioguided Surgery
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#377847" title="Click to go to the Author Index">
             Zhang, Hanyi
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#378565" title="Click to go to the Author Index">
             Deng, Kaizhong
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#347446" title="Click to go to the Author Index">
             Hu, Zhaoyang Jacopo
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#234039" title="Click to go to the Author Index">
             Huang, Baoru
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#123487" title="Click to go to the Author Index">
             Elson, Daniel
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3284" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#surgical_robotics__laparoscopy" title="Click to go to the Keyword Index">
               Surgical Robotics: Laparoscopy
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Radioguided surgery, such as sentinel lymph node biopsy, relies on the precise localization of radioactive targets by non-imaging gamma/beta detectors. Manual radioactive target detection based on visual display or audible indication of gamma level is highly dependent on the ability of the surgeon to track and interpret the spatial information. This paper presents a learning-based method to realize the autonomous radiotracer detection in robot-assisted surgeries by navigating the probe to the radioactive target. We proposed novel hybrid approach that combines deep reinforcement learning (DRL) with adaptive robotic scanning. The adaptive grid-based scanning could provide initial direction estimation while the DRL-based agent could efficiently navigate to the target utilising historical data. Simulation experiments demonstrate a 95% success rate, and improved efficiency and robustness compared to conventional techniques. Real-world evaluation on the da Vinci Research Kit (dVRK) further confirms the feasibility of the approach, achieving an 80% success rate in radiotracer detection. This method has the potential to enhance consistency, reduce operator dependency, and improve procedural accuracy in radioguided surgeries.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt18_07">
             15:45-15:50, Paper ThDT18.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4386'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Improving Probe Localization for Freehand 3D Ultrasound Using Lightweight Cameras
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#216297" title="Click to go to the Author Index">
             Huang, Dianye
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#107647" title="Click to go to the Author Index">
             Navab, Nassir
            </a>
           </td>
           <td class="r">
            TU Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#191552" title="Click to go to the Author Index">
             Jiang, Zhongliang
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4386" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_medical_robotics" title="Click to go to the Keyword Index">
               Computer Vision for Medical Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Ultrasound (US) probe localization relative to the examined subject is essential for freehand 3D US imaging, which offers significant clinical value due to its affordability and unrestricted field of view. However, existing methods often rely on expensive tracking systems or bulky probes, while recent US image-based deep learning methods suffer from accumulated errors during probe maneuvering. To address these challenges, this study proposes a versatile, cost-effective probe pose localization method for freehand 3D US imaging, utilizing two lightweight cameras. To eliminate accumulated errors during US scans, we introduce PoseNet, which directly predicts the probe's 6D pose relative to a preset world coordinate system based on camera observations. We first jointly train pose and camera image encoders based on pairs of 6D pose and camera observations densely sampled in simulation. This will encourage each pair of probe pose and its corresponding camera observation to share the same representation in latent space. To ensure the two encoders handle unseen images and poses effectively, we incorporate a triplet loss that enforces smaller differences in latent features between nearby poses compared to distant ones. Then, the pose decoder uses the latent representation of the camera images to predict the probe's 6D pose. To bridge the sim-to-real gap, in the real world, we use the trained image encoder and pose decoder for initial predictions, followed by an additional MLP layer to refine the estimated pose, improving accuracy. The results obtained from an arm phantom demonstrate the effectiveness of the proposed method, which notably surpasses state-of-the-art techniques, achieving average positional and rotational errors of 2.03 mm and 0.37 deg, respectively.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt19">
             <b>
              ThDT19
             </b>
            </a>
           </td>
           <td class="r">
            407
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thdt19" title="Click to go to the Program at a Glance">
             <b>
              Manufacturing and Assembly Processes
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#104592" title="Click to go to the Author Index">
             Fox, Dieter
            </a>
           </td>
           <td class="r">
            University of Washington
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#217568" title="Click to go to the Author Index">
             Fang, Kuan
            </a>
           </td>
           <td class="r">
            Cornell University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt19_01">
             15:15-15:20, Paper ThDT19.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1082'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Robot-Based Automatic Charging for Electric Vehicles Using Incremental Learning and Biomimetic Control
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#216237" title="Click to go to the Author Index">
             Zeng, Chao
            </a>
           </td>
           <td class="r">
            University of Liverpool
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#392691" title="Click to go to the Author Index">
             Ye, Dexi
            </a>
           </td>
           <td class="r">
            South China University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#168969" title="Click to go to the Author Index">
             Wang, Ning
            </a>
           </td>
           <td class="r">
            Sheffield Hallam University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420487" title="Click to go to the Author Index">
             Feng, Chen
            </a>
           </td>
           <td class="r">
            Zhejiang VIE Science &amp; Technology Co., Ltd
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#107047" title="Click to go to the Author Index">
             Yang, Chenguang
            </a>
           </td>
           <td class="r">
            University of Liverpool
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1082" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#compliant_assembly" title="Click to go to the Keyword Index">
               Compliant Assembly
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#incremental_learning" title="Click to go to the Keyword Index">
               Incremental Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#compliance_and_impedance_control" title="Click to go to the Keyword Index">
               Compliance and Impedance Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             With the growing popularity of electric vehicles, the demand for robot-based unmanned automatic charging has become both urgent and challenging. Two key challenges need to be addressed: how to efficiently locate the charging port, and how to compliantly insert the connector into the port. In this paper, we propose an incremental learning method based on the broad learning system to address the visual positioning error of the charging port. This method allows the robot to transfer and generalize the search skills learned in simulation to real-world scenarios. As a result, the robot can rapidly locate the charging port in real world environments without the need for complex contact state modeling, time-consuming data collection, or model retraining. Subsequently, a biomimetic admittance controller is designed to enable the robot to adapt its compliant behavior online during the plugging process. Finally, experiments are performed on a UR robot to verify the effectiveness of our method.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt19_02">
             15:20-15:25, Paper ThDT19.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2675'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              CC-STAR: An Estimation for Contact State Transition Using Reconstruction-Based Anomaly Detection for Peg-In-Hole Assembly
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#255221" title="Click to go to the Author Index">
             Lee, Haeseong
            </a>
           </td>
           <td class="r">
            Graduate School of Convergence Science and Technology, Seoul Nat
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#294364" title="Click to go to the Author Index">
             Sung, Eunho
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#302751" title="Click to go to the Author Index">
             You, Seungbin
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101200" title="Click to go to the Author Index">
             Park, Jaeheung
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2675" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#assembly" title="Click to go to the Keyword Index">
               Assembly
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#intelligent_and_flexible_manufacturing" title="Click to go to the Keyword Index">
               Intelligent and Flexible Manufacturing
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             For successful peg-in-hole assembly, predefined sub-tasks should be executed sequentially according to the current contact state. Therefore, recognizing contact state transitions is essential in order to determine whether to continue the current task or proceed to the next. In that context, learning-based solutions have shown outstanding results. However, these methods heavily rely on balanced datasets, which are challenging to obtain due to the short duration of certain contact states and rare failure cases. To address this issue, this paper proposes a framework for estimating contact state transitions using anomaly detection through input data reconstruction. The proposed framework operates in a semi-supervised manner, eliminating the need for balanced datasets during training. For input data reconstruction, a convolutional neural network is combined with a variational autoencoder to process various sensor measurements as a multivariate time series. Unlike traditional binary anomaly detection, the proposed anomaly detector scores reconstruction errors and leverages domain knowledge to identify various contact state transitions in the peg-in-hole assembly. The effectiveness of the proposed framework is validated through experiments using a torque-controlled dual manipulator system.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt19_03">
             15:25-15:30, Paper ThDT19.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2906'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Blox-Net: Generative Design-For-Robot-Assembly Using VLM Supervision, Physics Simulation, and a Robot with Reset
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422094" title="Click to go to the Author Index">
             Goldberg, Andrew
            </a>
           </td>
           <td class="r">
            University of California Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422107" title="Click to go to the Author Index">
             Kondap, Kavish
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#340609" title="Click to go to the Author Index">
             Qiu, Tianshuang
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#398609" title="Click to go to the Author Index">
             Ma, Zehan
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#311951" title="Click to go to the Author Index">
             Fu, Letian
            </a>
           </td>
           <td class="r">
            UC Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#234135" title="Click to go to the Author Index">
             Kerr, Justin
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#245956" title="Click to go to the Author Index">
             Huang, Huang
            </a>
           </td>
           <td class="r">
            University of California at Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#300083" title="Click to go to the Author Index">
             Chen, Kaiyuan
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#217568" title="Click to go to the Author Index">
             Fang, Kuan
            </a>
           </td>
           <td class="r">
            Cornell University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#107102" title="Click to go to the Author Index">
             Goldberg, Ken
            </a>
           </td>
           <td class="r">
            UC Berkeley
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2906" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#assembly" title="Click to go to the Keyword Index">
               Assembly
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#robotics_and_automation_in_construction" title="Click to go to the Keyword Index">
               Robotics and Automation in Construction
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#ai_based_methods" title="Click to go to the Keyword Index">
               AI-Based Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Generative AI systems have shown impressive capabilities in creating text, code, and images. Inspired by the importance of research in industrial Design for Assembly, we introduce a novel problem: Generative Design-for-Robot- Assembly (GDfRA). The task is to generate an assembly based on a natural language prompt (e.g., “giraffe”) and an image of available physical components, such as 3D-printed blocks. The output is an assembly, a spatial arrangement of these components, accompanied by instructions for a robot to build it. The output geometry must 1) resemble the requested object and 2) be reliably assembled by a 6 DoF robot arm with a suction gripper. We then present Blox-Net, a GDfRA system that com- bines generative vision language models with well-established methods in computer vision, simulation, perturbation analysis, motion planning, and physical robot experimentation to solve a class of GDfRA problems without human supervision. Blox-Net achieved a Top-1 accuracy of 63.5% in the semantic accuracy of its designed assemblies. Six designs, after Blox-Net’s automated pertubation redesign, were reliably assembled by a robot, achieving near-perfect success across 10 consecutive assembly iterations with human intervention only during reset prior to assembly. The entire pipeline from the textual word to reliable physical assembly is performed without human intervention.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt19_04">
             15:30-15:35, Paper ThDT19.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3875'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Geometry and Force-Informed Robotic Assembly with Small Relative Initial Deviations for Circular Electrical Connectors
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424130" title="Click to go to the Author Index">
             Wang, Zhenyu
            </a>
           </td>
           <td class="r">
            Huazhong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#203408" title="Click to go to the Author Index">
             Li, Xiangfei
            </a>
           </td>
           <td class="r">
            Huazhong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#140969" title="Click to go to the Author Index">
             Zhao, Huan
            </a>
           </td>
           <td class="r">
            Huazhong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425609" title="Click to go to the Author Index">
             Shao, Lingjun
            </a>
           </td>
           <td class="r">
            Huazhong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425594" title="Click to go to the Author Index">
             Zhang, Hao
            </a>
           </td>
           <td class="r">
            Huazhong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#195115" title="Click to go to the Author Index">
             Ding, Han
            </a>
           </td>
           <td class="r">
            Huazhong University of Science and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3875" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#assembly" title="Click to go to the Keyword Index">
               Assembly
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#compliant_assembly" title="Click to go to the Keyword Index">
               Compliant Assembly
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Circular electrical connectors (CECs) have a wide range of applications in scenarios that require reliable connections. However, sockets are often located in narrow scenes with random spatial orientations, complex lighting conditions, and obstructions from cables, making it difficult to accurately locate them through cameras. Besides, due to the complex geometric structure of CECs and the presence of electrode protection slots, the existing research on the assembly of cylindrical or polygonal pegs and holes may not be applicable to the assembly of such components. To this end, this article proposes a novel robotic assembly strategy for CECs with small relative initial deviations, whose core is to design a search trajectory and heuristic force strategy to perceive force/pose (F/P) discontinuity characteristics under different geometric constraints. This assembly strategy is independent of the CEC's size and is not affected by the socket's spatial orientation. The experiments with two different sizes of CECs on a robot equipped with a 6-dimensional force/torque (F/T) sensor are conducted, and the effectiveness and robustness of the proposed assembly strategy for CECs are demonstrated.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt19_05">
             15:35-15:40, Paper ThDT19.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4076'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              MatchMaker: Automated Asset Generation for Robotic Assembly
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#374524" title="Click to go to the Author Index">
             Wang, Yian
            </a>
           </td>
           <td class="r">
            Umass Amherst
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#290359" title="Click to go to the Author Index">
             Tang, Bingjie
            </a>
           </td>
           <td class="r">
            University of Southern California
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#268795" title="Click to go to the Author Index">
             Gan, Chuang
            </a>
           </td>
           <td class="r">
            IBM
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104592" title="Click to go to the Author Index">
             Fox, Dieter
            </a>
           </td>
           <td class="r">
            University of Washington
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#223847" title="Click to go to the Author Index">
             Mo, Kaichun
            </a>
           </td>
           <td class="r">
            NVIDIA
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#217912" title="Click to go to the Author Index">
             Narang, Yashraj
            </a>
           </td>
           <td class="r">
            NVIDIA
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#215819" title="Click to go to the Author Index">
             Akinola, Iretiayo
            </a>
           </td>
           <td class="r">
            Columbia University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4076" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#assembly" title="Click to go to the Keyword Index">
               Assembly
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_manufacturing" title="Click to go to the Keyword Index">
               Computer Vision for Manufacturing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robotic assembly remains a significant challenge due to complexities in visual perception, functional grasping, contact-rich manipulation, and performing high-precision tasks. Simulation-based learning and sim-to-real transfer have led to recent success in solving assembly tasks in the presence of object pose variation, perception noise, and control error; however, the development of a generalist (i.e., multi-task) agent for a broad range of assembly tasks has been limited by the need to manually curate assembly assets, which greatly constrains the number and diversity of assembly problems that can be used for policy learning. Inspired by recent success of using Generative AI to scale up robot learning, we propose MatchMaker, a pipeline to automatically generate diverse, simulation-compatible assembly asset pairs to facilitate learning assembly skills. Specifically, MatchMaker can 1) take a simulation-incompatible, interpenetrating asset pair as input, and automatically convert it into a simulation-compatible, interpenetration-free pair, 2) take an arbitrary single asset as input , and generate a geometrically-mating asset to create an asset pair, 3) automatically erode contact surfaces from (1) or (2) according to a user-specified clearance parameter to generate realistic parts.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt19_06">
             15:40-15:45, Paper ThDT19.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4282'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              CNSv2: Probabilistic Correspondence Encoded Neural Image Servo
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#299649" title="Click to go to the Author Index">
             Chen, Anzhe
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#254623" title="Click to go to the Author Index">
             Yu, Hongxiang
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426416" title="Click to go to the Author Index">
             Li, Shuxin
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#450806" title="Click to go to the Author Index">
             Chen, Yuxi
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#286412" title="Click to go to the Author Index">
             Zhou, Zhongxiang
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#171392" title="Click to go to the Author Index">
             Sun, WenTao
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#113216" title="Click to go to the Author Index">
             Xiong, Rong
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#156231" title="Click to go to the Author Index">
             Wang, Yue
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4282" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#assembly" title="Click to go to the Keyword Index">
               Assembly
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#intelligent_and_flexible_manufacturing" title="Click to go to the Keyword Index">
               Intelligent and Flexible Manufacturing
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#visual_servoing" title="Click to go to the Keyword Index">
               Visual Servoing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Visual servo based on traditional image matching methods often requires accurate keypoint correspondence for high precision control. However, keypoint detection or matching tends to fail in challenging scenarios with inconsistent illuminations or textureless objects, resulting significant performance degradation. Previous approaches, including our proposed Correspondence encoded Neural image Servo policy (CNS), attempted to alleviate these issues by integrating neural control strategies. While CNS shows certain improvement against error correspondence over conventional image-based controllers, it could not fully resolve the limitations arising from poor keypoint detection and matching. In this paper, we continue to address this problem and propose a new solution: Probabilistic Correspondence Encoded Neural Image Servo (CNSv2). CNSv2 leverages probabilistic feature matching to improve robustness in challenging scenarios. By redesigning the architecture to condition on multimodal feature matching, CNSv2 achieves high precision, improved robustness across diverse scenes and runs in real-time. We validate CNSv2 with simulations and real-world experiments, demonstrating its effectiveness in overcoming the limitations of detector-based methods in visual servo tasks.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt19_07">
             15:45-15:50, Paper ThDT19.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4588'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Supervised Representation Learning towards Generalizable Assembly State Recognition
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#407051" title="Click to go to the Author Index">
             Schoonbeek, Tim Jeroen
            </a>
           </td>
           <td class="r">
            Eindhoven University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#406828" title="Click to go to the Author Index">
             Balachandran, Goutham
            </a>
           </td>
           <td class="r">
            ASML
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#407048" title="Click to go to the Author Index">
             Onvlee, Hans
            </a>
           </td>
           <td class="r">
            ASML
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#406777" title="Click to go to the Author Index">
             Houben, Tim
            </a>
           </td>
           <td class="r">
            Eindhoven University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#407190" title="Click to go to the Author Index">
             Hung, Shao-Hsuan
            </a>
           </td>
           <td class="r">
            Eindhoven University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#407073" title="Click to go to the Author Index">
             Kustra, Jacek
            </a>
           </td>
           <td class="r">
            ASML
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#180080" title="Click to go to the Author Index">
             de With, Peter H.N.
            </a>
           </td>
           <td class="r">
            Eindhoven University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#406871" title="Click to go to the Author Index">
             van der Sommen, Fons
            </a>
           </td>
           <td class="r">
            Eindhoven University of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4588" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#representation_learning" title="Click to go to the Keyword Index">
               Representation Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_manufacturing" title="Click to go to the Keyword Index">
               Computer Vision for Manufacturing
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Assembly state recognition facilitates the execution of assembly procedures, offering feedback to enhance efficiency and minimize errors. However, recognizing assembly states poses challenges in scalability, since parts are frequently updated, and the robustness to execution errors remains underexplored. To address these challenges, this paper proposes an approach based on representation learning and the novel intermediate-state informed loss function modification (ISIL). ISIL leverages unlabeled transitions between states and demonstrates significant improvements in clustering and classification performance for all tested architectures and losses. Despite being trained exclusively on images without execution errors, thorough analysis on error states demonstrates that our approach accurately distinguishes between correct states and states with various types of execution errors. The integration of the proposed algorithm can offer meaningful assistance to workers and mitigate unexpected losses due to procedural mishaps in industrial settings. The code and data are publicly available.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt20">
             <b>
              ThDT20
             </b>
            </a>
           </td>
           <td class="r">
            408
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thdt20" title="Click to go to the Program at a Glance">
             <b>
              Agricultural Automation 3
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#180009" title="Click to go to the Author Index">
             Papageorgiou, Dimitrios
            </a>
           </td>
           <td class="r">
            Hellenic Mediterranean University
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#113264" title="Click to go to the Author Index">
             Berenson, Dmitry
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt20_01">
             15:15-15:20, Paper ThDT20.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('17'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Panoptic Segmentation with Partial Annotations for Agricultural Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#276395" title="Click to go to the Author Index">
             Weyler, Jan
            </a>
           </td>
           <td class="r">
            University of Bonn
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#218841" title="Click to go to the Author Index">
             Läbe, Thomas
            </a>
           </td>
           <td class="r">
            University of Bonn
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#137262" title="Click to go to the Author Index">
             Behley, Jens
            </a>
           </td>
           <td class="r">
            University of Bonn
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101642" title="Click to go to the Author Index">
             Stachniss, Cyrill
            </a>
           </td>
           <td class="r">
            University of Bonn
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab17" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#robotics_and_automation_in_agriculture_and_forestry" title="Click to go to the Keyword Index">
               Robotics and Automation in Agriculture and Forestry
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             A detailed analysis of agricultural fields is key toward reducing the use of agrochemicals to achieve a more sustainable crop production. To this end, agricultural robots equipped with vision-based systems offer the potential to detect individual plants in the field automatically. This capability enables targeted management actions in the field, effectively reducing the amount of agrochemicals. A primary target of such vision systems is to perform a panoptic segmentation, combining the task of semantic and instance segmentation. Recent methods use neural networks for this task, which typically have to be trained on densely annotated images containing the required ground truth information for each pixel. Gathering these dense annotations is generally daunting and requires domain experts' knowledge in the agricultural domain. In this paper, we propose a method to effectively reduce the annotation bottleneck and yet achieve high performance using partial annotations. These partial annotations contain ground truth information only for a subset of pixels per image and are thus much faster to obtain than dense annotations. We propose a novel set of losses that exploit measures from vector fields used in physics, i.e., divergence and curl, to effectively supervise predictions without ground truth annotations. The experimental evaluation shows that our approach outperforms several state-of-the-art methods targeting to reduce the amount of annotations.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt20_02">
             15:20-15:25, Paper ThDT20.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('501'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Robotic 3D Flower Pose Estimation for Small-Scale Urban Farms
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#338166" title="Click to go to the Author Index">
             Muriki, Venkata Harsh Suhith
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416581" title="Click to go to the Author Index">
             Teo, Hong Ray
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416621" title="Click to go to the Author Index">
             Sengupta, Ved
            </a>
           </td>
           <td class="r">
            Georgia Tech Research Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#139609" title="Click to go to the Author Index">
             Hu, Ai-Ping
            </a>
           </td>
           <td class="r">
            Georgia Tech Research Institute
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab501" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#robotics_and_automation_in_agriculture_and_forestry" title="Click to go to the Keyword Index">
               Robotics and Automation in Agriculture and Forestry
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The small scale of urban farms and the commercial availability of low-cost robots (such as the FarmBot) that automate simple tending tasks enable an accessible platform for plant phenotyping. We have used a FarmBot with a custom camera end-effector to estimate strawberry plant flower pose (for robotic pollination) from acquired 3D point cloud models. We describe a novel algorithm that translates individual occupancy grids along orthogonal axes of a point cloud to obtain 2D images corresponding to the six viewpoints. For each image, 2D object detection models for flowers are used to identify 2D bounding boxes which can be converted into the 3D space to extract flower point clouds. Pose estimation is performed by fitting three shapes (superellipsoids, paraboloids and planes) to the flower point clouds and compared with manually labeled ground truth. Our method successfully finds approximately 80% of flowers scanned using our customized FarmBot platform and has a mean flower pose error of 7.7 degrees, which is sufficient for robotic pollination and rivals previous results. All code will be made available at https://github.com/harshmuriki/flowerPose.git.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt20_03">
             15:25-15:30, Paper ThDT20.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('732'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Fault Management System for the Safety of Perception Systems in Highly Automated Agricultural Machines
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#393093" title="Click to go to the Author Index">
             Lee, Changjoo
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#124912" title="Click to go to the Author Index">
             Schätzle, Simon
            </a>
           </td>
           <td class="r">
            STW (Sensor-Technik Wiedemann GmbH)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#393098" title="Click to go to the Author Index">
             Lang, Stefan Andreas
            </a>
           </td>
           <td class="r">
            Sensor-Technik Wiedemann
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#401259" title="Click to go to the Author Index">
             Maier, Michael
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#138727" title="Click to go to the Author Index">
             Oksanen, Timo
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab732" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#robotics_and_automation_in_agriculture_and_forestry" title="Click to go to the Keyword Index">
               Robotics and Automation in Agriculture and Forestry
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#robot_safety" title="Click to go to the Keyword Index">
               Robot Safety
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Safe and reliable environmental perception is crucial for the highly automated or even autonomous operation of agriculture machines. However, developing such a system is challenging due to imperfect perception sensors. This article proposes a fault management system (FMS) for detecting, diagnosing, and mitigating risks that compromise the safety and reliability of perception systems. This article aims to develop an improved image quality safety model (IQSM) for the FMS to detect and diagnose the causes of performance insufficiencies in object detection. The IQSM exhibits remarkable performance, achieving an accuracy of about 98%, demonstrating its ability to effectively identify performance insufficiencies under pre-defined hazardous scenarios.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt20_04">
             15:30-15:35, Paper ThDT20.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3065'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Learning to Prune Branches in Modern Tree-Fruit Orchards
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#418317" title="Click to go to the Author Index">
             Jain, Abhinav
            </a>
           </td>
           <td class="r">
            Oregon State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#191086" title="Click to go to the Author Index">
             Grimm, Cindy
            </a>
           </td>
           <td class="r">
            Oregon State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#187769" title="Click to go to the Author Index">
             Lee, Stefan
            </a>
           </td>
           <td class="r">
            Oregon State University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3065" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#robotics_and_automation_in_agriculture_and_forestry" title="Click to go to the Keyword Index">
               Robotics and Automation in Agriculture and Forestry
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#visual_servoing" title="Click to go to the Keyword Index">
               Visual Servoing
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#field_robots" title="Click to go to the Keyword Index">
               Field Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Dormant tree pruning is labor-intensive but essential to maintaining modern highly-productive fruit orchards. In this work we present a closed-loop visuomotor controller for robotic pruning. The controller guides the cutter through a cluttered tree environment to reach a specified cut point and ensures the cutters are perpendicular to the branch. We train the controller using a novel orchard simulation that captures the geometric distribution of branches in a target apple orchard configuration. Unlike traditional methods requiring full 3D reconstruction, our controller uses just optical flow images from a wrist-mounted camera. We deploy our learned policy in simulation and the real-world for an example V-Trellis envy tree with zero-shot transfer, achieving a sim30% success rate -- approximately half the performance of an oracle planner.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt20_05">
             15:35-15:40, Paper ThDT20.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3793'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Towards Safe and Efficient Through-The-Canopy Autonomous Fruit Counting with UAVs
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422338" title="Click to go to the Author Index">
             Yang, Teaya
            </a>
           </td>
           <td class="r">
            UC Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#251322" title="Click to go to the Author Index">
             Ibrahimov, Roman
            </a>
           </td>
           <td class="r">
            UC Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#146039" title="Click to go to the Author Index">
             Mueller, Mark Wilfried
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3793" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__perception_and_autonomy" title="Click to go to the Keyword Index">
               Aerial Systems: Perception and Autonomy
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#agricultural_automation" title="Click to go to the Keyword Index">
               Agricultural Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present an autonomous aerial system for safe and efficient through-the-canopy fruit counting. Aerial robot applications in large-scale orchards face significant challenges due to the complexity of fine-tuning flight paths based on orchard layouts, canopy density, and plant variability. Through-the-canopy navigation is crucial for minimizing occlusion by leaves and branches but is more challenging due to the complex and dense environment compared to traditional over-the-canopy flights. Our system addresses these challenges by integrating: i) a high-fidelity simulation framework for global path planning, ii) a low-cost autonomy stack for canopy-level navigation and data collection, and iii) a robust workflow for fruit detection and counting using RGB images. We validate our approach through fruit counting with canopy-level aerial images and by demonstrating the autonomous navigation capabilities of our experimental vehicle.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt20_06">
             15:40-15:45, Paper ThDT20.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4096'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Language-Guided Object Search in Agricultural Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422205" title="Click to go to the Author Index">
             Balaji, Advaith
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425878" title="Click to go to the Author Index">
             Pradhan, Saket
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#113264" title="Click to go to the Author Index">
             Berenson, Dmitry
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4096" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#robotics_and_automation_in_agriculture_and_forestry" title="Click to go to the Keyword Index">
               Robotics and Automation in Agriculture and Forestry
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Creating robots that can assist in farms and gardens can help reduce the mental and physical workload experienced by farm workers. We tackle the problem of object search in a farm environment, providing a method that allows a robot to semantically reason about the location of an unseen target object among a set of previously seen objects in the environment using a Large Language Model (LLM). We leverage object-to-object semantic relationships to plan a path through the environment that will allow us to accurately and efficiently locate our target object while also reducing the overall distance traveled, without needing high-level room or area-level semantic relationships. During our evaluations, we found that our method outperformed a current state-of-the-art baseline and our ablations. Our offline testing yielded an average path efficiency of 84%, reflecting how closely the predicted path aligns with the ideal path. Upon deploying our system on the Boston Dynamics Spot robot in a real-world farm environment, we found that our system had a success rate of 80%, with a success weighted by path length of 0.67, which demonstrates a reasonable trade-off between task success and path efficiency under real-world conditions. The project website can be viewed at: adi-balaji.github.io/losae
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt20_07">
             15:45-15:50, Paper ThDT20.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4871'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Robotic Grape Inspection and Selective Harvesting in Vineyards
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#203904" title="Click to go to the Author Index">
             Stavridis, Sotiris
            </a>
           </td>
           <td class="r">
            Aristotle University of Thessaloniki
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#137296" title="Click to go to the Author Index">
             Droukas, Leonidas
            </a>
           </td>
           <td class="r">
            Aristotle University of Thessaloniki
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#103866" title="Click to go to the Author Index">
             Doulgeri, Zoe
            </a>
           </td>
           <td class="r">
            Aristotle University of Thessaloniki
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#180009" title="Click to go to the Author Index">
             Papageorgiou, Dimitrios
            </a>
           </td>
           <td class="r">
            Hellenic Mediterranean University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#171424" title="Click to go to the Author Index">
             Dimeas, Fotios
            </a>
           </td>
           <td class="r">
            Aristotle University of Thessaloniki
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#248927" title="Click to go to the Author Index">
             Soriano, Angel
            </a>
           </td>
           <td class="r">
            Robotnik Automation SL
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#219175" title="Click to go to the Author Index">
             Molina, Sergi
            </a>
           </td>
           <td class="r">
            University of Lincoln
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#252749" title="Click to go to the Author Index">
             Deiri, Ahmed Sami
            </a>
           </td>
           <td class="r">
            SAGA Robotics
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#218794" title="Click to go to the Author Index">
             Hutchinson, Michael
            </a>
           </td>
           <td class="r">
            Saga Robotics
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#120687" title="Click to go to the Author Index">
             Pulido Fentanes, Jaime
            </a>
           </td>
           <td class="r">
            Saga Robotics
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#339258" title="Click to go to the Author Index">
             Hroob, Ibrahim
            </a>
           </td>
           <td class="r">
            University of Lincoln
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#192132" title="Click to go to the Author Index">
             Polvara, Riccardo
            </a>
           </td>
           <td class="r">
            University of Lincoln
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104948" title="Click to go to the Author Index">
             Hanheide, Marc
            </a>
           </td>
           <td class="r">
            University of Lincoln
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#106258" title="Click to go to the Author Index">
             Cielniak, Grzegorz
            </a>
           </td>
           <td class="r">
            University of Lincoln
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#350132" title="Click to go to the Author Index">
             Samarinas, Nikiforos
            </a>
           </td>
           <td class="r">
            Laboratory of Remote Sensing, Spectroscopy, and GIS, School of A
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#350133" title="Click to go to the Author Index">
             Kateris, Dimitrios
            </a>
           </td>
           <td class="r">
            Centre for Research and Technology Hellas (CERTH)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#181168" title="Click to go to the Author Index">
             Bochtis, Dionysis
            </a>
           </td>
           <td class="r">
            CERTH
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#184414" title="Click to go to the Author Index">
             Peleka, Georgia
            </a>
           </td>
           <td class="r">
            CERTH, Thessaloniki Greece
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#401982" title="Click to go to the Author Index">
             Papadam, Stefanos
            </a>
           </td>
           <td class="r">
            Certh / Iti
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#184413" title="Click to go to the Author Index">
             Triantafyllou, Dimitra
            </a>
           </td>
           <td class="r">
            CERTH
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#296535" title="Click to go to the Author Index">
             Papadimitriou, Alexios
            </a>
           </td>
           <td class="r">
            Certh / Iti
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#293792" title="Click to go to the Author Index">
             Papadopoulos, Christos
            </a>
           </td>
           <td class="r">
            ITI/CERTH
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#184427" title="Click to go to the Author Index">
             Mariolis, Ioannis
            </a>
           </td>
           <td class="r">
            CERTH
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#191969" title="Click to go to the Author Index">
             Giakoumis, Dimitris
            </a>
           </td>
           <td class="r">
            Centre for Research and Technology Hellas
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#191967" title="Click to go to the Author Index">
             Tzovaras, Dimitrios
            </a>
           </td>
           <td class="r">
            Centre for Research and Technology Hellas
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4871" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#robotics_and_automation_in_agriculture_and_forestry" title="Click to go to the Keyword Index">
               Robotics and Automation in Agriculture and Forestry
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#bimanual_manipulation" title="Click to go to the Keyword Index">
               Bimanual Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Driven by the increasing food demand and the need for higher-quality cultivation, precision agriculture grows steadily during the last decade. It involves the application of mobile robots and intelligent robotic technologies in various agricultural field tasks, concerning a variety of crop types. Aiming at compensating for the lack of selective robotic harvesting solutions regarding the high-value crop of grapes, the EU-funded project BACCHUS develops an intelligent mobile robotic system, comprising two independent and cooperative robots: one for the grape inspection and collection of valuable data regarding their maturity level, and one for the bimanual harvesting of grapes in a human-inspired manner. Validated via real-field trials, the proposed autonomous system pushes forward the precision agriculture application for a particularly sensitive crop type in the challenging and heavily cluttered environment of vineyards, facilitating the selective harvesting of high-quality grapes.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt21">
             <b>
              ThDT21
             </b>
            </a>
           </td>
           <td class="r">
            410
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thdt21" title="Click to go to the Program at a Glance">
             <b>
              Diffusion for Manipulation
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#131170" title="Click to go to the Author Index">
             Pérez-D'Arpino, Claudia
            </a>
           </td>
           <td class="r">
            NVIDIA
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt21_01">
             15:15-15:20, Paper ThDT21.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1145'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              ProDapt: Proprioceptive Adaptation Using Long-Term Memory Diffusion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#309224" title="Click to go to the Author Index">
             Pizarro Bejarano, Federico
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420489" title="Click to go to the Author Index">
             Jones, Bryson
            </a>
           </td>
           <td class="r">
            NASA Jet Propulsion Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#245024" title="Click to go to the Author Index">
             Pastor, Daniel
            </a>
           </td>
           <td class="r">
            Caltech
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#270153" title="Click to go to the Author Index">
             Bowkett, Joseph
            </a>
           </td>
           <td class="r">
            NASA Jet Propulsion Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#124265" title="Click to go to the Author Index">
             Schoellig, Angela P.
            </a>
           </td>
           <td class="r">
            TU Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#106990" title="Click to go to the Author Index">
             Backes, Paul
            </a>
           </td>
           <td class="r">
            Jet Propulsion Laboratory
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1145" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#machine_learning_for_robot_control" title="Click to go to the Keyword Index">
               Machine Learning for Robot Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#space_robotics_and_automation" title="Click to go to the Keyword Index">
               Space Robotics and Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Diffusion models have revolutionized imitation learning, allowing robots to replicate complex behaviours. However, diffusion often relies on cameras and other exteroceptive sensors to observe the environment and lacks long-term memory. In space, military, and underwater applications, robots must be highly robust to failures in exteroceptive sensors, operating using only proprioceptive information. In this paper, we propose ProDapt, a method of incorporating long-term memory of previous contacts between the robot and the environment in the diffusion process, allowing it to complete tasks using only proprioceptive data. This is achieved by identifying "keypoints", essential past observations maintained as inputs to the policy. We test our approach using a UR10e robotic arm in both simulation and real experiments and demonstrate the necessity of this long-term memory for task completion.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt21_02">
             15:20-15:25, Paper ThDT21.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1247'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Latent Embedding Adaptation for Human Preference Alignment in Diffusion Planners
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#361860" title="Click to go to the Author Index">
             Ng, Wen Zheng Terence
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#361861" title="Click to go to the Author Index">
             Chen, Jianda
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#227285" title="Click to go to the Author Index">
             Xu, Yuan
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#256104" title="Click to go to the Author Index">
             Zhang, Tianwei
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1247" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#representation_learning" title="Click to go to the Keyword Index">
               Representation Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This work addresses the challenge of personalizing automated decision-making systems by introducing a resource-efficient approach that enables rapid adaptation to individual users' preferences. Our method leverages a pretrained conditional diffusion model with Preference Latent Embeddings (PLE), trained on a large, reward-free offline dataset. The PLE serves as a compact representation for capturing specific user preferences. By adapting the pretrained model using our proposed preference inversion method, which directly optimizes the learnable PLE, we achieve superior alignment with human preferences compared to existing solutions like Reinforcement Learning from Human Feedback (RLHF) and Low-Rank Adaptation (LoRA). To better reflect practical applications, we create a benchmark experiment using real human preferences on diverse, optimal trajectories.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt21_03">
             15:25-15:30, Paper ThDT21.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1602'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Joint Localization and Planning Using Diffusion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#291945" title="Click to go to the Author Index">
             Lao Beyer, Lukas
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#124153" title="Click to go to the Author Index">
             Karaman, Sertac
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1602" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#autonomous_vehicle_navigation" title="Click to go to the Keyword Index">
               Autonomous Vehicle Navigation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Diffusion models have been successfully applied to robotics problems such as manipulation and vehicle path planning. In this work, we explore their application to end-to-end navigation -- including both perception and planning -- by considering the problem of jointly performing global localization and path planning in known but arbitrary 2D environments. In particular, we introduce a diffusion model which produces collision-free paths in a global reference frame given an egocentric LIDAR scan, an arbitrary map, and a desired goal position. To this end, we implement diffusion in the space of paths in SE(2), and describe how to condition the denoising process on both obstacles and sensor observations. In our evaluation, we show that the proposed conditioning techniques enable generalization to realistic maps of considerably different appearance than the training environment, demonstrate our model's ability to accurately describe ambiguous solutions, and run extensive simulation experiments showcasing our model's use as a real-time, end-to-end localization and planning stack.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt21_04">
             15:30-15:35, Paper ThDT21.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2315'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Diverse Motion Planning with Stein Diffusion Trajectory Inference
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#399102" title="Click to go to the Author Index">
             Zeya, Yin
            </a>
           </td>
           <td class="r">
            Univeristy of Sydney
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#237502" title="Click to go to the Author Index">
             Lai, Tin
            </a>
           </td>
           <td class="r">
            University of Sydney
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#269416" title="Click to go to the Author Index">
             Barcelos, Lucas
            </a>
           </td>
           <td class="r">
            University of Sydney
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#363448" title="Click to go to the Author Index">
             Jacob, Jayadeep
            </a>
           </td>
           <td class="r">
            University of Sydney
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#397077" title="Click to go to the Author Index">
             Li, Yong Hui
            </a>
           </td>
           <td class="r">
            Univeristy of Sydney
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101688" title="Click to go to the Author Index">
             Ramos, Fabio
            </a>
           </td>
           <td class="r">
            University of Sydney, NVIDIA
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2315" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#probabilistic_inference" title="Click to go to the Keyword Index">
               Probabilistic Inference
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#integrated_planning_and_learning" title="Click to go to the Keyword Index">
               Integrated Planning and Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Acquiring prior knowledge of trajectory distributions in specific environments can significantly expedite the optimisation process in robot motion planning. Leveraging successful past plans and utilising trajectory generative models as priors offers a clear advantage. Previous studies have proposed various methods to harness these priors, such as using prior samples for initialisation or incorporating the prior distribution into trajectory optimisation through inference. Recently, diffusion models have demonstrated effectiveness in encoding multimodal data in high-dimensional settings. In this study, we propose a method that uses diffusion models as priors and employs Stein variational inference with Gaussian Process trajectories to integrate them into a batch inverse denoising process. This approach reduces the computation time required to approximate the posterior distribution of trajectories, particularly when adapting to new, unseen environments. Additionally, we incorporate path signatures into our method to enhance the diversity of the posterior distribution. To validate our approach, we conduct comparative assessments against multiple baseline methods across various scenarios, including 2D planar robots and robotic manipulators. Our experiments demonstrate that our method identifies the optimal solution with significantly reduced computational time.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt21_05">
             15:35-15:40, Paper ThDT21.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2474'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              The Ingredients for Robotic Diffusion Transformers
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#294953" title="Click to go to the Author Index">
             Dasari, Sudeep
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#191594" title="Click to go to the Author Index">
             Mees, Oier
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423019" title="Click to go to the Author Index">
             Zhao, Sebastian
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#341441" title="Click to go to the Author Index">
             Srirama, Mohan Kumar
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#156706" title="Click to go to the Author Index">
             Levine, Sergey
            </a>
           </td>
           <td class="r">
            UC Berkeley
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2474" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#machine_learning_for_robot_control" title="Click to go to the Keyword Index">
               Machine Learning for Robot Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In recent years roboticists have achieved remarkable progress in solving increasingly general tasks on dexterous robotic hardware by leveraging high capacity Transformer network architectures and generative diffusion models. Unfortunately, combining these two orthogonal improvements has proven surprisingly difficult, since there is no clear and well understood process for making important design choices. In this paper, we identify, study and improve key architectural design decisions for high-capacity diffusion transformer policies. The resulting models can efficiently solve diverse tasks on multiple robot embodiments, without the excruciating pain of per-setup hyper-parameter tuning. By combining the results of our investigation with our improved model components, we are able to present a novel architecture, named DiT-Block Policy, that significantly outperforms the state of the art in solving long-horizon (1500+ time-steps) dexterous tasks on a bi-manual ALOHA robot. In addition, we find that our policies show improved scaling performance when trained on 10 hours of highly multi-modal, language annotated ALOHA demonstration data. We hope this work will open the door for future robot learning techniques that leverage the efficiency of generative diffusion modeling with the scalability of large scale transformer architectures. Code, robot dataset, and videos are available at: https://dit-policy.github.io
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt21_06">
             15:40-15:45, Paper ThDT21.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4182'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Inference-Time Policy Steering through Human Interactions
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#355184" title="Click to go to the Author Index">
             Wang, Yanwei
            </a>
           </td>
           <td class="r">
            MIT
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#308986" title="Click to go to the Author Index">
             Wang, Lirui
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#311624" title="Click to go to the Author Index">
             Du, Yilun
            </a>
           </td>
           <td class="r">
            MIT
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#186993" title="Click to go to the Author Index">
             Sundaralingam, Balakumar
            </a>
           </td>
           <td class="r">
            NVIDIA Corporation
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#205333" title="Click to go to the Author Index">
             Yang, Xuning
            </a>
           </td>
           <td class="r">
            NVIDIA
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#198471" title="Click to go to the Author Index">
             Chao, Yu-Wei
            </a>
           </td>
           <td class="r">
            NVIDIA
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#131170" title="Click to go to the Author Index">
             Pérez-D'Arpino, Claudia
            </a>
           </td>
           <td class="r">
            NVIDIA
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104592" title="Click to go to the Author Index">
             Fox, Dieter
            </a>
           </td>
           <td class="r">
            University of Washington
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#140740" title="Click to go to the Author Index">
             Shah, Julie A.
            </a>
           </td>
           <td class="r">
            MIT
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4182" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Generative policies trained with human demonstrations can autonomously accomplish multimodal, long-horizon tasks. However, during inference, humans are often removed from the policy execution loop, limiting the ability to guide a pre-trained policy towards a specific sub-goal or trajectory shape among multiple predictions. Naive human intervention may inadvertently exacerbate distribution shift, leading to constraint violations or execution failures. To better align policy output with human intent without inducing out-of-distribution errors, we propose an Inference-Time Policy Steering (ITPS) framework that leverages human interactions to bias the generative sampling process, rather than fine-tuning the policy on interaction data. We evaluate ITPS across three simulated and real-world benchmarks, testing three forms of human interaction and associated alignment distance metrics. Among six sampling strategies, our proposed stochastic sampling with diffusion policy achieves the best trade-off between alignment and distribution shift. Videos are available at https://yanweiw.github.io/itps/.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt21_07">
             15:45-15:50, Paper ThDT21.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('5022'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Legibility Diffuser: Offline Imitation for Intent Expressive Motion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#374833" title="Click to go to the Author Index">
             Bronars, Matthew
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#285427" title="Click to go to the Author Index">
             Cheng, Shuo
            </a>
           </td>
           <td class="r">
            Gatech
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#158823" title="Click to go to the Author Index">
             Xu, Danfei
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab5022" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In human-robot collaboration, legible motion that conveys a robot's intentions and goals is known to improve safety, task efficiency, and user experience. Legible robot motion is typically generated using hand-designed cost functions and classical motion planners. However, with the rise of deep learning and data-driven robot policies, we need methods for training end-to-end on offline demonstration data. In this paper, we propose Legibility Diffuser, a diffusion-based policy that learns intent expressive motion directly from human demonstrations. By variably combining the noise predictions from a goal-conditioned diffusion model, we guide the robot's motion toward the most legible trajectory in the training dataset. We find that decaying the guidance weight over the course of the trajectory is critical for maintaining a high success rate while maximizing legibility.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt22">
             <b>
              ThDT22
             </b>
            </a>
           </td>
           <td class="r">
            411
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thdt22" title="Click to go to the Program at a Glance">
             <b>
              Imitation Learning 3
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#246919" title="Click to go to the Author Index">
             Bıyık, Erdem
            </a>
           </td>
           <td class="r">
            University of Southern California
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt22_01">
             15:15-15:20, Paper ThDT22.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1235'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Visually Robust Adversarial Imitation Learning from Videos with Contrastive Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#293013" title="Click to go to the Author Index">
             Giammarino, Vittorio
            </a>
           </td>
           <td class="r">
            Boston University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#415039" title="Click to go to the Author Index">
             Queeney, James
            </a>
           </td>
           <td class="r">
            Mitsubishi Electric Research Laboratories
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#125644" title="Click to go to the Author Index">
             Paschalidis, Ioannis
            </a>
           </td>
           <td class="r">
            Boston University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1235" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We propose C-LAIfO, a computationally efficient algorithm designed for imitation learning from videos in the presence of visual mismatch between agent and expert domains. We analyze the problem of imitation from expert videos with visual discrepancies, and introduce a solution for robust latent space estimation using contrastive learning and data augmentation. Provided a visually robust latent space, our algorithm performs imitation entirely within this space using off-policy adversarial imitation learning. We conduct a thorough ablation study to justify our design and test C-LAIfO on high-dimensional continuous robotic tasks. Additionally, we demonstrate how C-LAIfO can be combined with other reward signals to facilitate learning on a set of challenging hand manipulation tasks with sparse rewards. Our experiments show improved performance compared to baseline methods, highlighting the effectiveness of C-LAIfO. To ensure reproducibility, we open source our code.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt22_02">
             15:20-15:25, Paper ThDT22.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1268'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              RACER: Rich Language-Guided Failure Recovery Policies for Imitation Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#367887" title="Click to go to the Author Index">
             Dai, Yinpei
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#338629" title="Click to go to the Author Index">
             Lee, Jayjun
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#191587" title="Click to go to the Author Index">
             Fazeli, Nima
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#169794" title="Click to go to the Author Index">
             Chai, Joyce
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1268" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#data_sets_for_robot_learning" title="Click to go to the Keyword Index">
               Data Sets for Robot Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Developing robust and correctable visuomotor policies for robotic manipulation is challenging due to the lack of self-recovery mechanisms from failures and the limitations of simple language instructions in guiding robot actions. To address these issues, we propose a scalable data generation pipeline that automatically augments expert demonstrations with failure recovery trajectories and fine-grained language annotations for training. We then introduce Rich languAge-guided failure reCovERy (RACER), a supervisor-actor framework, which combines failure recovery data with rich language descriptions to enhance robot control. RACER features a vision-language model (VLM) that acts as an online supervisor, providing detailed language guidance for error correction and task execution, and a language-conditioned visuomotor policy as an actor to predict the next actions. Our experimental results show that RACER outperforms the state-of-the-art Robotic View Transformer (RVT) on RLbench across various evaluation settings, including standard long-horizon tasks, dynamic goal-change tasks and zero-shot unseen tasks, achieving superior performance in both simulated and real world environments. Videos and code are available at: https://rich-language-failure-recovery.github.io
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt22_02">
             15:20-15:25, Paper ThDT22.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2797'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              One-Shot Imitation under Mismatched Execution
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#311920" title="Click to go to the Author Index">
             Kedia, Kushal
            </a>
           </td>
           <td class="r">
            Cornell University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#354410" title="Click to go to the Author Index">
             Dan, Prithwish
            </a>
           </td>
           <td class="r">
            Cornell University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#450695" title="Click to go to the Author Index">
             Chao, Angela
            </a>
           </td>
           <td class="r">
            Cornell University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#450697" title="Click to go to the Author Index">
             Pace, Maximus
            </a>
           </td>
           <td class="r">
            Cornell University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#160608" title="Click to go to the Author Index">
             Choudhury, Sanjiban
            </a>
           </td>
           <td class="r">
            Cornell University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2797" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#representation_learning" title="Click to go to the Keyword Index">
               Representation Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#transfer_learning" title="Click to go to the Keyword Index">
               Transfer Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Human demonstrations as prompts are a powerful way to program robots to do long-horizon manipulation tasks. However, translating these demonstrations into robot-executable actions presents significant challenges due to execution mismatches in movement styles and physical capabilities. Existing methods for human-robot translation either depend on paired data, which is infeasible to scale, or rely heavily on frame-level visual similarities that often break down in practice. To address these challenges, we propose RHyME, a novel framework that automatically pairs human and robot trajectories using sequence-level optimal transport cost functions. Given long-horizon robot demonstrations, RHyME synthesizes semantically equivalent human videos by retrieving and composing short-horizon human clips. This approach facilitates effective policy training without the need for paired data. RHyME successfully imitates a range of cross-embodiment demonstrators, both in simulation and with a real human hand, achieving over 50% increase in task success compared to previous methods. We release our code and datasets at https://portal-cornell.github.io/rhyme/.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt22_03">
             15:25-15:30, Paper ThDT22.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1338'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Improving Vision-Language-Action Model with Online Reinforcement Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#341977" title="Click to go to the Author Index">
             Guo, Yanjiang
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419155" title="Click to go to the Author Index">
             Zhang, Jianke
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#311375" title="Click to go to the Author Index">
             Chen, Xiaoyu
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420360" title="Click to go to the Author Index">
             Ji, Xiang
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#352359" title="Click to go to the Author Index">
             Wang, Yen-Jen
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421144" title="Click to go to the Author Index">
             Hu, Yucheng
            </a>
           </td>
           <td class="r">
            Tsinghua
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#178628" title="Click to go to the Author Index">
             Chen, Jianyu
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1338" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#continual_learning" title="Click to go to the Keyword Index">
               Continual Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Recent studies have successfully integrated large vision-language models (VLMs) into low-level robotic control by supervised fine-tuning (SFT) with expert robotic datasets, resulting in what we term vision-language-action (VLA) models. Although the VLA models are powerful, how to improve these large models during interaction with environments remains an open question. In this paper, we explore how to further improve these VLA models via Reinforcement Learning (RL), a commonly used fine-tuning technique for large models. However, we find that directly applying online RL to large VLA models presents significant challenges, including training instability that severely impacts the performance of large models, and computing demands that exceed the capabilities of most local machines. To address these problems, we propose iRe-VLA framework, which iterates between Reinforcement Learning and supervised learning to effectively improve VLA models, leveraging the exploratory benefits of RL while maintaining the stability of supervised learning. Experiments in two simulated benchmarks and a real-world manipulation suite validate the effectiveness of our method.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt22_04">
             15:30-15:35, Paper ThDT22.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3328'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              MILE: Model-Based Intervention Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#403384" title="Click to go to the Author Index">
             Korkmaz, Yigit
            </a>
           </td>
           <td class="r">
            University of Southern California
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#246919" title="Click to go to the Author Index">
             Bıyık, Erdem
            </a>
           </td>
           <td class="r">
            University of Southern California
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3328" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#ai_based_methods" title="Click to go to the Keyword Index">
               AI-Based Methods
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#human_factors_and_human_in_the_loop" title="Click to go to the Keyword Index">
               Human Factors and Human-in-the-Loop
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Imitation learning techniques have been shown to be highly effective in real-world control scenarios, such as robotics. However, these approaches not only suffer from compounding error issues but also require human experts to provide complete trajectories. Although there exist interactive methods where an expert oversees the robot and intervenes if needed, these extensions usually only utilize the data collected during intervention periods and ignore the feedback signal hidden in non-intervention timesteps. In this work, we create a model to formulate how the interventions occur in such cases, and show that it is possible to learn a policy with just a handful of expert interventions. Our key insight is that it is possible to get crucial information about the quality of the current state and the optimality of the chosen action from expert feedback, regardless of the presence or the absence of intervention. We evaluate our method on various discrete and continuous simulation environments, a real-world robotic manipulation task, as well as a human subject study. Videos and the code can be found at https://liralab.usc.edu/mile.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt22_05">
             15:35-15:40, Paper ThDT22.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3339'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Validity Learning on Failures: Mitigating the Distribution Shift in Autonomous Vehicle Planning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#373779" title="Click to go to the Author Index">
             Arasteh, Fazel
            </a>
           </td>
           <td class="r">
            Noah's Ark Lab, Huawei
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#236861" title="Click to go to the Author Index">
             Elmahgiubi, Mohammed
            </a>
           </td>
           <td class="r">
            Huawei Technologies Inc
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#286278" title="Click to go to the Author Index">
             Khamidehi, Behzad
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#396612" title="Click to go to the Author Index">
             Mirkhani, Hamidreza
            </a>
           </td>
           <td class="r">
            Huawei Technologies Canada
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#305749" title="Click to go to the Author Index">
             Zhang, Weize
            </a>
           </td>
           <td class="r">
            Huawei
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#310909" title="Click to go to the Author Index">
             Cao, Tongtong
            </a>
           </td>
           <td class="r">
            Noah's Ark Lab, Huawei Technologies
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#237138" title="Click to go to the Author Index">
             Rezaee, Kasra
            </a>
           </td>
           <td class="r">
            Huawei Technologies
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3339" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The planning problem constitutes a fundamental aspect of the autonomous driving framework. Recent strides in representation learning have empowered vehicles to comprehend their surrounding environments, thereby facilitating the integration of learning-based planning strategies. Among these approaches, Imitation Learning stands out due to its notable training efficiency. However, traditional Imitation Learning methodologies encounter challenges associated with the covariate shift phenomenon. We propose Validity Learning on Failures, VL(on failure), as a remedy to address this issue. The essence of our method lies in deploying a pre-trained planner across diverse scenarios. Instances where the planner deviates from its immediate objectives, such as maintaining a safe distance from obstacles or adhering to traffic rules, are flagged as failures. The states corresponding to these failures are compiled into a new dataset, termed the failure dataset. Notably, the absence of expert annotations for this data precludes the applicability of standard imitation learning approaches. To facilitate learning from the closed-loop mistakes, we introduce the VL objective which aims to discern valid trajectories within the current environmental context. Experimental evaluations conducted on both reactive CARLA simulation and non-reactive log-replay simulations reveal substantial enhancements in closed-loop metrics such as Score, Progress, and Success Rate, underscoring the effectiveness of the proposed methodology. Further evaluations against the Bench2Drive benchmark demonstrate that VL(on failure) outperforms the state-of-the-art methods by a large margin.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt22_07">
             15:45-15:50, Paper ThDT22.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('5012'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Iteratively Adding Latent Human Knowledge within Trajectory Optimization Specifications Improves Learning and Task Outcomes
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#306736" title="Click to go to the Author Index">
             Chang, Christine T
            </a>
           </td>
           <td class="r">
            University of Colorado Boulder
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#400140" title="Click to go to the Author Index">
             Stull, Maria P
            </a>
           </td>
           <td class="r">
            University of Colorado Boulder
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#409378" title="Click to go to the Author Index">
             Crockett, Breanne
            </a>
           </td>
           <td class="r">
            University of Colorado Boulder
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#409379" title="Click to go to the Author Index">
             Jensen, Emily
            </a>
           </td>
           <td class="r">
            University of Colorado Boulder
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#288276" title="Click to go to the Author Index">
             Lohrmann, Clare
            </a>
           </td>
           <td class="r">
            University of Colorado Boulder
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#180396" title="Click to go to the Author Index">
             Hebert, Mitchell
            </a>
           </td>
           <td class="r">
            Draper
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#156445" title="Click to go to the Author Index">
             Hayes, Bradley
            </a>
           </td>
           <td class="r">
            University of Colorado Boulder
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab5012" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#human_factors_and_human_in_the_loop" title="Click to go to the Keyword Index">
               Human Factors and Human-in-the-Loop
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#human_robot_teaming" title="Click to go to the Keyword Index">
               Human-Robot Teaming
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Frictionless and understandable tasking is essential for leveraging human-autonomy teaming in commercial, military, and public safety applications. Existing technology for facilitating human teaming with uncrewed aerial vehicles (UAVs), utilizing planners or trajectory optimizers that incorporate human input, introduces a usability and operator capability gap by not explicitly effecting user upskilling by promoting system understanding or predictability. Supplementing annotated waypoints with natural language guidance affords an opportunity for both. In this work we investigate one-shot versus iterative input, introducing a testbed system based on government and industry UAV planning tools that affords inputs in the form of both natural language text and drawn annotations on a terrain map. The testbed uses an LLM-based subsystem to map user inputs into additional terms for the trajectory optimization objective function. We demonstrate through a human subjects study that prompting a human teammate to iteratively add latent knowledge to a trajectory optimization aids the user in learning how the system functions, elicits more desirable robot behaviors, and ultimately achieves better task outcomes.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt23">
             <b>
              ThDT23
             </b>
            </a>
           </td>
           <td class="r">
            412
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thdt23" title="Click to go to the Program at a Glance">
             <b>
              Autonomous Vehicle Perception 6
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#309454" title="Click to go to the Author Index">
             Dam, Tanmoy
            </a>
           </td>
           <td class="r">
            Emory University
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#313614" title="Click to go to the Author Index">
             Ding, Zhengming
            </a>
           </td>
           <td class="r">
            Tulane University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt23_01">
             15:15-15:20, Paper ThDT23.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('53'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              HybridOcc: NeRF Enhanced Transformer-Based Multi-Camera 3D Occupancy Prediction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#379298" title="Click to go to the Author Index">
             Zhao, Xiao
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#387957" title="Click to go to the Author Index">
             Chen, Bo
            </a>
           </td>
           <td class="r">
            FAW Group
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#367712" title="Click to go to the Author Index">
             Sun, Mingyang
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#320217" title="Click to go to the Author Index">
             Yang, Dingkang
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#387906" title="Click to go to the Author Index">
             Wang, Youxing
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#388132" title="Click to go to the Author Index">
             Zhang, Xukun
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#387889" title="Click to go to the Author Index">
             Li, Mingcheng
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#372579" title="Click to go to the Author Index">
             Kou, Dongliang
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#373615" title="Click to go to the Author Index">
             Wei, Xiaoyi
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#267890" title="Click to go to the Author Index">
             ZHang, Lihua
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab53" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_transportation" title="Click to go to the Keyword Index">
               Computer Vision for Transportation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#recognition" title="Click to go to the Keyword Index">
               Recognition
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Vision-based 3D semantic scene completion (SSC) describes autonomous driving scenes through 3D volume representations. However, the occlusion of invisible voxels by scene surfaces poses challenges to current SSC methods in hallucinating refined 3D geometry. This paper proposes HybridOcc, a hybrid 3D volume query proposal method generated by Transformer framework and NeRF representation and refined in a coarse-to-fine SSC prediction framework. HybridOcc aggregates contextual features through the Transformer paradigm based on hybrid query proposals while combining it with NeRF representation to obtain depth supervision. The Transformer branch contains multiple scales and uses spatial cross-attention for 2D to 3D transformation. The newly designed NeRF branch implicitly infers scene occupancy through volume rendering, including visible and invisible voxels, and explicitly captures scene depth rather than generating RGB color. Furthermore, we present an innovative occupancy-aware ray sampling method to orient the SSC task instead of focusing on the scene surface, further improving the overall performance. Extensive experiments on nuScenes and SemanticKITTI datasets demonstrate the effectiveness of our HybridOcc on the SSC task.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt23_02">
             15:20-15:25, Paper ThDT23.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('266'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Temporal Consistency for RGB-Thermal Data-Based Semantic Scene Understanding
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#369840" title="Click to go to the Author Index">
             Li, Haotian
            </a>
           </td>
           <td class="r">
            The Hong Kong Polytechnic University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#132768" title="Click to go to the Author Index">
             Chu, Henry
            </a>
           </td>
           <td class="r">
            The Hong Kong Polytechnic University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#178095" title="Click to go to the Author Index">
             Sun, Yuxiang
            </a>
           </td>
           <td class="r">
            City University of Hong Kong
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab266" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#automation_technologies_for_smart_cities" title="Click to go to the Keyword Index">
               Automation Technologies for Smart Cities
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#intelligent_transportation_systems" title="Click to go to the Keyword Index">
               Intelligent Transportation Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Semantic scene understanding is a fundamental capability for autonomous vehicles. Under challenging lighting conditions, such as nighttime and on-coming headlights, the semantic scene understanding performance using only RGB images are usually degraded. Thermal images can provide complementary information to RGB images, so many recent semantic segmentation networks have been proposed using RGB-Thermal (RGB-T) images. However, most existing networks focus only on improving segmentation accuracy for single image frames, omitting the information consistency between consecutive frames. To provide a solution to this issue, we propose a temporal-consistent framework for RGB-T semantic segmentation, which introduces a virtual view image generation module to synthesize a virtual image for the next moment, and a consistency loss function to ensure the segmentation consistency. We also propose an evaluation metric to measure both the accuracy and consistency for semantic segmentation. Experimental results show that our framework outperforms state-of-the-art methods.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt23_03">
             15:25-15:30, Paper ThDT23.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1723'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SaViD: Spectravista Aesthetic Vision Integration for Robust and Discerning 3D Object Detection in Challenging Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#309454" title="Click to go to the Author Index">
             Dam, Tanmoy
            </a>
           </td>
           <td class="r">
            Emory University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#377823" title="Click to go to the Author Index">
             Dharavath, Sanjay Bhargav
            </a>
           </td>
           <td class="r">
            Indian Institute of Technology, Kharagpur, India
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#377833" title="Click to go to the Author Index">
             Alam, Sameer
            </a>
           </td>
           <td class="r">
            Saab-NTU Joint Lab, Nanyang Technological University, Singapore
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#377836" title="Click to go to the Author Index">
             Lilith, Nimrod
            </a>
           </td>
           <td class="r">
            Saab-NTU Joint Lab, Nanyang Technological University, Singapore
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421782" title="Click to go to the Author Index">
             Maiti, Aniruddha
            </a>
           </td>
           <td class="r">
            ADP
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#377850" title="Click to go to the Author Index">
             Chakraborty, Supriyo
            </a>
           </td>
           <td class="r">
            Indian Institute of Technology, Kharagpur, India
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#271962" title="Click to go to the Author Index">
             Feroskhan, Mir
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1723" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#autonomous_vehicle_navigation" title="Click to go to the Keyword Index">
               Autonomous Vehicle Navigation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#sensor_fusion" title="Click to go to the Keyword Index">
               Sensor Fusion
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The fusion of LiDAR and camera sensors has demonstrated significant effectiveness in achieving accurate detection for short-range tasks in autonomous driving. However, this fusion approach could face challenges when dealing with long-range detection scenarios due to disparity between sparsity of LiDAR and high-resolution camera data. Moreover, sensor corruption introduces complexities that affect the ability to maintain robustness, despite the growing adoption of sensor fusion in this domain. We present SaViD, a novel framework comprised of a three-stage fusion alignment mechanism designed to address long-range detection challenges in the presence of natural corruption. The SaViD framework consists of three key elements: the Global Memory Attention Network (GMAN), which enhances the extraction of image features through offering a deeper understanding of global patterns; the Attentional Sparse Memory Network (ASMN), which enhances the integration of LiDAR and image features; and the KNNnectivity Graph Fusion (KGF), which enables the entire fusion of spatial information. SaViD achieves superior performance on the long-range detection Argoverse-2 (AV2) dataset with a performance improvement of 9.87% in AP value and an improvement of 2.39% in mAPH for L2 difficulties on the Waymo Open dataset (WOD). Comprehensive experiments are carried out to showcase its robustness against 14 natural sensor corruptions. SaViD exhibits a robust performance improvement of 31.43% for AV2 and 16.13% for WOD in RCE value compared to other existing fusion-based methods while considering all the corruptions for both datasets. Our code is available at href{https://anonymous.4open.science/r/SAVID-2A0D/README.m d}{textcolor{blue}{SaViD}}.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt23_04">
             15:30-15:35, Paper ThDT23.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2310'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              CRAB: Camera-Radar Fusion for Reducing Depth Ambiguity in Backward Projection Based View Transformation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#391965" title="Click to go to the Author Index">
             Lee, In-Jae
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#339600" title="Click to go to the Author Index">
             Hwang, Sihwan
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#253532" title="Click to go to the Author Index">
             Kim, Youngseok
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422009" title="Click to go to the Author Index">
             Kim, Wonjune
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#256712" title="Click to go to the Author Index">
             Kim, Sanmin
            </a>
           </td>
           <td class="r">
            Kookmin University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#254634" title="Click to go to the Author Index">
             Kum, Dongsuk
            </a>
           </td>
           <td class="r">
            KAIST
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2310" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_transportation" title="Click to go to the Keyword Index">
               Computer Vision for Transportation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Recently, camera-radar fusion-based 3D object detection methods in bird's eye view (BEV) have gained attention due to the complementary characteristics and cost-effectiveness of these sensors. Previous approaches using forward projection struggle with sparse BEV feature generation, while those employing backward projection overlook depth ambiguity, leading to false positives. In this paper, to address the aforementioned limitations, we propose a novel camera-radar fusion-based 3D object detection and segmentation model named CRAB (Camera-Radar fusion for reducing depth Ambiguity in Backward projection-based view transformation), using a backward projection that leverages radar to mitigate depth ambiguity. During the view transformation, CRAB aggregates perspective view image context features into BEV queries. It improves depth distinction among queries along the same ray by combining the dense but unreliable depth distribution from images with the sparse yet precise depth information from radar occupancy. We further introduce spatial cross-attention with a feature map containing radar context information to enhance the comprehension of the 3D scene. When evaluated on the nuScenes open dataset, our proposed approach achieves a state-of-the-art performance among backward projection-based camera-radar fusion methods with 62.4% NDS and 54.0% mAP in 3D object detection.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt23_05">
             15:35-15:40, Paper ThDT23.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2406'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Efficient 3D Perception on Multi-Sweep Point Cloud with Gumbel Spatial Pruning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#404009" title="Click to go to the Author Index">
             Li, Jianhao
            </a>
           </td>
           <td class="r">
            Beihang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#362426" title="Click to go to the Author Index">
             Sun, Tianyu
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#403997" title="Click to go to the Author Index">
             Zhang, Xueqian
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#404005" title="Click to go to the Author Index">
             Wang, Zhongdao
            </a>
           </td>
           <td class="r">
            Noah's Ark Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#404011" title="Click to go to the Author Index">
             Feng, Bailan
            </a>
           </td>
           <td class="r">
            Noah's Ark Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#405372" title="Click to go to the Author Index">
             Xu, Ke
            </a>
           </td>
           <td class="r">
            Beihang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2406" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_transportation" title="Click to go to the Keyword Index">
               Computer Vision for Transportation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper studies point cloud perception within outdoor environments. Existing methods face limitations in recognizing objects located at a distance or occluded, due to the sparse nature of outdoor point clouds. In this work, we observe a significant mitigation of this problem by accumulating multiple temporally consecutive LiDAR sweeps, resulting in a remarkable improvement in perception accuracy. However, the computation cost also increases, hindering previous approaches from utilizing a large number of LiDAR sweeps. To tackle this challenge, we find that a considerable portion of points in the accumulated point cloud is redundant, and discarding these points has minimal impact on perception accuracy. We introduce a simple yet effective Gumbel Spatial Pruning (GSP) layer that dynamically prunes points based on a learned end-to-end sampling. The GSP layer is decoupled from other network components and thus can be seamlessly integrated into existing point cloud network architectures. Extensive experiments show that our pruning strategy improves several perception algorithms in multiple tasks.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt23_06">
             15:40-15:45, Paper ThDT23.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2440'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              RoBiFusion: A Robust and Bidirectional Interaction Camera-LiDAR 3D Object Detection Framework
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423042" title="Click to go to the Author Index">
             Wen, Xubin
            </a>
           </td>
           <td class="r">
            Southeast University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422667" title="Click to go to the Author Index">
             Xia, Haifeng
            </a>
           </td>
           <td class="r">
            Southeast University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#313614" title="Click to go to the Author Index">
             Ding, Zhengming
            </a>
           </td>
           <td class="r">
            Tulane University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422414" title="Click to go to the Author Index">
             Xia, Siyu
            </a>
           </td>
           <td class="r">
            Southeast University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2440" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#intelligent_transportation_systems" title="Click to go to the Keyword Index">
               Intelligent Transportation Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#sensor_fusion" title="Click to go to the Keyword Index">
               Sensor Fusion
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Camera-LiDAR 3D object detection is currently becoming a crucial component in the field of autonomous driving perception. However, previous models only performed feature fusion in the deep-level BEV hierarchy when dealing with camera-LiDAR feature fusion. This approach lacks interaction with the shallow-level sensor features, which is beneficial in constructing the corresponding BEV features. However, a simple shallow-level feature interaction can introduce sensor noise caused by intrinsic and extrinsic camera calibration errors. To address this, we propose RoBiFusion, a novel camera-LiDAR 3D object detection framework designed for effective sensor feature interaction and mitigating sensor noise interference. This framework consists of three submodules: the Camera-LiDAR Feature Matching module, the LiDAR-to-Camera module, and the Camera-to-LiDAR module. Firstly, in the Camera-LiDAR Feature Matching module, we use the cross-attention module to dynamically match the camera features and the LiDAR features, which solves the problem of feature inconsistency caused by noise in the camera's intrinsic and extrinsic parameters. Secondly, in the LiDAR-to-Camera module, we propose a novel depth representation that can effectively mitigate LiDAR noise interference. Thirdly, in the Camera-to-LiDAR module, we introduce deformable attention to help LiDAR feature capture instance-level semantic features. Additionally, we design a novel differentiable and efficient grid sample module to accelerate the process since the bilinear grid sample module in deformable attention is time-consuming and not deployment-friendly. We compared RoBiFusion to the state-of-the-art BEVFusion on the nuScenes dataset and found that RoBiFusion surpasses BEVFusion by 1.5% mAP and 2.4% NDS. Furthermore, we designed a series of ablation experiments to verify the effectiveness of the aforementioned modules.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt23_07">
             15:45-15:50, Paper ThDT23.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2661'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Towards Accurate Semi-Supervised BEV 3D Object Detection with Depth-Aware Refinement and Denoising-Aided Alignment
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#413721" title="Click to go to the Author Index">
             Yang, Zhao
            </a>
           </td>
           <td class="r">
            Xi'an Jiaotong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#415918" title="Click to go to the Author Index">
             Shi, Yinan
            </a>
           </td>
           <td class="r">
            Technical University Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#340481" title="Click to go to the Author Index">
             Zhu, Jiangtong
            </a>
           </td>
           <td class="r">
            Xi'an Jiaotong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#415071" title="Click to go to the Author Index">
             Xu, Weixiang
            </a>
           </td>
           <td class="r">
            Institute of Automation, Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416118" title="Click to go to the Author Index">
             Liu, Longjun
            </a>
           </td>
           <td class="r">
            Xi'an Jiaotong University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2661" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Recently, camera-based Bird’s-Eye View (BEV) representation has gained significant traction in 3D object detection. However, training high-performance BEV 3D detectors typically requires a large number of annotated samples, which can be costly. Traditional semi-supervised methods for BEV 3D object detection face challenges including loss of rich depth information, inconsistent object representations across spaces, and unreliable pseudo label generation, leading to decreased accuracy and performance. Addressing this challenge, we pioneer the introduction of a semi-supervised BEV 3D object detection framework. Our approach leverages a small set of labeled data alongside a larger set of unlabeled data, significantly reducing annotation costs while maintaining robust detection performance. Firstly, we propose a depth-based self-refinement module to generate high-quality and stable pseudo labels, which can effectively regulate training with noisy labels. Secondly, we designed a denoising labels regression module that integrates denoising for both labeled and unlabeled data. Thirdly, in order to alleviate object inconsistency, we propose a consistent object-guided alignment method to ensure the consistency of objects in multi-spaces. Finally, our method can be easily plugged into various BEV 3D detection networks. Extensive experiments show that the proposed method achieves a new state-of-the-art compared to various camera-based 3D detectors tested on multiple public autonomous driving datasets.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thet1">
             <b>
              ThET1
             </b>
            </a>
           </td>
           <td class="r">
            302
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thet1" title="Click to go to the Program at a Glance">
             <b>
              Visual Perception and Learning
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#138554" title="Click to go to the Author Index">
             Zhang, Hao
            </a>
           </td>
           <td class="r">
            University of Massachusetts Amherst
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#415364" title="Click to go to the Author Index">
             Zhang, Jing
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet1_01">
             16:35-16:40, Paper ThET1.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('218'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Positioning in Congested Space by Combining Vision-Based and Proximity-Based Control
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#312850" title="Click to go to the Author Index">
             Thomas, John
            </a>
           </td>
           <td class="r">
            Institut Pascal
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101831" title="Click to go to the Author Index">
             Chaumette, Francois
            </a>
           </td>
           <td class="r">
            Inria Center at University of Rennes
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab218" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#sensor_based_control" title="Click to go to the Keyword Index">
               Sensor-based Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#visual_servoing" title="Click to go to the Keyword Index">
               Visual Servoing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper, we consider positioning in congested space within the framework of Sensor-based Control (SBC) using vision and proximity sensors. Vision acts as primary sensing modality for performing the positioning task, while proximity sensors complement it by ensuring that the robotic platform does not collide with objects in the workspace. Sensor information is combined in a shared manner using the QP formalism where ideas from safety-critical control are used to express inequality constraints. The proposed method is validated through various real experiments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet1_01">
             16:35-16:40, Paper ThET1.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3406'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Open-RGBT: Open-Vocabulary RGB-T Zero-Shot Semantic Segmentation in Open-World Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#412875" title="Click to go to the Author Index">
             Yu, Meng
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#205136" title="Click to go to the Author Index">
             Yue, Yufeng
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423314" title="Click to go to the Author Index">
             Yang, Luojie
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#313617" title="Click to go to the Author Index">
             He, Xunjie
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#128868" title="Click to go to the Author Index">
             Yang, Yi
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#128869" title="Click to go to the Author Index">
             Fu, Mengyin
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3406" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Semantic segmentation is a critical technique for effective scene understanding. Traditional RGB-T semantic segmentation models often struggle to generalize across diverse scenarios due to their reliance on pretrained models and predefined categories. Recent advancements in Visual Language Models (VLMs) have facilitated a shift from closed-set to open-vocabulary semantic segmentation methods. However, these models face challenges in dealing with intricate scenes, primarily due to the heterogeneity between RGB and thermal modalities. To address this gap, we present Open-RGBT, a novel open-vocabulary RGB-T semantic segmentation model. Specifically, we obtain instance-level detection proposals by incorporating visual prompts to enhance category understanding. Additionally, we employ the CLIP model to assess image-text similarity, which helps correct semantic consistency and mitigates ambiguities in category identification. Empirical evaluations demonstrate that Open-RGBT achieves superior performance in diverse and challenging real-world scenarios, even in the wild, significantly advancing the field of RGB-T semantic segmentation. The project page of Open-RGBT is available at https://OpenRGBT.github.io/.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet1_02">
             16:40-16:45, Paper ThET1.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('243'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SliceOcc: Indoor 3D Semantic Occupancy Prediction with Vertical Slice Representation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#312350" title="Click to go to the Author Index">
             Li, Jianing
            </a>
           </td>
           <td class="r">
            Nanjing University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#378097" title="Click to go to the Author Index">
             Lu, Ming
            </a>
           </td>
           <td class="r">
            Intel Labs
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#451419" title="Click to go to the Author Index">
             Liu, Juntao
            </a>
           </td>
           <td class="r">
            China Mobile Research Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#392582" title="Click to go to the Author Index">
             Wang, Hao
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#414572" title="Click to go to the Author Index">
             Gu, Chenyang
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#249923" title="Click to go to the Author Index">
             Zheng, Wenzhao
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#312353" title="Click to go to the Author Index">
             Du, Li
            </a>
           </td>
           <td class="r">
            Nanjing University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#355359" title="Click to go to the Author Index">
             Zhang, Shanghang
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab243" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_manufacturing" title="Click to go to the Keyword Index">
               Computer Vision for Manufacturing
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             3D semantic occupancy prediction is a crucial task in visual perception, demanding a simultaneous understanding of both scene geometry and semantics. It plays a pivotal role in 3D scene comprehension and holds great potential for various applications, such as robotic vision perception and autonomous driving. Many previous works leverage planar-based representations like Bird’s Eye View (BEV) and Tri-Perspective View (TPV), which aim to simplify the complexity of 3D scenes while preserving essential object information, thereby facilitating efficient scene representation. However, in dense indoor environments where occlusions are prevalent, directly applying these planar-based methods often leads to difficulties in capturing global semantic occupancy, ultimately degrading model performance. In this paper, we introduce a novel vertical slice representation, which divides the scene along the vertical axis and projects spatial point features onto the nearest pair of parallel planes. To harness these slice features, we propose SliceOcc, a camera-based model specifically tailored for indoor 3D semantic occupancy prediction. SliceOcc utilizes pairs of slice queries and cross-attention mechanisms to extract planar features from input images. These local planar features are then combined to form a global scene representation, which is employed for indoor occupancy estimation. Experimental results on the EmbodiedScan dataset demonstrate that SliceOcc achieves a mIoU of 15.45% across 81 indoor categories, setting a new state-of-the-art performance among RGB-based models for indoor 3D semantic occupancy prediction.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet1_03">
             16:45-16:50, Paper ThET1.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('416'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Bandwidth-Adaptive Spatiotemporal Correspondence Identification for Collaborative Perception
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#227860" title="Click to go to the Author Index">
             Gao, Peng
            </a>
           </td>
           <td class="r">
            North Carolina State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#342736" title="Click to go to the Author Index">
             Jose, Williard Joshua
            </a>
           </td>
           <td class="r">
            University of Massachusetts Amherst
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#138554" title="Click to go to the Author Index">
             Zhang, Hao
            </a>
           </td>
           <td class="r">
            University of Massachusetts Amherst
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab416" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Correspondence identification (CoID) is an essential capability for multi-robot collaborative perception, which allows a group of robots to consistently refer to the same objects in their own fields of view. In real-world applications, such as connected autonomous driving, connected vehicles cannot directly share their raw observations due to the limited communication bandwidth. To address this challenge, we propose a novel approach of bandwidth-adaptive spatiotemporal CoID for collaborative perception, where robots interactively select partial spatiotemporal observations to share with others, while adapting to the communication constraint that dynamically changes over time. We evaluate our approach over various scenarios in connected autonomous driving simulations. Experimental results have demonstrated that our approach enables CoID and adapts to the dynamic change of bandwidth constraints. In addition, our approach achieves 8%-56% overall improvements in terms of covisible object retrieval for CoID and data sharing efficiency, which outperforms the previous techniques and achieves the state-of-the-art performance. More information is available at: https://gaopeng5.github.io/acoid/.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet1_04">
             16:50-16:55, Paper ThET1.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('607'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Polyp-Gen: Realistic and Diverse Polyp Image Generation for Endoscopic Dataset Expansion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#418254" title="Click to go to the Author Index">
             Liu, Shengyuan
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#286502" title="Click to go to the Author Index">
             Chen, Zhen
            </a>
           </td>
           <td class="r">
            Centre for Artificial Intelligence and Robotics (CAIR), Hong Kon
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#415161" title="Click to go to the Author Index">
             Yang, Qiushi
            </a>
           </td>
           <td class="r">
            City University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419038" title="Click to go to the Author Index">
             Yu, Weihao
            </a>
           </td>
           <td class="r">
            Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419040" title="Click to go to the Author Index">
             Dong, Di
            </a>
           </td>
           <td class="r">
            Institute of Automation, Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421016" title="Click to go to the Author Index">
             Hu, Jiancong
            </a>
           </td>
           <td class="r">
            The Sixth Affiliated Hospital, Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#168158" title="Click to go to the Author Index">
             Yuan, Yixuan
            </a>
           </td>
           <td class="r">
            Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab607" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Automated diagnostic systems (ADS) have shown significant potential in the early detection of polyps during endoscopic examinations, thereby reducing the incidence of colorectal cancer. However, due to high annotation costs and strict privacy concerns, acquiring high-quality endoscopic images poses a considerable challenge in the development of ADS. Despite recent advancements in generating synthetic images for dataset expansion, existing endoscopic image generation algorithms failed to accurately generate the details of polyp boundary regions and typically required medical priors to specify plausible locations and shapes of polyps, which limited the realism and diversity of the generated images. To address these limitations, we present Polyp-Gen, the first full-automatic diffusion-based endoscopic image generation framework. Specifically, we devise a spatial-aware diffusion training scheme with a lesion-guided loss to enhance the structural context of polyp boundary regions. Moreover, to capture medical priors for the localization of potential polyp areas, we introduce a hierarchical retrieval-based sampling strategy to match similar fine-grained spatial features. In this way, our Polyp-Gen can generate realistic and diverse endoscopic images for building reliable ADS. Extensive experiments demonstrate the state-of-the-art generation quality and the synthetic images can improve the downstream polyp detection task. Additionally, our Polyp-Gen has shown remarkable zero-shot generalizability on other datasets. The source code is available at https://github.com/CUHK-AIM-Group/Polyp-Gen.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet1_05">
             16:55-17:00, Paper ThET1.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('803'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              DetailRefine: Towards Fine-Grained and Efficient Online Monocular 3D Reconstruction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#414104" title="Click to go to the Author Index">
             Chu, Fupeng
            </a>
           </td>
           <td class="r">
            Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#131466" title="Click to go to the Author Index">
             Cong, Yang
            </a>
           </td>
           <td class="r">
            Chinese Academy of Science, China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#368635" title="Click to go to the Author Index">
             Chen, Ronghan
            </a>
           </td>
           <td class="r">
            Sheyang Institute of Automation, Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab803" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Online monocular 3D reconstruction has attracted widespread attention as it promotes the application of robots in interactive scenarios. Most existing methods focus on 1) real-time reconstruction, 2) accurate voxel featuring learning, and 3) effective voxel sparsification algorithm. To this end, 1) they adopt a coarse-to-fine pipeline, where all non-empty voxels are sent to the next level for refinement. However, this results in over-refinement of flat regions, leading to unnecessary computational overhead. Furthermore, 2) advanced methods focus on exploring view visibility but overlook the discriminability among visible views, which limits the representation of learned voxel features. Moreover, 3) existing sparsification algorithms struggle to distinguish detailed and empty voxels, resulting in either the loss of detailed voxels or the retention of empty voxels. To tackle these challenges, 1) we present Dynamic Detail Refinement (DDR) to allocate more voxels to detailed regions for refinement, which could alleviate the computational burden. Furthermore, 2) we propose Discriminability-Aware Fusion (DAF) to focus on discriminative views, which helps to capture accurate voxel features. In addition, 3) we propose Hierarchical Hybrid Sparsification (HHS) to balance global completeness and local refinement, which helps to preserve detailed voxels at hierarchical levels effectively. Extensive experiments conducted on the representative ScanNet (V2) and 7-Scenes datasets demonstrate the superiority of the proposed method.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet1_07">
             17:05-17:10, Paper ThET1.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4264'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              DAP-LED: Learning Degradation-Aware Priors with CLIP for Joint Low-Light Enhancement and Deblurring
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422385" title="Click to go to the Author Index">
             Wang, Ling
            </a>
           </td>
           <td class="r">
            HKUST(GZ)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422043" title="Click to go to the Author Index">
             Wu, Chen
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#256235" title="Click to go to the Author Index">
             Wang, Lin
            </a>
           </td>
           <td class="r">
            Nanyang Technological University (NTU)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4264" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Autonomous vehicles and robots often struggle with reliable visual perception at night due to the low illumination and motion blur caused by the long exposure time of RGB cameras. Existing methods address this challenge by sequentially connecting the off-the-shelf pretrained low-light enhancement and deblurring models. Unfortunately, these methods often lead to noticeable artifacts (eg., color distortions) in the over-exposed regions or make it hardly possible to learn the motion cues of the dark regions. In this paper, we interestingly find vision-language models, eg., Contrastive Language-Image Pretraining (CLIP), can comprehensively perceive diverse degradation levels at night. In light of this, we propose a novel transformer-based joint learning framework, named DAP-LED, which can jointly achieve low-light enhancement and deblurring, benefiting downstream tasks, such as depth estimation, segmentation, and detection in the dark. The key insight is to leverage CLIP to adaptively learn the degradation levels from images at night. This subtly enables learning rich semantic information and visual representation for optimization of the joint tasks. To achieve this, we first introduce a CLIP-guided cross-fusion module to obtain multi-scale patch-wise degradation heatmaps from the image embeddings. Then, the heatmaps are fused via the designed CLIP-enhanced transformer blocks to retain useful degradation information for effective model optimization. Experimental results show that, compared to existing methods, our DAP-LED achieves state-of-the-art performance in the dark. Meanwhile, the enhanced results are demonstrated to be effective for three downstream tasks.	For demo and more results, please check the project page: url{https://vlislab22.github.io/dap-led/}.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet1_08">
             17:10-17:15, Paper ThET1.8
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4528'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              FusionSense: Bridging Common Sense, Vision, and Touch for Robust Sparse-View Reconstruction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#378542" title="Click to go to the Author Index">
             Fang, Irving
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426880" title="Click to go to the Author Index">
             Shi, Kairui
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426873" title="Click to go to the Author Index">
             He, Xujin
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#314055" title="Click to go to the Author Index">
             Tan, Siqi
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#378176" title="Click to go to the Author Index">
             Wang, Yifan
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#341569" title="Click to go to the Author Index">
             Zhao, Hanwen
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#220233" title="Click to go to the Author Index">
             Huang, Hung-Jui
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#172670" title="Click to go to the Author Index">
             Yuan, Wenzhen
            </a>
           </td>
           <td class="r">
            University of Illinois
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#160443" title="Click to go to the Author Index">
             Feng, Chen
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#415364" title="Click to go to the Author Index">
             Zhang, Jing
            </a>
           </td>
           <td class="r">
            NYU
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4528" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#force_and_tactile_sensing" title="Click to go to the Keyword Index">
               Force and Tactile Sensing
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Humans effortlessly integrate common-sense knowledge with sensory input from vision and touch to understand their surroundings. Emulating this capability, we introduce FusionSense, a novel 3D reconstruction framework that enables robots to fuse priors from foundation models with highly sparse observations from vision and tactile sensors. FusionSense addresses three key challenges: (i) How can robots efficiently acquire robust global shape information about the surrounding scene and objects? (ii) How can robots strategically select touch points on the object using geometric and common-sense priors? (iii) How can partial observations such as tactile signals improve the overall representation of the object? Our framework employs 3D Gaussian Splatting as a core representation and incorporates a hierarchical optimization strategy involving global structure construction, object visual hull pruning and local geometric constraints. This advancement results in fast and robust perception in environments with traditionally challenging objects that are transparent, reflective, or dark, enabling more downstream manipulation or navigation tasks. Experiments on real-world data suggest that our framework outperforms previously state-of-the-art sparse-view methods. All code and data are open-sourced on the project website.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thet2">
             <b>
              ThET2
             </b>
            </a>
           </td>
           <td class="r">
            301
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thet2" title="Click to go to the Program at a Glance">
             <b>
              Multi-Robot SLAM and Mapping
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet2_01">
             16:35-16:40, Paper ThET2.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('260'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Multi-Robot Object SLAM Using Distributed Variational Inference
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#297783" title="Click to go to the Author Index">
             Cao, Hanwen
            </a>
           </td>
           <td class="r">
            University of California, San Diego
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#330597" title="Click to go to the Author Index">
             Shreedharan, Sriram
            </a>
           </td>
           <td class="r">
            University of California, San Diego
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#149893" title="Click to go to the Author Index">
             Atanasov, Nikolay
            </a>
           </td>
           <td class="r">
            University of California, San Diego
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab260" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_slam" title="Click to go to the Keyword Index">
               Multi-Robot SLAM
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#distributed_robot_systems" title="Click to go to the Keyword Index">
               Distributed Robot Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#probability_and_statistical_methods" title="Click to go to the Keyword Index">
               Probability and Statistical Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Multi-robot simultaneous localization and mapping (SLAM) enables a robot team to achieve coordinated tasks by relying on a common map of the environment. Constructing a map by centralized processing of the robot observations is undesirable because it creates a single point of failure and requires pre-existing infrastructure and significant communication throughput. This paper formulates multi-robot object SLAM as a variational inference problem over a communication graph subject to consensus constraints on the object estimates maintained by different robots. To solve the problem, we develop a distributed mirror descent algorithm with regularization enforcing consensus among the communicating robots. Using Gaussian distributions in the algorithm, we also derive a distributed multi-state constraint Kalman filter (MSCKF) for multi-robot object SLAM. Experiments on real and simulated data show that our method improves the trajectory and object estimates, compared to individual-robot SLAM, while achieving better scaling to large robot teams, compared to centralized multi-robot SLAM.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet2_02">
             16:40-16:45, Paper ThET2.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1566'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              DVM-SLAM: Decentralized Visual Monocular Simultaneous Localization and Mapping for Multi-Agent Systems
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421746" title="Click to go to the Author Index">
             Bird, Joshua
            </a>
           </td>
           <td class="r">
            University of Cambridge
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#267264" title="Click to go to the Author Index">
             Blumenkamp, Jan
            </a>
           </td>
           <td class="r">
            University of Cambrdige
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#124025" title="Click to go to the Author Index">
             Prorok, Amanda
            </a>
           </td>
           <td class="r">
            University of Cambridge
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1566" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_slam" title="Click to go to the Keyword Index">
               Multi-Robot SLAM
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Cooperative Simultaneous Localization and Mapping (C-SLAM) enables multiple agents to work together in mapping unknown environments while simultaneously estimating their own positions. This approach enhances robustness, scalability, and accuracy by sharing information between agents, reducing drift, and enabling collective exploration of larger areas. In this paper, we present Decentralized Visual Monocular SLAM (DVM-SLAM), the first open-source decentralized monocular C-SLAM system. By only utilizing low-cost and light-weight monocular vision sensors, our system is well suited for small robots and micro aerial vehicles (MAVs). DVM-SLAM's real-world applicability is validated on physical robots with a custom collision avoidance framework, showcasing its potential in real-time multi-agent autonomous navigation scenarios. We also demonstrate comparable accuracy to state-of-the-art centralized monocular C-SLAM systems. We open-source our code and provide supplementary material online.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet2_03">
             16:45-16:50, Paper ThET2.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1696'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              TCAFF: Temporal Consistency for Robot Frame Alignment
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#354142" title="Click to go to the Author Index">
             Peterson, Mason B.
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#241322" title="Click to go to the Author Index">
             Lusk, Parker C.
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421931" title="Click to go to the Author Index">
             Avila, Antonio
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104610" title="Click to go to the Author Index">
             How, Jonathan
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1696" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_slam" title="Click to go to the Keyword Index">
               Multi-Robot SLAM
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In the field of collaborative robotics, the ability to communicate spatial information like planned trajectories and shared environment information is crucial. When no global position information is available (e.g., indoor or GPS-denied environments), agents must align their coordinate frames before shared spatial information can be properly expressed and interpreted. Coordinate frame alignment is particularly difficult when robots have no initial alignment and are affected by odometry drift. To this end, we develop a novel multiple hypothesis algorithm, called TCAFF, for aligning the coordinate frames of neighboring robots. TCAFF considers potential alignments from associating sparse open-set object maps and leverages temporal consistency to determine an initial alignment and correct for drift, all without any initial knowledge of neighboring robot poses. We demonstrate TCAFF being used for frame alignment in a collaborative object tracking application on a team of four robots tracking six pedestrians and show that TCAFF enables robots to achieve a tracking accuracy similar to that of a system with ground truth localization. The code and hardware dataset are available at https://github.com/mit-acl/tcaff.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet2_04">
             16:50-16:55, Paper ThET2.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1971'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Effective Heterogeneous Point Cloud-Based Place Recognition and Relative Localization for Ground and Aerial Vehicles
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#373693" title="Click to go to the Author Index">
             Mao, Rui
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#169114" title="Click to go to the Author Index">
             Cheng, Hui
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1971" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#range_sensing" title="Click to go to the Keyword Index">
               Range Sensing
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_slam" title="Click to go to the Keyword Index">
               Multi-Robot SLAM
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Place recognition and relative localization are crucial for realizing the potential of collaboration in ground and aerial robot teams. Many existing works focus only on ground robots and are not well-suited for heterogeneous robot systems in large-scale environments. In this paper, we propose a novel pipeline based on BEV density image, combined with an enhanced data structure, for place recognition in air-ground robotic collaboration systems. An efficient height alignment algorithm is proposed for relative localization. Extensive experiments on various types of public datasets validate the efficacy of our method compared to other SOTA works. We also show that our method is capable to detect inter- and intra-robot loop closures in a ground and aerial multi-session SLAM system.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet2_05">
             16:55-17:00, Paper ThET2.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2215'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Distributed Invariant Kalman Filter for Object-Level Multi-Robot Pose SLAM
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#388793" title="Click to go to the Author Index">
             Li, Haoying
            </a>
           </td>
           <td class="r">
            Chinese University of Hong Kong, Shenzhen
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#388795" title="Click to go to the Author Index">
             Zeng, Qingcheng
            </a>
           </td>
           <td class="r">
            The Hong Kong University of Science and Technology (Guangzhou)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423809" title="Click to go to the Author Index">
             Li, Haoran
            </a>
           </td>
           <td class="r">
            Chinese University of Hong Kong, Shenzhen
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423761" title="Click to go to the Author Index">
             Zhang, Yanglin
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong, Shenzhen
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#246757" title="Click to go to the Author Index">
             Wu, Junfeng
            </a>
           </td>
           <td class="r">
            The Chinese Unviersity of Hong Kong, Shenzhen
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2215" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#distributed_robot_systems" title="Click to go to the Keyword Index">
               Distributed Robot Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_slam" title="Click to go to the Keyword Index">
               Multi-Robot SLAM
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#autonomous_agents" title="Click to go to the Keyword Index">
               Autonomous Agents
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Cooperative localization and target tracking are essential for multi-robot systems to implement high-level tasks. To this end, we propose a distributed invariant Kalman filter~(KF) based on covariance intersection~(CI) for effective multi-robot pose estimation. The paper utilizes the object-level measurement models, which have condensed information further reducing the communication burden. Besides, by modeling states on special Lie groups, and representing uncertainty in corresponding Lie algebras, better linearity and consistency are obtained under the invariant KF framework. We also use a combination of CI and KF to avoid overly confident or conservative estimates in multi-robot systems with intricate and unknown correlations, and some level of robot degradation is acceptable through multi-robot collaboration. The simulation and real data experiment validate the practicability and superiority of the proposed algorithm.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet2_06">
             17:00-17:05, Paper ThET2.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3574'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              MT-PCR: Leveraging Modality Transformation for Large-Scale Point Cloud Registration with Limited Overlap
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#405810" title="Click to go to the Author Index">
             Wu, Yilong
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#231281" title="Click to go to the Author Index">
             Duan, Yifan
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424327" title="Click to go to the Author Index">
             Chen, Yuxi
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#376912" title="Click to go to the Author Index">
             Zhang, Xinran
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417277" title="Click to go to the Author Index">
             Shen, Yedong
            </a>
           </td>
           <td class="r">
            University of Science &amp; Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#148006" title="Click to go to the Author Index">
             Ji, Jianmin
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#291452" title="Click to go to the Author Index">
             Zhang, Yanyong
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#366042" title="Click to go to the Author Index">
             Zhang, Lu
            </a>
           </td>
           <td class="r">
            Institute of Artificial Intelligence, Hefei Comprehensive Nation
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3574" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_slam" title="Click to go to the Keyword Index">
               Multi-Robot SLAM
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__perception_and_autonomy" title="Click to go to the Keyword Index">
               Aerial Systems: Perception and Autonomy
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Large-scale scene point cloud registration with limited overlap is a challenging task due to computational load and constrained data acquisition. To tackle these issues, we propose a point cloud registration method, MT-PCR, based on Modality Transformation. MT-PCR leverages a Bird’s Eye View (BEV) capturing the maximal overlap information to improve the accuracy and utilizes images to provide complementary spatial features. Specifically, MT-PCR converts 3D point clouds to BEV images and estimates correspondence by 2D image keypoints extraction and matching. Subsequently, the 2D correspondence estimates are then transformed back to 3D point clouds using inverse mapping. We have applied MT-PCR to Terrestrial Laser Scanning (TLS) and Aerial Laser Scanning (ALS) point cloud registration on the GrAco dataset, involving 8 low-overlap, square-kilometer scale registration scenarios. Experiments and comparisons with commonly used methods demonstrate that MT-PCR can achieve superior accuracy and robustness in large-scale scenes with limited overlap.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet2_07">
             17:05-17:10, Paper ThET2.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4931'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Large-Scale Multi-Session Point-Cloud Map Merging
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#312661" title="Click to go to the Author Index">
             Wei, Hairuo
            </a>
           </td>
           <td class="r">
            The University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#370554" title="Click to go to the Author Index">
             Li, Rundong
            </a>
           </td>
           <td class="r">
            University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#283472" title="Click to go to the Author Index">
             Cai, Yixi
            </a>
           </td>
           <td class="r">
            KTH Royal Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#296567" title="Click to go to the Author Index">
             Yuan, Chongjian
            </a>
           </td>
           <td class="r">
            The University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#292033" title="Click to go to the Author Index">
             Ren, Yunfan
            </a>
           </td>
           <td class="r">
            The University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#338792" title="Click to go to the Author Index">
             Zou, Zuhao
            </a>
           </td>
           <td class="r">
            HongKong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#276992" title="Click to go to the Author Index">
             Wu, Huajie
            </a>
           </td>
           <td class="r">
            Hong Kong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#288528" title="Click to go to the Author Index">
             Zheng, Chunran
            </a>
           </td>
           <td class="r">
            The University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#352741" title="Click to go to the Author Index">
             Zhou, Shunbo
            </a>
           </td>
           <td class="r">
            Huawei
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#218816" title="Click to go to the Author Index">
             Xue, Kaiwen
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong, Shenzhen
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#204381" title="Click to go to the Author Index">
             Zhang, Fu
            </a>
           </td>
           <td class="r">
            University of Hong Kong
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4931" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_slam" title="Click to go to the Keyword Index">
               Multi-Robot SLAM
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper introduces LAMM, an open-source framework for large-scale multi-session 3D LiDAR point cloud map merging. LAMM can automatically integrate sub-maps from multiple agents carrying LiDARs with different scanning patterns, facilitating place feature extraction, data association, and global optimization in various environments. Our framework incorporates two key novelties that enable robust, accurate, large-scale map merging. The first novelty is a temporal bidirectional filtering mechanism that removes dynamic objects from 3D LiDAR point cloud data. This eliminates the effect of dynamic objects on the 3D map
             <p>
              model, providing higher-quality map merging results. The second novelty
              <p>
               is a robust and efficient outlier removal algorithm for detected loop closures. This algorithm ensures a high recall rate and a low false alarm rate in position retrieval, significantly reducing outliers in repetitive environments during large-scale merging. We evaluate our framework using various datasets, including KITTI, H
              </p>
             </p>
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thet3">
             <b>
              ThET3
             </b>
            </a>
           </td>
           <td class="r">
            303
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thet3" title="Click to go to the Program at a Glance">
             <b>
              Robotics and Automation in Life Science and Rescue Applications
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#303525" title="Click to go to the Author Index">
             Kaiser, Tanja Katharina
            </a>
           </td>
           <td class="r">
            University of Technology Nuremberg
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#107129" title="Click to go to the Author Index">
             Alterovitz, Ron
            </a>
           </td>
           <td class="r">
            University of North Carolina at Chapel Hill
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet3_01">
             16:35-16:40, Paper ThET3.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('998'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              The qPCRBot: Combining Automated Data Handling, Standardization, and Robotic Labware Transport for Better qPCR Measurements
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#319789" title="Click to go to the Author Index">
             Zwirnmann, Henning
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#309926" title="Click to go to the Author Index">
             Eckhoff, Moritz
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#319102" title="Click to go to the Author Index">
             Knobbe, Dennis
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#418653" title="Click to go to the Author Index">
             Fülöp, Dorian
            </a>
           </td>
           <td class="r">
            Technical University of Munich (TUM)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#380389" title="Click to go to the Author Index">
             Gabrielli, Andrea
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#108317" title="Click to go to the Author Index">
             Haddadin, Sami
            </a>
           </td>
           <td class="r">
            Mohamed Bin Zayed University of Artificial Intelligence
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab998" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#robotics_and_automation_in_life_sciences" title="Click to go to the Keyword Index">
               Robotics and Automation in Life Sciences
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#software_architecture_for_robotic_and_automation" title="Click to go to the Keyword Index">
               Software Architecture for Robotic and Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#biological_cell_manipulation" title="Click to go to the Keyword Index">
               Biological Cell Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Laboratory automation is a key driver for higher efficiency and reproducibility of experiments and measurements in natural science laboratories. One process that is particularly susceptible to both manual errors in the physical handling of labware, faulty data analyses, and incomplete reporting is the quantitative Polymerase Chain Reaction (qPCR). It is a ubiquitous analysis method in biolaboratories to amplify and measure the amount of a specific DNA sequence in a sample. Our system, which we call the
             <i>
              qPCRBot
             </i>
             , addresses these issues through three key pillars: automating data analysis and handling processes, standardizing data management and system communication protocols, and utilizing a robotic manipulator for labware transport. To achieve this, we developed a SiLA 2-based client-server architecture for unified and standardized access to both the qPCR device and the robot. For the manipulator, we implemented a Cartesian motion generator to ensure proper labware transport. We transform all experiment data to a standardized, XML-based format and integrate a widely-used Laboratory Information Management System for its storage. These developments collectively enable streamlined qPCR measurements without human interaction, thus enhancing both efficiency and reproducibility.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet3_02">
             16:40-16:45, Paper ThET3.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1086'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Distributed Pursuit of an Evader with Adaptive Robust Path Control under State Measurement Uncertainty
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#380569" title="Click to go to the Author Index">
             Rao, Kai
            </a>
           </td>
           <td class="r">
            East China University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#317419" title="Click to go to the Author Index">
             Yan, Huaicheng
            </a>
           </td>
           <td class="r">
            East China University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419406" title="Click to go to the Author Index">
             Huang, Zhihao
            </a>
           </td>
           <td class="r">
            East China University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419403" title="Click to go to the Author Index">
             Yang, Penghui
            </a>
           </td>
           <td class="r">
            East China University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#380567" title="Click to go to the Author Index">
             Lv, Yunkai
            </a>
           </td>
           <td class="r">
            East China University of Science and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1086" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#surveillance_robotic_systems" title="Click to go to the Keyword Index">
               Surveillance Robotic Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#search_and_rescue_robots" title="Click to go to the Keyword Index">
               Search and Rescue Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a distributed pursuit framework for environments with obstacles considering state measurement uncertainty. Our framework consists of two primary components: the computation of safe pursuit regions based on Voronoi cell (VC) and the solution of an adaptive robust path controller based on Control Barrier Function (CBF). Initially, the chance constrained obstacle-aware Voronoi cell (CCOVC) for each pursuer is constructed by calculating separation hyperplane and buffer terms. Subsequently, we formulate chance CBF and chance Control Lyapunov Function (CLF) constraints, using convex approximation to determine their upper bounds. We then find the adaptive robust path controller by solving a Quadratically Constrained Quadratic Program (QCQP). The advantage of this framework lies in its capability to adaptively compute the path controller and ensure robust collision avoidance among pursuers and with obstacles. Simulation and experimental results demonstrate the effectiveness and robustness of the proposed framework.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet3_03">
             16:45-16:50, Paper ThET3.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2618'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Multimodal Behaviour Trees for Robotic Laboratory Task Automation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#191516" title="Click to go to the Author Index">
             Fakhruldeen, Hatem
            </a>
           </td>
           <td class="r">
            University of Liverpool
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424511" title="Click to go to the Author Index">
             Raveendran Nambiar, Arvind
            </a>
           </td>
           <td class="r">
            University of Liverpool
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#308025" title="Click to go to the Author Index">
             Veeramani, Satheeshkumar
            </a>
           </td>
           <td class="r">
            University of Liverpool
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424522" title="Click to go to the Author Index">
             Tailor, Bonilkumar Vijaykumar
            </a>
           </td>
           <td class="r">
            University of Liverpool
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#386604" title="Click to go to the Author Index">
             Beyzaee Juneghani, Hadi
            </a>
           </td>
           <td class="r">
            University of Liverpool
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#232637" title="Click to go to the Author Index">
             Pizzuto, Gabriella
            </a>
           </td>
           <td class="r">
            University of Liverpool
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#312923" title="Click to go to the Author Index">
             Cooper, Andrew Ian
            </a>
           </td>
           <td class="r">
            University of Liverpool
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2618" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#robotics_and_automation_in_life_sciences" title="Click to go to the Keyword Index">
               Robotics and Automation in Life Sciences
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Laboratory robotics offer the capability to conduct experiments with a high degree of precision and reproducibility, with the potential to transform scientific research. Trivial and repeatable tasks; e.g., sample transportation for analysis and vial capping are well-suited for robots; if done successfully and reliably, chemists could contribute their efforts towards more critical research activities. Currently, robots can perform these tasks faster than chemists, but how reliable are they? Improper capping could result in human exposure to toxic chemicals which could be fatal. To ensure that robots perform these tasks as accurately as humans, sensory feedback is required to assess the progress of task execution. To address this, we propose a novel methodology based on behaviour trees with multimodal perception. Along with automating robotic tasks, this methodology also verifies the successful execution of the task, a fundamental requirement in safety-critical environments. The experimental evaluation was conducted on two lab tasks: sample vial capping and laboratory rack insertion. The results show high success rate, i.e., 88% for capping and 92% for insertion, along with strong error detection capabilities. This ultimately proves the robustness and reliability of our approach and that using multimodal behaviour trees should pave the way towards the next generation of robotic chemists.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet3_04">
             16:50-16:55, Paper ThET3.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2939'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Hierarchical Graph-Based Terrain-Aware Autonomous Navigation Approach for Complementary Multimodal Ground-Aerial Exploration
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#313126" title="Click to go to the Author Index">
             Patel, Akash
            </a>
           </td>
           <td class="r">
            Luleå University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#340572" title="Click to go to the Author Index">
             Valdes Saucedo, Mario Alberto
            </a>
           </td>
           <td class="r">
            Lulea University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#325063" title="Click to go to the Author Index">
             Stathoulopoulos, Nikolaos
            </a>
           </td>
           <td class="r">
            Luleå University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#277573" title="Click to go to the Author Index">
             Sankaranarayanan, Viswa Narayanan
            </a>
           </td>
           <td class="r">
            Lulea University of Techonology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#310358" title="Click to go to the Author Index">
             Tevetzidis, Ilias
            </a>
           </td>
           <td class="r">
            Luleå University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#199367" title="Click to go to the Author Index">
             Kanellakis, Christoforos
            </a>
           </td>
           <td class="r">
            LTU
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#105882" title="Click to go to the Author Index">
             Nikolakopoulos, George
            </a>
           </td>
           <td class="r">
            Luleå University of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2939" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#search_and_rescue_robots" title="Click to go to the Keyword Index">
               Search and Rescue Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#field_robots" title="Click to go to the Keyword Index">
               Field Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#cooperating_robots" title="Click to go to the Keyword Index">
               Cooperating Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Autonomous navigation in unknown environments is a fundamental challenge in robotics, particularly in coordinating ground and aerial robots to maximize exploration efficiency. This paper presents a novel approach that utilizes a hierarchical graph to represent the environment, encoding both geometric and semantic traversability. The framework enables the robots to compute a shared confidence metric, which helps the ground robot assess terrain and determine when deploying the aerial robot will extend exploration. The robot's confidence in traversing a path is based on factors such as predicted volumetric gain, path traversability, and collision risk. A hierarchy of graphs is used to maintain an efficient representation of traversability and frontier information through multi-resolution maps. Evaluated in a real subterranean exploration scenario, the approach allows the ground robot to autonomously identify zones that are no longer traversable but suitable for aerial deployment. By leveraging this hierarchical structure, the ground robot can selectively share graph information on confidence-assessed frontier targets from parts of the scene, enabling the aerial robot to navigate beyond obstacles and continue exploration.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet3_05">
             16:55-17:00, Paper ThET3.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2952'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Introducing Collaborative Robots As a First Step towards Autonomous Reprocessing of Medical Equipment
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#288650" title="Click to go to the Author Index">
             Voigt, Florian
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#239366" title="Click to go to the Author Index">
             Naceri, Abdeldjallil
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#108317" title="Click to go to the Author Index">
             Haddadin, Sami
            </a>
           </td>
           <td class="r">
            Mohamed Bin Zayed University of Artificial Intelligence
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2952" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#robotics_and_automation_in_life_sciences" title="Click to go to the Keyword Index">
               Robotics and Automation in Life Sciences
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#bimanual_manipulation" title="Click to go to the Keyword Index">
               Bimanual Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Ensuring the sterility of medical equipment, particularly endoscopes used in environments teeming with diverse pathogens and drug-resistant bacteria, is crucial for safe medical procedures. However, the complexity of endoscope reprocessing, which involves numerous dexterous manual manipulations, poses significant challenges. Achieving certification for sterilization requires precise, repetitive execution with strict tolerances. In this study, we propose a framework that automates the handling and storage of endoscopes right after the sterilization process and employs compliant collaborative robots to address these dexterous manipulation challenges. In the first stage, we identified the key manipulation skills involved in the process through observations and feedback from medical personnel. In the second stage, we proposed a system that employs a high-level action planner to orchestrate the removal and storage of endoscopes, integrating two collaborative robots and a linear unit. Through real-time force measurements, compliant control, task knowledge, and safety protocols, we establish a system that ensures the safety of both medical equipment and personnel in proximity. In our first experiment, we conducted 50 trials with a 100% reliability rate. Each trial had an execution time of 102 seconds, with a variance of 1.2 seconds. In our second experiment, we performed 10 trials with a human obstructing the transfer path, facing away from the robot. In all cases, the system successfully and promptly detected the collision. This work pioneers the automation of medical reprocessing in sterile environments using tactile robots and addresses the associated challenges.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet3_06">
             17:00-17:05, Paper ThET3.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3115'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              CloudTrack: Scalable UAV Tracking with Cloud Semantics
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#378083" title="Click to go to the Author Index">
             Blei, Yannik
            </a>
           </td>
           <td class="r">
            University of Technology Nuremberg
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#210949" title="Click to go to the Author Index">
             Krawez, Michael
            </a>
           </td>
           <td class="r">
            University of Technology Nuremberg
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424692" title="Click to go to the Author Index">
             Nilavadi, Nisarga
            </a>
           </td>
           <td class="r">
            University of Technology Nuremberg
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#303525" title="Click to go to the Author Index">
             Kaiser, Tanja Katharina
            </a>
           </td>
           <td class="r">
            University of Technology Nuremberg
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101785" title="Click to go to the Author Index">
             Burgard, Wolfram
            </a>
           </td>
           <td class="r">
            University of Technology Nuremberg
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3115" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#search_and_rescue_robots" title="Click to go to the Keyword Index">
               Search and Rescue Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#human_detection_and_tracking" title="Click to go to the Keyword Index">
               Human Detection and Tracking
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Nowadays, unmanned aerial vehicles (UAVs) are commonly used in search and rescue scenarios to gather information in the search area. The automatic identification of the person searched for in aerial footage could increase the autonomy of such systems, reduce the search time, and thus increase the missed person’s chances of survival. In this paper, we present a novel approach to perform semantically conditioned open vocabulary object tracking that is specifically designed to cope with the limitations of UAV hardware. Our approach has several advantages: It can run with verbal descriptions of the missing person, e.g., the color of the shirt, it does not require dedicated training to execute the mission, and can efficiently track a potentially moving person. Our experimental results demonstrate the versatility and efficacy of our approach. We publish the methods source code at https://github.com/utn-blei/CloudTrack.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet3_07">
             17:05-17:10, Paper ThET3.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3330'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              The Experiment Orchestration System (EOS): Comprehensive Foundation for Laboratory Automation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#354052" title="Click to go to the Author Index">
             Angelopoulos, Angelos
            </a>
           </td>
           <td class="r">
            University of North Carolina at Chapel Hill
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#418372" title="Click to go to the Author Index">
             Baykal, Cem
            </a>
           </td>
           <td class="r">
            University of North Carolina at Chapel Hill
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425248" title="Click to go to the Author Index">
             Kandel, Jade
            </a>
           </td>
           <td class="r">
            University of North Carolina at Chapel Hill
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#353011" title="Click to go to the Author Index">
             Verber, Matthew
            </a>
           </td>
           <td class="r">
            University of North Carolina at Chapel Hill
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#353295" title="Click to go to the Author Index">
             Cahoon, James
            </a>
           </td>
           <td class="r">
            University of North Carolina at Chapel Hill
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#107129" title="Click to go to the Author Index">
             Alterovitz, Ron
            </a>
           </td>
           <td class="r">
            University of North Carolina at Chapel Hill
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3330" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#robotics_and_automation_in_life_sciences" title="Click to go to the Keyword Index">
               Robotics and Automation in Life Sciences
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#software_architecture_for_robotic_and_automation" title="Click to go to the Keyword Index">
               Software Architecture for Robotic and Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#foundations_of_automation" title="Click to go to the Keyword Index">
               Foundations of Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             As scientific research in chemistry, materials science, and applied sciences becomes increasingly complex and data-driven, there is a growing need for efficient, scalable, and flexible automation to accelerate discoveries and reduce human burden and error in laboratories. We introduce the Experiment Orchestration System (EOS), an open-source software framework and runtime offering a comprehensive foundation for laboratory automation. EOS offers an extensible framework allowing users to define labs, devices, tasks, experiments, and optimization criteria using YAML and Python plugins, and also offers a distributed runtime for managing and executing automation. EOS has a central orchestrator that communicates with and controls laboratory equipment to execute tasks. EOS implements autonomous experiment campaigns, parameter optimization, task scheduling, result aggregation, and more. By providing a common infrastructure for laboratory automation, EOS aims to reduce automation implementation barriers and accelerate discoveries in science laboratories.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thet4">
             <b>
              ThET4
             </b>
            </a>
           </td>
           <td class="r">
            304
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thet4" title="Click to go to the Program at a Glance">
             <b>
              Bioinspiration and Biomimetics 3
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#103183" title="Click to go to the Author Index">
             Floreano, Dario
            </a>
           </td>
           <td class="r">
            Ecole Polytechnique Fédérale De Lausanne (EPFL)
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#107478" title="Click to go to the Author Index">
             Degani, Amir
            </a>
           </td>
           <td class="r">
            Technion - Israel Institute of Technology
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet4_01">
             16:35-16:40, Paper ThET4.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('314'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Design of a Bioinspired Jumping Mechanism for Self-Takeoff of Flapping Robot
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#376195" title="Click to go to the Author Index">
             Pan, Erzhen
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology, Shenzhen
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#387356" title="Click to go to the Author Index">
             Sun, Wei
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology Shenzhen
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#114039" title="Click to go to the Author Index">
             Xu, Wenfu
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology, Shenzhen
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab314" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#biologically_inspired_robots" title="Click to go to the Keyword Index">
               Biologically-Inspired Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#biomimetics" title="Click to go to the Keyword Index">
               Biomimetics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Most birds in nature rely on jumping for take-off. Flapping-Wing robots can flap and fly like birds but require an operator to take off, which are unable to generate sufficient lift to maintain flight at a low airspeed and must accelerate to take-off speed in a short time. It poses a challenge for the design of the jumping mechanism. This study is inspired by the jump-takeoff of birds and designs a simple and lightweight jumping leg, which is capable of storing and releasing the energy with only one degree of freedom. In addition, a prototype was developed and tested, with a wingspan of 2 meters and a mass of 1.6 kilograms, accelerating to 4 m/s in 52 milliseconds by jumping, achieving the jumping take-off from the ground.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet4_02">
             16:40-16:45, Paper ThET4.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1079'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Embodied Adaptive Sensing for Odor Concentration Maximization in Bio-Inspired Robotics
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#242280" title="Click to go to the Author Index">
             Homchanthanakul, Jettanan
            </a>
           </td>
           <td class="r">
            Vidyasirimedhi Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#187376" title="Click to go to the Author Index">
             Shigaki, Shunsuke
            </a>
           </td>
           <td class="r">
            National Institute of Informatics
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#120785" title="Click to go to the Author Index">
             Manoonpong, Poramate
            </a>
           </td>
           <td class="r">
            Vidyasirimedhi Institute of Science and Technology (VISTEC)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1079" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#biologically_inspired_robots" title="Click to go to the Keyword Index">
               Biologically-Inspired Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#neural_and_fuzzy_control" title="Click to go to the Keyword Index">
               Neural and Fuzzy Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Animals exhibit remarkable adaptability in sensing their environments, employing strategies that optimize information gathering. For instance, silk moths adjust their wing-flapping frequency to detect pheromones, while dogs modify their sniffing behavior by altering sniff height and frequency based on proximity to an odor source. Despite the potential to enhance odor detection for olfactory navigation by drawing inspiration from these natural mechanisms, many existing approaches focus on computationally intensive methods like multi-sensory integration or rely on multiple robots for odor localization, rather than leveraging embodied sensing. In this study, we propose an embodied adaptive sensing strategy that enhances odor detection by implementing an active odor sensor on a legged robot and applying a bio-inspired adaptive robot height control system for dynamically adapting the robot's height based on real-time gas concentration feedback. The control system employs a simple artificial hormone mechanism to regulate the robot height by processing gas concentration derivatives, mimicking biological adaptability. By utilizing the interaction between the active odor sensor, adaptive control system, and the legged body, this approach allows the robot to optimize its height online to capture the maximum gas concentration, thereby reducing the need for complex algorithms and high computational resources. As a result, it offers a more efficient solution for odor-driven tasks, with potential applications in real-world environments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet4_03">
             16:45-16:50, Paper ThET4.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2314'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SKOOTR: A SKating, Omni-Oriented, Tripedal Robot
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#374397" title="Click to go to the Author Index">
             Hung, Adam Joshua
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#318368" title="Click to go to the Author Index">
             Enninful Adu, Challen
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#153427" title="Click to go to the Author Index">
             Moore, Talia
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2314" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#biologically_inspired_robots" title="Click to go to the Keyword Index">
               Biologically-Inspired Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#biomimetics" title="Click to go to the Keyword Index">
               Biomimetics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In both animals and robots, locomotion capabilities are determined by the physical structure of the system. The majority of legged animals and robots are bilaterally symmetric, which facilitates locomotion with consistent headings and obstacle traversal, but leads to constraints in their turning ability. On the other hand, radially symmetric animals have demonstrated rapid turning abilities enabled by their omni-directional body plans. Radially symmetric tripedal robots are able to turn instantaneously, but are commonly constrained by needing to change direction with every step, resulting in inefficient and less stable locomotion. Inspired by the radial symmetry and maneuverability of brittle stars and octopuses, we introduce a novel design for a tripedal robot that has both frictional and rolling contacts. Additionally, a freely rotating central sphere provides an added contact point so the robot can retain a stable tripod base of support while lifting and pushing with any one of its legs. The SKating, Omni-Oriented, Tripedal Robot (SKOOTR) is more versatile and stable than existing tripedal robots. It is capable of multiple forward gaits, multiple turning maneuvers, obstacle traversal, and stair climbing. SKOOTR has been designed to facilitate customization for diverse applications: it is fully open-source, is constructed with 3D printed or off-the-shelf parts, and costs approximately 500 USD to build. A project page with CAD files, assembly guide, and links to the github repository is posted at https://www.embirlab.com/skootr.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet4_04">
             16:50-16:55, Paper ThET4.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3162'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              AllGaits: Learning All Quadruped Gaits and Transitions
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#216565" title="Click to go to the Author Index">
             Bellegarda, Guillaume
            </a>
           </td>
           <td class="r">
            EPFL
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#205571" title="Click to go to the Author Index">
             Shafiee, Milad
            </a>
           </td>
           <td class="r">
            EPFL
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#105018" title="Click to go to the Author Index">
             Ijspeert, Auke
            </a>
           </td>
           <td class="r">
            EPFL
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3162" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#biologically_inspired_robots" title="Click to go to the Keyword Index">
               Biologically-Inspired Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present a framework for learning a single policy capable of producing all quadruped gaits and transitions. The framework consists of a policy trained with deep reinforcement learning (DRL) to modulate the parameters of a system of abstract oscillators (i.e. Central Pattern Generator), whose output is mapped to joint commands through a pattern formation layer that sets the gait style, i.e. body height, swing foot ground clearance height, and foot offset. Different gaits are formed by changing the coupling between different oscillators, which can be instantaneously selected at any velocity by a user. With this framework, we systematically investigate which gait should be used at which velocity, and when gait transitions should occur from a Cost of Transport (COT), i.e. energy-efficiency, point of view. Additionally, we note how gait style changes as a function of locomotion speed for each gait to keep the most energy-efficient locomotion. While the currently most popular gait (trot) does not result in the lowest COT, we find that considering different co-dependent metrics such as mean base angular velocity and joint acceleration result in different 'optimal' gaits than those that minimize COT. We deploy our controller in various hardware experiments, focusing on 9 quadruped animal gaits, and demonstrate generalizability to novel and unseen gaits during training, and robustness to leg failures.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet4_05">
             16:55-17:00, Paper ThET4.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3306'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Bird-Inspired Tendon Coupling Improves Paddling Efficiency by Shortening Phase Transition Times
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#326082" title="Click to go to the Author Index">
             Lin, Jianfeng
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#164479" title="Click to go to the Author Index">
             Guo, Zhao
            </a>
           </td>
           <td class="r">
            Wuhan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#111036" title="Click to go to the Author Index">
             Badri-Spröwitz, Alexander
            </a>
           </td>
           <td class="r">
            Max Planck Institute for Intelligent Systems
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3306" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#biologically_inspired_robots" title="Click to go to the Keyword Index">
               Biologically-Inspired Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#biomimetics" title="Click to go to the Keyword Index">
               Biomimetics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#tendon_wire_mechanism" title="Click to go to the Keyword Index">
               Tendon/Wire Mechanism
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Drag-based swimming with rowing appendages, fins, and webbed feet is a widely adapted locomotion form in aquatic animals. To develop effective underwater and swimming vehicles, a wide range of bioinspired drag-based paddles have been proposed, often faced with a trade-off between propulsive efficiency and versatility. Webbed feet provide an effective propulsive force in the power phase, are light weight and robust, and can even be partially folded away in the recovery phase. However, during the transition between recovery and power phase, much time is lost folding and unfolding, leading to drag and reducing efficiency. In this work, we took inspiration from the coupling tendons of aquatic birds and utilized tendon coupling mechanisms to shorten the transition time between recovery and power phase. Results from our hardware experiments show that the proposed mechanisms improve propulsive efficiency by 2.0 and 2.4 times compared to a design without extensor tendons or based on passive paddle, respectively. We further report that distal leg joint clutching, which has been shown to improve efficiency in terrestrial walking, did not play an major role in swimming locomotion. In sum, we describe a new principle for an efficient, drag-based leg and paddle design, with potential relevance for the swimming mechanics in aquatic birds.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet4_06">
             17:00-17:05, Paper ThET4.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3590'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Bio-Inspired Sand-Rolling Robot: Effect of Body Shape on Sand Rolling Performance
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425066" title="Click to go to the Author Index">
             Liao, Xingjue
            </a>
           </td>
           <td class="r">
            University of Southern California
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425350" title="Click to go to the Author Index">
             Liu, Wenhao
            </a>
           </td>
           <td class="r">
            University of Southern California
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425331" title="Click to go to the Author Index">
             Wu, Hao
            </a>
           </td>
           <td class="r">
            University of Southern California
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#194949" title="Click to go to the Author Index">
             Qian, Feifei
            </a>
           </td>
           <td class="r">
            University of Southern California
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3590" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#biologically_inspired_robots" title="Click to go to the Keyword Index">
               Biologically-Inspired Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#biomimetics" title="Click to go to the Keyword Index">
               Biomimetics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#passive_walking" title="Click to go to the Keyword Index">
               Passive Walking
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The capability of effectively moving on complex terrains such as sand and gravel can empower our robots to robustly operate in outdoor environments, and assist with critical tasks such as environment monitoring, search-and-rescue, and supply delivery. Inspired by the Mount Lyell salamander's ability to curl its body into a loop and effectively roll down hill slopes, in this study we develop a sand-rolling robot and investigate how its locomotion performance is governed by the shape of its body. We experimentally tested three different body shapes: Hexagon, Quadrilateral, and Triangle. We found that Hexagon and Triangle can achieve a faster rolling speed on sand, but exhibited more frequent failures of getting stuck. Analysis of the interaction between robot and sand revealed the failure mechanism: the deformation of the sand produced a local ``sand incline'' underneath robot contact segments, increasing the effective region of supporting polygon (ERSP) and preventing the robot from shifting its center of mass (CoM) outside the ERSP to produce sustainable rolling. Based on this mechanism, a highly-simplified model successfully captured the critical body pitch for each rolling shape to produce sustained rolling on sand, and informed design adaptations that mitigated the locomotion failures and improved robot speed by more than 200%. Our results provide insights into how locomotors can utilize different morphological features to achieve robust rolling motion across deformable substrates.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet4_07">
             17:05-17:10, Paper ThET4.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4593'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Programmable Substrate to Study Robots Jumping from Non-Rigid Surfaces
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#256422" title="Click to go to the Author Index">
             Divi, Sathvik
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#195833" title="Click to go to the Author Index">
             Yim, Justin K.
            </a>
           </td>
           <td class="r">
            University of Illinois Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#193428" title="Click to go to the Author Index">
             Bedillion, Mark
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#103249" title="Click to go to the Author Index">
             Bergbreiter, Sarah
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4593" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#biologically_inspired_robots" title="Click to go to the Keyword Index">
               Biologically-Inspired Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#biomimetics" title="Click to go to the Keyword Index">
               Biomimetics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#compliance_and_impedance_control" title="Click to go to the Keyword Index">
               Compliance and Impedance Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This study presents the development, characterization, and demonstration of a tunable substrate for small jumping robots. Jumping robots in the literature are typically evaluated when jumping from rigid surfaces, in contrast to surfaces with more significant compliance or damping that are encountered in the natural world. The aim of this work is to create a physical substrate, or 'ground', for which the effective mass, compliance, and damping can be programmed. This system enables quick testing of various substrate conditions and also allows for the introduction of complex nonlinearities to analyze the interactions between latch-mediated spring actuation (LaMSA) systems and their environment. A mathematical model for the substrate is defined and the system is built with a fast brushless DC motor and controller running on a real-time target machine. The results illustrate the range of compliance and damping that can be achieved, as well as example jumps from the substrate using a 4 g jumper and a 108 g jumping robot.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thet5">
             <b>
              ThET5
             </b>
            </a>
           </td>
           <td class="r">
            305
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thet5" title="Click to go to the Program at a Glance">
             <b>
              Learning for Legged Locomotion 2
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#123323" title="Click to go to the Author Index">
             Havoutis, Ioannis
            </a>
           </td>
           <td class="r">
            University of Oxford
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet5_01">
             16:35-16:40, Paper ThET5.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('437'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Fine-Tuning Hard-To-Simulate Objectives for Quadruped Locomotion: A Case Study on Total Power Saving
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417045" title="Click to go to the Author Index">
             Nai, Ruiqian
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417049" title="Click to go to the Author Index">
             You, Jiacheng
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417051" title="Click to go to the Author Index">
             Cao, Liu
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417136" title="Click to go to the Author Index">
             Cui, Hanchen
            </a>
           </td>
           <td class="r">
            University of Minnesota Twin Cities
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417096" title="Click to go to the Author Index">
             Zhang, Shiyuan
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#220412" title="Click to go to the Author Index">
             Xu, Huazhe
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#191564" title="Click to go to the Author Index">
             Gao, Yang
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab437" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Legged locomotion is not just about mobility; it also encompasses crucial objectives such as energy efficiency, safety, and user experience, which are vital for real-world applications. However, key factors such as battery power consumption and stepping noise are often inaccurately modeled or missing in common simulators, leaving these aspects poorly optimized or unaddressed by current sim-to-real methods. Hand-designed proxies, such as mechanical power and foot contact forces, have been used to address these challenges but are often problem-specific and inaccurate. 			 In this paper, we propose a data-driven framework for fine-tuning locomotion policies, targeting these hard-to-simulate objectives. Our framework leverages real-world data to model these objectives and incorporates the learned model into simulation for policy improvement. We demonstrate the effectiveness of our framework on power saving for quadruped locomotion, achieving a significant 24-28% net reduction in total power consumption from the battery pack at various speeds. In essence, our approach offers a versatile solution for optimizing hard-to-simulate objectives in quadruped locomotion, providing an easy-to-adapt paradigm for continual improving with real-world knowledge.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet5_02">
             16:40-16:45, Paper ThET5.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('535'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Think on Your Feet: Seamless and Command-Adaptive Transition between Human-Like Locomotions
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#324723" title="Click to go to the Author Index">
             Huang, Huaxing
            </a>
           </td>
           <td class="r">
            Noetix Robotics
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417662" title="Click to go to the Author Index">
             Cui, Wenhao
            </a>
           </td>
           <td class="r">
            Noetix
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417675" title="Click to go to the Author Index">
             Zhang, Tonghe
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417874" title="Click to go to the Author Index">
             Li, Shengtao
            </a>
           </td>
           <td class="r">
            Noetix
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426189" title="Click to go to the Author Index">
             Han, Jinchao
            </a>
           </td>
           <td class="r">
            Noetix
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#418238" title="Click to go to the Author Index">
             Qin, Bangyu
            </a>
           </td>
           <td class="r">
            Noetix Robotics
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417795" title="Click to go to the Author Index">
             Zheng, Liang
            </a>
           </td>
           <td class="r">
            Noetix
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417799" title="Click to go to the Author Index">
             Tang, Ziyang
            </a>
           </td>
           <td class="r">
            Noetix Robotics
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426794" title="Click to go to the Author Index">
             Zhang, Tianchu
            </a>
           </td>
           <td class="r">
            Noetix Robotics
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#415203" title="Click to go to the Author Index">
             Hu, Chenxu
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417872" title="Click to go to the Author Index">
             Zhang, Shipu
            </a>
           </td>
           <td class="r">
            Noetix Robotics
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#352468" title="Click to go to the Author Index">
             Jiang, Zheyuan
            </a>
           </td>
           <td class="r">
            NOETIX Robotics
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab535" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#humanoid_and_bipedal_locomotion" title="Click to go to the Keyword Index">
               Humanoid and Bipedal Locomotion
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             While it is relatively easier to train humanoid robots to mimic specific locomotion skills, it is more challenging to learn from various motions and adhere to continuously changing commands. These robots must accurately track motion instructions, seamlessly transition between a variety of movements,} and master intermediate motions not present in their reference data. In this work, we propose a novel approach that integrates human-like motion transfer with precise velocity tracking by a series of improvements to classical imitation learning. To enhance generalization, we employ the Wasserstein divergence criterion (WGAN-div). Furthermore, a Hybrid Internal Model provides structured estimates of hidden states and velocity to enhance mobile stability and environment adaptability, while a curiosity bonus fosters exploration. Our comprehensive method promises highly human-like locomotion that adapts to varying velocity requirements, direct generalization to unseen motions and multitasking, as well as zero-shot transfer to the simulator and the real world across different terrains. These advancements are validated through simulations across various robot models and extensive real-world experiments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet5_03">
             16:45-16:50, Paper ThET5.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('821'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              RINA: Rapid Introspective Neural Adaptation for Out-Of-Distribution Payload Configurations on Quadruped Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#329987" title="Click to go to the Author Index">
             Youngquist, Oscar
            </a>
           </td>
           <td class="r">
            University of Massachusetts Amherst
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#138554" title="Click to go to the Author Index">
             Zhang, Hao
            </a>
           </td>
           <td class="r">
            University of Massachusetts Amherst
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab821" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#machine_learning_for_robot_control" title="Click to go to the Keyword Index">
               Machine Learning for Robot Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Adaptive locomotion is a fundamental capability for quadruped robots, particularly in real-world scenarios when they must transport novel or out-of-distribution (O.O.D.) payloads across diverse terrains. Previous learning-based methods often tightly couple a locomotion controller's learned parameters with the adaptation process, which requires extensive pre-training or slow online updates when encountering O.O.D. payloads. To enable adaptation of quadruped locomotion to O.O.D. payloads, we propose the novel Rapid Introspective Neural Adaptation (RINA) method that rapidly compensates for differences between expected and actual joint torques caused by O.O.D. payloads. RINA introduces an adaptive residual dynamics representation that decouples the learning model's parameters from those used for adaptation. A new neural operator network is introduced to learn a set of basis functions as the learning model, which are combined using linear coefficients to predict residual dynamics. Then, these residual dynamics are used to adjust the locomotion controller's output, compensating for additional torques induced by the O.O.D. payload. During execution, the mixing coefficients can be rapidly and introspectively adapted on-the-go to generate joint torque compensations for O.O.D. payloads, while keeping the learned basis functions unchanged. Experimental results have demonstrated that our RINA approach well addresses on-the-go O.O.D. payload adaptation on varied natural terrains without collecting and retraining on additional data and outperforms baseline methods.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet5_04">
             16:50-16:55, Paper ThET5.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('887'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Masked Sensory-Temporal Attention for Sensor Generalization in Quadruped Locomotion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#339599" title="Click to go to the Author Index">
             Liu, Dikai
            </a>
           </td>
           <td class="r">
            NVIDIA
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#256104" title="Click to go to the Author Index">
             Zhang, Tianwei
            </a>
           </td>
           <td class="r">
            Nanyang Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#342141" title="Click to go to the Author Index">
             Yin, Jianxiong
            </a>
           </td>
           <td class="r">
            NVIDIA
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#342292" title="Click to go to the Author Index">
             See, Simon
            </a>
           </td>
           <td class="r">
            NVIDIA
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab887" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             With the rising focus on quadrupeds, a generalized policy capable of handling different robot models and sensor inputs becomes highly beneficial. Although several methods have been proposed to address different morphologies, it remains a challenge for learning-based policies to manage various combinations of proprioceptive information. This paper presents Masked Sensory-Temporal Attention (MSTA), a novel transformer-based mechanism with masking for quadruped locomotion. It employs direct sensor-level attention to enhance the sensory-temporal understanding and handle different combinations of sensor data, serving as a foundation for incorporating unseen information. MSTA can effectively understand its states even with a large portion of missing information, and is flexible enough to be deployed on physical systems despite the long input sequence.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet5_05">
             16:55-17:00, Paper ThET5.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('888'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Robust Robot Walker: Learning Agile Locomotion Over Tiny Traps
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#414070" title="Click to go to the Author Index">
             Zhu, Shaoting
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#418803" title="Click to go to the Author Index">
             Huang, Runhan
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416173" title="Click to go to the Author Index">
             Mou, Linzhan
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#207381" title="Click to go to the Author Index">
             Zhao, Hang
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab888" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#ai_based_methods" title="Click to go to the Keyword Index">
               AI-Based Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Quadruped robots must exhibit robust walking capabilities in practical applications. In this work, we propose a novel approach that enables quadruped robots to pass various small obstacles, or "tiny traps". Existing methods often rely on exteroceptive sensors, which can be unreliable for detecting such tiny traps. To overcome this limitation, our approach focuses solely on proprioceptive inputs. We introduce a two-stage training framework incorporating a contact encoder and a classification head to learn implicit representations of different traps. Additionally, we design a set of tailored reward functions to improve both the stability of training and the ease of deployment for goal-tracking tasks. To benefit further research, we design a new benchmark for tiny trap task. Extensive experiments in both simulation and real-world settings demonstrate the effectiveness and robustness of our method. Appendix can be found in project page: https://robust-robot-walker.github.io/.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet5_06">
             17:00-17:05, Paper ThET5.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1094'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              FRASA: An End-To-End Reinforcement Learning Agent for Fall Recovery and Stand up of Humanoid Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#394243" title="Click to go to the Author Index">
             Gaspard, Clément
            </a>
           </td>
           <td class="r">
            LaBRI - University of Bordeaux
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#409742" title="Click to go to the Author Index">
             Duclusaud, Marc
            </a>
           </td>
           <td class="r">
            LaBRI - University of Bordeaux
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#163740" title="Click to go to the Author Index">
             Passault, Grégoire
            </a>
           </td>
           <td class="r">
            LaBRI
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#285942" title="Click to go to the Author Index">
             Daniel, Mélodie
            </a>
           </td>
           <td class="r">
            LaBRI - Université De Bordeaux
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#137536" title="Click to go to the Author Index">
             Ly, Olivier
            </a>
           </td>
           <td class="r">
            LaBRI - Bordeaux University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1094" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#humanoid_robot_systems" title="Click to go to the Keyword Index">
               Humanoid Robot Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#body_balancing" title="Click to go to the Keyword Index">
               Body Balancing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Humanoid robotics faces significant challenges in achieving stable locomotion and recovering from falls in dynamic environments. Traditional methods, such as Model Predictive Control (MPC) and Key Frame Based (KFB) routines, either require extensive fine-tuning or lack real-time adaptability. This paper introduces FRASA, a Deep Reinforcement Learning (DRL) agent that integrates fall recovery and stand up strategies into a unified framework. Leveraging the Cross-Q algorithm, FRASA significantly reduces training time and offers a versatile recovery strategy that adapts to unpredictable disturbances. Comparative tests on Sigmaban humanoid robots demonstrate FRASA superior performance against the KFB method deployed in the RoboCup 2023 by the Rhoban Team, world champion of the KidSize League.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet5_07">
             17:05-17:10, Paper ThET5.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1417'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              DreamFLEX: Learning Fault-Aware Quadrupedal Locomotion Controller for Anomaly Situation in Rough Terrains
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#314666" title="Click to go to the Author Index">
             Lee, Seunghyun
            </a>
           </td>
           <td class="r">
            KAIST (Korea Advanced Institute of Science and Technology)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#310811" title="Click to go to the Author Index">
             Nahrendra, I Made Aswin
            </a>
           </td>
           <td class="r">
            KAIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#319647" title="Click to go to the Author Index">
             Lee, Dongkyu
            </a>
           </td>
           <td class="r">
            KAIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#274099" title="Click to go to the Author Index">
             Yu, Byeongho
            </a>
           </td>
           <td class="r">
            KAIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#295392" title="Click to go to the Author Index">
             Oh, Minho
            </a>
           </td>
           <td class="r">
            KAIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#320847" title="Click to go to the Author Index">
             Lee, Hyeonwoo
            </a>
           </td>
           <td class="r">
            KAIST (Korea Advanced Institute of Science and Technology)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104490" title="Click to go to the Author Index">
             Myung, Hyun
            </a>
           </td>
           <td class="r">
            KAIST (Korea Advanced Institute of Science and Technology)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1417" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#robust_adaptive_control" title="Click to go to the Keyword Index">
               Robust/Adaptive Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Recent advances in quadrupedal robots have demonstrated impressive agility and the ability to traverse diverse terrains. However, hardware issues, such as motor overheating or joint locking, may occur during long-distance walking or traversing through rough terrains and lead to locomotion failures. Although several studies have proposed fault-tolerant control methods for quadrupedal robots, there are still challenges in traversing unstructured terrains. In this paper, we propose DreamFLEX, a robust fault-tolerant locomotion controller that enables a quadrupedal robot to traverse complex environments even under joint failure condition. DreamFLEX integrates an explicit failure estimation and modulation network that jointly estimates the robot's joint fault vector and utilizes this information to adapt the locomotion pattern to faulty conditions in real-time, enabling quadrupedal robots to maintain stability and performance in rough terrains. Experimental results demonstrate that DreamFLEX outperforms existing methods in both simulation and real-world scenarios, effectively managing hardware failures while maintaining robust locomotion performance.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet5_08">
             17:10-17:15, Paper ThET5.8
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4811'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Curriculum-Based Reinforcement Learning for Quadrupedal Jumping: A Reference-Free Design
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#382310" title="Click to go to the Author Index">
             Atanassov, Vassil
            </a>
           </td>
           <td class="r">
            University of Oxford
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#188071" title="Click to go to the Author Index">
             Ding, Jiatao
            </a>
           </td>
           <td class="r">
            Delft University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#117613" title="Click to go to the Author Index">
             Kober, Jens
            </a>
           </td>
           <td class="r">
            TU Delft
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#123323" title="Click to go to the Author Index">
             Havoutis, Ioannis
            </a>
           </td>
           <td class="r">
            University of Oxford
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#179240" title="Click to go to the Author Index">
             Della Santina, Cosimo
            </a>
           </td>
           <td class="r">
            TU Delft
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4811" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#machine_learning_for_robot_control" title="Click to go to the Keyword Index">
               Machine Learning for Robot Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Deep reinforcement learning (DRL) has emerged as a promising solution to mastering explosive and versatile quadrupedal jumping skills. However, current DRL-based frameworks usually rely on pre-existing reference trajectories obtained by capturing animal motions or transferring experience from existing controllers. This work aims to prove that learning dynamic jumping is possible without relying on imitating a reference trajectory by leveraging a curriculum design. Starting from a vertical in-place jump, we generalize the learned policy to forward and diagonal jumps and, finally, we learn to jump across obstacles. Conditioned on the desired landing location, orientation, and obstacle dimensions, the proposed approach yields a wide range of omnidirectional jumping motions in real-world experiments. Particularly we achieve a 90cm forward jump, exceeding all previous records for similar robots reported in the existing literature. Additionally, the robot can reliably execute continuous jumping on soft grassy grounds, which is especially remarkable as such conditions were not included in the training stage.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thet6">
             <b>
              ThET6
             </b>
            </a>
           </td>
           <td class="r">
            307
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thet6" title="Click to go to the Program at a Glance">
             <b>
              Perception for Manipulation 4
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#237146" title="Click to go to the Author Index">
             Gaidon, Adrien
            </a>
           </td>
           <td class="r">
            Toyota Research Institute
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet6_01">
             16:35-16:40, Paper ThET6.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('860'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              OmniShape: Zero-Shot Multi-Hypothesis Shape and Pose Estimation in the Real World
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#217166" title="Click to go to the Author Index">
             Liu, Katherine
            </a>
           </td>
           <td class="r">
            Toyota Research Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#211158" title="Click to go to the Author Index">
             Zakharov, Sergey
            </a>
           </td>
           <td class="r">
            Toyota Research Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#340938" title="Click to go to the Author Index">
             Chen, Dian
            </a>
           </td>
           <td class="r">
            Toyota Research Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#172403" title="Click to go to the Author Index">
             Ikeda, Takuya
            </a>
           </td>
           <td class="r">
            Woven by Toyota, Inc
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#308464" title="Click to go to the Author Index">
             Shakhnarovich, Gregory
            </a>
           </td>
           <td class="r">
            Toyota Technological Institute at Chicago
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#237146" title="Click to go to the Author Index">
             Gaidon, Adrien
            </a>
           </td>
           <td class="r">
            Toyota Research Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#117786" title="Click to go to the Author Index">
             Ambrus, Rares
            </a>
           </td>
           <td class="r">
            Toyota Research Institute
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab860" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We would like to estimate the pose and full shape of an object from a single observation, without assuming known 3D model or category. In this work, we propose OmniShape, the first method of its kind to enable probabilistic pose and shape estimation. OmniShape is based on the key insight that shape completion can be decoupled into two multi-modal distributions: one capturing how measurements project into a normalized object reference frame defined by the dataset and the other modelling a prior over object geometries represented as triplanar neural fields. By training separate conditional diffusion models for these two distributions, we enable sampling multiple hypotheses from the joint pose and shape distribution. OmniShape demonstrates compelling performance on challenging real world datasets.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet6_02">
             16:40-16:45, Paper ThET6.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('964'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Self-Supervised Learning of Reconstructing Deformable Linear Objects under Single-Frame Occluded View
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#375589" title="Click to go to the Author Index">
             Wang, Song
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420076" title="Click to go to the Author Index">
             Shen, Guanghui
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#319867" title="Click to go to the Author Index">
             Wu, Shirui
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#256557" title="Click to go to the Author Index">
             Wu, Dan
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab964" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Deformable linear objects (DLOs), such as ropes,cables, and rods, are common in various scenarios, and accurate occlusion reconstruction of them is crucial for effective robotic manipulation. Previous studies for DLO reconstruction either rely on supervised learning, which is limited by the availability of labeled real-world data, or geometric approaches, which fail to capture global features and often struggle with occlusions and complex shapes. This paper presents a novel DLO occlusion reconstruction framework that integrates self-supervised point cloud completion with traditional techniques like clustering, sorting, and fitting to generate ordered key points. A memory module is proposed to enhance the self-supervised training process by consolidating prototype information, while DLO shape constraints are utilized to improve reconstruction accuracy. Experimental results on both synthetic and real-world datasets demonstrate that our method outperforms state-of the-art algorithms, particularly in scenarios involving complex occlusions and intricate self-intersections.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet6_03">
             16:45-16:50, Paper ThET6.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1045'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              PseudoTouch: Efficiently Imaging the Surface Feel of Objects for Robotic Manipulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#236637" title="Click to go to the Author Index">
             Röfer, Adrian
            </a>
           </td>
           <td class="r">
            University of Freiburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#324871" title="Click to go to the Author Index">
             Heppert, Nick
            </a>
           </td>
           <td class="r">
            University of Freiburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#396333" title="Click to go to the Author Index">
             Ayad, Abdallah
            </a>
           </td>
           <td class="r">
            University of Freiburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#289597" title="Click to go to the Author Index">
             Chisari, Eugenio
            </a>
           </td>
           <td class="r">
            University of Freiburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#160427" title="Click to go to the Author Index">
             Valada, Abhinav
            </a>
           </td>
           <td class="r">
            University of Freiburg
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1045" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#force_and_tactile_sensing" title="Click to go to the Keyword Index">
               Force and Tactile Sensing
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#representation_learning" title="Click to go to the Keyword Index">
               Representation Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Tactile sensing is vital for human dexterous manipulation, however, it has not been widely used in robotics. Compact, low-cost sensing platforms can facilitate a change, but unlike their popular optical counterparts, they are difficult to deploy in high-fidelity tasks due to their low signal dimensionality and lack of a simulation model. To overcome these challenges, we introduce PseudoTouch which links high-dimensional structural information to low-dimensional sensor signals. It does so by learning a low-dimensional visual-tactile embedding, wherein we encode a depth patch from which we decode the tactile signal. We collect and train PseudoTouch on a dataset comprising aligned tactile and visual data pairs obtained through random touching of eight basic geometric shapes. We demonstrate the utility of our trained PseudoTouch model in two downstream tasks: object recognition and grasp stability prediction. In the object recognition task, we evaluate the learned embedding's performance on a set of five basic geometric shapes and five household objects. Using PseudoTouch, we achieve an object recognition accuracy 84% after just ten touches, surpassing a proprioception baseline. For the grasp stability task, we use ACRONYM labels to train and evaluate a grasp success predictor using PseudoTouch's predictions derived from virtual depth information. Our approach yields a 32% absolute improvement in accuracy compared to the baseline relying on partial point cloud data. We make the data, code, and trained models publicly available at https://pseudotouch.cs.uni-freiburg.de.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet6_04">
             16:50-16:55, Paper ThET6.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1095'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Segment Any Repeated Object
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#372603" title="Click to go to the Author Index">
             Liu, Yushi
            </a>
           </td>
           <td class="r">
            University Tübingen
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420244" title="Click to go to the Author Index">
             Graf, Christian
            </a>
           </td>
           <td class="r">
            Robert Bosch GmbH
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#139757" title="Click to go to the Author Index">
             Spies, Markus
            </a>
           </td>
           <td class="r">
            Bosch Center for Artificial Intelligence
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#253422" title="Click to go to the Author Index">
             Keuper, Margret
            </a>
           </td>
           <td class="r">
            University of Mannheim
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1095" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Understanding a scene in terms of objects and their properties is fundamental for various vision-based robotic applications, including item picking. To effectively clear a bin, a robot must comprehend objects as graspable entities, often without prior access to models of the target object. This study focuses on open world object segmentation with the additional requirement of assigning identical class labels for repeated instances of the same object. This capability enables item picking tasks with homogeneous bins, filtering out packaging material, and sorting tasks. We propose a novel pipeline for detecting repeated instances of identical objects, building on recent advancements in vision foundation models and exploring approaches for estimating object similarities based on feature embeddings or keypoint correspondence matching. Through a comprehensive experimental evaluation, we establish a new state-of-the-art on ARMBench repeated objects segmentation, a particularly challenging open problem in bin-picking robotics. Additionally, we demonstrate the real-world application of our method integrated into a robot picking cell to showcase its relevance to industrial use cases.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet6_05">
             16:55-17:00, Paper ThET6.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1594'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              ViTa-Zero: Zero-Shot Visuotactile Object 6D Pose Estimation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#324814" title="Click to go to the Author Index">
             Li, Hongyu
            </a>
           </td>
           <td class="r">
            Brown University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#291217" title="Click to go to the Author Index">
             Akl, James
            </a>
           </td>
           <td class="r">
            Amazon
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#173768" title="Click to go to the Author Index">
             Sridhar, Srinath
            </a>
           </td>
           <td class="r">
            Brown University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421915" title="Click to go to the Author Index">
             Brady, Tye
            </a>
           </td>
           <td class="r">
            Amazon
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#129456" title="Click to go to the Author Index">
             Padir, Taskin
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1594" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#force_and_tactile_sensing" title="Click to go to the Keyword Index">
               Force and Tactile Sensing
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#sensor_fusion" title="Click to go to the Keyword Index">
               Sensor Fusion
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Object 6D pose estimation is a critical challenge in robotics, particularly for manipulation tasks. While prior research combining visual and tactile (visuotactile) information has shown promise, these approaches often struggle with generalization due to the limited availability of visuotactile data. In this paper, we introduce ViTa-Zero, a zero-shot visuotactile pose estimation framework. Our key innovation lies in leveraging a visual model as its backbone and performing feasibility checking and test-time optimization based on physical constraints derived from tactile and proprioceptive observations. Specifically, we model the gripper-object interaction as a spring–mass system, where tactile sensors induce attractive forces, and proprioception generates repulsive forces. We validate our framework through experiments on a real-world robot setup, demonstrating its effectiveness across representative visual backbones and manipulation scenarios, including grasping, object picking, and bimanual handover. Compared to the visual models, our approach overcomes some drastic failure modes while tracking the in-hand object pose. In our experiments, our approach shows an average increase of 55% in AUC of ADD-S and 60% in ADD, along with an 80% lower position error compared to FoundationPose.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet6_06">
             17:00-17:05, Paper ThET6.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3953'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              DoorBot: Closed-Loop Task Planning and Manipulation for Door Opening in the Wild with Haptic Feedback
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#397238" title="Click to go to the Author Index">
             Wang, Zhi
            </a>
           </td>
           <td class="r">
            UIUC
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#340231" title="Click to go to the Author Index">
             Mo, Yuchen
            </a>
           </td>
           <td class="r">
            University of Illinois, Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#393269" title="Click to go to the Author Index">
             Jin, Shengmiao
            </a>
           </td>
           <td class="r">
            University of Illinois Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#172670" title="Click to go to the Author Index">
             Yuan, Wenzhen
            </a>
           </td>
           <td class="r">
            University of Illinois
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3953" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#force_and_tactile_sensing" title="Click to go to the Keyword Index">
               Force and Tactile Sensing
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mobile_manipulation" title="Click to go to the Keyword Index">
               Mobile Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robots operating in unstructured environments face significant challenges when interacting with everyday objects like doors. They particularly struggle to generalize across diverse door types and conditions. Existing vision-based and open-loop planning methods often lack the robustness to handle varying door designs, mechanisms, and push/pull configurations. In this work, we propose a haptic-aware closed-loop hierarchical control framework that enables robots to explore and open different unseen doors in the wild. Our approach leverages real-time haptic feedback, allowing the robot to adjust its strategy dynamically based on force feedback during manipulation. We test our system on 20 unseen doors across different buildings, featuring diverse appearances and mechanical types. Our framework achieves a 90% success rate, demonstrating its ability to generalize and robustly handle varied door-opening tasks. This scalable solution offers potential applications in broader open-world articulated object manipulation tasks.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet6_07">
             17:05-17:10, Paper ThET6.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4980'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SEDMamba: Enhancing Selective State Space Modelling with Bottleneck Mechanism and Fine-To-Coarse Temporal Fusion for Efficient Error Detection in Robot-Assisted Surgery
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#410394" title="Click to go to the Author Index">
             Xu, Jialang
            </a>
           </td>
           <td class="r">
            University College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416164" title="Click to go to the Author Index">
             Sirajudeen, Nazir
            </a>
           </td>
           <td class="r">
            University College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416160" title="Click to go to the Author Index">
             Boal, Matthew
            </a>
           </td>
           <td class="r">
            The Griffin Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426944" title="Click to go to the Author Index">
             Francis, Nader
            </a>
           </td>
           <td class="r">
            The Griffin Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#128031" title="Click to go to the Author Index">
             Stoyanov, Danail
            </a>
           </td>
           <td class="r">
            University College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#203999" title="Click to go to the Author Index">
             Mazomenos, Evangelos
            </a>
           </td>
           <td class="r">
            UCL
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4980" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_medical_robotics" title="Click to go to the Keyword Index">
               Computer Vision for Medical Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#surgical_robotics__laparoscopy" title="Click to go to the Keyword Index">
               Surgical Robotics: Laparoscopy
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Automated detection of surgical errors can improve robotic-assisted surgery. Despite promising progress, existing methods still face challenges in capturing rich temporal context to establish long-term dependencies while maintaining computational efficiency. In this paper, we propose a novel hierarchical model named SEDMamba, which incorporates the selective state space model (SSM) into surgical error detection, facilitating efficient long sequence modelling with linear complexity. SEDMamba enhances selective SSM with a bottleneck mechanism and fine-to-coarse temporal fusion (FCTF) to detect and temporally localize surgical errors in long videos. The bottleneck mechanism compresses and restores features within their spatial dimension, thereby reducing computational complexity. FCTF utilizes multiple dilated 1D convolutional layers to merge temporal information across diverse scale ranges, accommodating errors of varying duration. Our work also contributes the first-of-its-kind, frame-level, in-vivo surgical error dataset to support error detection in real surgical cases. Specifically, we deploy the clinically validated observational clinical human reliability assessment tool (OCHRA) to annotate the errors during suturing tasks in an open-source radical prostatectomy dataset (SAR-RARP50). Experimental results demonstrate that our SEDMamba outperforms state-of-the-art methods with at least 1.82% AUC and 3.80% AP performance gains with significantly reduced computational complexity. The corresponding error annotations, code and models will be released at https://github.com/wzjialang/SEDMamba.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thet7">
             <b>
              ThET7
             </b>
            </a>
           </td>
           <td class="r">
            309
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thet7" title="Click to go to the Program at a Glance">
             <b>
              Deep Learning Applications
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#284378" title="Click to go to the Author Index">
             Ostyn, Frederik
            </a>
           </td>
           <td class="r">
            Ghent University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet7_01">
             16:35-16:40, Paper ThET7.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('198'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Automated Generation of Transformations to Mitigate Sensor Hardware Migration in ADS
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#291783" title="Click to go to the Author Index">
             Von Stein, Meriel
            </a>
           </td>
           <td class="r">
            University of Virginia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#147791" title="Click to go to the Author Index">
             Elbaum, Sebastian
            </a>
           </td>
           <td class="r">
            University of Virginia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#385359" title="Click to go to the Author Index">
             Wang, Hongning
            </a>
           </td>
           <td class="r">
            University of Virginia
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab198" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#sensor_based_control" title="Click to go to the Keyword Index">
               Sensor-based Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#autonomous_vehicle_navigation" title="Click to go to the Keyword Index">
               Autonomous Vehicle Navigation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Autonomous driving systems (ADSs) rely on massive amounts of sensed data to train their underlying	machine-learned components. Common sensor hardware migrations can render an existing machine-learned pipeline inadequate. This necessitates the development of bespoke transformations to adapt new sensor data to the old learned model, or the retraining of a new model with new sensor data. These solutions are expensive, often performed reactively to sensor hardware migration, and rely on empirical reconstruction and validation metrics only which lack knowledge of the features important to the learned model. To address these challenges, we propose PreFixer, a technique that can systematically generate transformations for many types of sensor hardware migration during the ADS development lifecycle. PreFixer collects small datasets using colocated new and old sensors, and then uses that data and the output of the learned model to train an augmented encoder to learn a transformation that maps new sensor data to old sensor data. The trained encoder can then be deployed as a preprocessor to the old learned model. Our study shows that, for a common set of camera sensor hardware migrations, PreFixer can match or improve the performance of the best-performing baseline technique in terms of distance travelled safely with 10% of the training dataset, and take at most half of the training time.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet7_02">
             16:40-16:45, Paper ThET7.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('443'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Probabilistic Latent Variable Modeling for Dynamic Friction Identification and Estimation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#327016" title="Click to go to the Author Index">
             Vantilborgh, Victor
            </a>
           </td>
           <td class="r">
            Ghent University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#317263" title="Click to go to the Author Index">
             De Witte, Sander
            </a>
           </td>
           <td class="r">
            Ghent University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#284378" title="Click to go to the Author Index">
             Ostyn, Frederik
            </a>
           </td>
           <td class="r">
            Ghent University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#209623" title="Click to go to the Author Index">
             Lefebvre, Tom
            </a>
           </td>
           <td class="r">
            Ghent University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#208200" title="Click to go to the Author Index">
             Crevecoeur, Guillaume
            </a>
           </td>
           <td class="r">
            Ghent University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab443" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#industrial_robots" title="Click to go to the Keyword Index">
               Industrial Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#probabilistic_inference" title="Click to go to the Keyword Index">
               Probabilistic Inference
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Precise identification of dynamic models in robotics is essential to support dynamic simulations, control design, friction compensation, output torque estimation, etc. A longstanding challenge remains in the development and identification of friction models for robotic joints, given the numerous physical phenomena affecting the underlying friction dynamics which result into nonlinear characteristics and hysteresis behaviour in particular. These phenomena proof difficult to be modelled and captured accurately using physical analogies alone. This has motivated researchers to shift from physics-based to data-driven models. Currently, these methods are still limited in their ability to generalize effectively to typical industrial robot deployement, characterized by high- and low-velocity operations and frequent direction reversals. Empirical observations motivate the use of dynamic friction models but these remain particulary challenging to establish. To address the current limitations, we propose to account for unidentified dynamics in the robot joints using latent dynamic states. The friction model may then utilize both the dynamic robot state and additional information encoded in the latent state to evaluate the friction torque. We cast this stochastic and partially unsupervised identification problem as a standard probabilistic representation learning problem. In this work both the friction model and latent state dynamics are parametrized as neural networks and are integrated in the conventional lumped parameter dynamic robot model. The complete dynamics model is directly learned from the noisy encoder measurements in the robot joints. We use the Expectation-Maximisation (EM) algorithm to find a Maximum Likelihood Estimate (MLE) of the model parameters. The effectiveness of the proposed method is validated in terms of open-loop prediction accuracy in comparison with baseline methods, using the Kuka KR6 R700 as a test platform.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet7_03">
             16:45-16:50, Paper ThET7.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1028'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Learning Three-Dimensional Bin Packing with Adjustable-Order Semi-Online Setting
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416152" title="Click to go to the Author Index">
             Yin, Hao
            </a>
           </td>
           <td class="r">
            Southwest Jiaotong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416415" title="Click to go to the Author Index">
             Zhang, Chenxi
            </a>
           </td>
           <td class="r">
            Southwest Jiaotong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420321" title="Click to go to the Author Index">
             Chen, Fan
            </a>
           </td>
           <td class="r">
            Southwest Jiaotong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420315" title="Click to go to the Author Index">
             He, Hongjie
            </a>
           </td>
           <td class="r">
            Southwest Jiaotong University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1028" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#industrial_robots" title="Click to go to the Keyword Index">
               Industrial Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The online setting brings greater flexibility and practicality to the three-dimensional bin packing problem (3D-BPP) but at the cost of algorithm performance. Existing methods mitigate the performance impact by introducing semi-online settings with look-ahead or buffer zones. However, these methods either fail to fundamentally alter the packing order or reduce packing efficiency. This paper proposes a novel semi-online setting that allows for the observation of multiple items and the selection of one for packing, thereby adjusting the packing order without reducing packing efficiency. We do work for solving the semi-online packing problem via reinforcement learning which faces two real-world challenges: (1) a variable and difficult-to-predict number of observed items, and (2) the obstruction of robotic arm movement by already packed items. On the one hand, we design a policy network capable of adapting to variable item quantities. On the other hand, we introduce a guided bottom-up packing reward function to free up space for robotic arm motion. We show that our method outperforms the baselines in terms of space utilization with the condition of observing at least two items. Further experiments demonstrate the functionality of our reward function, which can guide a virtual robot to complete packing tasks.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet7_04">
             16:50-16:55, Paper ThET7.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1928'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Multiple Rotation Averaging with Constrained Reweighting Deep Matrix Factorization
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#365670" title="Click to go to the Author Index">
             Li, Shiqi
            </a>
           </td>
           <td class="r">
            Xi'an Jiaotong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#285126" title="Click to go to the Author Index">
             Zhu, Jihua
            </a>
           </td>
           <td class="r">
            Xi'an Jiaotong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#365633" title="Click to go to the Author Index">
             Xie, Yifan
            </a>
           </td>
           <td class="r">
            Xi'an Jiaotong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#372481" title="Click to go to the Author Index">
             Hu, Naiwen
            </a>
           </td>
           <td class="r">
            Xi'an Jiaotong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422581" title="Click to go to the Author Index">
             Zhu, Mingchen
            </a>
           </td>
           <td class="r">
            University of California, Davis
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#285411" title="Click to go to the Author Index">
             Li, Zhongyu
            </a>
           </td>
           <td class="r">
            Xi'an Jiaotong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#395630" title="Click to go to the Author Index">
             Wang, Di
            </a>
           </td>
           <td class="r">
            Xi'an Jiaotong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#351349" title="Click to go to the Author Index">
             Lu, Huimin
            </a>
           </td>
           <td class="r">
            Southeast University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1928" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Multiple rotation averaging plays a crucial role in computer vision and robotics domains. The conventional optimization-based methods optimize a nonlinear cost function based on certain noise assumptions, while most previous learning-based methods require ground truth labels in the supervised training process. Recognizing the handcrafted noise assumption may not be reasonable in all real-world scenarios, this paper proposes an effective rotation averaging method for mining data patterns in a learning manner while avoiding the requirement of labels. Specifically, we apply deep matrix factorization to directly solve the multiple rotation averaging problem in free linear space. For deep matrix factorization, we design a neural network model, which is explicitly low-rank and symmetric to better suit the background of multiple rotation averaging. Meanwhile, we utilize a spanning tree-based edge filtering to suppress the influence of rotation outliers. What's more, we also adopt a reweighting scheme and dynamic depth selection strategy to further improve the robustness. Our method synthesizes the merit of both optimization-based and learning-based methods. Experimental results on various datasets validate the effectiveness of our proposed method.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet7_05">
             16:55-17:00, Paper ThET7.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1987'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Magnetometer-Calibrated Hybrid Transformer for Robust Inertial Tracking in Robotics
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420115" title="Click to go to the Author Index">
             Zheng, Xinzhe
            </a>
           </td>
           <td class="r">
            The University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421083" title="Click to go to the Author Index">
             Ji, Sijie
            </a>
           </td>
           <td class="r">
            California Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#384385" title="Click to go to the Author Index">
             Pan, Yipeng
            </a>
           </td>
           <td class="r">
            The University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420314" title="Click to go to the Author Index">
             Zhang, Kaiwen
            </a>
           </td>
           <td class="r">
            The University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#131357" title="Click to go to the Author Index">
             Pan, Jia
            </a>
           </td>
           <td class="r">
            University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422686" title="Click to go to the Author Index">
             Wu, Chenshu
            </a>
           </td>
           <td class="r">
            The University of Hong Kong
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1987" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Inertial tracking is vital for autonomous robots and has gained popularity with the ubiquity of low-cost Inertial Measurement Units (IMUs) and deep learning-powered tracking algorithms. Existing works, however, have not fully utilized IMU measurements, particularly magnetometers, nor maximized the potential of deep learning to achieve the desired accuracy. To bridge the gap, we introduce NeurIT, which employs a Time-Frequency Block-recurrent Transformer (TF-BRT) at its core, combining RNN and Transformer to learn both time-frequency representative features. To fully utilize IMU information, we strategically employ differentiation of body-frame magnetometers for orientation calibration in a sensor fusion manner. Experiments conducted in diverse environments show that NeurIT maintains a mere 1-meter tracking error over a 300-meter distance, surpassing state-of-the-art baselines by 48.21% on unseen data. NeurIT also performs comparably to the visual-inertial approach (Tango Phone) in vision-favored conditions and surpasses it in plain environments. We share the code and data to promote further research: https://github.com/aiot-lab/NeurIT.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet7_06">
             17:00-17:05, Paper ThET7.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3064'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              MotionGlot: A Multi-Embodied Motion Generation Model
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#297562" title="Click to go to the Author Index">
             Harithas, Sudarshan S
            </a>
           </td>
           <td class="r">
            Brown University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#173768" title="Click to go to the Author Index">
             Sridhar, Srinath
            </a>
           </td>
           <td class="r">
            Brown University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3064" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#ai_based_methods" title="Click to go to the Keyword Index">
               AI-Based Methods
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#representation_learning" title="Click to go to the Keyword Index">
               Representation Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper introduces MotionGlot, a model that can generate motion across multiple embodiments with different action dimensions, such as quadruped robots and human bodies. By leveraging the well-established training procedures commonly used in large language models (LLMs), we introduce an instruction-tuning template specifically designed for motion related tasks. Our approach demonstrates that the principles underlying LLM training can be successfully adapted to learn a wide range of motion generation tasks across multiple embodiments with different action dimensions. We demonstrate the various abilities of MotionGlot on a set of 6 tasks and report an average improvement of 35.3% across tasks. Additionally, we contribute two new datasets: (1) a dataset of expert controlled quadruped locomotion with approximately 48,000 trajectories paired with direction-based text annotations, and (2) a dataset of over 23,000 situational text prompts for human motion generation tasks. Finally, we conduct hardware experiments to validate the capabilities of our system in real-world applications.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet7_07">
             17:05-17:10, Paper ThET7.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4395'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Retinex-BEVFormer: Using Retinex to Enhance Multi-View Image-Based BEV Detector in Low Light Scenes
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426542" title="Click to go to the Author Index">
             Liu, Xuan
            </a>
           </td>
           <td class="r">
            Beihang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#410836" title="Click to go to the Author Index">
             Xiong, Zhongxia
            </a>
           </td>
           <td class="r">
            Beihang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426617" title="Click to go to the Author Index">
             Yao, Ziying
            </a>
           </td>
           <td class="r">
            Beihang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#307333" title="Click to go to the Author Index">
             Wu, Xinkai
            </a>
           </td>
           <td class="r">
            Beihang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4395" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#intelligent_transportation_systems" title="Click to go to the Keyword Index">
               Intelligent Transportation Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Multi-view image-based BEV (Bird's Eye View) 3D perception is gaining attention as an alternative to high-cost LiDAR systems and has achieved notable success. However, there is a significant safety concern for future image-based BEV autonomous driving in low-light conditions (such as nighttime) while the limited research on BEV detectors for these scenes. In this paper, we attempt to enhance low-light BEV perception with illumination-guided feature fusion. We propose Retinex-BEVFormer, which uses illumination information generated by the Retinex theory to enhance the model's robustness to varying lighting conditions and improve detection performance in low-light scenes. Additionally, to address the illumination estimation discontinuity from multi-view images that can adversely affect detection, we propose the MVB-Retinex module, which balances illumination estimation by leveraging overlapping regions between adjacent images. Notably, our proposed method is a plug-and-play module that can be applied to any image-based BEV detector method and does not require any additional ground truth supervision. We conduct extensive experiments on the Nuscenes dataset, validating our algorithm in nighttime and daytime scenes. Compared to the baseline, our algorithm achieves a 2.9% increase in mAP on the validation set with minimal computational cost, especially showing a 3.6% improvement in nighttime scene. The experiments demonstrate that our Retinex-BEVFormer effectively improves detection performance in low-light conditions and enhances performance under normal illumination, indicating increased robustness of the BEV detector.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thet8">
             <b>
              ThET8
             </b>
            </a>
           </td>
           <td class="r">
            311
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thet8" title="Click to go to the Program at a Glance">
             <b>
              Collision Avoidance 2
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#192735" title="Click to go to the Author Index">
             Bylard, Andrew
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet8_01">
             16:35-16:40, Paper ThET8.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('869'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Reactive Collision Avoidance for Safe Agile Navigation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#320007" title="Click to go to the Author Index">
             Saviolo, Alessandro
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#398232" title="Click to go to the Author Index">
             Picello, Niko
            </a>
           </td>
           <td class="r">
            University of Padova
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#297802" title="Click to go to the Author Index">
             Mao, Jeffrey
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419641" title="Click to go to the Author Index">
             Verma, Rishabh
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#142369" title="Click to go to the Author Index">
             Loianno, Giuseppe
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab869" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__perception_and_autonomy" title="Click to go to the Keyword Index">
               Aerial Systems: Perception and Autonomy
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__mechanics_and_control" title="Click to go to the Keyword Index">
               Aerial Systems: Mechanics and Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Reactive collision avoidance is essential for agile robots navigating complex and dynamic environments, enabling real-time obstacle response. However, this task is inherently challenging because it requires a tight integration of perception, planning, and control, which traditional methods often handle separately, resulting in compounded errors and delays. This paper introduces a novel approach that unifies these tasks into a single reactive framework using solely onboard sensing and computing. Our method combines nonlinear model predictive control with adaptive control barrier functions, directly linking perception-driven constraints to real-time planning and control. Constraints are determined by using a neural network to refine noisy RGB-D data, enhancing depth accuracy, and selecting points with the minimum time-to-collision to prioritize the most immediate threats. To maintain a balance between safety and agility, a heuristic dynamically adjusts the optimization process, preventing overconstraints in real time. Extensive experiments with an agile quadrotor demonstrate effective collision avoidance across diverse indoor and outdoor environments, without requiring environment-specific tuning or explicit mapping.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet8_02">
             16:40-16:45, Paper ThET8.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('881'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Hardware-Accelerated Ray Tracing for Discrete and Continuous Collision Detection on GPUs
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#418819" title="Click to go to the Author Index">
             Sui, Sizhe
            </a>
           </td>
           <td class="r">
            University of Texas, Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#110037" title="Click to go to the Author Index">
             Sentis, Luis
            </a>
           </td>
           <td class="r">
            The University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#192735" title="Click to go to the Author Index">
             Bylard, Andrew
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab881" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#collision_avoidance" title="Click to go to the Keyword Index">
               Collision Avoidance
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computational_geometry" title="Click to go to the Keyword Index">
               Computational Geometry
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a set of simple and intuitive robot collision detection algorithms that show substantial scaling improvements for high geometric complexity and large numbers of collision queries by leveraging hardware-accelerated ray tracing on GPUs. It is the first leveraging hardware-accelerated ray-tracing for direct volume mesh-to-mesh discrete collision detection and applying it to continuous collision detection. We introduce two methods: Ray-Traced Discrete-Pose Collision Detection for exact robot mesh to obstacle mesh collision detection, and Ray-Traced Continuous Collision Detection for robot sphere representation to obstacle mesh swept collision detection, using piecewise-linear or quadratic B-splines. For robot link meshes totaling 24k triangles and obstacle meshes of over 190k triangles, our methods were up to 2.8 times faster in batched discrete-pose queries than a state-of-the-art GPU-based method using a sphere robot representation. For the same obstacle mesh scene, our sphere-robot continuous collision detection was up to 7 times faster depending on trajectory batch size. We also performed detailed measurements of the volume coverage accuracy of various sphere/mesh pose/path representations to provide insight into the tradeoffs between speed and accuracy of different robot collision detection methods.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet8_03">
             16:45-16:50, Paper ThET8.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1737'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Collision Avoidance in Model Predictive Control Using Velocity Damper
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#386987" title="Click to go to the Author Index">
             Haffemayer, Arthur
            </a>
           </td>
           <td class="r">
            LAAS-CNRS
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#296707" title="Click to go to the Author Index">
             Jordana, Armand
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#296608" title="Click to go to the Author Index">
             De Matteïs, Ludovic
            </a>
           </td>
           <td class="r">
            LAAS-CNRS
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#386988" title="Click to go to the Author Index">
             Wojciechowski, Krzysztof
            </a>
           </td>
           <td class="r">
            LAAS-CNRS
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#107817" title="Click to go to the Author Index">
             Righetti, Ludovic
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101933" title="Click to go to the Author Index">
             Lamiraux, Florent
            </a>
           </td>
           <td class="r">
            CNRS
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#103132" title="Click to go to the Author Index">
             Mansard, Nicolas
            </a>
           </td>
           <td class="r">
            CNRS
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1737" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#collision_avoidance" title="Click to go to the Keyword Index">
               Collision Avoidance
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We propose an advanced method for controlling the motion of a manipulator robot with strict collision avoidance in dynamic environments, leveraging a velocity damper constraint. Unlike conventional distance-based constraints, which tend to saturate near obstacles to reach optimality, the velocity damper constraint considers both distance and relative velocity, ensuring a safer separation. This constraint is incorporated into a model predictive control framework and enforced as a hard constraint through analytical derivatives supplied to the numerical solver. The approach has been fully implemented on a Franka Emika Panda robot and validated through experimental trials, demonstrating effective collision avoidance during dynamic tasks and robustness to unmodeled disturbances. An efficient open-source implementation along examples are provided here: url{https://gepettoweb.laas.fr/articles/haffemayer2025.html}.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet8_04">
             16:50-16:55, Paper ThET8.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1802'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              On the Synthesis of Reactive Collision-Free Whole-Body Robot Motions: A Complementarity-Based Approach
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#355185" title="Click to go to the Author Index">
             Yao, Haowen
            </a>
           </td>
           <td class="r">
            Technical Univerity of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#238518" title="Click to go to the Author Index">
             Laha, Riddhiman
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#205992" title="Click to go to the Author Index">
             Sinha, Anirban
            </a>
           </td>
           <td class="r">
            GE Aerospace Research
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#335157" title="Click to go to the Author Index">
             Hall, Jonas
            </a>
           </td>
           <td class="r">
            Boston University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#156255" title="Click to go to the Author Index">
             Figueredo, Luis
            </a>
           </td>
           <td class="r">
            University of Nottingham (UoN)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#107599" title="Click to go to the Author Index">
             Chakraborty, Nilanjan
            </a>
           </td>
           <td class="r">
            Stony Brook University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#108317" title="Click to go to the Author Index">
             Haddadin, Sami
            </a>
           </td>
           <td class="r">
            Mohamed Bin Zayed University of Artificial Intelligence
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1802" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#whole_body_motion_planning_and_control" title="Click to go to the Keyword Index">
               Whole-Body Motion Planning and Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reactive_and_sensor_based_planning" title="Click to go to the Keyword Index">
               Reactive and Sensor-Based Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper is about generating motion plans for high degree-of-freedom systems that account for both static and dynamic collisions along the entire body. A particular class of mathematical programs with complementarity constraints become useful in this regard. Optimization-based planners can tackle confined space trajectory planning while being cognizant of robot and (mostly static) obstacle constraints. However, handling moving obstacles is non-trivial in a real-time setting. To this end, we present the FLIQC (Fast LInear Quadratic Complementarity based) motion planner. Our reactive planner employs a novel motion model that captures the entire rigid robot as well as the obstacle geometry and ensures non- penetration between the surfaces due to the imposed constraint. We perform thorough comparative studies with the state-of- the-art, which demonstrate improved performance. Extensive simulation and hardware experiments validate our claim of generating continuous and real-time motion plans at 1 kHz for modern collaborative robots with constant minimal parameters.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet8_05">
             16:55-17:00, Paper ThET8.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1991'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Rapid Dynamic Obstacle Avoidance for UAVs Enhanced by DVS and Neuromorphic Computing
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419899" title="Click to go to the Author Index">
             Wang, Siyang
            </a>
           </td>
           <td class="r">
            Xi'an Jiaotong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419892" title="Click to go to the Author Index">
             Yu, Sheng
            </a>
           </td>
           <td class="r">
            Xi'an Jiaotong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419887" title="Click to go to the Author Index">
             Liang, Tingbang
            </a>
           </td>
           <td class="r">
            Xi'an Jiaotong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419893" title="Click to go to the Author Index">
             Shi, Yilin
            </a>
           </td>
           <td class="r">
            Xi’an Jiaotong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422455" title="Click to go to the Author Index">
             Ma, Yongqiang
            </a>
           </td>
           <td class="r">
            Xi'an Jiaotong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#377403" title="Click to go to the Author Index">
             Ren, Pengju
            </a>
           </td>
           <td class="r">
            Xi'an Jiaotong University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1991" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#collision_avoidance" title="Click to go to the Keyword Index">
               Collision Avoidance
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__applications" title="Click to go to the Keyword Index">
               Aerial Systems: Applications
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#force_control" title="Click to go to the Keyword Index">
               Force Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Achieving rapid and accurate dynamic obstacle avoidance is crucial for enhancing the survivability of unmanned aerial vehicles (UAVs) in hazardous conditions. To accomplish dynamic obstacle avoidance, sensors with high temporal resolution and efficient processing models are required. Dynamic vision sensors (DVS) fulfill the sensing requirements, while spiking neural networks (SNNs) address the processing demands. In this paper, we develop an end-to-end obstacle avoidance algorithm for UAVs using only a single monocular DVS as the sensor and further enhance accuracy and speed through our proposed mechanisms. The algorithm consists of three components: ego-motion compensation, an SNN model for movement analysis, and a force filter inspired by spiking neurons. In movement analysis, we propose the temporal potential pooling (TPP) and incremental event (EI) mechanisms to accelerate our SNN model. The real-flight experiments confirm that our algorithm achieves approximately 90% accuracy with a processing latency as low as 4ms on a GPU, surpassing state-of-the-art methods. Ablation studies show that the proposed method maintains high accuracy in movement detection while significantly reducing computational time. Our method operates in real-time, achieves high accuracy, and is feasible across a wide range of environments. Our code is available at https://github.com/AmperiaWang/oanet_s1 for reproducibility.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet8_06">
             17:00-17:05, Paper ThET8.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2239'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Efficient Collision Detection Framework for Enhancing Collision-Free Robot Motion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422558" title="Click to go to the Author Index">
             Zhu, Xiankun
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#369777" title="Click to go to the Author Index">
             Xin, Yucheng
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#308276" title="Click to go to the Author Index">
             Li, Shoujie
            </a>
           </td>
           <td class="r">
            Tsinghua Shenzhen International Graduate School
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#189167" title="Click to go to the Author Index">
             Liu, Houde
            </a>
           </td>
           <td class="r">
            Shenzhen Graduate School, Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#210797" title="Click to go to the Author Index">
             Xia, Chongkun
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#241749" title="Click to go to the Author Index">
             Liang, Bin
            </a>
           </td>
           <td class="r">
            Center for Artificial Intelligence and Robotics, Graduate School
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2239" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#collision_avoidance" title="Click to go to the Keyword Index">
               Collision Avoidance
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#integrated_planning_and_learning" title="Click to go to the Keyword Index">
               Integrated Planning and Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reactive_and_sensor_based_planning" title="Click to go to the Keyword Index">
               Reactive and Sensor-Based Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Fast and efficient collision detection is essential for motion generation in robotics. In this paper, we propose an efficient collision detection framework based on the Signed Distance Field (SDF) of robots, seamlessly integrated with a self-collision detection module. Firstly, we decompose the robot's SDF using forward kinematics and leverage multiple extremely lightweight networks in parallel to efficiently approximate the SDF. Moreover, we introduce support vector machines to integrate the self-collision detection module into the framework, which we refer to as the SDF-SC framework. Using statistical features, our approach unifies the representation of collision distance for both SDF and self-collision detection. During this process, we maintain and utilize the differentiable properties of the framework to optimize collision-free robot trajectories. Finally, we develop a reactive motion controller based on our framework, enabling real-time avoidance of multiple dynamic obstacles. While maintaining high accuracy, our framework achieves inference speeds up to five times faster than previous methods. Experimental results on the Franka robotic arm demonstrate the effectiveness of our approach.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet8_07">
             17:05-17:10, Paper ThET8.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3166'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Differentiable Composite Neural Signed Distance Fields for Robot Navigation in Dynamic Indoor Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#407984" title="Click to go to the Author Index">
             Bukhari, Syed Talha
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#344972" title="Click to go to the Author Index">
             Lawson, Daniel
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#201200" title="Click to go to the Author Index">
             Qureshi, Ahmed H.
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3166" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Neural Signed Distance Fields (SDFs) provide a differentiable environment representation to readily obtain collision checks and well-defined gradients for robot navigation tasks. However, updating neural SDFs as the scene evolves entails re-training, which is tedious, time consuming, and inefficient, making it unsuitable for robot navigation with limited field-of-view in dynamic environments. Towards this objective, we propose a compositional framework of neural SDFs to solve robot navigation in indoor environments using only an onboard RGB-D sensor. Our framework embodies a dual mode procedure for trajectory optimization, with different modes using complementary methods of modeling collision costs and collision avoidance gradients. The primary stage queries the robot body's SDF, swept along the route to goal, at the obstacle point cloud, enabling swift local optimization of trajectories. The secondary stage infers the visible scene's SDF by aligning and composing the SDF representations of its constituents, providing better informed costs and gradients for trajectory optimization. The dual mode procedure combines the best of both stages, achieving a success rate of 98%, 14.4% higher than baseline with comparable amortized plan time on iGibson 2.0. We also demonstrate its effectiveness in adapting to real-world indoor scenarios.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet8_08">
             17:10-17:15, Paper ThET8.8
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4781'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              On the Evaluation of Collision Probability Along a Path
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#380838" title="Click to go to the Author Index">
             Paiola, Lorenzo
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#108951" title="Click to go to the Author Index">
             Grioli, Giorgio
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#103420" title="Click to go to the Author Index">
             Bicchi, Antonio
            </a>
           </td>
           <td class="r">
            Fondazione Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4781" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#risk" title="Click to go to the Keyword Index">
               Risk
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#collision_avoidance" title="Click to go to the Keyword Index">
               Collision Avoidance
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#probability_and_statistical_methods" title="Click to go to the Keyword Index">
               Probability and Statistical Methods
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#robot_safety" title="Click to go to the Keyword Index">
               Robot Safety
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Characterizing the risk of operations is a fundamental requirement in robotics, and a crucial ingredient of safe planning. The problem is multifaceted, with multiple definitions arising in the vast recent literature fitting different application scenarios and leading to different computational approaches. A basic element shared by most frameworks is the definition and evaluation of the probability of collision for a mobile object in an environment with obstacles. We observe that, even in basic cases, different interpretations are possible. This paper proposes an index we call Risk Density, which offers a theoretical link between conceptually distant assumptions about the interplay of single collision events along a continuous path. We show how this index can be used to approximate the collision probability in the case where the robot evolves along a nominal continuous curve from random initial conditions. Indeed under this hypothesis the proposed approximation outperforms some well-established methods either in accuracy or computational cost.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thet9">
             <b>
              ThET9
             </b>
            </a>
           </td>
           <td class="r">
            312
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thet9" title="Click to go to the Program at a Glance">
             <b>
              Task and Motion Planning 4
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#167990" title="Click to go to the Author Index">
             Bera, Aniket
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet9_01">
             16:35-16:40, Paper ThET9.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('606'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Fast and Accurate Task Planning Using Neuro-Symbolic Language Models and Multi-Level Goal Decomposition
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#394864" title="Click to go to the Author Index">
             Kwon, Minseo
            </a>
           </td>
           <td class="r">
            Ewha Womans University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#202844" title="Click to go to the Author Index">
             Kim, Yaesol
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104431" title="Click to go to the Author Index">
             Kim, Young J.
            </a>
           </td>
           <td class="r">
            Ewha Womans University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab606" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#task_planning" title="Click to go to the Keyword Index">
               Task Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#task_and_motion_planning" title="Click to go to the Keyword Index">
               Task and Motion Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In robotic task planning, symbolic planners using rule-based representations like PDDL are effective but struggle with long-sequential tasks in complicated environments due to exponentially increasing search space. Meanwhile, LLM-based approaches, which are grounded in artificial neural networks, offer faster inference and commonsense reasoning but suffer from lower success rates. To address the limitations of the current symbolic (slow speed) or LLM-based approaches (low accuracy), we propose a novel neuro-symbolic task planner that decomposes complex tasks into subgoals using LLM and carries out task planning for each subgoal using either symbolic or MCTS-based LLM planners, depending on the subgoal complexity. This decomposition reduces planning time and improves success rates by narrowing the search space and enabling LLMs to focus on more manageable tasks. Our method significantly reduces planning time while maintaining high success rates across task planning domains, as well as real-world and simulated robotics environments. More details are available at http://graphics.ewha.ac.kr/LLMTAMP/.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet9_02">
             16:40-16:45, Paper ThET9.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('773'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              OpenBench: A New Benchmark and Baseline for Semantic Navigation in Smart Logistics
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#323453" title="Click to go to the Author Index">
             Wang, Junhui
            </a>
           </td>
           <td class="r">
            Macau University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#397724" title="Click to go to the Author Index">
             Huo, Dongjie
            </a>
           </td>
           <td class="r">
            Beijing University of Chemical Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#418725" title="Click to go to the Author Index">
             Xu, ZeHui
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#338957" title="Click to go to the Author Index">
             Shi, Yongliang
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#418729" title="Click to go to the Author Index">
             Yan, Yimin
            </a>
           </td>
           <td class="r">
            University of Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#337828" title="Click to go to the Author Index">
             Wang, Yuanxin
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#204100" title="Click to go to the Author Index">
             Gao, Chao
            </a>
           </td>
           <td class="r">
            University of Cambridge
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#144434" title="Click to go to the Author Index">
             Qiao, Yan
            </a>
           </td>
           <td class="r">
            Macau University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#165639" title="Click to go to the Author Index">
             Zhou, Guyue
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab773" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#autonomous_vehicle_navigation" title="Click to go to the Keyword Index">
               Autonomous Vehicle Navigation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#task_and_motion_planning" title="Click to go to the Keyword Index">
               Task and Motion Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#engineering_for_robotic_systems" title="Click to go to the Keyword Index">
               Engineering for Robotic Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The increasing demand for efficient last-mile delivery in smart logistics underscores the role of autonomous robots in enhancing operational efficiency and reducing costs. Traditional navigation methods, which depend on high-precision maps, are resource-intensive, while learning-based approaches often struggle with generalization in real-world scenarios. To address these challenges, this work proposes the Openstreetmap-enhanced oPen-air sEmantic Navigation (OPEN) system that combines foundation models with classic algorithms for scalable outdoor navigation. The system leverages OpenStreetMap (OSM) for flexible map representation, thereby eliminating the need for extensive pre-mapping efforts. It also employs Large Language Models (LLMs) to comprehend delivery instructions and Vision-Language Models (VLMs) for global localization, map updates, and house number recognition. To compensate the limitations of existing benchmarks that are inadequate for assessing last-mile delivery, this work introduces a new benchmark specifically designed for outdoor navigation in residential areas, reflecting the real-world challenges faced by autonomous delivery systems. Extensive experiments validate the effectiveness of the proposed system in enhancing navigation efficiency and reliability. To facilitate further research, our code and benchmark are publicly available.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet9_03">
             16:45-16:50, Paper ThET9.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1800'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              KARMA: Augmenting Embodied AI Agents with Long-And-Short Term Memory Systems
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#413328" title="Click to go to the Author Index">
             Wang, Zixuan
            </a>
           </td>
           <td class="r">
            Institute of Automation, Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#223416" title="Click to go to the Author Index">
             Yu, Bo
            </a>
           </td>
           <td class="r">
            Shenzhen Institute of Artificial Intelligence and Robotics for S
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421972" title="Click to go to the Author Index">
             Zhao, Junzhe
            </a>
           </td>
           <td class="r">
            Alibaba
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422321" title="Click to go to the Author Index">
             Sun, Wenhao
            </a>
           </td>
           <td class="r">
            Institute of Computing Technology, Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422268" title="Click to go to the Author Index">
             Hou, Sai
            </a>
           </td>
           <td class="r">
            Beijing Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#196330" title="Click to go to the Author Index">
             Liang, Shuai
            </a>
           </td>
           <td class="r">
            Institute of Computing Technology, Chinese Academy of Sciences (
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421970" title="Click to go to the Author Index">
             Hu, Xing
            </a>
           </td>
           <td class="r">
            Institute of Computing Technology, Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#223647" title="Click to go to the Author Index">
             Han, Yinhe
            </a>
           </td>
           <td class="r">
            Institute of Computing Technology, Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421945" title="Click to go to the Author Index">
             Gan, Yiming
            </a>
           </td>
           <td class="r">
            Institute of Computing Technology, Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1800" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#ai_based_methods" title="Click to go to the Keyword Index">
               AI-Based Methods
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#task_planning" title="Click to go to the Keyword Index">
               Task Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             负责执行互连的、长序列的家务任务经常面临上下文记忆的困难，导致任务执行效率低下和错误。为了解决这个问题，我们引入了 KARMA，一种创新的记忆系统它集成了长期和短期记忆模块，增强大型语言模型 （LLM） 以进行规划通过记忆增强提示进行具身代理。 业 区分长期记忆和短期记忆，其中长期记忆捕获全面的 3D 场景图作为环境的表示，而短期记忆动态记录对象位置的变化，并且 国家。 这种双重记忆结构允许代理检索相关的过去场景体验，从而提高任务规划的准确性和效率。 短期 内存采用有效和自适应内存替换的策略，确保保留关键信息同时丢弃不太相关的数据
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet9_04">
             16:50-16:55, Paper ThET9.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2469'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Socratic Planner: Self-QA-Based Zero-Shot Planning for Embodied Instruction Following
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#351688" title="Click to go to the Author Index">
             Shin, Suyeon
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#312294" title="Click to go to the Author Index">
             Jeon, Sujin
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#351447" title="Click to go to the Author Index">
             Kim, Junghyun
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#351450" title="Click to go to the Author Index">
             Kang, Gi-Cheon
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#133606" title="Click to go to the Author Index">
             Zhang, Byoung-Tak
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2469" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#task_and_motion_planning" title="Click to go to the Keyword Index">
               Task and Motion Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#ai_based_methods" title="Click to go to the Keyword Index">
               AI-Based Methods
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#task_planning" title="Click to go to the Keyword Index">
               Task Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Embodied Instruction Following (EIF) is the task of executing natural language instructions by navigating and interacting with objects in interactive environments. A key challenge in EIF is compositional task planning, typically addressed through supervised learning or few-shot in-context learning with labeled data. To this end, we introduce the Socratic Planner, a self-QA-based zero-shot planning method that infers an appropriate plan without any further training. The Socratic Planner first facilitates the Large Language Model (LLM) in performing self-questioning and answering, which in turn helps generate a sequence of subgoals. While executing the subgoals, an embodied agent may encounter unexpected situations, such as unforeseen obstacles. The Socratic Planner then adjusts plans based on dense visual feedback through a visually-grounded re-planning mechanism. Experiments demonstrate the effectiveness of the Socratic Planner, outperforming current state-of-the-art planning models on the ALFRED benchmark across all metrics, particularly excelling in long-horizon tasks that demand complex inference. We further demonstrate real-world applicability through deployment on a physical robot.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet9_05">
             16:55-17:00, Paper ThET9.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4228'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Hypergraph-Based Coordinated Task Allocation and Socially-Aware Navigation for Multi-Robot Systems
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#324108" title="Click to go to the Author Index">
             Wang, Weizheng
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#167990" title="Click to go to the Author Index">
             Bera, Aniket
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#164680" title="Click to go to the Author Index">
             Min, Byung-Cheol
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4228" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#task_and_motion_planning" title="Click to go to the Keyword Index">
               Task and Motion Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             A team of multiple robots seamlessly and safely working in human-filled public environments requires adaptive task allocation and socially-aware navigation that account for dynamic human behavior. Current approaches struggle with highly dynamic pedestrian movement and the need for flexible task allocation. We propose Hyper-SAMARL, a hypergraph-based system for multi-robot task allocation and socially-aware navigation, leveraging multi-agent reinforcement learning (MARL). Hyper-SAMARL models the environmental dynamics between robots, humans, and points of interest (POIs) using a hypergraph, enabling adaptive task assignment and socially-compliant navigation through a hypergraph diffusion mechanism. Our framework, trained with MARL, effectively captures interactions between robots and humans, adapting tasks based on real-time changes in human activity. Experimental results demonstrate that Hyper-SAMARL outperforms baseline models in terms of social navigation, task completion efficiency, and adaptability in various simulated scenarios.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet9_06">
             17:00-17:05, Paper ThET9.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4536'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Bootstrapping Object-Level Planning with Large Language Models
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#192313" title="Click to go to the Author Index">
             Paulius, David
            </a>
           </td>
           <td class="r">
            Brown University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#115697" title="Click to go to the Author Index">
             Agostini, Alejandro
            </a>
           </td>
           <td class="r">
            University of Innsbruck
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#354573" title="Click to go to the Author Index">
             Quartey, Benedict
            </a>
           </td>
           <td class="r">
            Brown University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#106428" title="Click to go to the Author Index">
             Konidaris, George
            </a>
           </td>
           <td class="r">
            Brown University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4536" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#task_and_motion_planning" title="Click to go to the Keyword Index">
               Task and Motion Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#ai_based_methods" title="Click to go to the Keyword Index">
               AI-Based Methods
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#task_planning" title="Click to go to the Keyword Index">
               Task Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We introduce a new method that extracts knowledge from a large language model (LLM) to produce object-level plans, which describe high-level changes to object state, and uses them to bootstrap task and motion planning (TAMP). Existing work uses LLMs to directly output task plans or generate goals in representations like PDDL. However, these methods fall short because they rely on the LLM to do the actual planning or output a hard-to-satisfy goal. Our approach instead extracts knowledge from an LLM in the form of plan schemas as an object-level representation called functional object-oriented networks (FOON), from which we automatically generate PDDL subgoals. Our method markedly outperforms alternative planning strategies in completing several pick-and-place tasks in simulation.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet9_07">
             17:05-17:10, Paper ThET9.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4771'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              GPT-4V(ision) for Robotics: Multimodal Task Planning from Human Demonstration
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#223806" title="Click to go to the Author Index">
             Wake, Naoki
            </a>
           </td>
           <td class="r">
            Microsoft
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#346725" title="Click to go to the Author Index">
             Kanehira, Atsushi
            </a>
           </td>
           <td class="r">
            Microsoft
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#165824" title="Click to go to the Author Index">
             Sasabuchi, Kazuhiro
            </a>
           </td>
           <td class="r">
            Microsoft
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#103862" title="Click to go to the Author Index">
             Takamatsu, Jun
            </a>
           </td>
           <td class="r">
            Microsoft
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101287" title="Click to go to the Author Index">
             Ikeuchi, Katsushi
            </a>
           </td>
           <td class="r">
            Microsoft
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4771" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#task_and_motion_planning" title="Click to go to the Keyword Index">
               Task and Motion Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#task_planning" title="Click to go to the Keyword Index">
               Task Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We introduce a pipeline that enhances a general-purpose Vision Language Model, GPT-4V(ision), to facilitate one-shot visual teaching for robotic manipulation. This system analyzes videos of humans performing tasks and outputs executable robot programs that incorporate insights into affordances. The process begins with GPT-4V analyzing the videos to obtain textual explanations of environmental and action details. A GPT-4-based task planner then encodes these details into a symbolic task plan. Subsequently, vision systems spatially and temporally ground the task plan in the videos—objects are identified using an open-vocabulary object detector, and hand-object interactions are analyzed to pinpoint moments of grasping and releasing. This spatiotemporal grounding allows for the gathering of affordance information (e.g., grasp types, waypoints, and body postures) critical for robot execution. Experiments across various scenarios demonstrate the method's efficacy in achieving real robots' operations from human demonstrations in a one-shot manner. Meanwhile, quantitative tests have revealed instances of hallucination in GPT-4V, highlighting the importance of incorporating human supervision within the pipeline. The prompts of GPT-4V/GPT-4 are available at this project page: https://microsoft.github.io/GPT4Vision-Robot-Manipulation-Prompts/
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet9_08">
             17:10-17:15, Paper ThET9.8
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4947'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Action Contextualization: Adaptive Task Planning and Action Tuning Using Large Language Models
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#288618" title="Click to go to the Author Index">
             Gupta, Sthithpragya
            </a>
           </td>
           <td class="r">
            Ecole Polytechnique Federale De Lausanne
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#255033" title="Click to go to the Author Index">
             Yao, Kunpeng
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#334874" title="Click to go to the Author Index">
             Niederhauser, Loïc
            </a>
           </td>
           <td class="r">
            EPFL
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#106862" title="Click to go to the Author Index">
             Billard, Aude
            </a>
           </td>
           <td class="r">
            EPFL
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4947" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#task_and_motion_planning" title="Click to go to the Keyword Index">
               Task and Motion Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#task_planning" title="Click to go to the Keyword Index">
               Task Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#ai_based_methods" title="Click to go to the Keyword Index">
               AI-Based Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Large Language Models (LLMs) present a promising frontier in robotic task planning by leveraging extensive human knowledge. Nevertheless, the current literature often overlooks the critical aspects of robots' adaptability and error correction. This work aims to overcome this limitation by enabling robots to modify their motions and select the most suitable task plans based on the context. We introduce a novel framework to achieve action contextualization, aimed at tailoring robot actions to the context of specific tasks, thereby enhancing adaptability through applying LLM-derived contextual insights. Our framework integrates motion metrics that evaluate robot performances for each motion to resolve redundancy in planning. Moreover, it supports online feedback between the robot and the LLM, enabling immediate modifications to the task plans and corrections of errors. An overall success rate of 81.25% has been achieved through extensive experimental validation. Finally, when integrated with dynamical system (DS)-based robot controllers, the robotic arm-hand system demonstrates its proficiency in autonomously executing LLM-generated motion plans for sequential table-clearing tasks, rectifying errors without human intervention, and showcasing robustness against external disturbances. Our proposed framework also features the potential to be integrated with modular control approaches, significantly enhancing robots' adaptability and autonomy in performing sequential tasks in the real world.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thet10">
             <b>
              ThET10
             </b>
            </a>
           </td>
           <td class="r">
            313
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thet10" title="Click to go to the Program at a Glance">
             <b>
              Multi-Robot Systems and Tools
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#107102" title="Click to go to the Author Index">
             Goldberg, Ken
            </a>
           </td>
           <td class="r">
            UC Berkeley
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet10_01">
             16:35-16:40, Paper ThET10.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('828'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              CognitiveOS: Large Multimodal Model Based System to Endow Any Type of Robot with Generative AI
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#368162" title="Click to go to the Author Index">
             Lykov, Artem
            </a>
           </td>
           <td class="r">
            Skolkovo Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#378116" title="Click to go to the Author Index">
             Konenkov, Mikhail
            </a>
           </td>
           <td class="r">
            Skolkovo Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#395968" title="Click to go to the Author Index">
             Gbagbe, Koffivi Fidele
            </a>
           </td>
           <td class="r">
            Skolkovo Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#395970" title="Click to go to the Author Index">
             Litvinov, Mikhail
            </a>
           </td>
           <td class="r">
            Skolkovo Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#367883" title="Click to go to the Author Index">
             Davletshin, Denis
            </a>
           </td>
           <td class="r">
            Skolkovo Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#241638" title="Click to go to the Author Index">
             Fedoseev, Aleksey
            </a>
           </td>
           <td class="r">
            Skolkovo Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#218091" title="Click to go to the Author Index">
             Altamirano Cabrera, Miguel
            </a>
           </td>
           <td class="r">
            Skolkovo Institute of Science and Technology (Skoltech), Moscow,
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#396001" title="Click to go to the Author Index">
             Peter Vimalathas, Robinroy
            </a>
           </td>
           <td class="r">
            Intelligent Space Robotics Laboratory, Skolkovo Institute of Sci
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#224247" title="Click to go to the Author Index">
             Tsetserukou, Dzmitry
            </a>
           </td>
           <td class="r">
            Skolkovo Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab828" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#cognitive_control_architectures" title="Click to go to the Keyword Index">
               Cognitive Control Architectures
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multi_modal_perception_for_hri" title="Click to go to the Keyword Index">
               Multi-Modal Perception for HRI
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#cooperating_robots" title="Click to go to the Keyword Index">
               Cooperating Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper introduces CognitiveOS, the first operating system designed for cognitive robots capable of functioning across diverse robotic platforms. CognitiveOS is structured as a multi-agent system comprising modules built upon a transformer architecture, facilitating communication through an internal monologue format. These modules collectively empower the robot to tackle intricate real-world tasks. The paper delineates the operational principles of the system along with descriptions of its nine distinct modules. The modular design endows the system with distinctive advantages over traditional end-to-end methodologies, notably in terms of adaptability and scalability. The system's modules are configurable, modifiable, or deactivatable depending on the task requirements, while new modules can be seamlessly integrated. This system serves as a foundational resource for researchers and developers in the cognitive robotics domain, alleviating the burden of constructing a cognitive robot system from scratch. Experimental findings demonstrate the system's advanced task comprehension and adaptability across varied tasks, robotic platforms, and module configurations, underscoring its potential for real-world applications. Moreover, in the category of Reasoning it outperformed CognitiveDog (by 15%) and RT2 (by 31%), achieving the highest to date rate of 77%. We provide a code repository and dataset for the replication of CognitiveOS: https://github.com/Arcwy0/cognitiveos
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet10_02">
             16:40-16:45, Paper ThET10.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1156'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              CLSTR: Capability-Level System for Tracking Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#302645" title="Click to go to the Author Index">
             Bejarano, Alexandra
            </a>
           </td>
           <td class="r">
            Colorado School of Mines
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#194717" title="Click to go to the Author Index">
             Bonial, Claire
            </a>
           </td>
           <td class="r">
            US Army Research Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#174132" title="Click to go to the Author Index">
             Williams, Tom
            </a>
           </td>
           <td class="r">
            Colorado School of Mines
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1156" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             For human operators to effectively task teams of robots, it is critical that they maintain situational awareness about the status of those robots. However, maintaining this situational awareness becomes particularly difficult when there are dynamic changes not only in the members of the robot team, but also in the capabilities of those robots. Prior work has shown that situational awareness can be supported through interfaces that effectively visualize task-relevant information. As such, in this work, we introduce a Capability-Level System for Tracking Robots (CLSTR), a new visualization for supporting operators to maintain an appropriate level of situational awareness over the capabilities of a dynamic robot team. In evaluating CLSTR through an online human-subject study (n=123), we found that a combination of different visual elements within an interface like the use of icons to summarize robot capabilities and animations to indicate team changes can help operators maintain awareness over robot teams.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet10_03">
             16:45-16:50, Paper ThET10.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1934'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Mitigating Side Effects in Multi-Agent Systems Using Blame Assignment
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#226226" title="Click to go to the Author Index">
             Rustagi, Pulkit
            </a>
           </td>
           <td class="r">
            Oregon State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#245656" title="Click to go to the Author Index">
             Saisubramanian, Sandhya
            </a>
           </td>
           <td class="r">
            Oregon State University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1934" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#planning_under_uncertainty" title="Click to go to the Keyword Index">
               Planning under Uncertainty
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#path_planning_for_multiple_mobile_robots_or_agents" title="Click to go to the Keyword Index">
               Path Planning for Multiple Mobile Robots or Agents
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             When independently trained or designed robots are deployed in a shared environment, their combined actions can lead to unintended negative side effects (NSEs). To ensure safe and efficient operation, robots must optimize task performance while minimizing the penalties associated with NSEs, balancing individual objectives with collective impact. We model the problem of mitigating NSEs in a cooperative multi-agent system as a bi-objective lexicographic decentralized Markov decision process. We assume independence of transitions and rewards with respect to the robots' tasks, but the joint NSE penalty creates a form of dependence in this setting. To improve scalability, the joint NSE penalty is decomposed into individual penalties for each robot using credit assignment, which facilitates decentralized policy computation. We empirically demonstrate, using mobile robots and in simulation, the effectiveness and scalability of our approach in mitigating NSEs. Code:
             <i>
              https://tinyurl.com/RECON-NSE-Mitigation
             </i>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet10_04">
             16:50-16:55, Paper ThET10.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2491'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Decentralized Drone Swaps for Online Rebalancing of Drone Delivery Tasks
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#365758" title="Click to go to the Author Index">
             Vakil, Kamran
            </a>
           </td>
           <td class="r">
            Boston University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#167155" title="Click to go to the Author Index">
             Pierson, Alyssa
            </a>
           </td>
           <td class="r">
            Boston University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2491" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#networked_robots" title="Click to go to the Keyword Index">
               Networked Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#sensor_networks" title="Click to go to the Keyword Index">
               Sensor Networks
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Recent research has seen the advancement of drone depot models as a promising way to allocate drones for large-scale task completion. Applications of these drone depot models include data collection, environmental monitoring, package delivery, and more. This paper focuses on sharing agents between static depots for task allocation based on expected demand. We model the problem as a Binary Nonlinear Program, then derive an iterative neighborhood search based on solving a series of Binary Linear Programs to drive towards the optimal configuration of agents for each depot. We show that our method is more tractable than a Branch and Bound approach for this model as problem complexity grows. We also show through simulations that with near optimal allocation between local depots, the overall system performance will outperform greedy and non-sharing approaches.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet10_05">
             16:55-17:00, Paper ThET10.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2716'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Fairness-Oriented Control Framework for Safety-Critical Multi-Robot Systems: Alternative Authority Control
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#338881" title="Click to go to the Author Index">
             Shi, Lei
            </a>
           </td>
           <td class="r">
            Johns Hopkins University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423286" title="Click to go to the Author Index">
             Liu, Qichao
            </a>
           </td>
           <td class="r">
            University of Wisconsin–Madison
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#190582" title="Click to go to the Author Index">
             Zhou, Cheng
            </a>
           </td>
           <td class="r">
            Tencent
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#181507" title="Click to go to the Author Index">
             Li, Xiong
            </a>
           </td>
           <td class="r">
            Tencent
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2716" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#intelligent_transportation_systems" title="Click to go to the Keyword Index">
               Intelligent Transportation Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#collision_avoidance" title="Click to go to the Keyword Index">
               Collision Avoidance
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper proposes a fair control framework for multi-robot systems, which integrates the newly introduced Alternative Authority Control (AAC) and Flexible Control Barrier Function (F-CBF). Control authority refers to a single robot which can plan its trajectory while considering others as moving obstacles, meaning the other robots do not have authority to plan their own paths. The AAC method dynamically distributes the control authority, enabling fair and coordinated movement across the system. This approach significantly improves computational efficiency, scalability, and robustness in complex environments. The proposed F-CBF extends traditional CBFs by incorporating obstacle shape, velocity, and orientation. F-CBF enhances safety by accurate dynamic obstacle avoidance. The framework is validated through simulations in multi-robot scenarios, demonstrating its safety, robustness and computational efficiency.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet10_06">
             17:00-17:05, Paper ThET10.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2758'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              FogROS2-PLR: Probabilistic Latency-Reliability for Cloud Robotics
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#300083" title="Click to go to the Author Index">
             Chen, Kaiyuan
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#205739" title="Click to go to the Author Index">
             Tian, Nan
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#378073" title="Click to go to the Author Index">
             Juette, Christian
            </a>
           </td>
           <td class="r">
            Bosch Research
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#340609" title="Click to go to the Author Index">
             Qiu, Tianshuang
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#269026" title="Click to go to the Author Index">
             Ren, Liu
            </a>
           </td>
           <td class="r">
            Robert Bosch North America Research Technology Center
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#238701" title="Click to go to the Author Index">
             Kubiatowicz, John
            </a>
           </td>
           <td class="r">
            UC Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#107102" title="Click to go to the Author Index">
             Goldberg, Ken
            </a>
           </td>
           <td class="r">
            UC Berkeley
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2758" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#networked_robots" title="Click to go to the Keyword Index">
               Networked Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#cellular_and_modular_robots" title="Click to go to the Keyword Index">
               Cellular and Modular Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#engineering_for_robotic_systems" title="Click to go to the Keyword Index">
               Engineering for Robotic Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Cloud robotics enables robots to offload complex computational tasks to cloud servers for performance, cost, and ease of management. However, the network and cloud computing infrastructure are not designed for reliable timing guarantee, leading to fluctuating Quality-of-Service (QoS). In this work, we formulate an impossibility triangle of latency reliability, singleton deployment and commodity hardware. The theorem implicates that providing replicated resources with uncorrelated failures exponentially reduces the probability of missing a deadline. We present FogROS2-Probabilistic Latency Reliability (RLR) that uses multiple independent network interfaces to send requests to replicated cloud resources and uses the first response back. We design routing mechanisms to discover, connect, and route through non-default network interfaces on robots. FogROS2-PLR optimizes the selection of interfaces to servers by minimizing the probability of missing a deadline. We conduct a cloud-connected driving experiment with two 5G service providers, demonstrating FogROS2-PLR effectively provides smooth service quality even if one of the service providers experiences low coverage and base station handover. We use 99 Percentile (P99) latency to evaluate anomalous long-tail latency behavior. In the experiment, FogROS2-PLR improves P99 latency by up to 3.7x compared to using one service provider. We deploy FogROS2-PLR on a physical Stretch 3 robot with an indoor human-tracking task. Even in a fully covered Wi-Fi and 5G environment, FogROS2-PLR improves the responsiveness of the robot by reducing 36% of mean latency and 33% P99 latency. Code and supplementary can be found on website.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet10_07">
             17:05-17:10, Paper ThET10.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4176'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Jointly Assigning Processes to Machines and Generating Plans for Autonomous Mobile Robots in a Smart Factory
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#287072" title="Click to go to the Author Index">
             Leet, Christopher
            </a>
           </td>
           <td class="r">
            University of Southern California
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424268" title="Click to go to the Author Index">
             Sciortino, Aidan
            </a>
           </td>
           <td class="r">
            University of Rochester
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#107126" title="Click to go to the Author Index">
             Koenig, Sven
            </a>
           </td>
           <td class="r">
            University of Southern California
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4176" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#path_planning_for_multiple_mobile_robots_or_agents" title="Click to go to the Keyword Index">
               Path Planning for Multiple Mobile Robots or Agents
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#industrial_robots" title="Click to go to the Keyword Index">
               Industrial Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             A modern smart factory runs a manufacturing procedure using a collection of programmable machines. Typically, materials are ferried between these machines using a team of mobile robots. To embed a manufacturing procedure in a smart factory, a factory operator must a) assign its processes to the smart factory's machines and b) determine how agents should carry materials between machines. A good embedding maximizes the smart factory's throughput; the rate at which it outputs products. Existing smart factory management systems solve the aforementioned problems sequentially, limiting the throughput that they can achieve. In this paper we introduce ACES, the Anytime Cyclic Embedding Solver, the first solver which jointly optimizes the assignment of processes to machines and the assignment of paths to agents. We evaluate ACES and show that it can scale to real industrial scenarios.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thet11">
             <b>
              ThET11
             </b>
            </a>
           </td>
           <td class="r">
            314
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thet11" title="Click to go to the Program at a Glance">
             <b>
              Physical Human-Robot Interaction
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#104840" title="Click to go to the Author Index">
             Song, Kai-Tai
            </a>
           </td>
           <td class="r">
            National Yang Ming Chiao Tung University
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#110186" title="Click to go to the Author Index">
             Secchi, Cristian
            </a>
           </td>
           <td class="r">
            Univ. of Modena &amp; Reggio Emilia
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet11_01">
             16:35-16:40, Paper ThET11.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('415'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Control Scheme for Collaborative Object Transportation between a Human and a Quadruped Robot Using the MIGHTY Suction Cup
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416930" title="Click to go to the Author Index">
             Plotas, Konstantinos
            </a>
           </td>
           <td class="r">
            Hellenic Mediterranean University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#325275" title="Click to go to the Author Index">
             Papadakis, Emmanouil
            </a>
           </td>
           <td class="r">
            Foundation for Research and Technology - Hellas
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#354670" title="Click to go to the Author Index">
             Drosakis, Drosakis
            </a>
           </td>
           <td class="r">
            Foundation for Research and Technology–Hellas
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#116891" title="Click to go to the Author Index">
             Trahanias, Panos
            </a>
           </td>
           <td class="r">
            Foundation for Research and Technology – Hellas (FORTH)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#180009" title="Click to go to the Author Index">
             Papageorgiou, Dimitrios
            </a>
           </td>
           <td class="r">
            Hellenic Mediterranean University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab415" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#physical_human_robot_interaction" title="Click to go to the Keyword Index">
               Physical Human-Robot Interaction
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#compliance_and_impedance_control" title="Click to go to the Keyword Index">
               Compliance and Impedance Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this work, a control scheme for human-robot collaborative object transportation is proposed, considering a quadruped robot equipped with the MIGHTY suction cup that serves both as a gripper for holding the object and a force/torque sensor. The proposed control scheme is based on the notion of admittance control, and incorporates a variable damping term aiming towards increasing the controllability of the human and, at the same time, decreasing her/his effort. Furthermore, to ensure that the object is not detached from the suction cup during the collaboration, an additional control signal is proposed, which is based on a barrier artificial potential. The proposed control scheme is proven to be passive and its performance is demonstrated through experimental evaluations conducted using the Unitree Go1 robot equipped with the MIGHTY suction cup.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet11_02">
             16:40-16:45, Paper ThET11.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('669'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              DTRT: Enhancing Human Intent Estimation and Role Allocation for Physical Human-Robot Collaboration
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#376959" title="Click to go to the Author Index">
             Liu, Haotian
            </a>
           </td>
           <td class="r">
            Institute of Automation, Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#272554" title="Click to go to the Author Index">
             Tong, Yuchuang
            </a>
           </td>
           <td class="r">
            The Institute of Automation of the Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#121974" title="Click to go to the Author Index">
             Zhang, Zhengtao
            </a>
           </td>
           <td class="r">
            Institute of Automation, Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab669" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#physical_human_robot_interaction" title="Click to go to the Keyword Index">
               Physical Human-Robot Interaction
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#intention_recognition" title="Click to go to the Keyword Index">
               Intention Recognition
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In physical Human-Robot Collaboration (pHRC), accurate human intent estimation and rational human-robot role allocation are crucial for safe and efficient assistance. Existing methods that rely on short-term motion data for intention estimation lack multi-step prediction capabilities, hindering their ability to sense intent changes and adjust human-robot assignments autonomously, resulting in potential discrepancies. To address these issues, we propose a Dual Transformer-based Robot Trajectron (DTRT) featuring a hierarchical architecture, which harnesses human-guided motion and force data to rapidly capture human intent changes, enabling accurate trajectory predictions and dynamic robot behavior adjustments for effective collaboration. Specifically, human intent estimation in DTRT uses two Transformer-based Conditional Variational Autoencoders (CVAEs), incorporating robot motion data in obstacle-free case with human-guided trajectory and force for obstacle avoidance. Additionally, Differential Cooperative Game Theory (DCGT) is employed to synthesize predictions based on human-applied forces, ensuring robot behavior align with human intention. Compared to state-of-the-art (SOTA) methods, DTRT incorporates human dynamics into long-term prediction, providing an accurate understanding of intention and enabling rational role allocation, achieving robot autonomy and maneuverability. Experiments demonstrate DTRT's accurate intent estimation and superior collaboration performance.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet11_03">
             16:45-16:50, Paper ThET11.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('969'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Learning-Based Dynamic Robot-To-Human Handover
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#391811" title="Click to go to the Author Index">
             Kim, Hyeonseong
            </a>
           </td>
           <td class="r">
            Korea University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#339644" title="Click to go to the Author Index">
             Kim, Chanwoo
            </a>
           </td>
           <td class="r">
            Korea University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#172093" title="Click to go to the Author Index">
             Pan, Matthew
            </a>
           </td>
           <td class="r">
            Queen's University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#179481" title="Click to go to the Author Index">
             Lee, Kyungjae
            </a>
           </td>
           <td class="r">
            Korea University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#163618" title="Click to go to the Author Index">
             Choi, Sungjoon
            </a>
           </td>
           <td class="r">
            Korea University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab969" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#physical_human_robot_interaction" title="Click to go to the Keyword Index">
               Physical Human-Robot Interaction
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#human_aware_motion_planning" title="Click to go to the Keyword Index">
               Human-Aware Motion Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#learning_from_experience" title="Click to go to the Keyword Index">
               Learning from Experience
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a novel learning-based approach to dynamic robot-to-human handover, addressing the challenges of delivering objects to a moving receiver. We hypothesize that dynamic handover, where the robot adjusts to the receiver’s movements, results in more efficient and comfortable interaction compared to static handover, where the receiver is assumed to be stationary. To validate this, we developed a nonparametric method for generating continuous handover motion, conditioned on the receiver's movements, and trained the model using a dataset of 1,000 human-to-human handover demonstrations. We integrated preference learning for improved handover effectiveness and applied impedance control to ensure user safety and adaptiveness. The approach was evaluated in both simulation and real-world settings, with user studies demonstrating that dynamic handover significantly reduces handover time and improves user comfort compared to static methods. Videos and demonstrations of our approach are available at https://zerotohero7886.github.io/dyn-r2h-handover/.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet11_04">
             16:50-16:55, Paper ThET11.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1188'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Novel Dynamic Motion Primitives Framework for Safe Human-Robot Collaboration
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#277746" title="Click to go to the Author Index">
             Pupa, Andrea
            </a>
           </td>
           <td class="r">
            University of Modena and Reggio Emilia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420685" title="Click to go to the Author Index">
             Di Vittorio, Filippo
            </a>
           </td>
           <td class="r">
            University of Modena and Reggio Emilia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#110186" title="Click to go to the Author Index">
             Secchi, Cristian
            </a>
           </td>
           <td class="r">
            Univ. of Modena &amp; Reggio Emilia
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1188" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#safety_in_hri" title="Click to go to the Keyword Index">
               Safety in HRI
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Learning by demonstration techniques are gaining popularity within the human-robot collaboration (HRC) scenarios. This is because they allow to deeply exploit the versatility of collaborative robots. In this context, dynamic motion primitives (DMPs) have become a standard method for enabling human operators to easily teach tasks to robots. However, DMPs have two main limitations. First, they may encounter difficulties in generalizing some tasks, which can lead to non-intuitive behavior. Second, it is not guaranteed that the output of DMPs is compliant with ISO/TS 15066, which provides guidelines for assessing safety in collaborative scenarios. This work aims to address these two issues by introducing a novel control pipeline. This pipeline leverages a new variant of DMPs, called Swap DMPs (SDMPs), introduced in this work. The SDMPs enable a more intuitive behavior when the robot reproduces the learned task. Subsequently, SDMPs are encoded into a new optimization problem that ensures the robot complies with the Speed and Separation Monitoring (SSM) collaborative mode. The proposed approach has been experimentally validated and compared with traditional DMPs in both simulation and a real scenario, where a UR5e and a human operator collaborate on a polishing task.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet11_05">
             16:55-17:00, Paper ThET11.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1334'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Depth Restoration of Hand-Held Transparent Objects for Human-To-Robot Handover
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#379901" title="Click to go to the Author Index">
             Yu, Ran
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#334528" title="Click to go to the Author Index">
             Yu, Haixin
            </a>
           </td>
           <td class="r">
            Tsinghua Shenzhen International Graduate School
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#308276" title="Click to go to the Author Index">
             Li, Shoujie
            </a>
           </td>
           <td class="r">
            Tsinghua Shenzhen International Graduate School
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#373239" title="Click to go to the Author Index">
             Huang, Yan
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#320066" title="Click to go to the Author Index">
             Song, Ziwu
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#318552" title="Click to go to the Author Index">
             Ding, Wenbo
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1334" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#multi_modal_perception_for_hri" title="Click to go to the Keyword Index">
               Multi-Modal Perception for HRI
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Transparent objects are common in daily life, while their optical properties pose challenges for RGB-D cameras to capture accurate depth information. This issue is further amplified when these objects are hand-held, as hand occlusions further complicate depth estimation. For assistant robots, however, accurately perceiving hand-held transparent objects is critical to effective human-robot interaction. This paper presents a Hand-Aware Depth Restoration (HADR) method based on creating an implicit neural representation function from a single RGB-D image. The proposed method utilizes hand posture as an important guidance to leverage semantic and geometric information of hand-object interaction. To train and evaluate the proposed method, we create a high-fidelity synthetic dataset named TransHand-14K with a real-to-sim data generation scheme. Experiments show that our method has better performance and generalization ability compared with existing methods. We further develop a real-world human-to-robot handover system based on HADR, demonstrating its potential in human-robot interaction applications.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet11_06">
             17:00-17:05, Paper ThET11.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2422'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Leveraging Semantic and Geometric Information for Zero-Shot Robot-To-Human Handover
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#414675" title="Click to go to the Author Index">
             Liu, Jiangshan
            </a>
           </td>
           <td class="r">
            Southern University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417543" title="Click to go to the Author Index">
             Dong, Wenlong
            </a>
           </td>
           <td class="r">
            Southern University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#205946" title="Click to go to the Author Index">
             Wang, Jiankun
            </a>
           </td>
           <td class="r">
            Southern University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#100160" title="Click to go to the Author Index">
             Meng, Max Q.-H.
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2422" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#physical_human_robot_interaction" title="Click to go to the Keyword Index">
               Physical Human-Robot Interaction
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#robot_companions" title="Click to go to the Keyword Index">
               Robot Companions
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#grasping" title="Click to go to the Keyword Index">
               Grasping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Human-robot interaction (HRI) encompasses a wide range of collaborative tasks, with handover being one of the most fundamental. As robots become more integrated into human environments, the potential for service robots to assist in handing objects to humans is increasingly promising. In robot-to-human (R2H) handover, selecting the optimal grasp is crucial for success, as it requires avoiding interference with the human's preferred grasp region and minimizing intrusion into their workspace. Existing methods either inadequately consider geometric information or rely on data-driven approaches, which often struggle to generalize across diverse objects. To address these limitations, we propose a novel zero-shot system that combines semantic and geometric information to generate optimal handover grasps. Our method first identifies grasp regions using semantic knowledge from vision-language models (VLMs) and, by incorporating customized visual prompts, achieves finer granularity in region grounding. A grasp is then selected based on grasp distance and approach angle to maximize human ease and avoid interference. We validate our approach through ablation studies and real-world comparison experiments. Results demonstrate that our system improves handover success rates and provides a more user-preferred interaction experience. Videos, appendixes and more are available at https://sites.google.com/view/vlm-handover/.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet11_07">
             17:05-17:10, Paper ThET11.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4928'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Human-To-Robot Handover Control of an Autonomous Mobile Robot Based on Hand-Masked Object Pose Estimation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104840" title="Click to go to the Author Index">
             Song, Kai-Tai
            </a>
           </td>
           <td class="r">
            National Yang Ming Chiao Tung University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#390745" title="Click to go to the Author Index">
             Huang, Yu-Yun
            </a>
           </td>
           <td class="r">
            National Yang Ming Chiao Tung University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4928" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#grasping" title="Click to go to the Keyword Index">
               Grasping
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#visual_servoing" title="Click to go to the Keyword Index">
               Visual Servoing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a human-to-robot handover design for an Autonomous Mobile Robot (AMR). The developed control system enables the AMR to navigate to a specific person and grasp the object that the person wants to handover. This paper proposes a motion planning algorithm for grasping an unseen object held in hand. Through hand detection and segmentation, the hand region is masked and removed from the acquired depth image, which is used to estimate the object pose for grasping. For grasp pose determination, we propose to add the Convolutional Block Attention Module (CBAM) to the Generative Grasping Convolutional Neural Network (GGCNN) model to enhance the recognition rate. For the object-grasp task, the AMR localizes the object in person’s hand, and uses the Model Predictive Control (MPC)-based controller to simultaneously control the mobile base and manipulator to grasp the object. A laboratory-developed mobile manipulator, equipped with a 6-DoF TM5M-900 is used for experimental verification. The experimental results show an average handover success rate of 81% for five different objects.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thet12">
             <b>
              ThET12
             </b>
            </a>
           </td>
           <td class="r">
            315
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thet12" title="Click to go to the Program at a Glance">
             <b>
              Motion Control 2
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet12_01">
             16:35-16:40, Paper ThET12.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('91'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Learning Multimodal Confidence for Intention Recognition in Human-Robot Interaction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#368106" title="Click to go to the Author Index">
             Zhao, Xiyuan
            </a>
           </td>
           <td class="r">
            Southeast University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#138561" title="Click to go to the Author Index">
             Li, Huijun
            </a>
           </td>
           <td class="r">
            Southeast University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#350780" title="Click to go to the Author Index">
             Miao, Tianyuan
            </a>
           </td>
           <td class="r">
            Southeast University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#374817" title="Click to go to the Author Index">
             Zhu, Xianyi
            </a>
           </td>
           <td class="r">
            Southeast University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#317301" title="Click to go to the Author Index">
             Wei, Zhikai
            </a>
           </td>
           <td class="r">
            Southeast University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#408767" title="Click to go to the Author Index">
             Tan, Lifen
            </a>
           </td>
           <td class="r">
            China Astronaut Research and Training Center
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#112790" title="Click to go to the Author Index">
             Song, Aiguo
            </a>
           </td>
           <td class="r">
            Southeast University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab91" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#multi_modal_perception_for_hri" title="Click to go to the Keyword Index">
               Multi-Modal Perception for HRI
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#human_factors_and_human_in_the_loop" title="Click to go to the Keyword Index">
               Human Factors and Human-in-the-Loop
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The rapid development of collaborative robotics has provided a new possibility of helping the elderly who has difficulties in daily life, allowing robots to operate according to specific intentions. However, efficient human-robot cooperation requires natural, accurate and reliable intention recognition in shared environments. The current paramount challenge for this is reducing the uncertainty of multimodal fused intention to be recognized and reasoning adaptively a more reliable result despite current interactive condition. In this work we propose a novel learning-based multimodal fusion framework Batch Multimodal Confidence Learning for Opinion Pool (BMCLOP). Our approach combines Bayesian multimodal fusion method and batch confidence learning algorithm to improve accuracy, uncertainty reduction and success rate given the interactive condition. In particular, the generic and practical multimodal intention recognition framework can be easily extended further. Our desired assistive scenarios consider three modalities gestures, speech and gaze, all of which produce categorical distributions over all the finite intentions. The proposed method is validated with a six-DoF robot through extensive experiments and exhibits high performance compared to baselines.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet12_02">
             16:40-16:45, Paper ThET12.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2157'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Optimize and Coordinate Multiple DMPs under Constraints to Achieve a Collaborative Manipulation Task
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#266298" title="Click to go to the Author Index">
             Kordia, Ali H.
            </a>
           </td>
           <td class="r">
            Instituto Superior Técnico
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104914" title="Click to go to the Author Index">
             Melo, Francisco S.
            </a>
           </td>
           <td class="r">
            Instituto Superior Tecnico
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2157" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#motion_control" title="Click to go to the Keyword Index">
               Motion Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#planning__scheduling_and_coordination" title="Click to go to the Keyword Index">
               Planning, Scheduling and Coordination
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper addresses a significant challenge in achieving collaborative tasks; how can a robot or multiple robots, endowed with a library of pre-learned primitive movements, generate multiple simultaneous coordinated robotic movements, adapting and optimizing those in the library, to complete one collaborative task? This work can thus be seen as a follow-up to the work with a motion presented as dynamic movement primitive (DMP) that now considers collaborative tasks and the existence of multiple robots/manipulators. Specifically, we start with a simple task using one DMP and extend it to accommodate the coordinated execution of multiple DMPs in robots with multiple manipulators or---alternatively---multiple robots with a single manipulator. We investigate mechanisms to jointly optimize multiple DMPs to perform one task in a coordinated fashion. The joint trajectory is built from initial DMPs learned for a single manipulator, and its optimization must comply with task-specific constraints. We illustrate the application of our approach both in a simulated environment and in a simulated and real Baxter robot.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet12_03">
             16:45-16:50, Paper ThET12.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2271'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Modified Resistance Model for Magnetic Honeycomb Robots to Navigate in Low Reynolds Number Fluids
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#379678" title="Click to go to the Author Index">
             Zou, Leyao
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423891" title="Click to go to the Author Index">
             Ma, Shihao
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423906" title="Click to go to the Author Index">
             Liu, Yi
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#360908" title="Click to go to the Author Index">
             Dong, Xinyang
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#377348" title="Click to go to the Author Index">
             Zhou, Ziqing
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#340423" title="Click to go to the Author Index">
             Ouyang, Chun
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#322500" title="Click to go to the Author Index">
             Gan, Zhongxue
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2271" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#motion_control" title="Click to go to the Keyword Index">
               Motion Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#micro_nano_robots" title="Click to go to the Keyword Index">
               Micro/Nano Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In recent years, magnetically controlled microrobots have garnered significant attention. This paper presents the H-robot, a self-designed microrobot featuring an innovative structure. The H-robot features a honeycomb porous spherical design specifically engineered to enhance cargo capacity. A new dynamic model for this structure has been developed for low Reynolds number fluid environments, along with a robust backstepping sliding mode control (RBSMC) strategy. Experiments were conducted in a calibrated magnetic field generated by a magnetic field generator to achieve precise motion control. The results demonstrate that the H-robot accurately tracks standard trajectories, with root mean square errors (RMSE) of 9.09×10−4 m for the Number-8 path and 8.29×10−4 m for the S-shaped path. Additionally, the proposed resistance model enhances tracking accuracy by 73.61% compared to traditional models, effectively adjusting the dynamic behavior of the H-robot in low Reynolds number fluids and significantly improving its motion performance. Finally, path planning experiments in a maze demonstrate the H-robot’s ability to navigate and avoid obstacles.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet12_04">
             16:50-16:55, Paper ThET12.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2855'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Manual, Semi or Fully Autonomous Flipper Control? a Framework for Fair Comparison
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424737" title="Click to go to the Author Index">
             Číhala, Valentýn
            </a>
           </td>
           <td class="r">
            Ceske Vysoke Uceni Technicke V Praze, FEL
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#195222" title="Click to go to the Author Index">
             Pecka, Martin
            </a>
           </td>
           <td class="r">
            Ceske Vysoke Uceni Technicke V Praze, FEL
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#107657" title="Click to go to the Author Index">
             Svoboda, Tomas
            </a>
           </td>
           <td class="r">
            Ceske Vysoke Uceni Technicke V Praze, FEL
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#134222" title="Click to go to the Author Index">
             Zimmermann, Karel
            </a>
           </td>
           <td class="r">
            Ceske Vysoke Uceni Technicke V Praze, FEL
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2855" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#motion_control" title="Click to go to the Keyword Index">
               Motion Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#software_tools_for_benchmarking_and_reproducibility" title="Click to go to the Keyword Index">
               Software Tools for Benchmarking and Reproducibility
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We investigated the performance of existing semi- and fully autonomous methods for controlling flipper-based skid-steer robots. Our study involves reimplementation of these methods for fair comparison and it introduces a novel semi-autonomous control policy that provides a compelling trade-off among current state-of-the-art approaches. We also propose new metrics for assessing cognitive load and traversal quality and offer a benchmarking interface for generating Quality-Load graphs from recorded data. Our results, presented in a 2D Quality-Load space, demonstrate that the new control policy effectively bridges the gap between autonomous and manual control methods. Additionally, we reveal a surprising fact that fully manual, continuous control of all six degrees of freedom remains highly effective when performed by an experienced operator on a well-designed analog controller from third person view.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet12_05">
             16:55-17:00, Paper ThET12.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3577'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Safety-Critical Locomotion of Biped Robots in Infeasible Paths: Overcoming Obstacles During Navigation Toward Destination
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#142294" title="Click to go to the Author Index">
             Lee, Jaemin
            </a>
           </td>
           <td class="r">
            North Carolina State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#296633" title="Click to go to the Author Index">
             Dai, Min
            </a>
           </td>
           <td class="r">
            California Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#201505" title="Click to go to the Author Index">
             Kim, Jeeseop
            </a>
           </td>
           <td class="r">
            Caltech
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#116276" title="Click to go to the Author Index">
             Ames, Aaron
            </a>
           </td>
           <td class="r">
            California Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3577" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#motion_control" title="Click to go to the Keyword Index">
               Motion Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#robot_safety" title="Click to go to the Keyword Index">
               Robot Safety
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#humanoid_and_bipedal_locomotion" title="Click to go to the Keyword Index">
               Humanoid and Bipedal Locomotion
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper proposes a safety-critical locomotion control framework employed for legged robots exploring through infeasible path in obstacle-rich environments. Our research focus is on achieving safe and robust locomotion where robots confront unavoidable obstacles en route to their designated destination. Through the utilization of outcomes from physical interactions with unknown objects, we establish a hierarchy among the safety-critical conditions avoiding the obstacles. This hierarchy enables the generation of a safe reference trajectory that adeptly mitigates conflicts among safety conditions and reduce the risk while controlling the robot toward its destination without additional motion planning methods. In addition, robust bipedal locomotion is achieved by utilizing the Hybrid Linear Inverted Pendulum model, coupled with a disturbance observer addressing a disturbance from the physical interaction.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet12_06">
             17:00-17:05, Paper ThET12.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3747'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Optimal Framework for Constrained Admittance Path-Following Control
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425939" title="Click to go to the Author Index">
             Besi, Giulio
            </a>
           </td>
           <td class="r">
            University of Modena and Reggio Emilia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#277746" title="Click to go to the Author Index">
             Pupa, Andrea
            </a>
           </td>
           <td class="r">
            University of Modena and Reggio Emilia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#110186" title="Click to go to the Author Index">
             Secchi, Cristian
            </a>
           </td>
           <td class="r">
            Univ. of Modena &amp; Reggio Emilia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#158070" title="Click to go to the Author Index">
             Ferraguti, Federica
            </a>
           </td>
           <td class="r">
            Università Degli Studi Di Modena E Reggio Emilia
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3747" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#physical_human_robot_interaction" title="Click to go to the Keyword Index">
               Physical Human-Robot Interaction
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#compliance_and_impedance_control" title="Click to go to the Keyword Index">
               Compliance and Impedance Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this article, an optimal controller for achieving constrained admittance control is proposed. This controller strictly adheres to the constraint boundaries while ensuring minimal variations in kinematic energy. The proposed method integrates admittance control for human-robot interaction with the Udwadia-Kalaba equations for constrained motion into a unified framework. The proposed architecture has been tested and validated both with simulations and real tests on a 6-DoF UR5e robot. The results demonstrate that the proposed architecture outperforms virtual fixtures, one of the most commonly used techniques to implement effective path-following control.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet12_07">
             17:05-17:10, Paper ThET12.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4026'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Robust Orientation Control of Robot Manipulator Using Orientation Disturbance Observer
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#347993" title="Click to go to the Author Index">
             Choi, Kiyoung
            </a>
           </td>
           <td class="r">
            Deagu Gyeongbuk Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#378571" title="Click to go to the Author Index">
             Song, JunHo
            </a>
           </td>
           <td class="r">
            Daegu Gyeongbuk Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#216543" title="Click to go to the Author Index">
             Yun, WonBum
            </a>
           </td>
           <td class="r">
            Korea Institute of Robotics and Technology Convergence
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#129982" title="Click to go to the Author Index">
             Oh, Sehoon
            </a>
           </td>
           <td class="r">
            DGIST
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4026" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#motion_control" title="Click to go to the Keyword Index">
               Motion Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#dynamics" title="Click to go to the Keyword Index">
               Dynamics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a robust control algorithm for precise orientation control of robot manipulators using a Disturbance Observer (DOB) specifically designed for orientation dynamics. Our approach addresses the challenges of 3D orientation control by incorporating various orientation representations, such as Euler angles, quaternions, and exponential coordinates, and analyzing their impact on DOB performance. Through theoretical analysis and experimental validation, we demonstrate the effectiveness of our method in achieving high-precision orientation control under uncertainties and disturbances. This work offers a comprehensive framework for robust orientation control, advancing the application of DOB in complex robotic tasks.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet12_08">
             17:10-17:15, Paper ThET12.8
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4232'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Predictive Kinematic Coordinate Control for Aerial Manipulators Based on Modified Kinematics Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426284" title="Click to go to the Author Index">
             Li, Zhengzhen
            </a>
           </td>
           <td class="r">
            Westlake University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#356189" title="Click to go to the Author Index">
             Shen, Jiahao
            </a>
           </td>
           <td class="r">
            Westlake University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#406823" title="Click to go to the Author Index">
             Ji, Mengyu
            </a>
           </td>
           <td class="r">
            Westlake University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#346443" title="Click to go to the Author Index">
             Cao, Huazi
            </a>
           </td>
           <td class="r">
            Beihang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#193378" title="Click to go to the Author Index">
             Zhao, Shiyu
            </a>
           </td>
           <td class="r">
            Westlake University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4232" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#motion_control" title="Click to go to the Keyword Index">
               Motion Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#aerial_systems__mechanics_and_control" title="Click to go to the Keyword Index">
               Aerial Systems: Mechanics and Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#kinematics" title="Click to go to the Keyword Index">
               Kinematics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             High-precision manipulation has always been a developmental goal for aerial manipulators. This paper investigates the kinematic coordinate control issue in aerial manipulators. We propose a predictive kinematic coordinate control method based on model learning, which includes a learning-based modified kinematic model and a model predictive control (MPC) scheme based on weight allocation. Compared to existing methods, our proposed approach offers several attractive features. First, the kinematic model incorporates closed-loop dynamics characteristics and online residual learning. Compared to methods that do not consider closed-loop dynamics and residuals, our proposed method has improved accuracy by 59.6%. Second, a MPC method that considers weight allocation has been proposed, which can coordinate the motion strategies of quadcopters and manipulators. Compared to methods that do not consider weight allocation, the proposed method can meet the requirements of more tasks. The proposed approach is verified through complex trajectory tracking and moving target tracking experiments. The results validate the effectiveness of the proposed method.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thet13">
             <b>
              ThET13
             </b>
            </a>
           </td>
           <td class="r">
            316
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thet13" title="Click to go to the Program at a Glance">
             <b>
              Resiliency and Security 2
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#101579" title="Click to go to the Author Index">
             Ueda, Jun
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet13_01">
             16:35-16:40, Paper ThET13.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('247'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Affine Transformation-Based Perfectly Undetectable False Data Injection Attacks on Remote Manipulator Kinematic Control with Attack Detector
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101579" title="Click to go to the Author Index">
             Ueda, Jun
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#342017" title="Click to go to the Author Index">
             Blevins, Jacob
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab247" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#networked_robots" title="Click to go to the Keyword Index">
               Networked Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#failure_detection_and_recovery" title="Click to go to the Keyword Index">
               Failure Detection and Recovery
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_control" title="Click to go to the Keyword Index">
               Motion Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper demonstrates the viability of perfectly undetectable affine transformation attacks against robotic manipulators where intelligent attackers can inject multiplicative and additive false data while remaining completely hidden from system users. The attacker can implement these communication line attacks by satisfying three Conditions presented in this work. These claims are experimentally validated on a FANUC 6 degree of freedom manipulator by comparing a nominal (non-attacked) trial and a detectable attack case against three
             <p>
              perfectly undetectable trajectory attack Scenarios: scaling, reflection, and shearing. The results show similar observed end effector error for the attack Scenarios and the nominal case, indicating that the perfectly undetectable affine transformation attack
              <p>
               method keeps the attacker perfectly hidden while enabling them to attack manipulator trajectories.
              </p>
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet13_02">
             16:40-16:45, Paper ThET13.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('449'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              CDA: Covert Deception Attacks in Multi-Agent Resource Scheduling
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#284379" title="Click to go to the Author Index">
             Hao, Wei
            </a>
           </td>
           <td class="r">
            Nanjing University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#285737" title="Click to go to the Author Index">
             Liu, Jia
            </a>
           </td>
           <td class="r">
            Nanjing University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#285694" title="Click to go to the Author Index">
             Li, Wenjie
            </a>
           </td>
           <td class="r">
            Nanjng University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#285738" title="Click to go to the Author Index">
             Chen, Lijun
            </a>
           </td>
           <td class="r">
            Nanjing University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab449" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#robot_safety" title="Click to go to the Keyword Index">
               Robot Safety
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#swarm_robotics" title="Click to go to the Keyword Index">
               Swarm Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this letter, we address the critical security concerns in multi-agent systems, where illegal infiltration is commonly used to convert agents into malicious entities. Existing research predominantly focuses on explicit malicious attack patterns. Our work introduces a covert deception attack framework in the context of multi-agent resource scheduling scenarios. We first highlight vulnerabilities in scheduling strategies based on time and path costs. Exploiting these weaknesses, an infiltrated agent clandestinely gathers motion characteristics of other agents while posing as a teammate. Using these motion characteristics, the infiltrated agent employs an LSTM architecture to learn and predict congestion areas, thereby designing attack paths with greater time efficiency. This approach allows the infiltrated agent to secure additional resources and evade capture more effectively. Validation through simulation and real-world experiments demonstrates the feasibility and effectiveness of our approach, underscoring the importance of evaluating covert attacks in risk assessments within multi-agent systems.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet13_03">
             16:45-16:50, Paper ThET13.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1352'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Early Model-Based Safety Analysis for Collaborative Robotic Systems (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421248" title="Click to go to the Author Index">
             Manjunath, Meenakshi
            </a>
           </td>
           <td class="r">
            Technical University of Applied Sciences Würzburg-Schweinfurt
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421246" title="Click to go to the Author Index">
             Jesus Raja, Jeshwitha
            </a>
           </td>
           <td class="r">
            Technical University of Applied Sciences Würzburg-Schweinfurt
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#224756" title="Click to go to the Author Index">
             Daun, Marian
            </a>
           </td>
           <td class="r">
            Technical University of Applied Sciences Würzburg-Schweinfurt
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1352" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#safety_in_hri" title="Click to go to the Keyword Index">
               Safety in HRI
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#intelligent_and_flexible_manufacturing" title="Click to go to the Keyword Index">
               Intelligent and Flexible Manufacturing
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#modeling__control__and_learning_for_soft_robots" title="Click to go to the Keyword Index">
               Modeling, Control, and Learning for Soft Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The current era is marked by an accelerated digitization of manufacturing processes, with robotic systems increasingly integrated into various workflows. Yet, despite significant advancements, it is impractical to fully automate certain tasks due to prohibitive costs and technical constraints. As a result, there’s a growing emphasis on human-robot collaboration (HRC) for intricate operations. In HRC scenarios, humans and robots co-inhabit the same work environment, operating side by side. More than just mere coexistence in the same space, they actively collaborate on shared tasks, thus raising the stakes in terms of safety. The dynamic behavior of robots must be synchronized with the anticipated and unexpected human actions, adding another layer of complexity to the safety considerations. It is essential to conduct comprehensive safety analyses that identify potential risks that pose harm to the human operator. As a proactive measure to foster early-stage safety and risk analysis, we propose the use of goal models. The approach enables the specification of safety threats within the HRC context, thereby facilitating the development of safety tasks and supportive monitoring mechanisms. This approach helps in the refinement and implementation of safety measures, ensuring a secure and productive environment for human-robot collaboration.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet13_04">
             16:50-16:55, Paper ThET13.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1863'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Investigating Security Threats in Multi-Tenant ROS 2 Systems
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419739" title="Click to go to the Author Index">
             Xia, Lichen
            </a>
           </td>
           <td class="r">
            University of Delaware
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420674" title="Click to go to the Author Index">
             Gao, Xing
            </a>
           </td>
           <td class="r">
            University of Delaware
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#291510" title="Click to go to the Author Index">
             Shi, Weisong
            </a>
           </td>
           <td class="r">
            University of Delaware
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1863" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#software_architecture_for_robotic_and_automation" title="Click to go to the Keyword Index">
               Software Architecture for Robotic and Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robot Operating System (ROS) has been widely used to develop robotic applications. The first generation of ROS generally lacks security features, and ROS 2 is introduced with security support. However, security concerns still exist for running ROS in practical multi-tenant environments. In this paper, we conduct an in-depth investigation into the security of ROS 2. We focus on vulnerabilities in ROS nodes and topics and intend to explore methods to break the isolation and security mechanisms systematically. We devise a set of strategies that can be exploited by attackers to escalate privilege or cause information leakage in a multi-tenant environment. These attacks can bypass existing isolation and security mechanisms, including ROS 2’s native security module. To validate our findings, we employ simulations across various real-world scenarios to demonstrate how attackers could exploit these vulnerabilities to bypass existing security mechanisms. Finally, we present several defense practices to mitigate these identified threats.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet13_05">
             16:55-17:00, Paper ThET13.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2230'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Multi-Task Robustness Enhancement Framework against Various Adversarial Patches
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416993" title="Click to go to the Author Index">
             Jing, Lihua
            </a>
           </td>
           <td class="r">
            Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417003" title="Click to go to the Author Index">
             Wang, Rui
            </a>
           </td>
           <td class="r">
            Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416995" title="Click to go to the Author Index">
             Li, Runbo
            </a>
           </td>
           <td class="r">
            Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416988" title="Click to go to the Author Index">
             Zhu, Zixuan
            </a>
           </td>
           <td class="r">
            Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421249" title="Click to go to the Author Index">
             Wei, Xingxing
            </a>
           </td>
           <td class="r">
            Beihang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2230" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Autonomous systems leveraging visual perception face a rising threat from adversarial patches, jeopardizing their robustness. Existing defense methods adaptable to various pre-trained models typically rely on observed patch characteristics or prior attack data, having difficulty adapting to new threats. This study innovatively focuses on modeling patch attack behavior instead of existing patches, proposing a unified robustness enhancement framework against various adversarial patches. Through self-supervised learning, we accurately locate diverse adversarial patches without prior attack knowledge. Furthermore, we introduce an efficient adaptive patch inpainting method to mitigate patch impact while maintaining visual coherence. Experiments show that our methods effectively boost the robustness of visual perception models against various adversarial patches across different tasks.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet13_06">
             17:00-17:05, Paper ThET13.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3211'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Perfectly Undetectable False Data Injection Attacks on Encrypted Bilateral Teleoperation System Based on Dynamic Symmetry and Malleability
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#342077" title="Click to go to the Author Index">
             Kwon, Hyukbin
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#346856" title="Click to go to the Author Index">
             Kawase, Hiroaki
            </a>
           </td>
           <td class="r">
            The University of Electro-Communications
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#310054" title="Click to go to the Author Index">
             Nieves-Vazquez, Heriberto Andres
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101325" title="Click to go to the Author Index">
             Kogiso, Kiminao
            </a>
           </td>
           <td class="r">
            The University of Electro-Communications
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101579" title="Click to go to the Author Index">
             Ueda, Jun
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3211" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#telerobotics_and_teleoperation" title="Click to go to the Keyword Index">
               Telerobotics and Teleoperation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#networked_robots" title="Click to go to the Keyword Index">
               Networked Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#dynamics" title="Click to go to the Keyword Index">
               Dynamics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper investigates the vulnerability of bilateral teleoperation systems to perfectly undetectable False Data Injection Attacks (FDIAs). Teleoperation, one of major applications in robotics, involves a leader manipulator operated by a human and a follower manipulator at a remote site, connected via a communication channel. While this setup enables operation in challenging environments, it also introduces cybersecurity risks, particularly in the communication link. The paper focuses on a specific class of cyberattacks: perfectly undetectable FDIAs, where attackers alter signals without leaving detectable traces at all. Compared to previous research on linear and first-order nonlinear systems, this paper examines bilateral teleoperation systems with second-order nonlinear manipulator dynamics. The paper derives mathematical conditions based on Lie Group theory that enable such attacks, demonstrating how an attacker can modify the follower manipulator's motion while the operator perceives normal operation through the leader device. This vulnerability challenges conventional detection methods based on observable changes and highlights the need for advanced security measures in teleoperation systems. To validate the theoretical results, the paper presents experimental demonstrations using a teleoperation system connecting robots in the US and Japan.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thet14">
             <b>
              ThET14
             </b>
            </a>
           </td>
           <td class="r">
            402
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thet14" title="Click to go to the Program at a Glance">
             <b>
              Hand and Gripper Design
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#192394" title="Click to go to the Author Index">
             Plecnik, Mark
            </a>
           </td>
           <td class="r">
            University of Notre Dame
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet14_01">
             16:35-16:40, Paper ThET14.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1386'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Novel Under-Actuated Gripper Based on Passive-Locking Mechanism for Stable Gripping under Environmental Constraints
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#365396" title="Click to go to the Author Index">
             Yang, Seokjun
            </a>
           </td>
           <td class="r">
            Kwangwoon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#103310" title="Click to go to the Author Index">
             Lee, Sungon
            </a>
           </td>
           <td class="r">
            Hanyang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#102803" title="Click to go to the Author Index">
             Yang, Woosung
            </a>
           </td>
           <td class="r">
            Kwangwoon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1386" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#grippers_and_other_end_effectors" title="Click to go to the Keyword Index">
               Grippers and Other End-Effectors
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mechanism_design" title="Click to go to the Keyword Index">
               Mechanism Design
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#grasping" title="Click to go to the Keyword Index">
               Grasping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a novel under-actuated two finger gripper that passively adapts to various environments and maintains its grip posture using a passive-locking mechanism. The proposed mechanism features fingers with three phalanges, each incorporating four-bar and eight-bar linkages arranged in parallel. These linkages perform crucial functions, including maintaining the grip angle and ensuring passive characteristics during pinch grips. Previous grippers with passive mechanisms and three-phalanx fingers faced issues with gripping instability, particularly when changes in the passive joint angle were caused by object inertia or external lateral forces. To address this problem, we propose a new passive-locking mechanism utilizing an eight-bar linkage. This innovative design is engineered to adapt to environmental conditions, establish a secure grip, and maintain the grip angle of the passive joint after the grip is achieved. To demonstrate the advantages of the proposed mechanism, this paper conducts a fingertip force vector analysis and a mobility analysis according to the pinch sequence. It also details the derivation process and principles of the mechanism. The gripper’s operational range and gripping force are examined through kinematic analysis and verified by simulation. Furthermore, the study shows that the proposed mechanism effectively responds to environmental constraints, even in environments with obstacles surrounding the object. Comparative experiments with and without a contact bar indicate that the proposed gripper can stably secure an object in scenarios involving swing motions and external forces of approximately 5N.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet14_02">
             16:40-16:45, Paper ThET14.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1899'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Juzu Type Gripper That Can Change Both Shape and Firmness
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#389704" title="Click to go to the Author Index">
             Hara, Shunya
            </a>
           </td>
           <td class="r">
            Osaka University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#125046" title="Click to go to the Author Index">
             Fukuda, Osamu
            </a>
           </td>
           <td class="r">
            Saga University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101838" title="Click to go to the Author Index">
             Higashimori, Mitsuru
            </a>
           </td>
           <td class="r">
            Osaka University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1899" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#grippers_and_other_end_effectors" title="Click to go to the Keyword Index">
               Grippers and Other End-Effectors
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mechanism_design" title="Click to go to the Keyword Index">
               Mechanism Design
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a novel gripper capable of actively changing both shape and firmness. The gripper increases its grasp ability by changing its finger posture and firmness suitable for given target objects. In the proposed gripper, each finger is constructed by serially connecting multiple Juzu units. By controlling the angles between neighboring Juzu units individually using two actuators used for sending and bending, arbitrary finger shapes can be generated. In addition, by controlling the tension of the wire that penetrated all Juzu units in each finger, the friction between Juzu units is adjusted and the firmness of finger can be varied. A prototype gripper was designed and developed, and experiments to evaluate the capabilities of changing shape and firmness were conducted. Furthermore, through experiments of preshaping and grasping for various objects with different shape and size, the validity of the proposed method was demonstrated.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet14_03">
             16:45-16:50, Paper ThET14.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3073'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Direct-Drive Gripper Designed by Ellipse Synthesis across Two Output Modes
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#257211" title="Click to go to the Author Index">
             Ramesh, Shashank
            </a>
           </td>
           <td class="r">
            University of Notre Dame
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#192394" title="Click to go to the Author Index">
             Plecnik, Mark
            </a>
           </td>
           <td class="r">
            University of Notre Dame
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3073" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#mechanism_design" title="Click to go to the Keyword Index">
               Mechanism Design
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#grippers_and_other_end_effectors" title="Click to go to the Keyword Index">
               Grippers and Other End-Effectors
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#kinematics" title="Click to go to the Keyword Index">
               Kinematics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             There are many ways for a gripper to estimate the forces between its fingers. If powered by direct-drive brushless motors, then one technique is to measure their current. This is not the most accurate technique, but it is simple, keeps the sensor remote, and requires no new components. The estimation involves multiplying current signals through by the torque constant and the inverse transpose of the Jacobian. The Jacobian either amplifies the signal from fingertip force to motor current (at the cost of tip force production), or diminishes it (with the gain of tip force production), indicating an inherent trade-off. However, the Jacobian is a function of configuration, and for any workspace point there are multiple configurations (multiple inverse kinematics solutions), therefore a selection of Jacobian exists. For a given workspace point, the number of Jacobian choices is just a few, but these choices can be designed (through dimensional synthesis) to overcome the trade-off. The problem can be framed as velocity ellipse synthesis over multiple output modes. In this work, we conduct optimal synthesis to compute a new gripper design. The gripper was built and tested. It transitions between two different modes: sense mode and grip mode. Sense mode can sense forces 3 times smaller than grip mode. Grip mode can exert forces 4 times greater than sense mode.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet14_04">
             16:50-16:55, Paper ThET14.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4171'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Mechanisms and Computational Design of Multi-Modal End-Effector with Force Sensing Using Gated Networks
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#275673" title="Click to go to the Author Index">
             Tanaka, Yusuke
            </a>
           </td>
           <td class="r">
            University of California, Los Angeles
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#410540" title="Click to go to the Author Index">
             Zhu, Alvin
            </a>
           </td>
           <td class="r">
            University of California Los Angeles
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#196551" title="Click to go to the Author Index">
             Lin, Richard
            </a>
           </td>
           <td class="r">
            UC Los Angeles
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#133682" title="Click to go to the Author Index">
             Mehta, Ankur
            </a>
           </td>
           <td class="r">
            UCLA
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#106488" title="Click to go to the Author Index">
             Hong, Dennis
            </a>
           </td>
           <td class="r">
            UCLA
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4171" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#grippers_and_other_end_effectors" title="Click to go to the Keyword Index">
               Grippers and Other End-Effectors
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#climbing_robots" title="Click to go to the Keyword Index">
               Climbing Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In limbed robotics, end-effectors must serve dual functions, such as both feet for locomotion and grippers for grasping, which presents design challenges. This paper introduces a multi-modal end-effector capable of transitioning between flat and line foot configurations while providing grasping capabilities. MAGPIE integrates 8-axis force sensing using proposed mechanisms with hall effect sensors, enabling both contact and tactile force measurements. We present a computational design framework for our sensing mechanism that accounts for noise and interference, allowing for desired sensitivity and force ranges and generating ideal inverse models. The hardware implementation of MAGPIE is validated through experiments, demonstrating its capability as a foot and verifying the performance of the sensing mechanisms, ideal models, and gated network-based models.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet14_05">
             16:55-17:00, Paper ThET14.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4899'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Single-Motor-Driven (4 + 2)-Fingered Robotic Gripper Capable of Expanding the Workable Space in the Extremely Confined Environment
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#190314" title="Click to go to the Author Index">
             Nishimura, Toshihiro
            </a>
           </td>
           <td class="r">
            Kanazawa University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#412334" title="Click to go to the Author Index">
             Akasaka, Keisuke
            </a>
           </td>
           <td class="r">
            Kanazawa University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#367125" title="Click to go to the Author Index">
             Ishikawa, Subaru
            </a>
           </td>
           <td class="r">
            Kanazawa University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#103658" title="Click to go to the Author Index">
             Watanabe, Tetsuyou
            </a>
           </td>
           <td class="r">
            Kanazawa University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4899" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#grippers_and_other_end_effectors" title="Click to go to the Keyword Index">
               Grippers and Other End-Effectors
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mechanism_design" title="Click to go to the Keyword Index">
               Mechanism Design
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#grasping" title="Click to go to the Keyword Index">
               Grasping
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This study proposes a novel robotic gripper that can expand workable spaces in a target environment to pick up objects from confined spaces. The proposed gripper is most effective for retrieving objects from deformable environments, such as taking an object out of a drawstring bag, or for extracting target objects located behind surrounding objects. The proposed gripper achieves both work-space expansion and grasping motion by using only a single motor. The gripper is equipped with four outer fingers for expanding the environment and two inner fingers for grasping an object. The inner and outer fingers move in different directions for their respective functions of grasping and spatial expansion. To realize two different movements of the fingers, a novel self-motion switching mechanism that switches between the functions as feed-screw and rack-and-pinion mechanisms is developed. The mechanism switches the motions according to the magnitude of the force applied to the inner fingers. This paper presents the mechanism design of the developed gripper, including the self-motion switching mechanism and the actuation strategy for expanding the workable space. The mechanical analysis is also presented, and the analysis result is validated experimentally. Moreover, an automatic object-picking system using the developed gripper is constructed to evaluate the gripper.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet14_06">
             17:00-17:05, Paper ThET14.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('5036'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Three-Finger Adaptive Gripper with Finger-Embedded Suction Cups for Enhanced Object Grasping Mechanism
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#355919" title="Click to go to the Author Index">
             Yoon, Jimin
            </a>
           </td>
           <td class="r">
            Sungkyunkwan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#250411" title="Click to go to the Author Index">
             Jeong, Heeyeon
            </a>
           </td>
           <td class="r">
            Sungkyunkwan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#217963" title="Click to go to the Author Index">
             Park, Jae Hyeong
            </a>
           </td>
           <td class="r">
            Sungkwunkwan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#274605" title="Click to go to the Author Index">
             Gong, Young Jin
            </a>
           </td>
           <td class="r">
            SungKyunKwan University(SKKU)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#310368" title="Click to go to the Author Index">
             Shin, Dongsu
            </a>
           </td>
           <td class="r">
            Sungkyunkwan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#402720" title="Click to go to the Author Index">
             Seo, Hyeon-Woong
            </a>
           </td>
           <td class="r">
            Sungkyunkwan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#248389" title="Click to go to the Author Index">
             Moon, Seung Jae
            </a>
           </td>
           <td class="r">
            Sungkyunkwan, Mechanical Engineering, Robottory
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#102438" title="Click to go to the Author Index">
             Choi, Hyouk Ryeol
            </a>
           </td>
           <td class="r">
            Sungkyunkwan University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab5036" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#grippers_and_other_end_effectors" title="Click to go to the Keyword Index">
               Grippers and Other End-Effectors
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#grasping" title="Click to go to the Keyword Index">
               Grasping
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mechanism_design" title="Click to go to the Keyword Index">
               Mechanism Design
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             With the growth of logistics automation, there is an increasing demand for advanced grippers. This study presents a gripper that integrates suction cups into the fingertips to overcome the limitations of traditional robotic gripping methods. Designed with a 5-degree-of-freedom (DOF) structure, the gripper allows for angle adjustment of the suction cups, facilitating effective grasping in various environments. Its adaptive grasping mechanism simplifies control by using the fingertips and distal phalanxes to cage objects without manually controlling them. The versatility of the gripper was tested by performing hybrid finger-suction gripping, as well as conventional finger and suction gripping. These advanced gripping strategies are designed to enhance flexibility and efficiency in logistics automation when handling a diverse range of objects.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thet15">
             <b>
              ThET15
             </b>
            </a>
           </td>
           <td class="r">
            403
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thet15" title="Click to go to the Program at a Glance">
             <b>
              Datasets and Benchmarking
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#149815" title="Click to go to the Author Index">
             Sintov, Avishai
            </a>
           </td>
           <td class="r">
            Tel-Aviv University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet15_01">
             16:35-16:40, Paper ThET15.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('47'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Syn-Mediverse: A Multimodal Synthetic Dataset for Intelligent Scene Understanding of Healthcare Facilities
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#243678" title="Click to go to the Author Index">
             Mohan, Rohit
            </a>
           </td>
           <td class="r">
            University of Freiburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#342408" title="Click to go to the Author Index">
             Arce y de la Borbolla, José
            </a>
           </td>
           <td class="r">
            University of Freiburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#368592" title="Click to go to the Author Index">
             Mokhtar, Sassan
            </a>
           </td>
           <td class="r">
            University of Freiburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#205661" title="Click to go to the Author Index">
             Cattaneo, Daniele
            </a>
           </td>
           <td class="r">
            University of Freiburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#160427" title="Click to go to the Author Index">
             Valada, Abhinav
            </a>
           </td>
           <td class="r">
            University of Freiburg
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab47" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#data_sets_for_robotic_vision" title="Click to go to the Keyword Index">
               Data Sets for Robotic Vision
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_medical_robotics" title="Click to go to the Keyword Index">
               Computer Vision for Medical Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Safety and efficiency are paramount in healthcare facilities where the lives of patients are at stake. Despite the adoption of robots to assist medical staff in challenging tasks such as complex surgeries, human expertise is still indispensable. The next generation of autonomous healthcare robots hinges on their capacity to perceive and understand their complex and frenetic environments. While deep learning models are increasingly used for this purpose, they require extensive annotated training data which is impractical to obtain in real-world healthcare settings. To bridge this gap, we present Syn-Mediverse, the first hyper-realistic multimodal synthetic dataset of diverse healthcare facilities. Syn-Mediverse contains over 48,000 images from a simulated industry-standard optical tracking camera and provides more than 1.5M annotations spanning five different scene understanding tasks including depth estimation, object detection, semantic segmentation, instance segmentation, and panoptic segmentation. We demonstrate the complexity of our dataset by evaluating the performance on a broad range of state- of-the-art baselines for each task. To further advance research on scene understanding of healthcare facilities, along with the public dataset we provide an online evaluation benchmark available at http://syn-mediverse.cs.uni-freiburg.de.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet15_02">
             16:40-16:45, Paper ThET15.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3258'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              STEER: Flexible Robotic Manipulation Via Dense Language Grounding
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#268839" title="Click to go to the Author Index">
             Smith, Laura
            </a>
           </td>
           <td class="r">
            UC Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#218251" title="Click to go to the Author Index">
             Irpan, Alexander
            </a>
           </td>
           <td class="r">
            Google
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#340717" title="Click to go to the Author Index">
             Gonzalez Arenas, Montserrat
            </a>
           </td>
           <td class="r">
            Google
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#315580" title="Click to go to the Author Index">
             Kirmani, Sean
            </a>
           </td>
           <td class="r">
            Google DeepMind
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#249187" title="Click to go to the Author Index">
             Kalashnikov, Dmitry
            </a>
           </td>
           <td class="r">
            Google Brain
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#233045" title="Click to go to the Author Index">
             Shah, Dhruv
            </a>
           </td>
           <td class="r">
            Google DeepMind
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#241254" title="Click to go to the Author Index">
             Xiao, Ted
            </a>
           </td>
           <td class="r">
            Google DeepMind
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3258" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#data_sets_for_robot_learning" title="Click to go to the Keyword Index">
               Data Sets for Robot Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#big_data_in_robotics_and_automation" title="Click to go to the Keyword Index">
               Big Data in Robotics and Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The complexity of the real world demands robotic systems that can intelligently adapt to unseen situations. We present STEER, a robot learning framework that bridges high-level, commonsense reasoning with precise, flexible low-level control. Our approach translates complex situational awareness into actionable low-level behavior through training language-grounded policies with dense annotation. By structuring policy training around fundamental, modular manipulation skills expressed in natural language, STEER exposes an expressive interface for humans or Vision-Language Models (VLMs) to intelligently orchestrate the robot's behavior by reasoning about the task and context. Our experiments demonstrate the skills learned via STEER can be combined to synthesize novel behaviors to adapt to new situations or perform completely new tasks without additional data collection or training.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet15_03">
             16:45-16:50, Paper ThET15.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3605'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              MBE-ARI: A Multimodal Dataset Mapping Bi-Directional Engagement in Animal-Robot Interaction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422411" title="Click to go to the Author Index">
             Noronha, Ian
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424465" title="Click to go to the Author Index">
             Jawaji, Advait Prasad
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422523" title="Click to go to the Author Index">
             Soto, Juan
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#249450" title="Click to go to the Author Index">
             An, Jiajun
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#165030" title="Click to go to the Author Index">
             Gu, Yan
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#278740" title="Click to go to the Author Index">
             Kaur, Upinder
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3605" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#gesture__posture_and_facial_expressions" title="Click to go to the Keyword Index">
               Gesture, Posture and Facial Expressions
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#data_sets_for_robot_learning" title="Click to go to the Keyword Index">
               Data Sets for Robot Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multi_modal_perception_for_hri" title="Click to go to the Keyword Index">
               Multi-Modal Perception for HRI
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Animal-robot interaction (ARI) remains an unexplored challenge in robotics, as robots struggle to interpret the complex, multimodal communication cues of animals, such as body language, movement, and vocalizations. Unlike human-robot interaction, which benefits from established datasets and frameworks, animal-robot interaction lacks the foundational resources needed to facilitate meaningful bidirectional communication. To bridge this gap, we present the MBE-ARI (Multimodal Bidirectional Engagement in Animal-Robot Interaction), a novel multimodal dataset that captures detailed interactions between a legged robot and cows. The dataset includes synchronized RGB-D streams from multiple viewpoints, annotated with body pose and activity labels across interaction phases, offering an unprecedented level of detail for ARI research. Additionally, we introduce a full-body pose estimation model tailored for quadruped animals, capable of tracking 39 keypoints with a mean average precision (mAP) of 92.7%, outperforming existing benchmarks in animal pose estimation. The MBE-ARI dataset and our pose estimation framework lay a robust foundation for advancing research in animal-robot interaction, providing essential tools for developing perception, reasoning, and interaction frameworks needed for effective collaboration between robots and animals. The dataset and resources are publicly available at https://github.com/RISELabPurdue/MBE-ARI/, inviting further exploration and development in this critical area.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet15_04">
             16:50-16:55, Paper ThET15.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4918'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Diffusion-Based Data Generator for Training Object Recognition Models in Ultra-Range Distance
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#305896" title="Click to go to the Author Index">
             Bamani Beeri, Eran
            </a>
           </td>
           <td class="r">
            Tel Aviv University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#383600" title="Click to go to the Author Index">
             Nissinman, Eden
            </a>
           </td>
           <td class="r">
            Tel-Aviv University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#383601" title="Click to go to the Author Index">
             Koenigsberg, Lisa
            </a>
           </td>
           <td class="r">
            Tel-Aviv University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#293162" title="Click to go to the Author Index">
             Meir, Inbar
            </a>
           </td>
           <td class="r">
            Tel Aviv University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#149815" title="Click to go to the Author Index">
             Sintov, Avishai
            </a>
           </td>
           <td class="r">
            Tel-Aviv University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4918" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#data_sets_for_robotic_vision" title="Click to go to the Keyword Index">
               Data Sets for Robotic Vision
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#gesture__posture_and_facial_expressions" title="Click to go to the Keyword Index">
               Gesture, Posture and Facial Expressions
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#recognition" title="Click to go to the Keyword Index">
               Recognition
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Object recognition, commonly performed by a camera, is a fundamental requirement for robots to complete complex tasks. Some tasks require recognizing objects far from the robot's camera. A challenging example is Ultra-Range Gesture Recognition (URGR) in human-robot interaction where the user exhibits directive gestures at a distance of up to 25~m from the robot. However, training a model to recognize hardly visible objects located in ultra-range requires an exhaustive collection of a significant amount of labeled samples. The generation of synthetic training datasets is a recent solution to the lack of real-world data, while unable to properly replicate the realistic visual characteristics of distant objects in images. In this letter, we propose the Diffusion in Ultra-Range (DUR) framework based on a Diffusion model to generate labeled images of distant objects in various scenes. The DUR generator receives a desired distance and class (e.g., gesture) and outputs a corresponding synthetic image. We apply DUR to train a URGR model with directive gestures in which fine details of the gesturing hand are challenging to distinguish. DUR is compared to other types of generative models showcasing superiority both in fidelity and in recognition success rate when training a URGR model. More importantly, training a DUR model on a limited amount of real data and then using it to generate synthetic data for training a URGR model outperforms directly training the URGR model on real data. The synthetic-based URGR model is also demonstrated in gesture-based direction of a ground robot.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet15_05">
             16:55-17:00, Paper ThET15.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('5046'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              MovingCables: Moving Cable Segmentation Method and Dataset
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#243035" title="Click to go to the Author Index">
             Holesovsky, Ondrej
            </a>
           </td>
           <td class="r">
            Czech Technical University in Prague
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#223965" title="Click to go to the Author Index">
             Skoviera, Radoslav
            </a>
           </td>
           <td class="r">
            Czech Institute of Informatics, Robotics, and Cybernetics; Czech
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#168627" title="Click to go to the Author Index">
             Hlavac, Vaclav
            </a>
           </td>
           <td class="r">
            Czech Technical University in Prague
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab5046" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#data_sets_for_robotic_vision" title="Click to go to the Keyword Index">
               Data Sets for Robotic Vision
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Manipulating cluttered cables, hoses or ropes is challenging for both robots and humans. Humans often simplify these perceptually challenging tasks by pulling or pushing tangled cables and observing the resulting motions. We propose to use a similar interactive perception principle to aid robotic cable manipulation. A fundamental building block of such an endeavor is a cable motion segmentation method that densely labels moving cable image pixels. This letter presents MovingCables, a moving cable dataset, which we hope will motivate the development and evaluation of cable motion segmentation algorithms. The dataset consists of real-world image sequences automatically annotated with ground truth segmentation masks and optical flow. In addition, we propose a cable motion segmentation method and evaluate its performance on the new dataset.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thet16">
             <b>
              ThET16
             </b>
            </a>
           </td>
           <td class="r">
            404
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thet16" title="Click to go to the Program at a Glance">
             <b>
              Soft Sensors
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#168436" title="Click to go to the Author Index">
             Stuart, Hannah
            </a>
           </td>
           <td class="r">
            UC Berkeley
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#129946" title="Click to go to the Author Index">
             Monje, Concepción A.
            </a>
           </td>
           <td class="r">
            University Carlos III of Madrid
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet16_01">
             16:35-16:40, Paper ThET16.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('36'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Dynamic Contact Force Estimation Via Integration of Soft Sensor Based on Fiber Bragg Grating and Series Elastic Actuator
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#310705" title="Click to go to the Author Index">
             Na, Hyunbin
            </a>
           </td>
           <td class="r">
            DGIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#212121" title="Click to go to the Author Index">
             Lee, Hyunwook
            </a>
           </td>
           <td class="r">
            Gyeongsang National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#310714" title="Click to go to the Author Index">
             Park, Chang Hyun
            </a>
           </td>
           <td class="r">
            Pusan National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#310717" title="Click to go to the Author Index">
             Kim, Gyeong Hun
            </a>
           </td>
           <td class="r">
            Pusan National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#310719" title="Click to go to the Author Index">
             Kim, Chang-Seok
            </a>
           </td>
           <td class="r">
            Pusan National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#129982" title="Click to go to the Author Index">
             Oh, Sehoon
            </a>
           </td>
           <td class="r">
            DGIST
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab36" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#force_and_tactile_sensing" title="Click to go to the Keyword Index">
               Force and Tactile Sensing
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#compliant_joints_and_mechanisms" title="Click to go to the Keyword Index">
               Compliant Joints and Mechanisms
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#flexible_robotics" title="Click to go to the Keyword Index">
               Flexible Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Research on interactive force measurement in robotics follows two trends: distributed force sensing using soft tactile sensors and centered force sensing using rigid sensors. This study proposes a novel force sensing mechanism and algorithm to integrate the two approaches taking advantage of a soft tactile sensor and rigid actuator based on spring. Soft tactile sensors allow for gentle contact with humans, but have limited recovery and measurable force range. The rigidity of a spring-based actuator is utilized to address their force estimation issues. This allows for estimating a wider range of forces while maintaining the softness. The paper presents a novel approach for integrating two sensors using sophisticated algorithms. Specifically, a deep neural network is developed to estimate the contact location through the tactile sensor. Subsequently, a state-space observer is proposed based on the dynamic characteristics of the robot link, which integrates the network output and the torque measurements obtained from a spring-based actuator. This algorithm provides accurate force estimation during dynamic behavior and enables a wide measurable force range across the entire area of the robot link. The efficacy of the proposed mechanism and algorithm is validated through rigorous experimentation, demonstrating the fast recovery characteristics and accuracy.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet16_02">
             16:40-16:45, Paper ThET16.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('827'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Piezoresistive Printable Strain Sensor for Monitoring and Control of Soft Robotic Links
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#404399" title="Click to go to the Author Index">
             Sánchez, Claudia
            </a>
           </td>
           <td class="r">
            University Carlos III of Madrid
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#418677" title="Click to go to the Author Index">
             Rodriguez, Daniel
            </a>
           </td>
           <td class="r">
            AIMPLAS
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419077" title="Click to go to the Author Index">
             Otero, Susana
            </a>
           </td>
           <td class="r">
            AIMPLAS
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#129946" title="Click to go to the Author Index">
             Monje, Concepción A.
            </a>
           </td>
           <td class="r">
            University Carlos III of Madrid
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab827" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#soft_sensors_and_actuators" title="Click to go to the Keyword Index">
               Soft Sensors and Actuators
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#additive_manufacturing" title="Click to go to the Keyword Index">
               Additive Manufacturing
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#flexible_robotics" title="Click to go to the Keyword Index">
               Flexible Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Integrating sensors into soft links with complex geometries without compromising their flexibility, precision, or structural integrity remains one of the main challenges in soft robotics. This article presents the design, fabrication, and electromechanical evaluation of a 3D-printed flexible strain sensor tailored for monitoring and controlling these links. By combining Fused Filament Fabrication (FFF) and Direct Ink Writing (DIW) technologies, we manufactured a sensor composed of a thermoplastic polyurethane (TPU) substrate and a pattern of silver (Ag) nanoparticles ink, ensuring high flexibility and conductivity. We performed electromechanical tests to assess the sensor's performance, including three-point bending tests, cyclic loading to evaluate its durability, and angular deflection measurements to confirm its precision in detecting bending angles. The sensor demonstrated efficient piezoresistive behavior within a defined working range between 3% and 8% of flexure strain with a Gauge Factor (GF) of 0.24 and stable repeatability. We also tested its integration into a soft link, showing that the sensor maintains flexibility and accuracy during deformation.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet16_03">
             16:45-16:50, Paper ThET16.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1761'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              AnySkin: Plug-And-Play Skin Sensing for Robotic Touch
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#256454" title="Click to go to the Author Index">
             Bhirangi, Raunaq Mahesh
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420623" title="Click to go to the Author Index">
             Pattabiraman, Venkatesh
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420647" title="Click to go to the Author Index">
             Erciyes, Mehmet Enes
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420631" title="Click to go to the Author Index">
             Cao, Yifeng
            </a>
           </td>
           <td class="r">
            Columbia University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#189501" title="Click to go to the Author Index">
             Hellebrekers, Tess
            </a>
           </td>
           <td class="r">
            Meta AI Research
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#161575" title="Click to go to the Author Index">
             Pinto, Lerrel
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1761" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#soft_sensors_and_actuators" title="Click to go to the Keyword Index">
               Soft Sensors and Actuators
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#sensorimotor_learning" title="Click to go to the Keyword Index">
               Sensorimotor Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#transfer_learning" title="Click to go to the Keyword Index">
               Transfer Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             While tactile sensing is widely accepted as an important and useful sensing modality, its use pales in comparison to other sensory modalities like vision and proprioception. AnySkin addresses the critical challenges that impede the use of tactile sensing -- versatility, replaceability, and data reusability. Building on the simplistic design of ReSkin, and decoupling the sensing electronics from the sensing interface, AnySkin simplifies integration making it as straightforward as putting on a phone case and connecting a charger. Furthermore, AnySkin is the first uncalibrated tactile-sensor with cross-instance generalizability of learned manipulation policies. To summarize, this work makes three key contributions: first, we introduce a streamlined fabrication process and a design tool for creating an adhesive-free, durable and easily replaceable magnetic tactile sensor; second, we characterize slip detection and policy learning with the AnySkin sensor; and third, we demonstrate zero-shot generalization of models trained on one instance of AnySkin to new instances, and compare it with popular existing tactile solutions like DIGIT and ReSkin. Code, design files, and videos of policy experiments can be found on https://any-skin.github.io
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet16_04">
             16:50-16:55, Paper ThET16.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3098'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Proximity and Visuotactile Point Cloud Fusion for Contact Patches in Extreme Deformation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#219913" title="Click to go to the Author Index">
             Yin, Jessica
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#282828" title="Click to go to the Author Index">
             Shah, Paarth
            </a>
           </td>
           <td class="r">
            University of Oxford
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#176232" title="Click to go to the Author Index">
             Kuppuswamy, Naveen
            </a>
           </td>
           <td class="r">
            Toyota Research Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#292691" title="Click to go to the Author Index">
             Beaulieu, Andrew
            </a>
           </td>
           <td class="r">
            Toyota Research Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#278108" title="Click to go to the Author Index">
             Uttamchandani, Avinash
            </a>
           </td>
           <td class="r">
            Toyota Research Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#206962" title="Click to go to the Author Index">
             Castro, Alejandro
            </a>
           </td>
           <td class="r">
            Toyota Research Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#294881" title="Click to go to the Author Index">
             Pikul, James
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#105841" title="Click to go to the Author Index">
             Tedrake, Russ
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3098" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#force_and_tactile_sensing" title="Click to go to the Keyword Index">
               Force and Tactile Sensing
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#soft_sensors_and_actuators" title="Click to go to the Keyword Index">
               Soft Sensors and Actuators
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#soft_robot_materials_and_design" title="Click to go to the Keyword Index">
               Soft Robot Materials and Design
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Visuotactile sensors are a popular tactile sensing strategy due to high-fidelity estimates of local object geometry. However, existing algorithms for processing raw sensor inputs to useful intermediate signals such as contact patches struggle in high-deformation regimes. This is due to physical constraints imposed by sensor hardware and small-deformation assumptions used by mechanics-based models. In this work, we propose a fusion algorithm for proximity and visuotactile point clouds for contact patch segmentation, entirely independent from membrane mechanics. This algorithm exploits the synchronous, high spatial resolution proximity and visuotactile modalities enabled by an extremely deformable, selectively transmissive soft membrane, which uses visible light for visuotactile sensing and infrared light for proximity depth. We evaluate our contact patch algorithm in low (10%), medium (60%), and high (100%+) strain states. We compare our method against three baselines: proximity-only, tactile-only, and a first principles mechanics model. Our approach outperforms all baselines with an average RMSE under 2.8 mm of the contact patch geometry across all strain ranges. We demonstrate our contact patch algorithm in four applications: varied stiffness membranes, torque and shear-induced wrinkling, closed loop control, and pose estimation.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet16_05">
             16:55-17:00, Paper ThET16.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3545'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Spatial Sensitivity Equalization of ERT-Based Robotic Skin through Gauge Factor Distribution Optimization
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#256382" title="Click to go to the Author Index">
             Cho, Junhwi
            </a>
           </td>
           <td class="r">
            KAIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#338501" title="Click to go to the Author Index">
             Chung, Hyunjo
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology (KAIST)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#184906" title="Click to go to the Author Index">
             Park, Kyungseo
            </a>
           </td>
           <td class="r">
            Daegu Gyeongbuk Institute of Science and Technology (DGIST)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#115549" title="Click to go to the Author Index">
             Kim, Jung
            </a>
           </td>
           <td class="r">
            KAIST
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3545" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#force_and_tactile_sensing" title="Click to go to the Keyword Index">
               Force and Tactile Sensing
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#touch_in_hri" title="Click to go to the Keyword Index">
               Touch in HRI
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#soft_sensors_and_actuators" title="Click to go to the Keyword Index">
               Soft Sensors and Actuators
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Electrical Resistance Tomography (ERT) has emerged as a promising technology for large-area robotic skin due to its ability to reconstruct pressure distribution over extensive regions using a few sparsely distributed electrodes. Despite ERT’s potential to reconstruct the external forces applied on 3D surfaces, the uneven distribution of spatial sensitivity leads to significant errors in identifying the physical quantities of contacts, inhibiting this technique from being an effective tactile sensor. To address this issue, this paper proposes a method to equalize the spatial sensitivity by modulating the conductivity of ERT sensors through topology optimization. In a simulation environment, the sensor's conductive domain was converted into a binary image and optimized to equalize spatial sensitivity and reduce disparities between low and high-sensitivity areas. Additionally, we present a sensor fabrication method with a complex optimized conductive patch pattern from simulation by applying screen printing techniques. The effectiveness of the implemented spatial sensitivity equalization was validated by comparing it to a conventional ERT sensor in both simulations and real-world environments. The proposed sensitivity optimization method expands the use of ERT-based sensors for distributed tactile sensing in physical human-robot interaction scenarios.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet16_06">
             17:00-17:05, Paper ThET16.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3910'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Milli-Scale AcousTac Sensing Using Soft Helmholtz Resonators
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#313877" title="Click to go to the Author Index">
             Aderibigbe, Jadesola
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#274991" title="Click to go to the Author Index">
             Li, Monica
            </a>
           </td>
           <td class="r">
            Yale University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#285589" title="Click to go to the Author Index">
             Lee, Jungpyo
            </a>
           </td>
           <td class="r">
            University of California, Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#168436" title="Click to go to the Author Index">
             Stuart, Hannah
            </a>
           </td>
           <td class="r">
            UC Berkeley
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3910" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#soft_sensors_and_actuators" title="Click to go to the Keyword Index">
               Soft Sensors and Actuators
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#force_and_tactile_sensing" title="Click to go to the Keyword Index">
               Force and Tactile Sensing
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#soft_robot_applications" title="Click to go to the Keyword Index">
               Soft Robot Applications
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Acoustic transmission, or sound, can effectively communicate information over distances through various media. We focus on generating acoustic transmission using pneumatically driven resonators for wireless tactile sensing without the need for any electronics at the end-effector or contact point. We explore the relationship between emitted frequency and the geometry of the resonance chamber. When a normal compressive force is applied to the end cap, the compliant resonant cavity deforms, leading to an increase in frequency measurable by an external microphone. Prior work uses tube resonators with fipple attachments. In the present work, we study whether a different smaller audible cylindrical resonator with air blown across the entryway can be utilized instead. We test the utility of the Helmholtz resonator model in predicting the experimental frequency response. Resonance is often modeled for rigid cavities, presenting unique challenges in predicting resonance for the design of soft resonating taxels.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet16_07">
             17:05-17:10, Paper ThET16.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('5018'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Enhanced Model-Free Dynamic State Estimation for a Soft Robot Finger Using an Embedded Optical Waveguide Sensor
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#317672" title="Click to go to the Author Index">
             Krauss, Henrik
            </a>
           </td>
           <td class="r">
            Keio University, Faculty of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#103693" title="Click to go to the Author Index">
             Takemura, Kenjiro
            </a>
           </td>
           <td class="r">
            Keio University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab5018" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#modeling__control__and_learning_for_soft_robots" title="Click to go to the Keyword Index">
               Modeling, Control, and Learning for Soft Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#soft_sensors_and_actuators" title="Click to go to the Keyword Index">
               Soft Sensors and Actuators
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#machine_learning_for_robot_control" title="Click to go to the Keyword Index">
               Machine Learning for Robot Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this letter, an advanced stretchable optical waveguide sensor is implemented into a multidirectional PneuNet soft actuator to enhance dynamic state estimation through a NARX neural network. The stretchable waveguide featuring a semidivided core design from previous work is sensitive to multiple strain modes. It is integrated into a soft finger actuator with two pressure chambers that replicates human finger motions. The soft finger, designed for applications in soft robotic grippers or hands, is viewed in isolation under pneumatic actuation controlled by motorized linear stages. The research first characterizes the soft finger's workspace and sensor response. Subsequently, three dynamic state estimators are developed using NARX architecture, differing in the degree of incorporating the optical waveguide sensor response. Evaluation on a testing path reveals that the full sensor response significantly improves end effector position estimation, reducing mean error by 51% from 5.70 mm to 2.80 mm, compared to only 21% improvement to 4.53 mm using the estimator representing a single core waveguide design. The letter concludes by discussing the application of these estimators for (open-loop) model-predictive control and recommends future focus on advanced, structured soft (optical) sensors for model-free state estimation and control of soft robots.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thet17">
             <b>
              ThET17
             </b>
            </a>
           </td>
           <td class="r">
            405
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thet17" title="Click to go to the Program at a Glance">
             <b>
              Design and Control
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#199582" title="Click to go to the Author Index">
             Le Goff, Leni Kenneth
            </a>
           </td>
           <td class="r">
            Edinburgh Napier University
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#129456" title="Click to go to the Author Index">
             Padir, Taskin
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet17_01">
             16:35-16:40, Paper ThET17.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1120'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Efficient and Diverse Generative Robot Designs Using Evolution and Intrinsic Motivation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#199582" title="Click to go to the Author Index">
             Le Goff, Leni Kenneth
            </a>
           </td>
           <td class="r">
            UPMC
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#257187" title="Click to go to the Author Index">
             Smith, Simón C.
            </a>
           </td>
           <td class="r">
            Edinburgh Napier University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1120" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#evolutionary_robotics" title="Click to go to the Keyword Index">
               Evolutionary Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#methods_and_tools_for_robot_system_design" title="Click to go to the Keyword Index">
               Methods and Tools for Robot System Design
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#embodied_cognitive_science" title="Click to go to the Keyword Index">
               Embodied Cognitive Science
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Methods for generative design of robot physical configurations can automatically find optimal and innovative solutions for challenging tasks in complex environments. The vast search-space includes the physical design-space and the controller parameter-space, making it a challenging problem in machine learning and optimisation in general. Evolutionary algorithms (EAs) have shown promising results in generating robot designs via gradient-free optimisation. Morpho-evolution with learning (MEL) uses EAs to concurrently generate robot designs and learn the optimal parameters of the controllers. Two main issues prevent MEL from scaling to higher complexity tasks: i) computational cost and ii) premature convergence to sub-optimal designs. To address these issues, we propose combining morpho-evolution with intrinsic motivations. Intrinsically motivated behaviour arises from embodiment and simple learning rules without external guidance. We use a homeokinetic controller that generates exploratory behaviour in a few seconds with minimal knowledge of the robot’s design. Homeokinesis replaces costly learning phases, reducing computational time and favouring diversity, preventing premature convergence. We compare our approach with current MEL methods in several downstream tasks. The generated designs score higher in all the tasks, are more diverse, and are quickly generated compared to morpho-evolution with static parameters. Source and containers available at github.com/AutonomousRoboticEvolution.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet17_02">
             16:40-16:45, Paper ThET17.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2525'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Novel Hybrid Hysteresis Modeling Method for Multiloop-Asymmetry Hysteresis Behavior of Nonlinear Compliant Actuators
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#237744" title="Click to go to the Author Index">
             Zhou, Libo
            </a>
           </td>
           <td class="r">
            Zhejiang University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424122" title="Click to go to the Author Index">
             Xu, Lingpeng
            </a>
           </td>
           <td class="r">
            Zhejiang University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#322159" title="Click to go to the Author Index">
             Ou, Linlin
            </a>
           </td>
           <td class="r">
            Zhejiang University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#322143" title="Click to go to the Author Index">
             Yu, Xinyi
            </a>
           </td>
           <td class="r">
            Zhejiang University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424145" title="Click to go to the Author Index">
             Feng, Yalei
            </a>
           </td>
           <td class="r">
            Midea Group
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#102798" title="Click to go to the Author Index">
             Bai, Shaoping
            </a>
           </td>
           <td class="r">
            Aalborg University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2525" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#compliant_joints_and_mechanisms" title="Click to go to the Keyword Index">
               Compliant Joints and Mechanisms
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#prosthetics_and_exoskeletons" title="Click to go to the Keyword Index">
               Prosthetics and Exoskeletons
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#wearable_robotics" title="Click to go to the Keyword Index">
               Wearable Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Nonlinear compliant actuators are being increasingly used in human-robot interaction scenarios due to their inherent flexibility. However, a limitation is that nonlinear hysteresis exists, which will degrade the force/torque tracking performance if the hysteresis is not modeled accurately. Moreover, the existing methods are difficult to deal with the multi-loop asymmetry hysteresis. In this work, we present a novel modeling method, in which the hysteresis curves are decoupled into nonlinear reference lines and symmetrical hysteresis loops. A hybrid hysteresis model based on power function and Maxwellslip model is then developed to fit the nonlinear reference lines and symmetrical hysteresis loops respectively. Experiments were conducted on a nonlinear compliant actuator and the results show that the root-mean-square-errors (RMSE) of the hysteresis model decreases by 24.4% when compared with the Maxwellslip based hysteresis model.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet17_03">
             16:45-16:50, Paper ThET17.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2746'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Dynamic Mode Decomposition with Sonomyography and Electromyography for Predictive Modeling of Lower Limb Exoskeleton Walking
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#315150" title="Click to go to the Author Index">
             Lambeth, Krysten
            </a>
           </td>
           <td class="r">
            North Carolina State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424643" title="Click to go to the Author Index">
             Xue, Xiangming
            </a>
           </td>
           <td class="r">
            North Carolina State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#311696" title="Click to go to the Author Index">
             Singh, Mayank
            </a>
           </td>
           <td class="r">
            North Carolina State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#187424" title="Click to go to the Author Index">
             Huang, He (Helen)
            </a>
           </td>
           <td class="r">
            North Carolina State University and University of North Carolina
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#133414" title="Click to go to the Author Index">
             Sharma, Nitin
            </a>
           </td>
           <td class="r">
            North Carolina State University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2746" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#model_learning_for_control" title="Click to go to the Keyword Index">
               Model Learning for Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#prosthetics_and_exoskeletons" title="Click to go to the Keyword Index">
               Prosthetics and Exoskeletons
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#rehabilitation_robotics" title="Click to go to the Keyword Index">
               Rehabilitation Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The nonlinear dynamics required to model walking with multi-joint lower limb exoskeleton assistance results in high computational burden. To address this, we derive a Koopman-based linearized model of the human-exoskeleton system using electromyography and ultrasound-derived metrics of volitional muscle activity during exoskeleton-assisted walking. Data are collected from one participant with spinal cord injury (SCI) and two participants with no disabilities. Various electromyography and ultrasound-derived features in addition to normalized motor currents are used to derive predictive models, and we identify which muscle activation metrics produce the most accurate model for each subject. For both subjects without disabilities, the most accurate model uses only ultrasound-derived echogenicity as a metric of muscle activity, while the most accurate model for the subject with SCI uses only EMG wave length. Furthermore, the inclusion of ground reaction force increases the prediction accuracy of all models for one participant with no disabilities while decreasing the accuracy of most models for the participant with SCI. For all subjects, the most accurate subject-specific linear model has a root-mean-square error (averaged across limb segment angles) of &lt;8°.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet17_04">
             16:50-16:55, Paper ThET17.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3658'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Data-Driven Sampling Based Stochastic MPC for Skid-Steer Mobile Robot Navigation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#344993" title="Click to go to the Author Index">
             Trivedi, Ananya
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#379572" title="Click to go to the Author Index">
             Prajapati, Sarvesh
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425744" title="Click to go to the Author Index">
             Shirgaonkar, Anway Prasad
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#226883" title="Click to go to the Author Index">
             Zolotas, Mark
            </a>
           </td>
           <td class="r">
            Toyota Research Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#129456" title="Click to go to the Author Index">
             Padir, Taskin
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3658" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#model_learning_for_control" title="Click to go to the Keyword Index">
               Model Learning for Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#planning_under_uncertainty" title="Click to go to the Keyword Index">
               Planning under Uncertainty
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#robust_adaptive_control" title="Click to go to the Keyword Index">
               Robust/Adaptive Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Traditional approaches to motion modeling for skid-steer robots struggle to capture nonlinear tire-terrain dynamics, especially during high-speed maneuvers. In this paper, we tackle such nonlinearities by enhancing a dynamic unicycle model with Gaussian Process (GP) regression outputs. This enables us to develop an adaptive, uncertainty-informed navigation formulation. We solve the resultant stochastic optimal control problem using a chance-constrained Model Predictive Path Integral (MPPI) control method. This approach formulates obstacle avoidance and path-following as chance constraints, accounting for residual uncertainties from the GP to ensure safety and reliability in control. Leveraging GPU acceleration, we efficiently manage the non-convex nature of the problem, ensuring real-time performance. Our approach unifies path-following and obstacle avoidance across different terrains, unlike prior works which typically focus on one or the other. We compare our GP-MPPI method against unicycle and data-driven kinematic models within the MPPI framework. In simulations, our approach shows superior tracking accuracy and obstacle avoidance. We further validate our approach through hardware experiments on a skid-steer robot platform, demonstrating its effectiveness in high-speed navigation. The GPU implementation of the proposed method and supplementary video footage are available at https://stochasticmppi.github.io.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet17_05">
             16:55-17:00, Paper ThET17.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3990'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Agile Mobility with Rapid Online Adaptation Via Meta-Learning and Uncertainty-Aware MPPI
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#311910" title="Click to go to the Author Index">
             Kalaria, Dvij
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#355073" title="Click to go to the Author Index">
             Xue, Haoru
            </a>
           </td>
           <td class="r">
            University of California Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#379763" title="Click to go to the Author Index">
             Xiao, Wenli
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#326263" title="Click to go to the Author Index">
             Tao, Tony
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#236627" title="Click to go to the Author Index">
             Shi, Guanya
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104857" title="Click to go to the Author Index">
             Dolan, John M.
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3990" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#robust_adaptive_control" title="Click to go to the Keyword Index">
               Robust/Adaptive Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#machine_learning_for_robot_control" title="Click to go to the Keyword Index">
               Machine Learning for Robot Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#representation_learning" title="Click to go to the Keyword Index">
               Representation Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Modern non-linear model-based controllers require an accurate physics model and model parameters to be able to control mobile robots at their limits. Also, due to surface slipping at high speeds, the friction parameters may continually change (like tire degradation in autonomous racing), and the controller may need to adapt rapidly. Many works derive a task-specific robot model with a parameter adaptation scheme that works well for the task but requires a lot of effort and tuning for each platform and task. In this work, we design a full model-learning-based controller based on meta pre-training that can very quickly adapt using few-shot dynamics data to any wheel-based robot with any model parameters, while also reasoning about model uncertainty. We demonstrate our results in small-scale numeric simulation, the large-scale Unity simulator, and on a medium-scale hardware platform with a wide range of settings. We show that our results are comparable to domain-specific well-engineered controllers, and have excellent generalization performance across all scenarios
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet17_06">
             17:00-17:05, Paper ThET17.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4598'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Variable Transmission Mechanisms for Robotic Applications: A Review
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#215088" title="Click to go to the Author Index">
             Park, Jihyuk
            </a>
           </td>
           <td class="r">
            Yeungnam University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#392088" title="Click to go to the Author Index">
             Lee, Joon
            </a>
           </td>
           <td class="r">
            Sogang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#234227" title="Click to go to the Author Index">
             Seo, Hyung-Tae
            </a>
           </td>
           <td class="r">
            Kyonggi University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#179932" title="Click to go to the Author Index">
             Jeong, Seokhwan
            </a>
           </td>
           <td class="r">
            Mechanical Eng., Sogang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4598" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#mechanism_design" title="Click to go to the Keyword Index">
               Mechanism Design
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#actuation_and_joint_mechanisms" title="Click to go to the Keyword Index">
               Actuation and Joint Mechanisms
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#compliant_joints_and_mechanisms" title="Click to go to the Keyword Index">
               Compliant Joints and Mechanisms
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Actuators play a crucial role in robotics, determining the force and speed capabilities necessary for varied tasks, directly affecting the performance of the robotic system. With the growing reliance on robotics in both industrial applications and daily life, innovative actuator research has expanded significantly. Despite advances, traditional actuators encounter limitations in performance and operational range due to inherent physical constraints. To address these challenges, variable transmission mechanisms (VTMs) have emerged over the past decade as one of the alternative solutions, enhancing the adaptability and efficiency of robotic systems. However, there is currently a lack of survey articles that comprehensively cover the mechanisms and working principles of VTMs in robotics. This review article fills this gap by offering an extensive analysis of VTM applications in robotics. It categorizes VTMs based on their mechanisms and principles, presents case studies on both commercial and experimental VTMs, and provides insights into future
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet17_07">
             17:05-17:10, Paper ThET17.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4599'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Continuously Variable Transmission and Stiffness Actuator Based on Actively Variable Four-Bar Linkage for Highly Dynamic Robot Systems
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#366291" title="Click to go to the Author Index">
             Hur, Jungwoo
            </a>
           </td>
           <td class="r">
            Sogang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#309546" title="Click to go to the Author Index">
             Song, Hangyeol
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#179932" title="Click to go to the Author Index">
             Jeong, Seokhwan
            </a>
           </td>
           <td class="r">
            Mechanical Eng., Sogang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4599" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#mechanism_design" title="Click to go to the Keyword Index">
               Mechanism Design
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#actuation_and_joint_mechanisms" title="Click to go to the Keyword Index">
               Actuation and Joint Mechanisms
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#compliant_joints_and_mechanisms" title="Click to go to the Keyword Index">
               Compliant Joints and Mechanisms
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a novel actuation mechanism that combines a continuously variable transmission (CVT) mechanism with a variable stiffness actuator (VSA) for highly dynamic robot systems such as legged robots. The CVT effectively changes the input-output transmission ratio of the system, thereby extending the operational torque-speed range. Concurrently, the VSA adjusts the system stiffness, altering its compliance characteristics. Both CVT and VSA are seamlessly integrated into a single four-bar linkage mechanism, with their active features enabled by an actively variable link within this linkage. This CVT-VSA mechanism offers a range of dynamic advantages by inversely varying transmission ratio and stiffness, which includes impact mitigation, torque or speed amplification, and expanded control bandwidth. The implementation and efficacy of the CVT-VSA mechanism in a legged robot were tested and validated through a series of experiments.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thet18">
             <b>
              ThET18
             </b>
            </a>
           </td>
           <td class="r">
            406
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thet18" title="Click to go to the Program at a Glance">
             <b>
              Planning under Uncertainty 3
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#190197" title="Click to go to the Author Index">
             Fridovich-Keil, David
            </a>
           </td>
           <td class="r">
            The University of Texas at Austin
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet18_01">
             16:35-16:40, Paper ThET18.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1423'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Data-Driven Aggressive Autonomous Racing Framework Utilizing Local Trajectory Planning with Velocity Prediction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#372819" title="Click to go to the Author Index">
             Li, Zhouheng
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420875" title="Click to go to the Author Index">
             Zhou, Bei
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#309246" title="Click to go to the Author Index">
             Hu, Cheng
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#183442" title="Click to go to the Author Index">
             Xie, Lei
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#119160" title="Click to go to the Author Index">
             Su, Hongye
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1423" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#integrated_planning_and_learning" title="Click to go to the Keyword Index">
               Integrated Planning and Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#integrated_planning_and_control" title="Click to go to the Keyword Index">
               Integrated Planning and Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#constrained_motion_planning" title="Click to go to the Keyword Index">
               Constrained Motion Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The development of autonomous driving has boosted the research on autonomous racing. However, existing local trajectory planning methods have difficulty planning trajectories with optimal velocity profiles at racetracks with sharp corners, thus weakening the performance of autonomous racing. To address this problem, we propose a local trajectory planning method that integrates Velocity Prediction based on Model Predictive Contouring Control (VPMPCC). The optimal parameters of VPMPCC are learned through Bayesian Optimization (BO) based on a proposed novel Objective Function adapted to Racing (OFR). Specifically, VPMPCC achieves velocity prediction by encoding the racetrack as a reference velocity profile and incorporating it into the optimization problem. This method optimizes the velocity profile of local trajectories, especially at corners with significant curvature. The proposed OFR balances racing performance with vehicle safety, ensuring safe and efficient BO training. In the simulation, the number of training iterations for OFR-based BO is reduced by 42.86% compared to the state-of-the-art method. The optimal simulation-trained parameters are then applied to a real-world F1TENTH vehicle without retraining. During prolonged racing on a custom-built racetrack featuring significant sharp corners, the mean projected velocity of VPMPCC reaches 93.18% of the vehicle's handling limits. The released code is available at https://github.com/zhouhengli/VPMPCC.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet18_02">
             16:40-16:45, Paper ThET18.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1460'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              RLPP: A Residual Method for Zero-Shot Real-World Autonomous Racing on Scaled Platforms
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#335798" title="Click to go to the Author Index">
             Ghignone, Edoardo
            </a>
           </td>
           <td class="r">
            ETH
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#335794" title="Click to go to the Author Index">
             Baumann, Nicolas
            </a>
           </td>
           <td class="r">
            ETH
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#309246" title="Click to go to the Author Index">
             Hu, Cheng
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#415900" title="Click to go to the Author Index">
             Wang, Jonathan
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#183442" title="Click to go to the Author Index">
             Xie, Lei
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#180368" title="Click to go to the Author Index">
             Carron, Andrea
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#335799" title="Click to go to the Author Index">
             Magno, Michele
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1460" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#field_robots" title="Click to go to the Keyword Index">
               Field Robots
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#wheeled_robots" title="Click to go to the Keyword Index">
               Wheeled Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Autonomous racing presents a complex environment requiring robust controllers capable of making rapid decisions under dynamic conditions. While traditional controllers based on tire models are reliable, they often demand extensive tuning or system identification. RL methods offer significant potential due to their ability to learn directly from interaction, yet they typically suffer from the sim-to-real gap, where policies trained in simulation fail to perform effectively in the real world. In this paper, we propose RLPP, a residual RL framework that enhances a PP controller with an RL-based residual. This hybrid approach leverages the reliability and interpretability of PP while using RL to fine-tune the controller's performance in real-world scenarios. Extensive testing on the F1TENTH platform demonstrates that RLPP improves lap times of the baseline controllers by up to 6.37%, closing the gap to the SotA methods by more than 52% and providing reliable performance in zero-shot real-world deployment, overcoming key challenges associated with the sim-to-real transfer and reducing the performance gap from simulation to reality by more than 8-fold when compared to the baseline RL controller. The RLPP framework is made available as an open-source tool, encouraging further exploration and advancement in autonomous racing research. The code is available at: www.github.com/forzaeth/rlpp.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet18_03">
             16:45-16:50, Paper ThET18.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1541'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Uncertainty-Aware Probabilistic Risk Quantification of SOTIF for Autonomous Vehicles
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#412375" title="Click to go to the Author Index">
             Yao, Botao
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#413042" title="Click to go to the Author Index">
             Huang, Shuohan
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#413049" title="Click to go to the Author Index">
             Liu, Chuanyi
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#413043" title="Click to go to the Author Index">
             Han, Peiyi
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#413044" title="Click to go to the Author Index">
             Lin, Jie
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#413048" title="Click to go to the Author Index">
             Duan, Shaoming
            </a>
           </td>
           <td class="r">
            Pengcheng Laboratory
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1541" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#collision_avoidance" title="Click to go to the Keyword Index">
               Collision Avoidance
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#intelligent_transportation_systems" title="Click to go to the Keyword Index">
               Intelligent Transportation Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Ensuring the Safety of the Intended Functionality (SOTIF) for autonomous vehicles (AVs) is critical. Effective risk assessment helps AVs make decisions and avoid risks. However, existing methods face challenges due to environmental uncertainties, insufficient multi-dimensional risk quantification, and limited predictive accuracy. To address this challenge, we propose an uncertainty-aware probabilistic risk assessment framework that quantifies the risk of AVs violating safety constraints and calculates the expected average severity of such violations in uncertain environments. We first establish a general SOTIF risk model to characterize the static risk of the AV and surrounding traffic participants. Following this, we introduce a method for predicting dynamic uncertainty risks, resulting in probabilistic risk quantification. This framework accounts for multi-dimensional uncertainties and enhances safety under dynamic conditions. Extensive evaluations across typical traffic scenarios—including highways, intersections, and roundabouts—demonstrate that our method outperforms typical algorithms like Time Headway (THW) and Time-to-Collision (TTC). Empirical studies in extreme scenarios further validate the framework's ability to reduce risks and improve system generalization. The related code is available at: https://github.com/idslab-autosec/risk_uncertainty.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet18_04">
             16:50-16:55, Paper ThET18.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1821'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Think Deep and Fast: Learning Neural Nonlinear Opinion Dynamics from Inverse Dynamic Games for Split-Second Interactions
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#220692" title="Click to go to the Author Index">
             Hu, Haimin
            </a>
           </td>
           <td class="r">
            Princeton University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#198069" title="Click to go to the Author Index">
             Fernández Fisac, Jaime
            </a>
           </td>
           <td class="r">
            Princeton University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#119551" title="Click to go to the Author Index">
             Leonard, Naomi
            </a>
           </td>
           <td class="r">
            Princeton University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#325804" title="Click to go to the Author Index">
             Gopinath, Deepak
            </a>
           </td>
           <td class="r">
            Northwestern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#159948" title="Click to go to the Author Index">
             DeCastro, Jonathan
            </a>
           </td>
           <td class="r">
            Cornell University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#173551" title="Click to go to the Author Index">
             Rosman, Guy
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1821" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#human_aware_motion_planning" title="Click to go to the Keyword Index">
               Human-Aware Motion Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Non-cooperative interactions commonly occur in multi-agent scenarios such as car racing, where an ego vehicle can choose to overtake the rival, or stay behind it until a safe overtaking “corridor” opens. While an expert human can do well at making such time-sensitive decisions, autonomous agents are incapable of rapidly reasoning about complex, potentially conflicting options, leading to suboptimal behaviors such as deadlocks. Recently, the nonlinear opinion dynamics (NOD) model has proven to exhibit fast opinion formation and avoidance of decision deadlocks. However, NOD modeling parameters are oftentimes assumed fixed, limiting their applicability in complex and dynamic environments. It remains an open challenge to determine such parameters automatically and adaptively, accounting for the ever-changing environment. In this work, we propose for the first time a learning-based and game-theoretic approach to synthesize a Neural NOD model from expert demonstrations, given as a dataset containing (possibly incomplete) state and action trajectories of interacting agents. We demonstrate Neural NOD’s ability to make fast and deadlock-free decisions in a simulated autonomous racing example. We find that Neural NOD consistently outperforms the state-of-the-art data-driven inverse game baseline in terms of safety and overtaking performance.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet18_05">
             16:55-17:00, Paper ThET18.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2945'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Online Risk-Bounded Graph-Based Local Planning for Autonomous Driving with Theoretical Guarantees
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#367890" title="Click to go to the Author Index">
             Ahmad, Abdulrahman
            </a>
           </td>
           <td class="r">
            Khalifa University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#267359" title="Click to go to the Author Index">
             Khonji, Majid
            </a>
           </td>
           <td class="r">
            Khalifa University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#283068" title="Click to go to the Author Index">
             Elbassioni, Khaled
            </a>
           </td>
           <td class="r">
            Khalifa University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101631" title="Click to go to the Author Index">
             Dias, Jorge
            </a>
           </td>
           <td class="r">
            Khalifa University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#332870" title="Click to go to the Author Index">
             Al-Sumaiti, Ameena
            </a>
           </td>
           <td class="r">
            Khalifa University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2945" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#constrained_motion_planning" title="Click to go to the Keyword Index">
               Constrained Motion Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#collision_avoidance" title="Click to go to the Keyword Index">
               Collision Avoidance
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#planning_under_uncertainty" title="Click to go to the Keyword Index">
               Planning under Uncertainty
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Risk-bounded motion planning in dynamic environments for autonomous driving presents complex challenges, particularly in solving the nonconvex problem of ensuring continuous, safe, and real-time navigation towards a destination. This paper introduces an online graph-based local planning approach constrained by a user-defined driving style in terms of a risk budget Delta for the entire mission. Our online approach assigns a risk bound to each motion planning decision, ensuring that the total risk consumed remains within Delta. First, we construct a spatial lattice graph that adheres to the vehicle's curvature constraints. Then, the trajectory planning problem is reformulated as an online optimization problem, where decisions must be made sequentially without prior knowledge of future events. Therefore, we propose a reduction to the problem to be online multiple-choice knapsack problem (ON-MCKP), where the knapsack items are candidate paths generated by solving constrained shortest-path problems. To solve the ON-MCKP, we deploy online algorithms that offer theoretical guarantees on the risk allocation throughout the entire mission. The effectiveness of our method is demonstrated empirically, showing significant improvements in the objective without violating safety constraints.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet18_06">
             17:00-17:05, Paper ThET18.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3535'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Dashing for the Golden Snitch: Multi-Drone Time-Optimal Motion Planning with Multi-Agent Reinforcement Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423254" title="Click to go to the Author Index">
             Wang, Xian
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#354528" title="Click to go to the Author Index">
             Zhou, Jin
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423284" title="Click to go to the Author Index">
             Feng, Yuanli
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#388363" title="Click to go to the Author Index">
             Mei, Jiahao
            </a>
           </td>
           <td class="r">
            Zhejiang University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#163844" title="Click to go to the Author Index">
             Chen, Jiming
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#230382" title="Click to go to the Author Index">
             Li, Shuo
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3535" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Recent innovations in autonomous drones have facilitated time-optimal flight in single-drone configurations, and enhanced maneuverability in multi-drone systems by applying optimal control and learning-based methods. However, few studies have achieved time-optimal motion planning for multi-drone systems, particularly during highly agile maneuvers or in dynamic scenarios. This paper presents a decentralized policy network using multi-agent reinforcement learning for time-optimal multi-drone flight. To strike a balance between flight efficiency and collision avoidance, we introduce a soft collision-free mechanism inspired by optimization-based methods. By customizing PPO in a centralized training, decentralized execution (CTDE) fashion, we unlock higher efficiency and stability in training while ensuring lightweight implementation. Extensive simulations show that, despite slight performance trade-offs compared to single-drone systems, our multi-drone approach maintains near-time-optimal performance with a low collision rate. Real-world experiments validate our method, with two quadrotors using the same network as in simulation achieving a maximum speed of 13.65 m/s and a maximum body rate of 13.4 rad/s in a 5.5 m × 5.5 m × 2.0 m space across various tracks, relying entirely on onboard computation.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet18_07">
             17:05-17:10, Paper ThET18.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4856'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Kernel-Based Metrics Learning for Uncertain Opponent Vehicle Trajectory Prediction in Autonomous Racing
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#332849" title="Click to go to the Author Index">
             Lee, Hojin
            </a>
           </td>
           <td class="r">
            Ulsan National Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#314740" title="Click to go to the Author Index">
             Nam, Youngim
            </a>
           </td>
           <td class="r">
            Ulsan National Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#378204" title="Click to go to the Author Index">
             Lee, Sanghun
            </a>
           </td>
           <td class="r">
            Ulsan Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#305540" title="Click to go to the Author Index">
             Kwon, Cheolhyeon
            </a>
           </td>
           <td class="r">
            Ulsan National Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4856" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#planning_under_uncertainty" title="Click to go to the Keyword Index">
               Planning under Uncertainty
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#integrated_planning_and_learning" title="Click to go to the Keyword Index">
               Integrated Planning and Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#machine_learning_for_robot_control" title="Click to go to the Keyword Index">
               Machine Learning for Robot Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Autonomous racing confronts significant challenges in safely overtaking Opponent Vehicles (OVs) that exhibit uncertain trajectories, stemming from unknown driving policies. To address these challenges, this study proposes heterogeneous kernel metrics for Deep Kernel Learning (DKL), designed to robustly capture the diverse driving policies of OVs, and carry out precise trajectory predictions along with the associated uncertainties. A key virtue of the proposed kernel metrics lies in their ability to align similar driving policies and disjoin dissimilar ones in an unsupervised manner, given the observed interactions between the Ego Vehicle (EV) and OVs. The efficacy of the proposed method is substantiated through experimental studies on a 1/10th scale racecar platform, demonstrating improved prediction accuracy and thereby safely overtaking against OVs. Furthermore, our method is computationally efficient for onboard computing units, affirming its viability in fast-paced racing environments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet18_08">
             17:10-17:15, Paper ThET18.8
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4876'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Inferring Occluded Agent Behavior in Dynamic Games from Noise Corrupted Observations
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#292737" title="Click to go to the Author Index">
             Qiu, Tianyu
            </a>
           </td>
           <td class="r">
            University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#190197" title="Click to go to the Author Index">
             Fridovich-Keil, David
            </a>
           </td>
           <td class="r">
            The University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4876" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#planning_under_uncertainty" title="Click to go to the Keyword Index">
               Planning under Uncertainty
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In mobile robotics and autonomous driving, it is natural to model agent interactions as the Nash equilibrium of a noncooperative, dynamic game. These methods inherently rely on observations from sensors such as lidars and cameras to identify agents participating in the game and, therefore, have difficulty when some agents are occluded. To address this limitation, this paper presents an occlusion-aware game-theoretic inference method to estimate the locations of potentially occluded agents, and simultaneously infer the intentions of both visible and occluded agents, which best accounts for the observations of visible agents. Additionally, we propose a receding horizon planning strategy based on an occlusion-aware contingency game designed to navigate in scenarios with potentially occluded agents. Monte Carlo simulations validate our approach, demonstrating that it accurately estimates the game model and trajectories for both visible and occluded agents using noisy observations of visible agents. Our planning pipeline significantly enhances navigation safety when compared to occlusion-ignorant baseline as well.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thet19">
             <b>
              ThET19
             </b>
            </a>
           </td>
           <td class="r">
            407
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thet19" title="Click to go to the Program at a Glance">
             <b>
              Manufacturing and Processes
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#290614" title="Click to go to the Author Index">
             Zhou, Zhengxue
            </a>
           </td>
           <td class="r">
            University of Liverpool
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet19_01">
             16:35-16:40, Paper ThET19.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('861'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Domain Randomization for Object Detection in Manufacturing Applications Using Synthetic Data: A Comprehensive Study
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#413555" title="Click to go to the Author Index">
             Zhu, Xiaomeng
            </a>
           </td>
           <td class="r">
            KTH and Scania CV AB
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419414" title="Click to go to the Author Index">
             Henningsson, Jacob
            </a>
           </td>
           <td class="r">
            Uppsala University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417864" title="Click to go to the Author Index">
             Li, Duruo
            </a>
           </td>
           <td class="r">
            Scania CV AB
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417881" title="Click to go to the Author Index">
             Mårtensson, Pär
            </a>
           </td>
           <td class="r">
            Scania
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#183970" title="Click to go to the Author Index">
             Hanson, Lars
            </a>
           </td>
           <td class="r">
            Skövde University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#106818" title="Click to go to the Author Index">
             Björkman, Mårten
            </a>
           </td>
           <td class="r">
            KTH
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#170470" title="Click to go to the Author Index">
             Maki, Atsuto
            </a>
           </td>
           <td class="r">
            KTH Royal Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab861" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_manufacturing" title="Click to go to the Keyword Index">
               Computer Vision for Manufacturing
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#data_sets_for_robotic_vision" title="Click to go to the Keyword Index">
               Data Sets for Robotic Vision
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper addresses key aspects of domain randomization in generating synthetic data for manufacturing object detection applications. To this end, we present a comprehensive data generation pipeline that reflects different factors: object characteristics, background, illumination, camera settings, and post-processing. We also introduce the Synthetic Industrial Parts Object Detection dataset (SIP15-OD) consisting of 15 objects from three industrial use cases under varying environments as a test bed for the study, while also employing an industrial dataset publicly available for robotic applications. In our experiments, we present more abundant results and insights into the feasibility as well as challenges of sim-to-real object detection. In particular, we identified material properties, rendering methods, post-processing, and distractors as important factors. Our method, leveraging these, achieves top performance on the public dataset with Yolov8 models trained exclusively on synthetic data; mAP@50 scores of 96.4% for the robotics dataset, and 94.1%, 99.5%, and 95.3% across three of the SIP15-OD use cases, respectively. The results showcase the effectiveness of the proposed domain randomization, potentially covering the distribution close to real data for the applications.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet19_02">
             16:40-16:45, Paper ThET19.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2537'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Component-Aware Unsupervised Logical Anomaly Generation for Industrial Anomaly Detection
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#415147" title="Click to go to the Author Index">
             Tong, Xuan
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#413517" title="Click to go to the Author Index">
             Chang, Yang
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419845" title="Click to go to the Author Index">
             Zhao, Qing
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419798" title="Click to go to the Author Index">
             Yu, Jiawen
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419826" title="Click to go to the Author Index">
             Wang, Boyang
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419415" title="Click to go to the Author Index">
             Lin, Junxiong
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#413488" title="Click to go to the Author Index">
             Lin, Yuxuan
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419031" title="Click to go to the Author Index">
             Mai, Xinji
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#418973" title="Click to go to the Author Index">
             Wang, Haoran
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#419852" title="Click to go to the Author Index">
             Tao, Zeng
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#413481" title="Click to go to the Author Index">
             Wang, Yan
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#110529" title="Click to go to the Author Index">
             Zhang, Wenqiang
            </a>
           </td>
           <td class="r">
            Fudan University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2537" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_manufacturing" title="Click to go to the Keyword Index">
               Computer Vision for Manufacturing
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Anomaly detection is critical in industrial manufacturing for ensuring product quality and improving efficiency in automated processes. The scarcity of anomalous samples limits traditional detection methods, making anomaly generation essential for expanding the data repository. However, recent generative models often produce unrealistic anomalies increasing false positives, or require real-world anomaly samples for training. In this work, we treat anomaly generation as a compositional problem and propose ComGEN, a component-aware and unsupervised framework that addresses the gap in logical anomaly generation. Our method comprises a multi-component learning strategy to disentangle visual components, followed by subsequent generation editing procedures. Disentangled text-to-component pairs, revealing intrinsic logical constraints, conduct attention-guided residual mapping and model training with iteratively matched references across multiple scales. Experiments on the MVTecLOCO dataset confirm the efficacy of ComGEN, achieving the best AUROC score of 91.2%. Additional experiments on the real-world scenario of Diesel Engine and widely-used MVTecAD dataset demonstrate significant performance improvements when integrating simulated anomalies generated by ComGEN into automated production workflows.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet19_03">
             16:45-16:50, Paper ThET19.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2768'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Use the Force, Bot! - Force-Aware ProDMP with Event-Based Replanning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#418699" title="Click to go to the Author Index">
             Lödige, Paul Werner
            </a>
           </td>
           <td class="r">
            Karlsruhe Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#341099" title="Click to go to the Author Index">
             Li, Maximilian Xiling
            </a>
           </td>
           <td class="r">
            Karlsruhe Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#169287" title="Click to go to the Author Index">
             Lioutikov, Rudolf
            </a>
           </td>
           <td class="r">
            Karlsruhe Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2768" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Movement Primitives (MPs) are a well-established method for representing and generating modular robot trajectories. This work presents FA-ProDMP, a novel approach that introduces force awareness to Probabilistic Dynamic Movement Primitives (ProDMP). FA-ProDMP adapts trajectories during runtime to account for measured and desired forces, offering smooth trajectories and capturing position and force correlations across multiple demonstrations. FA-ProDMPs support multiple axes of force, making them agnostic to Cartesian or joint space control. This versatility makes FA-ProDMP a valuable tool for learning contact rich manipulation tasks, such power plug insertion. To reliably evaluate FA-ProDMP, this work additionally introduces a modular, 3D printed task suite called POEMPEL, inspired by the popular Lego Technic pins. POEMPEL mimics industrial peg-in-hole assembly tasks with force requirements and offers multiple parameters of adjustment, such as position, orientation and plug stiffness level, thereby varying the direction and amount of required forces. Our experiments demonstrate that FA-ProDMP outperforms other MP formulations on the POEMPEL setup and a electrical power plug insertion task, thanks to its replanning capabilities based on measured forces. These findings highlight how FA-ProDMP enhances the performance of robotic systems in contact-rich manipulation tasks.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet19_04">
             16:50-16:55, Paper ThET19.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2934'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Reinforcement Learning on Reconfigurable Hardware: Overcoming Material Variability in Laser Material Processing
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#343479" title="Click to go to the Author Index">
             Masinelli, Giulio
            </a>
           </td>
           <td class="r">
            EPFL
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#327908" title="Click to go to the Author Index">
             Rajani, Chang
            </a>
           </td>
           <td class="r">
            Swiss Federal Laboratories for Materials Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424811" title="Click to go to the Author Index">
             Hoffmann, Patrik
            </a>
           </td>
           <td class="r">
            Empa
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#343009" title="Click to go to the Author Index">
             Wasmer, Kilian
            </a>
           </td>
           <td class="r">
            EMPA
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#217718" title="Click to go to the Author Index">
             Atienza, David
            </a>
           </td>
           <td class="r">
            Epfl Sti Imt Esl
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2934" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#manufacturing__maintenance_and_supply_chains" title="Click to go to the Keyword Index">
               Manufacturing, Maintenance and Supply Chains
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#hardware_software_integration_in_robotics" title="Click to go to the Keyword Index">
               Hardware-Software Integration in Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Ensuring consistent processing quality is challenging in laser processes due to varying material properties and surface conditions. Although some approaches have shown promise in solving this problem via automation, they often rely on predetermined targets or are limited to simulated environments. To address these shortcomings, we propose a novel real-time reinforcement learning approach for laser process control, implemented on a Field Programmable Gate Array to achieve real-time execution. Our experimental results from laser welding tests on stainless steel samples with a range of surface roughnesses validated the method's ability to adapt autonomously, without relying on reward engineering or prior setup information. Specifically, the algorithm learned the optimal power profile for each unique surface characteristic, demonstrating significant improvements over hand-engineered optimal constant power strategies — up to 23% better performance on rougher surfaces and 7% on mixed surfaces. This approach represents a significant advancement in automating and optimizing laser processes, with potential applications across multiple industries.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet19_05">
             16:55-17:00, Paper ThET19.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2964'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              GenCo: A Dual LVLM Generate-Correct Framework for Adaptive Peg-In-Hole Robotics
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#290614" title="Click to go to the Author Index">
             Zhou, Zhengxue
            </a>
           </td>
           <td class="r">
            University of Liverpool
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#308025" title="Click to go to the Author Index">
             Veeramani, Satheeshkumar
            </a>
           </td>
           <td class="r">
            University of Liverpool
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#191516" title="Click to go to the Author Index">
             Fakhruldeen, Hatem
            </a>
           </td>
           <td class="r">
            University of Liverpool
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424534" title="Click to go to the Author Index">
             Uyanik, Seda
            </a>
           </td>
           <td class="r">
            University of Liverpool
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#312923" title="Click to go to the Author Index">
             Cooper, Andrew Ian
            </a>
           </td>
           <td class="r">
            University of Liverpool
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2964" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#perception_action_coupling" title="Click to go to the Keyword Index">
               Perception-Action Coupling
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#cognitive_control_architectures" title="Click to go to the Keyword Index">
               Cognitive Control Architectures
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#industrial_robots" title="Click to go to the Keyword Index">
               Industrial Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Recent advances in Vision Language Models (VLMs) have enhanced their application in robotics, encompassing both high-level task planning and low-level action control. Despite their strong performance across various robotic tasks, even for zero-shot scenarios, most VLM applications remain open-loop, adhering to a plan-and-execute paradigm without mechanisms to assess task completion. To address this limitation, we propose GenCo, a Generate-Correct framework designed to automate a peg-in-hole task using a UR5e robot. This framework integrates an VLM-based motion generator and motion expert, working collaboratively to refine and correct actions during robotic task execution. Both VLM agents are fine-tuned using the pre-trained LLaVA, enhancing adaptability and scaling efficiently to diverse tasks. Our experiments demonstrate the adaptiveness of the framework, improving the success rate for the peg-in-hole task by 12.75% compared to a single VLM open-loop method. Notably, in unseen scenarios, the success rate for a triangular peg was increased by 15%, and for a random-shaped peg by 17%, underscoring the system's effectiveness in handling novel tasks. Adaptive testing under varied camera positions demonstrated robust performance, affirming reliability despite shifts in the visual input. The framework is also designed to be lightweight and efficient, facilitating broader adoption and practical deployment. Access to our code and model is provided here: https://github.com/Zhengxuez/generate_correct
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet19_06">
             17:00-17:05, Paper ThET19.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3263'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              ASCENT: Autonomous Skill Learning Toward Complex Embodied Tasks with Foundation Models
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423652" title="Click to go to the Author Index">
             Wu, Haolin
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#337730" title="Click to go to the Author Index">
             Liu, Yuecheng
            </a>
           </td>
           <td class="r">
            Huawei Noah's Ark Lab
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#340686" title="Click to go to the Author Index">
             Dong, Junyi
            </a>
           </td>
           <td class="r">
            Cornell University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#242365" title="Click to go to the Author Index">
             Zhang, Heng
            </a>
           </td>
           <td class="r">
            Huawei
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#352739" title="Click to go to the Author Index">
             Mao, Sitong
            </a>
           </td>
           <td class="r">
            ShenZhen Huawei Cloud Computing Technologies Co., Ltd
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#103003" title="Click to go to the Author Index">
             Wang, Hesheng
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#435529" title="Click to go to the Author Index">
             Wu, Weigang
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#195314" title="Click to go to the Author Index">
             Zhou, Shunbo
            </a>
           </td>
           <td class="r">
            Huawei
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3263" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#domestic_robotics" title="Click to go to the Keyword Index">
               Domestic Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Collecting data from simulated scenarios for training robotic skills provides a safer and more controllable alternative to real-world environments. However, it demands considerable effort, including the manual construction of simulation environments, the careful design of tasks, and the challenge of obtaining effective trajectories. These limitations hinder the efficiency of data collection from simulated scenarios. In this paper, we leverage the prior knowledge of Large Language Models (LLMs) and Large Multimodal Models (LMMs) to generate simulated scenarios and embodied tasks. We introduce a novel framework, ASCENT (Autonomous Skill learning toward Complex Embodied tasks with fouNdaTion models), designed to efficiently accomplish these tasks and generate trajectory data. ASCENT features a fully autonomous skill learning mechanism based on AI agent. During task training, the AI agent identifies suitable atomic skills from an atomic skill library to either directly complete the task or serve as an initial policy for further training. Newly acquired atomic skills are subsequently added to the library. To address training failures and enhance efficiency, the AI agent uses an LLM to automatically optimize the skill training process based on feedback received from simulations. Experimental results indicate that the number of training steps required for learning new tasks can be reduced by up to 65.9%.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet19_07">
             17:05-17:10, Paper ThET19.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3405'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Ms. NAMI: Multimodal Semantic Navigation on Relative Metric Intention Graph
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421318" title="Click to go to the Author Index">
             Zhai, Shichao
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#291053" title="Click to go to the Author Index">
             Cui, Yuxiang
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#378112" title="Click to go to the Author Index">
             Ye, Shuhao
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#361265" title="Click to go to the Author Index">
             Yu, Xuan
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#352739" title="Click to go to the Author Index">
             Mao, Sitong
            </a>
           </td>
           <td class="r">
            ShenZhen Huawei Cloud Computing Technologies Co., Ltd
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#195314" title="Click to go to the Author Index">
             Zhou, Shunbo
            </a>
           </td>
           <td class="r">
            Huawei
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#113216" title="Click to go to the Author Index">
             Xiong, Rong
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#156231" title="Click to go to the Author Index">
             Wang, Yue
            </a>
           </td>
           <td class="r">
            Zhejiang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3405" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#domestic_robotics" title="Click to go to the Keyword Index">
               Domestic Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#autonomous_vehicle_navigation" title="Click to go to the Keyword Index">
               Autonomous Vehicle Navigation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Embodied navigation in unknown environments presents the significant challenge of integrating tasks with multimodal goals into a unified framework. In this paper, we propose the Multimodal Semantic Navigation on Relative Metric Intention Graph (Ms. NAMI), a framework that integrates various navigation tasks with multimodal goals based on a relative topo-metric intention graph. A reinforcement learning based policy with a concise action space, consisting of frontier nodes and intention nodes, is designed to guide the agent to select reasonable sub-goals. A sparse reward design is introduced to reduce bias during training. Additionally, several engineering optimizations are implemented to enhance overall performance. The experimental results indicate that our method can achieve robust navigation performance in a variety of unknown environments.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thet20">
             <b>
              ThET20
             </b>
            </a>
           </td>
           <td class="r">
            408
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thet20" title="Click to go to the Program at a Glance">
             <b>
              Agricultural Automation 4
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#123478" title="Click to go to the Author Index">
             Hauser, Kris
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#137262" title="Click to go to the Author Index">
             Behley, Jens
            </a>
           </td>
           <td class="r">
            University of Bonn
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet20_01">
             16:35-16:40, Paper ThET20.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('176'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Towards Autonomous Crop Monitoring: Inserting Sensors in Cluttered Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#245862" title="Click to go to the Author Index">
             Lee, Moonyoung
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#383458" title="Click to go to the Author Index">
             Berger, Aaron
            </a>
           </td>
           <td class="r">
            Harvard University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#383499" title="Click to go to the Author Index">
             Guri, Dominic
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#227053" title="Click to go to the Author Index">
             Zhang, Kevin
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#390772" title="Click to go to the Author Index">
             Coffey, Lisa
            </a>
           </td>
           <td class="r">
            Iowa State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101976" title="Click to go to the Author Index">
             Kantor, George
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#124019" title="Click to go to the Author Index">
             Kroemer, Oliver
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab176" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#agricultural_automation" title="Click to go to the Keyword Index">
               Agricultural Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#robotics_and_automation_in_agriculture_and_forestry" title="Click to go to the Keyword Index">
               Robotics and Automation in Agriculture and Forestry
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#hardware_software_integration_in_robotics" title="Click to go to the Keyword Index">
               Hardware-Software Integration in Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Monitoring crop nutrients can aid farmers in optimizing fertilizer use. Many existing robots rely on visionbased phenotyping, however, which can only indirectly estimate nutrient deficiencies once crops have undergone visible color changes. We present a contact-based phenotyping robot platform that can directly insert nitrate sensors into cornstalks to proactively monitor macronutrient levels in crops. This task is challenging because inserting such sensors requires subcentimeter precision in an environment which contains high levels of clutter, lighting variation, and occlusion. To address these challenges, we develop a robust perception-action pipeline to grasp stalks, and create a custom robot gripper which mechanically aligns the sensor before inserting it into the stalk. Through experimental validation on 48 unique stalks in a cornfield in Iowa, we demonstrate our platform’s capability of detecting a stalk with 94% success, grasping a stalk with 90% success, and inserting a sensor with 60% success. In addition to developing an autonomous phenotyping research platform, we share key challenges and insights obtained from deployment in the field. Our research platform is open-sourced, with additional information available at https://kantor-lab.github.io/cornbot.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet20_02">
             16:40-16:45, Paper ThET20.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('582'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Dataset and Benchmark for Shape Completion of Fruits for Agricultural Robotics
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#274849" title="Click to go to the Author Index">
             Magistri, Federico
            </a>
           </td>
           <td class="r">
            University of Bonn
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#218841" title="Click to go to the Author Index">
             Läbe, Thomas
            </a>
           </td>
           <td class="r">
            University of Bonn
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#312976" title="Click to go to the Author Index">
             Marks, Elias Ariel
            </a>
           </td>
           <td class="r">
            University of Bonn
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#320282" title="Click to go to the Author Index">
             Nagulavancha, Sumanth
            </a>
           </td>
           <td class="r">
            University of Bonn
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#284469" title="Click to go to the Author Index">
             Pan, Yue
            </a>
           </td>
           <td class="r">
            University of Bonn
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#279508" title="Click to go to the Author Index">
             Smitt, Claus
            </a>
           </td>
           <td class="r">
            University of Bonn
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#157720" title="Click to go to the Author Index">
             Klingbeil, Lasse
            </a>
           </td>
           <td class="r">
            University of Bonn
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#217748" title="Click to go to the Author Index">
             Halstead, Michael Allan
            </a>
           </td>
           <td class="r">
            Bonn University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#192583" title="Click to go to the Author Index">
             Kuhlmann, Heiner
            </a>
           </td>
           <td class="r">
            University of Bonn
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#191378" title="Click to go to the Author Index">
             McCool, Christopher Steven
            </a>
           </td>
           <td class="r">
            University of Bonn
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#137262" title="Click to go to the Author Index">
             Behley, Jens
            </a>
           </td>
           <td class="r">
            University of Bonn
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101642" title="Click to go to the Author Index">
             Stachniss, Cyrill
            </a>
           </td>
           <td class="r">
            University of Bonn
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab582" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#robotics_and_automation_in_agriculture_and_forestry" title="Click to go to the Keyword Index">
               Robotics and Automation in Agriculture and Forestry
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#data_sets_for_robotic_vision" title="Click to go to the Keyword Index">
               Data Sets for Robotic Vision
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#agricultural_automation" title="Click to go to the Keyword Index">
               Agricultural Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             As the world population is expected to reach 10 billion by 2050, our agricultural production system needs to double its productivity despite a decline of human workforce in the agricultural sector. Autonomous robotic systems are one promising pathway to increase productivity by taking over labor-intensive manual tasks like fruit picking. To be effective, such systems need to monitor and interact with plants and fruits precisely, which is challenging due to the cluttered nature of agricultural environments causing, for example, strong occlusions. Thus, being able to estimate the complete 3D shapes of objects in presence of occlusions is crucial for automating operations such as fruit harvesting. In this paper, we propose the first publicly available 3D shape completion dataset for agricultural vision systems. We provide an RGB-D dataset for estimating the 3D shape of fruits. Specifically, our dataset contains RGB-D frames of single sweet peppers in lab conditions but also in a commercial greenhouse. For each fruit, we additionally collected high-precision point clouds that we use as ground truth. For acquiring the ground truth shape, we developed a measuring process that allows us to record data of real sweet pepper plants, both in the lab and in the greenhouse with high precision, and determine the shape of the sensed fruits. We release our dataset, consisting of almost 7,000 RGB-D frames belonging to more than 100 different fruits. We provide segmented RGB-D frames, with camera intrinsics to easily obtain colored point clouds, together with the corresponding high-precision, occlusion-free point clouds obtained with a high-precision laser scanner. We additionally enable evaluation of shape completion approaches on a hidden test set through a public challenge on a benchmark server.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet20_03">
             16:45-16:50, Paper ThET20.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1425'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Novel Control Strategy for Offset Points Tracking in the Context of Agricultural Robotics
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#412219" title="Click to go to the Author Index">
             Ngnepiepaye Wembe, Stephane
            </a>
           </td>
           <td class="r">
            University of Clermont Auvergne, French National Research Instit
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#150306" title="Click to go to the Author Index">
             Rousseau, Vincent
            </a>
           </td>
           <td class="r">
            IRSTEA
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#236390" title="Click to go to the Author Index">
             Laconte, Johann
            </a>
           </td>
           <td class="r">
            French National Research Institute for Agriculture, Food and The
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#103723" title="Click to go to the Author Index">
             Lenain, Roland
            </a>
           </td>
           <td class="r">
            INRAE
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1425" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#agricultural_automation" title="Click to go to the Keyword Index">
               Agricultural Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_control" title="Click to go to the Keyword Index">
               Motion Control
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#robotics_and_automation_in_agriculture_and_forestry" title="Click to go to the Keyword Index">
               Robotics and Automation in Agriculture and Forestry
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper, we present a novel method to control a rigidly connected location on the vehicle, such as a point on the implement in case of agricultural tasks. Agricultural robots are transforming modern farming by enabling precise and efficient operations, replacing humans in arduous tasks while reducing the use of chemicals. Traditionally, path-following algorithms are designed to guide the vehicle’s center along a predefined trajectory. However, since the actual agronomic task is performed by the implement, it is essential to control a specific point on the tool itself rather than the vehicle’s center. As such, we present in this paper two approaches for achieving the control of an offset point on the robot. The first approach adapts existing control laws, initially intended for the rear axle’s midpoint, to manage the desired lateral deviation. The second approach employs backstepping control techniques to create a control law that directly targets the implement. We conduct real-world experiments, highlighting the limitations of traditional approaches for offset point control, and demonstrating the strengths and weaknesses of the proposed methods.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet20_04">
             16:50-16:55, Paper ThET20.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2011'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Towards Over-Canopy Autonomous Navigation: Crop-Agnostic LiDAR-Based Crop-Row Detection in Arable Fields
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#398773" title="Click to go to the Author Index">
             Liu, Ruiji
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#281103" title="Click to go to the Author Index">
             Yandun, Francisco
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101976" title="Click to go to the Author Index">
             Kantor, George
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2011" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#agricultural_automation" title="Click to go to the Keyword Index">
               Agricultural Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reactive_and_sensor_based_planning" title="Click to go to the Keyword Index">
               Reactive and Sensor-Based Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#field_robots" title="Click to go to the Keyword Index">
               Field Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Autonomous navigation is crucial for various robotics applications in agriculture. However, many existing methods depend on RTK-GPS devices, which can be susceptible to loss of radio signal or intermittent reception of corrections from the internet. Consequently, research has increasingly focused on using RGB cameras for crop-row detection, though challenges persist when dealing with grown plants. This paper introduces a LiDAR-based navigation system that can achieve crop-agnostic over-canopy autonomous navigation in row-crop fields, even when the canopy fully blocks the inter-row spacing. Our algorithm can detect crop rows across diverse scenarios, encompassing various crop types, growth stages, illumination conditions, the presence of weeds, curved rows, and discontinuities. Without utilizing a global localization method (i.e., based on GPS), our navigation system can perform autonomous navigation in these challenging scenarios, detect the end of the crop rows, and navigate to the next crop row autonomously, providing a crop-agnostic approach to navigate an entire field. The proposed navigation system has undergone tests in various simulated and real agricultural fields, achieving an average cross-track error of 3.55 cm without human intervention. The system has been deployed on a customized UGV robot, which can be reconfigured depending on the field conditions.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet20_05">
             16:55-17:00, Paper ThET20.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2433'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Safe Leaf Manipulation for Accurate Shape and Pose Estimation of Occluded Fruits
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#332182" title="Click to go to the Author Index">
             Yao, Shaoxiong
            </a>
           </td>
           <td class="r">
            University of Illinois Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#295112" title="Click to go to the Author Index">
             Pan, Sicong
            </a>
           </td>
           <td class="r">
            University of Bonn
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#113220" title="Click to go to the Author Index">
             Bennewitz, Maren
            </a>
           </td>
           <td class="r">
            University of Bonn
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#123478" title="Click to go to the Author Index">
             Hauser, Kris
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2433" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#agricultural_automation" title="Click to go to the Keyword Index">
               Agricultural Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Fruit monitoring plays an important role in crop management, and rising global fruit consumption combined with labor shortages necessitates automated monitoring with robots. However, occlusions from plant foliage often hinder accurate shape and pose estimation. Therefore, we propose an active fruit shape and pose estimation method that physically manipulates occluding leaves to reveal hidden fruits. This paper introduces a framework that plans robot actions to maximize visibility and minimize leaf damage. We developed a novel scene-consistent shape completion technique to improve fruit estimation under heavy occlusion and utilize a perception-driven deformation graph model to predict leaf deformation during planning. Experiments on artificial and real sweet pepper plants demonstrate that our method enables robots to safely move leaves aside, exposing fruits for accurate shape and pose estimation, outperforming baseline methods. Project page: https://shaoxiongyao.github.io/lmap-ssc/.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet20_06">
             17:00-17:05, Paper ThET20.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3472'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Autonomous Sensor Exchange and Calibration for Cornstalk Nitrate Monitoring Robot
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424400" title="Click to go to the Author Index">
             Lee, Janice Seungyeon
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424653" title="Click to go to the Author Index">
             Detlefsen, Thomas
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424429" title="Click to go to the Author Index">
             Lawande, Shara
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424472" title="Click to go to the Author Index">
             Ghatge, Saudamini
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424627" title="Click to go to the Author Index">
             Ramesh Shanthi, Shrudhi
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424531" title="Click to go to the Author Index">
             Mukkamala, Sruthi
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101976" title="Click to go to the Author Index">
             Kantor, George
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#124019" title="Click to go to the Author Index">
             Kroemer, Oliver
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3472" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#robotics_and_automation_in_agriculture_and_forestry" title="Click to go to the Keyword Index">
               Robotics and Automation in Agriculture and Forestry
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#grippers_and_other_end_effectors" title="Click to go to the Keyword Index">
               Grippers and Other End-Effectors
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#agricultural_automation" title="Click to go to the Keyword Index">
               Agricultural Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Interactive sensors are an important component of robotic systems but often require manual replacement due to wear and tear. Automating this process can enhance system autonomy and facilitate long-term deployment. We developed an autonomous sensor exchange and calibration system for an agriculture crop monitoring robot that inserts a nitrate sensor into cornstalks. A novel gripper and replacement mechanism, featuring a reliable funneling design, were developed to enable efficient and reliable sensor exchanges. To maintain consistent nitrate sensor measurement, an on-board sensor calibration station was integrated to provide in-field sensor cleaning and calibration. The system was deployed at the Ames Curtis Farm in June 2024, where it successfully inserted nitrate sensors with high accuracy into 30 cornstalks with a 77% success rate.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet20_07">
             17:05-17:10, Paper ThET20.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3493'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Enhancing Agricultural Environment Perception Via Active Vision and Zero-Shot Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424844" title="Click to go to the Author Index">
             La Greca, Michele Carlo
            </a>
           </td>
           <td class="r">
            Politecnico Di Milano
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#375950" title="Click to go to the Author Index">
             Usuelli, Mirko
            </a>
           </td>
           <td class="r">
            Politecnico Di Milano
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#103628" title="Click to go to the Author Index">
             Matteucci, Matteo
            </a>
           </td>
           <td class="r">
            Politecnico Di Milano
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3493" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#robotics_and_automation_in_agriculture_and_forestry" title="Click to go to the Keyword Index">
               Robotics and Automation in Agriculture and Forestry
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#agricultural_automation" title="Click to go to the Keyword Index">
               Agricultural Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Agriculture, fundamental for human sustenance, faces unprecedented challenges. The need for efficient, human-cooperative, and sustainable farming methods has never been greater. The core contributions of this work involve leveraging Active Vision (AV) techniques and Zero-Shot Learning (ZSL) to improve the robot's ability to perceive and interact with agricultural environment in the context of fruit harvesting. The AV Pipeline implemented within ROS 2 integrates the Next-Best View (NBV) Planning for 3D environment reconstruction through a dynamic 3D Occupancy Map. Our system allows the robotics arm to dynamically plan and move to the most informative viewpoints and explore the environment, updating the 3D reconstruction using semantic information produced through ZSL models. Simulation and real-world experimental results demonstrate our system's effectiveness in complex visibility conditions, outperforming traditional and static predefined planning methods. ZSL segmentation models employed, such as YOLO World + EfficientViT SAM, exhibit high-speed performance and accurate segmentation, allowing flexibility when dealing with semantic information in unknown agricultural contexts without requiring any fine-tuning process.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet20_08">
             17:10-17:15, Paper ThET20.8
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4708'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              CitDet: A Benchmark Dataset for Citrus Fruit Detection
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#352195" title="Click to go to the Author Index">
             James, Jordan
            </a>
           </td>
           <td class="r">
            University of Texas at Arlington
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#373258" title="Click to go to the Author Index">
             Manching, Heather K.
            </a>
           </td>
           <td class="r">
            North Carolina State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#373260" title="Click to go to the Author Index">
             Mattia, Matthew R.
            </a>
           </td>
           <td class="r">
            USDA Agricultural Research Service
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#373261" title="Click to go to the Author Index">
             Bowman, Kim D.
            </a>
           </td>
           <td class="r">
            USDA Agricultural Research Service
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#373262" title="Click to go to the Author Index">
             Hulse-Kemp, Amanda M.
            </a>
           </td>
           <td class="r">
            US Department of Agriculture
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#157520" title="Click to go to the Author Index">
             Beksi, William J.
            </a>
           </td>
           <td class="r">
            The University of Texas at Arlington
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4708" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#agricultural_automation" title="Click to go to the Keyword Index">
               Agricultural Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#data_sets_for_robotic_vision" title="Click to go to the Keyword Index">
               Data Sets for Robotic Vision
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this letter, we present a new dataset to advance the state of the art in detecting citrus fruit and accurately estimate yield on trees affected by the Huanglongbing (HLB) disease in orchard environments via imaging. Despite the fact that significant progress has been made in solving the fruit detection problem, the lack of publicly available datasets has complicated direct comparison of results. For instance, citrus detection has long been of interest to the agricultural research community, yet there is an absence of work, particularly involving public datasets of citrus affected by HLB. To address this issue, we enhance state-of-the-art object detection methods for use in typical orchard settings. Concretely, we provide high-resolution images of citrus trees located in an area known to be highly affected by HLB, along with high-quality bounding box annotations of citrus fruit. Fruit on both the trees and the ground are labeled to allow for identification of fruit location, which contributes to advancements in yield estimation and potential measure of HLB impact via fruit drop. The dataset consists of over 32,000 bounding box annotations for fruit instances contained in 579 high-resolution images. In summary, our contributions are the following: (i) we introduce a novel dataset along with baseline performance benchmarks on multiple contemporary object detection algorithms, (ii) we show the ability to accurately capture fruit location on tree or on ground, and finally (ii) we present a correlation of our results with yield estimations.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thet21">
             <b>
              ThET21
             </b>
            </a>
           </td>
           <td class="r">
            410
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thet21" title="Click to go to the Program at a Glance">
             <b>
              Integrating Motion Planning and Learning 3
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="ICRA25_AuthorIndexWeb.html#105490" title="Click to go to the Author Index">
             Balakirsky, Stephen
            </a>
           </td>
           <td class="r">
            Georgia Tech
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#168590" title="Click to go to the Author Index">
             Solovey, Kiril
            </a>
           </td>
           <td class="r">
            Technion--Israel Institute of Technology
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet21_01">
             16:35-16:40, Paper ThET21.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('244'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Transformer-Enhanced Motion Planner: Attention-Guided Sampling for State-Specific Decision Making
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#382163" title="Click to go to the Author Index">
             Zhuang, Lei
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101775" title="Click to go to the Author Index">
             Zhao, Jingdong
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#382150" title="Click to go to the Author Index">
             Li, Yuntao
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#307110" title="Click to go to the Author Index">
             Xu, Zichun
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology, School of Mechatronics Engineeri
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#201854" title="Click to go to the Author Index">
             Zhao, Liangliang
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#101144" title="Click to go to the Author Index">
             Liu, Hong
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab244" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Sampling-based motion planning (SBMP) algorithms are renowned for their robust global search capabilities. However, the inherent randomness in their sampling mechanisms often results in inconsistent path quality and limited search efficiency. In response to these challenges, this work proposes a novel deep learning-based motion planning framework, named Transformer-Enhanced Motion Planner (TEMP), which synergizes a Co-Regulation Environmental Information Encoder (CEIE) with a Motion Planning Transformer (MPT). CEIE converts scenario data into encoded environmental information (EEI), providing MPT with an insightful understanding of the environment. MPT leverages an attention mechanism to dynamically recalibrate its focus on EEI, task objectives, and historical planning data, refining the sampling node generation. To demonstrate the capabilities of TEMP, we train our model using a dataset consisting of planning results produced by RRT*. CEIE and MPT are collaboratively trained, enabling CEIE to autonomously learn and extract patterns from environmental data, thereby forming informative representations that MPT can more effectively interpret and utilize for motion planning. Subsequently, we systematically evaluate TEMP's efficacy across diverse dimensions and assess it in out-of-distribution real-world scenarios, demonstrating that TEMP achieves exceptional performance metrics and a heightened degree of generalizability compared to state-of-the-art SBMPs.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet21_02">
             16:40-16:45, Paper ThET21.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2237'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              From Configuration-Space Clearance to Feature-Space Margin: Sample Complexity in Learning-Based Collision Detection
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423773" title="Click to go to the Author Index">
             Tubul, Sapir
            </a>
           </td>
           <td class="r">
            Technion - Israel Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#204752" title="Click to go to the Author Index">
             Tamar, Aviv
            </a>
           </td>
           <td class="r">
            Technion
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#168590" title="Click to go to the Author Index">
             Solovey, Kiril
            </a>
           </td>
           <td class="r">
            Technion--Israel Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#159588" title="Click to go to the Author Index">
             Salzman, Oren
            </a>
           </td>
           <td class="r">
            Technion
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2237" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#integrated_planning_and_learning" title="Click to go to the Keyword Index">
               Integrated Planning and Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#probability_and_statistical_methods" title="Click to go to the Keyword Index">
               Probability and Statistical Methods
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#collision_avoidance" title="Click to go to the Keyword Index">
               Collision Avoidance
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Motion planning is a central challenge in robotics, with learning-based approaches gaining significant attention in recent years. Our work focuses on a specific aspect of these approaches: using machine-learning techniques, particularly Support Vector Machines (SVM), to evaluate whether robot configurations are collision free, an operation termed “collision detection”. Despite the growing popularity of these methods, there is a lack of theory supporting their efficiency and prediction accuracy. This is in stark contrast to the rich theoretical results of machine-learning methods in general and of SVMs in particular. Our work bridges this gap by analyzing the sample complexity of an SVM classifier for learning-based collision detection in motion planning. We bound the number of samples needed to achieve a specified accuracy at a given confidence level. This result is stated in terms relevant to robot motion planning such as the system’s clearance. Building on these theoretical results, we propose a collision-detection algorithm that can also provide statistical guarantees on the algorithm’s error in classifying robot configurations as collision-free or not.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet21_03">
             16:45-16:50, Paper ThET21.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2317'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              CTSAC: Curriculum-Based Transformer Soft Actor-Critic for Goal-Oriented Robot Exploration
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#250625" title="Click to go to the Author Index">
             Yang, Chunyu
            </a>
           </td>
           <td class="r">
            China University of Mining and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417378" title="Click to go to the Author Index">
             Bi, Shengben
            </a>
           </td>
           <td class="r">
            China University of Mining and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#393813" title="Click to go to the Author Index">
             Xu, Yihui
            </a>
           </td>
           <td class="r">
            China University of Mining and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#412484" title="Click to go to the Author Index">
             Zhang, Xin
            </a>
           </td>
           <td class="r">
            China University of Mining and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2317" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#integrated_planning_and_learning" title="Click to go to the Keyword Index">
               Integrated Planning and Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#planning_under_uncertainty" title="Click to go to the Keyword Index">
               Planning under Uncertainty
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             With the increasing demand for efficient and flexible robotic exploration solutions, Reinforcement Learning (RL) is becoming a promising approach in the field of autonomous robotic exploration. However, current RL-based exploration algorithms often face limited environmental reasoning capabilities, slow convergence rates, and substantial challenges in Sim-To-Real (S2R) transfer. To address these issues, we propose a Curriculum Learning-based Transformer Reinforcement Learning Algorithm (CTSAC) aimed at improving both exploration efficiency and transfer performance. To enhance the robot's reasoning ability, a Transformer is integrated into the perception network of the Soft Actor-Critic (SAC) framework, leveraging historical information to improve the farsightedness of the strategy. A periodic review-based curriculum learning is proposed, which enhances training efficiency while mitigating catastrophic forgetting during curriculum transitions. Training is conducted on the ROS-Gazebo continuous robotic simulation platform, with LiDAR clustering optimization to further reduce the S2R gap. Experimental results demonstrate the CTSAC algorithm outperforms the state-of-the-art non-learning and learning-based algorithms in terms of success rate and success rate-weighted exploration time. Moreover, real-world experiments validate the strong S2R transfer capabilities of CTSAC.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet21_04">
             16:50-16:55, Paper ThET21.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3311'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Guiding Long-Horizon Task and Motion Planning with Vision Language Models
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#338934" title="Click to go to the Author Index">
             Yang, Zhutian
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#183173" title="Click to go to the Author Index">
             Garrett, Caelan
            </a>
           </td>
           <td class="r">
            NVIDIA
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104592" title="Click to go to the Author Index">
             Fox, Dieter
            </a>
           </td>
           <td class="r">
            University of Washington
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#106519" title="Click to go to the Author Index">
             Lozano-Perez, Tomas
            </a>
           </td>
           <td class="r">
            MIT
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#103153" title="Click to go to the Author Index">
             Kaelbling, Leslie
            </a>
           </td>
           <td class="r">
            MIT
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3311" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#integrated_planning_and_learning" title="Click to go to the Keyword Index">
               Integrated Planning and Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#task_and_motion_planning" title="Click to go to the Keyword Index">
               Task and Motion Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#mobile_manipulation" title="Click to go to the Keyword Index">
               Mobile Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Vision-Language Models (VLM) can generate plausible high-level plans when prompted with a goal, the context, an image of the scene, and any planning constraints. However, there is no guarantee that the predicted actions are geometrically and kinematically feasible for a particular robot embodiment. As a result, many prerequisite steps such as opening drawers to access objects are often omitted. Task and motion planners can generate motion trajectories that respect the geometric feasibility of actions and insert physically necessary actions, but do not scale to everyday problems that require common-sense knowledge and involve large state spaces comprised of many variables. We leverage the VLM for 1) system dynamics (i.e. recipe) and 2) search help. We propose VLM-TAMP, a hierarchical planning algorithm that leverages a VLM to generate intermediate subgoals that guide the sampling of a task and motion planner. When a subgoal or action cannot be refined, the VLM is queried again for replanning. We evaluate VLM-TAMP on kitchen tasks where a robot must accomplish cooking goals that require performing 30-50 actions in sequence and interacting with up to 21 objects. We found that VLM-TAMP substantially outperforms baselines that rigidly and independently execute VLM-generated action sequences (success rate 50 to 100% versus 0%, average task completion percentage 72 to 100% versus 15 to 45%). See the project site https://zt-yang.github.io/vlm-tamp-robot/ for more information.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet21_05">
             16:55-17:00, Paper ThET21.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4101'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              CrowdSurfer: Sampling Optimization Augmented with Vector-Quantized Variational AutoEncoder for Dense Crowd Navigation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422282" title="Click to go to the Author Index">
             Kumar, Naman
            </a>
           </td>
           <td class="r">
            Robotics Research Center, IIIT Hyderabad, India
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422192" title="Click to go to the Author Index">
             Singha, Antareep
            </a>
           </td>
           <td class="r">
            Robotics Research Center, IIIT Hyderabad
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#360486" title="Click to go to the Author Index">
             Nanwani, Laksh
            </a>
           </td>
           <td class="r">
            Robotics Research Center, IIIT Hyderabad, India
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422187" title="Click to go to the Author Index">
             Potdar, Dhruv
            </a>
           </td>
           <td class="r">
            Robotics Research Center, IIIT Hyderabad, India
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422182" title="Click to go to the Author Index">
             Ramakrishnan, Tarun
            </a>
           </td>
           <td class="r">
            Robotics Research Center, IIIT Hyderabad, India
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#277644" title="Click to go to the Author Index">
             Rastgar, Fatemeh
            </a>
           </td>
           <td class="r">
            Örebro University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#354183" title="Click to go to the Author Index">
             Idoko, Simon
            </a>
           </td>
           <td class="r">
            University of Tartu
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#123110" title="Click to go to the Author Index">
             Singh, Arun Kumar
            </a>
           </td>
           <td class="r">
            University of Tartu
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#102906" title="Click to go to the Author Index">
             Krishna, Madhava
            </a>
           </td>
           <td class="r">
            IIIT Hyderabad
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4101" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#integrated_planning_and_learning" title="Click to go to the Keyword Index">
               Integrated Planning and Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#collision_avoidance" title="Click to go to the Keyword Index">
               Collision Avoidance
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Navigation amongst densely packed crowds remains a challenge for mobile robots. The complexity increases further if the environment layout changes, making the prior computed global plan infeasible. In this paper, we show that it is possible to dramatically enhance crowd navigation by just improving the local planner. Our approach combines generative modelling with inference time optimization to generate sophisticated long-horizon local plans at interactive rates. More specifically, we train a Vector Quantized Variational AutoEncoder to learn a prior over the expert trajectory distribution conditioned on the perception input. At run-time, this is used as an initialization for a sampling-based optimizer for further refinement. Our approach does not require any sophisticated prediction of dynamic obstacles and yet provides state-of-the-art performance. In particular, we compare against the recent DRL-VO approach and show a 40% improvement in success rate and a 6% improvement in travel time.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet21_06">
             17:00-17:05, Paper ThET21.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4165'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              CLIMB: Language-Guided Continual Learning for Task Planning with Iterative Model Building
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#424675" title="Click to go to the Author Index">
             Byrnes, Walker
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#226976" title="Click to go to the Author Index">
             Bogdanovic, Miroslav
            </a>
           </td>
           <td class="r">
            University of Toronto
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#422319" title="Click to go to the Author Index">
             Balakirsky, Avi
            </a>
           </td>
           <td class="r">
            The Ohio State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#105490" title="Click to go to the Author Index">
             Balakirsky, Stephen
            </a>
           </td>
           <td class="r">
            Georgia Tech
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#155114" title="Click to go to the Author Index">
             Garg, Animesh
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4165" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#integrated_planning_and_learning" title="Click to go to the Keyword Index">
               Integrated Planning and Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#continual_learning" title="Click to go to the Keyword Index">
               Continual Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#incremental_learning" title="Click to go to the Keyword Index">
               Incremental Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Intelligent and reliable task planning is a core capability for generalized robotics, which requires a descriptive domain representation that sufficiently models all object and state information for the scene. We present CLIMB, a continual learning framework for robot task planning that leverages foundation models and feedback from execution to guide the construction of domain models. CLIMB can build a model from a natural language description, learn non-obvious predicates while solving tasks, and store that information for future problems. We demonstrate the ability of CLIMB to improve performance in common planning environments compared to baseline methods. We also developed the BlocksWorld++ domain, a simulated environment with an easily usable real counterpart, together with a curriculum of tasks with progressing difficulty to evaluate continual learning.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet21_07">
             17:05-17:10, Paper ThET21.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4446'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Safe Multi-Agent Navigation Guided by Goal-Conditioned Safe Reinforcement Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#255670" title="Click to go to the Author Index">
             Feng, Meng
            </a>
           </td>
           <td class="r">
            MIT
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#398366" title="Click to go to the Author Index">
             Parimi, Viraj
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#106049" title="Click to go to the Author Index">
             Williams, Brian
            </a>
           </td>
           <td class="r">
            MIT
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4446" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#integrated_planning_and_learning" title="Click to go to the Keyword Index">
               Integrated Planning and Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#robot_safety" title="Click to go to the Keyword Index">
               Robot Safety
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Safe navigation is essential for autonomous systems operating in hazardous environments. Traditional planning methods are effective for solving long-horizon tasks but depend on the availability of a graph representation with predefined distance metrics. In contrast, safe Reinforcement Learning (RL) is capable of learning complex behaviors without relying on manual heuristics but fails to solve long-horizon tasks, particularly in goal-conditioned and multi-agent scenarios.
             <p>
              In this paper, we introduce a novel method that integrates the strengths of both planning and safe RL. Our method leverages goal-conditioned RL (GCRL) and safe RL to learn a goal-conditioned policy for navigation while concurrently estimating cumulative distance and safety levels using learned value functions via an automated self-training algorithm. By constructing a graph with states from the replay buffer, our method prunes unsafe edges and generates a waypoint-based plan that the agent then executes by following those waypoints sequentially until their goal locations are reached. This graph pruning and planning approach via the learned value functions allows our approach to flexibly balance the trade-off between faster and safer routes especially over extended horizons.
              <p>
               Utilizing this unified high-level graph and a shared low-level safe GCRL policy, we extend this approach to address the multi-agent safe navigation problem. In particular, we leverage Conflict-Based Search (CBS) to create waypoint-based plans for multiple agents allowing for their safer navigation over extended horizons. This integration enhances the scalability of goal-conditioned safe RL in multi-agent scenarios, enabling efficient coordination among agents. Extensive benchmarking against state-of-the-art baselines demonstrates the effectiveness of our method in achieving distance goals safely for multiple agents in complex and hazardous environments. More details can be found at https://safe-visual-mapf-mers.mit.csail.mit.
              </p>
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet21_08">
             17:10-17:15, Paper ThET21.8
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4596'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Motion Planning for 2-DOF Transformable Wheel Robots Using Reinforcement Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#334282" title="Click to go to the Author Index">
             Park, Inha
            </a>
           </td>
           <td class="r">
            Hanyang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#222392" title="Click to go to the Author Index">
             Ryu, Sijun
            </a>
           </td>
           <td class="r">
            Hanyang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#294814" title="Click to go to the Author Index">
             Won, Jeeho
            </a>
           </td>
           <td class="r">
            Hanyang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#346521" title="Click to go to the Author Index">
             Yoon, Hyeongyu
            </a>
           </td>
           <td class="r">
            Hanyang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#295034" title="Click to go to the Author Index">
             Kim, SangGyun
            </a>
           </td>
           <td class="r">
            Hanyang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#151887" title="Click to go to the Author Index">
             Kim, Hwa Soo
            </a>
           </td>
           <td class="r">
            Kyonggi University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#140339" title="Click to go to the Author Index">
             Seo, TaeWon
            </a>
           </td>
           <td class="r">
            Hanyang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4596" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#model_learning_for_control" title="Click to go to the Keyword Index">
               Model Learning for Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Transformable robots have been developed to perform various tasks using flexible methods. However, the transformation properties present challenges in controlling and planning motion strategies, as the system model changes when transformations occur. To address this issue, we propose a planning framework based on artificial intelligence, called Geometric Manipulability Reinforcement Learning (GM-RL). GM-RL consists of two components: the manipulability estimator and the motion planner. The manipulability estimator employs graph neural networks (GNN) to provide action guidelines based on the dynamic manipulability of the transformable robots. The motion planner generates transformation plans using reinforcement learning (RL). The activation ratio alpha adjusts the ratio of the guideline accepted between the two components. In experiments utilizing a 2-DoF transformable wheel called STEP, GM-RL with alpha=0.5 generated an optimal transformation plan with an average dynamic manipulability measure of 0.0424, the highest measure compared to pure dynamic manipulability and reinforcement learning. A real-world experiment demonstrated that the transformation plan is efficient for overcoming stairs.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thet22">
             <b>
              ThET22
             </b>
            </a>
           </td>
           <td class="r">
            411
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thet22" title="Click to go to the Program at a Glance">
             <b>
              Imitation Learning for Manipulation 2
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet22_01">
             16:35-16:40, Paper ThET22.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1941'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Towards Effective Utilization of Mixed-Quality Demonstrations in Robotic Manipulation Via Segment-Level Selection and Optimization
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#374187" title="Click to go to the Author Index">
             Chen, Jingjing
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#300234" title="Click to go to the Author Index">
             Fang, Hongjie
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#288067" title="Click to go to the Author Index">
             Fang, Hao-Shu
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#224610" title="Click to go to the Author Index">
             Lu, Cewu
            </a>
           </td>
           <td class="r">
            ShangHai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1941" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Data is crucial for robotic manipulation, as it underpins the development of robotic systems for complex tasks. While high-quality, diverse datasets enhance the performance and adaptability of robotic manipulation policies, collecting extensive expert-level data is resource-intensive. Consequently, many current datasets suffer from quality inconsistencies due to operator variability, highlighting the need for methods to utilize mixed-quality data effectively. To mitigate these issues, we propose "Select Segments to Imitate" (S2I), a framework that selects and optimizes mixed-quality demonstration data at the segment level, while ensuring plug-and-play compatibility with existing robotic manipulation policies. The framework has three components: demonstration segmentation dividing origin data into meaningful segments, segment selection using contrastive learning to find high-quality segments, and trajectory optimization to refine suboptimal segments for better policy learning. We evaluate S2I through comprehensive experiments in simulation and real-world environments across six tasks, demonstrating that with only 3 expert demonstrations for reference, S2I can improve the performance of various downstream policies when trained with mixed-quality demonstrations. Project website: https://tonyfang.net/s2i/.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet22_02">
             16:40-16:45, Paper ThET22.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2626'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              DABI: Evaluation of Data Augmentation Methods Using Downsampling in Bilateral Control-Based Imitation Learning with Images
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#283791" title="Click to go to the Author Index">
             Kobayashi, Masato
            </a>
           </td>
           <td class="r">
            Osaka University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#386830" title="Click to go to the Author Index">
             Buamanee, Thanpimon
            </a>
           </td>
           <td class="r">
            Osaka University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#206267" title="Click to go to the Author Index">
             Uranishi, Yuki
            </a>
           </td>
           <td class="r">
            Osaka University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2626" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Autonomous robot manipulation is a complex and continuously evolving robotics field. This paper focuses on data augmentation methods in imitation learning. Imitation learning consists of three stages: data collection from experts, learning model, and execution. However, collecting expert data requires manual effort and is time-consuming. Additionally, as sensors have different data acquisition intervals, preprocessing such as downsampling to match the lowest frequency is necessary. Downsampling enables data augmentation and also contributes to the stabilization of robot operations. In light of this background, this paper proposes the Data Augmentation Method for Bilateral Control-Based Imitation Learning with Images, called "DABI". DABI collects robot joint angles, velocities, and torques at 1000 Hz, and uses images from gripper and environmental cameras captured at 100 Hz as the basis for data augmentation. This enables a tenfold increase in data. In this paper, we collected just 5 expert demonstration datasets. We trained the bilateral control Bi-ACT model with the unaltered dataset and two augmentation methods for comparative experiments and conducted real-world experiments. The results confirmed a significant improvement in success rates, thereby proving the effectiveness of DABI. For additional material, please check:https://mertcookimg.github.io/dabi
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet22_03">
             16:45-16:50, Paper ThET22.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3151'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Learning from Imperfect Demonstrations with Self-Supervision for Robotic Manipulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#341578" title="Click to go to the Author Index">
             Wu, Kun
            </a>
           </td>
           <td class="r">
            Syracuse University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#338817" title="Click to go to the Author Index">
             Liu, Ning
            </a>
           </td>
           <td class="r">
            Beijing Innovation Center of Humanoid Robotics
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#338848" title="Click to go to the Author Index">
             Zhao, Zhen
            </a>
           </td>
           <td class="r">
            Midea Group
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#374477" title="Click to go to the Author Index">
             Qiu, Di
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#377137" title="Click to go to the Author Index">
             Li, Jinming
            </a>
           </td>
           <td class="r">
            Shanghai University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#336671" title="Click to go to the Author Index">
             Che, Zhengping
            </a>
           </td>
           <td class="r">
            X-Humanoid
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#338492" title="Click to go to the Author Index">
             Xu, Zhiyuan
            </a>
           </td>
           <td class="r">
            Midea Group
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#322428" title="Click to go to the Author Index">
             Tang, Jian
            </a>
           </td>
           <td class="r">
            Midea Group (Shanghai) Co., Ltd
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3151" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Improving data utilization, especially for imperfect data from task failures, is crucial for robotic manipulation due to the challenging, time-consuming, and expensive data collection process in the real world. Current imitation learning (IL) typically discards imperfect data, focusing solely on successful expert data. While reinforcement learning (RL) can learn from explorations and failures, the sim2real gap and its reliance on dense reward and online exploration make it difficult to apply effectively in real-world scenarios. In this work, we aim to conquer the challenge of leveraging imperfect data without the need for reward information to improve the model performance for robotic manipulation in an offline manner. Specifically, we introduce a Self-Supervised Data Filtering framework (SSDF) that combines expert and imperfect data to compute quality scores for failed trajectory segments. High-quality segments from the failed data are used to expand the training dataset. Then, the enhanced dataset can be used with any downstream policy learning method for robotic manipulation tasks. Extensive experiments on the ManiSkill2 benchmark built on the high-fidelity Sapien simulator and real-world robotic manipulation tasks using the Franka robot arm demonstrated that the SSDF can accurately expand the training dataset with high-quality imperfect data and improve the success rates for all robotic manipulation tasks.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet22_04">
             16:50-16:55, Paper ThET22.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3624'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              MATCH POLICY: A Simple Pipeline from Point Cloud Registration to Manipulation Policies
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#335215" title="Click to go to the Author Index">
             Huang, Haojie
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#376503" title="Click to go to the Author Index">
             Liu, Haotian
            </a>
           </td>
           <td class="r">
            Worcester Polytechnic Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#237093" title="Click to go to the Author Index">
             Wang, Dian
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#336694" title="Click to go to the Author Index">
             Walters, Robin
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#106889" title="Click to go to the Author Index">
             Platt, Robert
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3624" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#transfer_learning" title="Click to go to the Keyword Index">
               Transfer Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Many manipulation tasks require the robot to rearrange objects relative to one another. Such tasks can be described as a sequence of relative poses between parts of a set of rigid bodies. In this work, we propose Match Policy, a simple but novel pipeline for solving high-precision pick and place tasks. Instead of predicting actions directly, our method registers the pick and place targets to the stored demonstrations. This transfers action inference into a point cloud registration task and enables us to realize nontrivial manipulation policies without any training. Match Policy is designed to solve high-precision tasks with a key-frame setting. By leveraging the geometric interaction and the symmetries of the task, it achieves extremely high sample efficiency and generalizability to unseen configurations. We demonstrate its state-of-the-art performance across various tasks on RLbench benchmark compared with several strong baselines and test it on a real robot with six tasks.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet22_05">
             16:55-17:00, Paper ThET22.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('3781'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Self-Improving Autonomous Underwater Manipulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416620" title="Click to go to the Author Index">
             Liu, Ruoshi
            </a>
           </td>
           <td class="r">
            Columbia University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#293927" title="Click to go to the Author Index">
             Ha, Huy
            </a>
           </td>
           <td class="r">
            Columbia University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#255086" title="Click to go to the Author Index">
             Hou, Mengxue
            </a>
           </td>
           <td class="r">
            University of Notre Dame
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#193621" title="Click to go to the Author Index">
             Song, Shuran
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#279048" title="Click to go to the Author Index">
             Vondrick, Carl
            </a>
           </td>
           <td class="r">
            Columbia
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab3781" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#sensorimotor_learning" title="Click to go to the Keyword Index">
               Sensorimotor Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#marine_robotics" title="Click to go to the Keyword Index">
               Marine Robotics
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Underwater robotic manipulation faces significant challenges due to complex fluid dynamics and unstructured environments, causing most manipulation systems to rely heavily on human teleoperation. In this paper, we introduce AquaBot, a fully autonomous manipulation system that combines behavior cloning from human demonstrations with self-learning optimization to improve beyond human teleoperation performance. With extensive real-world experiments, we demonstrate AquaBot's versatility across diverse manipulation tasks, including object grasping, trash sorting, and rescue retrieval. Our real-world experiments show that AquaBot's self-optimized policy outperforms a human operator by 41% in speed. AquaBot represents a promising step towards autonomous and self-improving underwater manipulation systems.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet22_06">
             17:00-17:05, Paper ThET22.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4373'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              DexMimicGen: Automated Data Generation for Bimanual Dexterous Manipulation Via Imitation Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#342351" title="Click to go to the Author Index">
             Jiang, Zhenyu
            </a>
           </td>
           <td class="r">
            The Unversity of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#425410" title="Click to go to the Author Index">
             Xie, Yuqi
            </a>
           </td>
           <td class="r">
            University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#385959" title="Click to go to the Author Index">
             Lin, Kevin
            </a>
           </td>
           <td class="r">
            Stanford
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#241233" title="Click to go to the Author Index">
             Xu, Zhenjia
            </a>
           </td>
           <td class="r">
            Columbia University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#319728" title="Click to go to the Author Index">
             Wan, Weikang
            </a>
           </td>
           <td class="r">
            Peking University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#210582" title="Click to go to the Author Index">
             Mandlekar, Ajay Uday
            </a>
           </td>
           <td class="r">
            NVIDIA
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#280356" title="Click to go to the Author Index">
             Fan, Linxi
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#203352" title="Click to go to the Author Index">
             Zhu, Yuke
            </a>
           </td>
           <td class="r">
            The University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4373" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#big_data_in_robotics_and_automation" title="Click to go to the Keyword Index">
               Big Data in Robotics and Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Imitation learning from human demonstrations is an effective means to teach robots manipulation skills. But data acquisition is a major bottleneck in applying this paradigm more broadly, due to the high costs and human efforts involved. There has been significant interest in imitation learning for bimanual dexterous robots, like humanoids. Unfortunately, data collection is even more challenging here due to the difficulty of simultaneously controlling the two arms and multi-fingered hands. Automated data generation in simulation is a compelling, scalable alternative to fuel this need for training data. To this end, we introduce DexMimicGen, a large-scale automated data generation system that synthesizes trajectories from a handful of human demonstrations for bimanual robots with dexterous hands. We present a collection of simulation environments in the setting of bimanual dexterous manipulation, spanning a range of manipulation behaviors and different requirements for coordination among the two arms. We generate 21K demos across these tasks from just 60 source human demos and study the effect of several data generation and policy learning decisions on agent performance. Finally, we present a real-to-sim-to-real pipeline and deploy it on a real-world humanoid can sorting task. Generated datasets, simulation environments and additional results are at dexmimicgen.github.io.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet22_07">
             17:05-17:10, Paper ThET22.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4864'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              The Art of Imitation: Learning Long-Horizon Manipulation Tasks from Few Demonstrations
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#363009" title="Click to go to the Author Index">
             von Hartz, Jan Ole
            </a>
           </td>
           <td class="r">
            University of Freiburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#177085" title="Click to go to the Author Index">
             Welschehold, Tim
            </a>
           </td>
           <td class="r">
            Albert-Ludwigs-Universität Freiburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#160427" title="Click to go to the Author Index">
             Valada, Abhinav
            </a>
           </td>
           <td class="r">
            University of Freiburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#107316" title="Click to go to the Author Index">
             Boedecker, Joschka
            </a>
           </td>
           <td class="r">
            University of Freiburg
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4864" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#sensorimotor_learning" title="Click to go to the Keyword Index">
               Sensorimotor Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Task Parametrized Gaussian Mixture Models (TP-GMM) are a sample-efficient method for learning object-centric robot manipulation tasks. However, there are several open challenges to applying TP-GMMs in the wild. In this work, we tackle three crucial challenges synergistically. First, end-effector velocities are non-Euclidean and thus hard to model using standard GMMs. We thus propose to factorize the robot's end-effector velocity into its direction and magnitude, and model them using Riemannian GMMs. Second, we leverage the factorized velocities to segment and sequence skills from complex demonstration trajectories. Through the segmentation, we further align skill trajectories and hence leverage time as a powerful inductive bias. Third, we present a method to automatically detect relevant task parameters per skill from visual observations. Our approach enables learning complex manipulation tasks from just five demonstrations while using only RGB-D observations. Extensive experimental evaluations on RLBench demonstrate that our approach achieves state-of-the-art performance with 20-fold improved sample efficiency. Our policies generalize across different environments, object instances, and object positions, while the learned skills are reusable.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet22_08">
             17:10-17:15, Paper ThET22.8
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4329'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              ZeroMimic: Distilling Robotic Manipulation Skills from Web Videos
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#266455" title="Click to go to the Author Index">
             Shi, Junyao
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426583" title="Click to go to the Author Index">
             Zhao, Zhuolun
            </a>
           </td>
           <td class="r">
            University of Pennsylvania, Skild AI
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426477" title="Click to go to the Author Index">
             Wang, Tianyou
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#450688" title="Click to go to the Author Index">
             Pedroza, Ian
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#450705" title="Click to go to the Author Index">
             Luo, Amy
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#450682" title="Click to go to the Author Index">
             Wang, Jie
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#290983" title="Click to go to the Author Index">
             Ma, Yecheng Jason
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#207182" title="Click to go to the Author Index">
             Jayaraman, Dinesh
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4329" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#sensorimotor_learning" title="Click to go to the Keyword Index">
               Sensorimotor Learning
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#transfer_learning" title="Click to go to the Keyword Index">
               Transfer Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Many recent advances in robotic manipulation have come through imitation learning, yet these rely largely on mimicking a particularly hard-to-acquire form of demonstrations: those collected on the same robot in the same room with the same objects as the trained policy must handle at test time. In contrast, large pre-recorded human video datasets demonstrating manipulation skills in-the-wild already exist, which contain valuable information for robots. Is it possible to distill a repository of useful robotic skill policies out of such data without any additional requirements on robot-specific demonstrations or exploration? We present the first such system ZeroMimic, that generates immediately deployable image goal-conditioned skill policies for several common categories of manipulation tasks (opening, closing, pouring, pick&amp;place, cutting, and stirring) each capable of acting upon diverse objects and across diverse unseen task setups. ZeroMimic is carefully designed to exploit recent advances in semantic and geometric visual understanding of human videos, together with modern grasp affordance detectors and imitation policy classes. After training ZeroMimic on the popular EpicKitchens dataset of ego-centric human videos, we evaluate its out-of-the-box performance in varied real-world and simulated kitchen settings with two different robot embodiments, demonstrating its impressive abilities to handle these varied tasks. To enable plug-and-play reuse of ZeroMimic policies on other task setups and robots, we release software and policy checkpoints of our skill policies.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thet23">
             <b>
              ThET23
             </b>
            </a>
           </td>
           <td class="r">
            412
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRA25_ProgramAtAGlanceWeb.html#thet23" title="Click to go to the Program at a Glance">
             <b>
              Autonomous Vehicle Perception 7
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="ICRA25_AuthorIndexWeb.html#112540" title="Click to go to the Author Index">
             Zhou, MengChu
            </a>
           </td>
           <td class="r">
            New Jersey Institute of Technology
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet23_01">
             16:35-16:40, Paper ThET23.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('119'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Object Importance Estimation Using Counterfactual Reasoning for Intelligent Driving
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#375878" title="Click to go to the Author Index">
             Gupta, Pranay
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#233286" title="Click to go to the Author Index">
             Biswas, Abhijat
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#148680" title="Click to go to the Author Index">
             Admoni, Henny
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#150386" title="Click to go to the Author Index">
             Held, David
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab119" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#autonomous_vehicle_navigation" title="Click to go to the Keyword Index">
               Autonomous Vehicle Navigation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#intelligent_transportation_systems" title="Click to go to the Keyword Index">
               Intelligent Transportation Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The ability to identify important objects in a complex and dynamic driving environment is essential for autonomous driving agents to make safe and efficient driving decisions. It also helps assistive driving systems decide when to alert drivers. We tackle object importance estimation in a data-driven fashion and introduce HOIST -Human-annotated Object Importance in Simulated Traffic. HOIST contains driving scenarios with human-annotated importance labels for vehicles and pedestrians. We additionally propose a novel approach that relies on counterfactual reasoning to estimate an object's importance. We generate counterfactual scenarios by modifying the motion of objects and ascribe importance based on how the modifications affect the ego vehicle's driving. Our approach outperforms strong baselines for the task of object importance estimation on HOIST. We also perform ablation studies to justify our design choices and show the significance of the different components of our proposed approach.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet23_02">
             16:40-16:45, Paper ThET23.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1112'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              3D Multi-Modal Object Detection Based on Cross-Attention Feature Fusion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#348093" title="Click to go to the Author Index">
             Jhong, Sin-Ye
            </a>
           </td>
           <td class="r">
            Tamkang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420479" title="Click to go to the Author Index">
             Ho, Min-Hsuan
            </a>
           </td>
           <td class="r">
            National Taiwan University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#420523" title="Click to go to the Author Index">
             Lu, Si-Yu
            </a>
           </td>
           <td class="r">
            National Taiwan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#348095" title="Click to go to the Author Index">
             Chen, Yung-Yao
            </a>
           </td>
           <td class="r">
            National Taiwan University of Science and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1112" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#sensor_fusion" title="Click to go to the Keyword Index">
               Sensor Fusion
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In Advanced Driver Assistance Systems (ADAS), environmental perception and object detection are crucial for ensuring safe autonomous driving. Single-modality systems often struggle under adverse weather conditions, underscoring the need for multi-modal approaches. Current fusion methods typically rely on simplistic concatenation of multi-modal fea-tures, which neglects semantic alignment and does not fully exploit inter-modal correlations. This paper proposes a cross-attention feature fusion specifically designed to enhance the global correlation between camera and radar features. By dynamically adjusting feature weights through cross-attention, our approach significantly improves feature integration. Fur-thermore, we propose a depth-weighted voting fusion strategy to select the most accurate sensor depth, thereby enhancing decision-making stability. Experimental results on the nuScenes dataset show substantial improvements, with mean Average Precision (mAP) of 0.399 and mean Average Translation Error (mATE) of 0.602, highlighting the effectiveness of our approach in enhancing the robustness and accuracy of multi-modal fusion.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet23_03">
             16:45-16:50, Paper ThET23.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1284'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Multi-Modality Test-Time Adaptation for Semantic Segmentation in Robotic Perception
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#416420" title="Click to go to the Author Index">
             Liu, Yan
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen Univerisity
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#267291" title="Click to go to the Author Index">
             Zhu, Hongyuan
            </a>
           </td>
           <td class="r">
            A*STAR
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#381530" title="Click to go to the Author Index">
             Zhang, Ye
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#238831" title="Click to go to the Author Index">
             Lei, Yinjie
            </a>
           </td>
           <td class="r">
            Sichuan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#316828" title="Click to go to the Author Index">
             Guo, Yulan
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1284" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#sensor_fusion" title="Click to go to the Keyword Index">
               Sensor Fusion
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Test-Time Adaptation (TTA) adjusts pre-trained models among unlabeled unseen environments during the test phase, making it more practical for robotic applications. However, the constant changes of the physical world create significant domain gaps between the received data during robot deployment and the source data used for training. In addition, existing methods mainly focus on a single modality, {e.g.}, RGB images, limiting the application of these methods in multi-modality input scenarios. In this work, we propose a Deep Multi-modality Aggregation Test-time Adaptation (DMATA) method to address the above mentioned issues. To prevent the domain shifts from disrupting the adaptation process, we first propose a Momentum-based Teacher-Student (MTS) framework. Since the teacher model and the student model contain complementary information, we design an Uncertainty-Guide (UG) feature fusion block to fuse the teacher model and student model of each modality. Finally, we introduce a 3D-Guide-2D (3G2) feature fusion block to extract spatial information from RGB images. In this way, 2D feature extraction is enhanced.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet23_04">
             16:50-16:55, Paper ThET23.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1286'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              MDC-Seg: Multi-Directional Convolution-Based Semantic Segmentation for LiDAR Point Clouds
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#373845" title="Click to go to the Author Index">
             Ouyang, Xin
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#217143" title="Click to go to the Author Index">
             Qian, Xiaolong
            </a>
           </td>
           <td class="r">
            Northeastern University, China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#219010" title="Click to go to the Author Index">
             Zhang, Yunzhou
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#353395" title="Click to go to the Author Index">
             Shen, You
            </a>
           </td>
           <td class="r">
            Northeastern University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#395137" title="Click to go to the Author Index">
             Wang, Guiyuan
            </a>
           </td>
           <td class="r">
            Jiangsu Shuguang Optoelectronics Co., Ltd., Yangzhou, China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#395142" title="Click to go to the Author Index">
             Liu, Wei
            </a>
           </td>
           <td class="r">
            Jiangsu Shuguang Optoelectronics Co., Ltd., Yangzhou, China
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1286" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             LiDAR point clouds 3D semantic segmentation enables efficient and accurate environmental sensing for intelligent vehicles and autonomous robots, greatly advancing these domains. Existing advanced methods use 3D sparse convolutional often suffer from a small Effective Receptive Field (ERF), limiting context sensing and challenging high-performance segmentation. Building on this observation, we propose MDC-Seg for efficient ERF enlargement. We design Multi-directional Convolution (MDConv), which simultaneously performs sparse feature encoding on the Bird's Eye View (BEV) and Range View (RV) planes to enlarge the ERF of 3D sparse convolution. To enhance feature fusion in MDConv, we introduce an attention mechanism and design an efficient multi-feature fusion (EMFF) module suitable for both 3D and 2D sparse features.To improve segmentation accuracy, we design a point-voxel constraint (PVC) module to handle edge voxels containing multiple point cloud categories, optimizing the final inference results. These modules add minimal memory and inference time but significantly improve performance compared to the baseline. Extensive experiments benchmarks on SemanticKITTI achieve excellent performance, while supplementary experiments on nuScenes also yield good results, demonstrating the superiority of MDC-Seg. The source code is available at https://github.com/OYgreat-river/MDC-Seg.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet23_05">
             16:55-17:00, Paper ThET23.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1293'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Illumination Adaptation for SAM to Achieve Accurate Segmentation of Images Taken in Low-Light Scenes
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#372943" title="Click to go to the Author Index">
             Mu, Hongmin
            </a>
           </td>
           <td class="r">
            Beijing University of Chemical Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#112540" title="Click to go to the Author Index">
             Zhou, MengChu
            </a>
           </td>
           <td class="r">
            New Jersey Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#111763" title="Click to go to the Author Index">
             Cao, Zhengcai
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1293" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Achieving accurate segmentation in low-light scenes is challenging due to 1) severe domain shift encountered when models trained on daylight data are applied to such scenes and 2) lack of	large-scale fine-grained labels in low-light conditions. A good idea is to use the generalization capabilities of segmentation foundation models like Segment Anything Model (SAM) to address the scarcity of annotated data. However, applying SAM to low-light scenes faces a severe domain shift issue due to the lack of inductive bias in effectively transforming low-light features into natural-light ones. To address this issue, we propose to adapt SAM for low-light scenes. To reduce the reliance on labels of low-light data, we develop a self-training method that makes SAM generate source-free predictions. To reduce the domain gap between low-light target data and SAM's natural-light trained data, we design a transformation head that enhances low-light features prior to the application of SAM. We further propose a domain shift compensation loss that trains our model to select a domain-adaptation-optimal illumination-enhanced feature map. Experimental results demonstrate that our method well outperforms the state of the art on the Dark Zurich and Nighttime Driving datasets.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet23_06">
             17:00-17:05, Paper ThET23.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('1329'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              4DRadDet: Cluster-Queried Enhanced 3D Object Detection with 4D Radar
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421162" title="Click to go to the Author Index">
             Weng, Caien
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#417294" title="Click to go to the Author Index">
             Bi, Xin
            </a>
           </td>
           <td class="r">
            College of Automotive Studies，Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421132" title="Click to go to the Author Index">
             Tong, Panpan
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#421134" title="Click to go to the Author Index">
             Eichberger, Arno
            </a>
           </td>
           <td class="r">
            Graz University of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab1329" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#intelligent_transportation_systems" title="Click to go to the Keyword Index">
               Intelligent Transportation Systems
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             3D object detection plays a critical role in advancing autonomous driving technology. To improve perception capabilities while maintaining low costs and ensuring performance in adverse weather conditions, 4D radar has emerged as a promising alternative for 3D object detection. However, current methods fail to fully exploit raw data and density information of 4D radar point clouds to tackle challenges like sparse data and noise. To address these limitations and make use of the unique Doppler velocity information provided by 4D radar, we propose a novel approach called 4DRadDet, which uses cross-attention fusion with cluster-queried techniques for 3D object detection. The 4DRadDet model uses a specially designed incremental clustering method to cluster potential object point clouds, reducing measurement errors from limited radar angular resolution and signal multipath effects. The cross-attention feature fusion (CAFF) module enhances network performance by querying the clustered point cloud feature map, allowing the network to leverage reliable prior information from the clustered point cloud to better detect potential objects. Our experimental evaluations on the View-of-Delft (VoD) dataset demonstrate the effectiveness of 4DRadDet, showcasing state-of-the-art performance. Specifically, 4DRadDet achieves a 3D mean average precision (mAP3D) of 51.44% and a bird's-eye view mean average precision (mAPBEV) of 57.07%. Our proposed method demonstrates impressive inference times and achieves real-time detection capabilities.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet23_07">
             17:05-17:10, Paper ThET23.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4959'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Robust Visual Localization System with HD Map Based on Joint Probabilistic Data Association
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#406150" title="Click to go to the Author Index">
             Gu, Zizhen
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#364263" title="Click to go to the Author Index">
             Cheng, Shaowu
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#364265" title="Click to go to the Author Index">
             Wang, Chuan
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#364155" title="Click to go to the Author Index">
             Wang, Ruihan
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#406782" title="Click to go to the Author Index">
             Zhao, Yong
            </a>
           </td>
           <td class="r">
            Harbin Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4959" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#autonomous_vehicle_navigation" title="Click to go to the Keyword Index">
               Autonomous Vehicle Navigation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Localization based on a high-definition (HD) map is a pivotal technology for autonomous driving. Nonetheless, establishing precise data association (DA) between detected landmarks and map landmarks presents a formidable challenge when leveraging prior information on maps. Traditional DA algorithms relying on nearest-neighbor methods only partially mitigate the ambiguity in DA caused by missed or false detections from the perception module, especially in complex and challenging environments. In this letter, we propose a novel joint probability data association (JPDA) algorithm. By integrating joint probability encompassing semantic likelihood, local spatial likelihood, and global structural likelihood of landmarks, alongside incorporating inter-frame temporal continuity of DA, the proposed algorithm can effectively rectify the erroneous DA. Additionally, we also introduce a max-mixture factor graph optimization framework, which couples the measurements of landmarks and odometry for pose estimation. Building upon these methods, a high-precision and robust visual semantic localization system employing consumer-level sensors has been developed. Experiments conducted on public datasets and real urban roads validate the efficacy of the proposed system in providing more robust and accurate localization results for autonomous driving vehicles.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thet23_08">
             17:10-17:15, Paper ThET23.8
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4349'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SALON: Self-Supervised Adaptive Learning for Off-Road Navigation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#291380" title="Click to go to the Author Index">
             Sivaprakasam, Matthew
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#291385" title="Click to go to the Author Index">
             Triest, Samuel
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#215359" title="Click to go to the Author Index">
             Ho, Cherie
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#253458" title="Click to go to the Author Index">
             Aich, Shubhra
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University Robotics Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#423387" title="Click to go to the Author Index">
             Lew, Jeric Jieyi
            </a>
           </td>
           <td class="r">
            National University of Singapore
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#426404" title="Click to go to the Author Index">
             Adu, Isaiah
            </a>
           </td>
           <td class="r">
            Pennsylvania State University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#171185" title="Click to go to the Author Index">
             Wang, Wenshan
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRA25_AuthorIndexWeb.html#104304" title="Click to go to the Author Index">
             Scherer, Sebastian
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4349" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRA25_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#learning_from_experience" title="Click to go to the Keyword Index">
               Learning from Experience
              </a>
              ,
              <a href="ICRA25_KeywordIndexWeb.html#field_robots" title="Click to go to the Keyword Index">
               Field Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Autonomous robot navigation in off-road environments presents a number of challenges due to its lack of structure, making it difficult to handcraft robust heuristics for diverse scenarios. While learned methods using hand labels or self-supervised data improve generalizability, they often require a tremendous amount of data and can be vulnerable to domain shifts. To improve generalization in novel environments, recent works have incorporated adaptation and self-supervision to develop autonomous systems that can learn from their own experiences online. However, current works often rely on significant prior data, for example minutes of human teleoperation data for each terrain type, which is difficult to scale with more environments and robots. To address these limitations, we propose SALON, a perception-action framework for textit{fast} adaptation of traversability estimates with textit{minimal} human input. SALON rapidly learns online from experience while avoiding out of distribution terrains to produce adaptive and risk-aware cost and speed maps. Within textit{seconds} of collected experience, our results demonstrate comparable navigation performance over kilometer-scale courses in diverse off-road terrain as methods trained on 100-1000x more data. We additionally show promising results on significantly different robots in different environments. Our code is available at https://theairlab.org/SALON.
            </div>
           </td>
          </tr>
         </table>
        </div>
        <p>
         <br/>
        </p>
        <p>
         <br/>
        </p>
       </td>
       <td height="100%" style="background-color:#2C1A77;" width="5">
       </td>
      </tr>
      <tr>
       <td alt="" border="0" colspan="4" height="8" style="background-color:#2C1A77;" valign="center" width="100%">
        <p align="center">
         <span style="font-size:8pt;line-height:10pt;color:#fff;">
          Technical Content ©
IEEE Robotics &amp; Automation Society
         </span>
        </p>
       </td>
      </tr>
      <tr>
       <td colspan="4" width="100%">
        <p align="right">
         <span style="text-decoration:none;">
          <img align="right" border="0" src="/images/pc_logo_small.png" style="margin-left: 10px; margin-right: 10px"/>
          This site is protected
by copyright and trademark laws under US and International law.
          <br/>
          All rights
reserved. © 2002-2025 PaperCept, Inc.
          <br/>
          Page generated 2025-04-22  17:14:37 PST
          <a href="" onclick="window.open('/conferences/scripts/about.pl','tc','width=1000,scrollbars=yes'); return false">
           Terms
of use
          </a>
         </span>
        </p>
       </td>
      </tr>
     </table>
    </body>
   </div>
  </form>
 </body>
</html>
